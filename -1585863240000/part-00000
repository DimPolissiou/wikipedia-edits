{"title_page": "David Peyman", "text_new": "{{Infobox officeholder\n | honorific_prefix   = \n | name               = David Peyman\n | native_name        = <!--The person's name in their own language, if different.-->\n | native_name_lang   = <!--ISO 639-1 code, e.g., \"fr\" for French. If more than one, use {{lang}} in |native_name= instead.-->\n | honorific_suffix   = \n | image              = David Peyman.jpg\n | image_size         = \n | image_upright      = \n | smallimage         = <!--If this is specified, \"image\" should not be.-->\n | alt                = \n | caption            = \n | order              = \n | office             = Deputy Assistant Secretary of State for Counter Threat Finance and Sanctions\n | status             = <!--If this is specified, overrides Incumbent.-->\n | term_start         = August 1, 2018\n | term_end           = <!-- Add data only when the actual term has ended, not for terms which will end in the future. (Per usage guideline.) -->\n | alongside          = <!--For two or more people serving in the same position from the same district.  (e.g. United States Senators.)-->\n | monarch            = \n | president          = [[Donald Trump]]\n | governor_general   = \n | primeminister      = \n | taoiseach          = \n | chancellor         = \n | governor           = \n | chair              = \n | vicepresident      = \n | viceprimeminister  = \n | deputy             = \n | lieutenant         = \n | vicechair          = \n | succeeding         = <!--For President-elect or equivalent-->\n | parliamentarygroup = \n | constituency       = \n | majority           = \n | predecessor        = \n | successor          = \n | birth_name      = \n | birth_date      =  <!-- {{Birth date and age|YYYY|MM|DD}} -->\n | birth_place     = \n | citizenship     = \n | nationality     = American\n | party           = [[Republican Party (United States)|Republican Party]]\n | otherparty      =  <!--For additional political affiliations-->\n | height          =  <!-- \"X cm\", \"X m\"  or \"X ft Y in\" plus optional reference (conversions are automatic) -->\n | spouse          = \n | partner         =  <!--For those with a domestic partner and not married-->\n | relations       = \n | children        = \n | parents         =  <!-- overrides mother and father parameters -->\n | mother          =  <!-- may be used (optionally with father parameter) in place of parents parameter (displays \"Parent(s)\" as label) -->\n | father          =  <!-- may be used (optionally with mother parameter) in place of parents parameter (displays \"Parent(s)\" as label) -->\n | relatives       = \n | residence       = \n | education       = [[University of California, Los Angeles|UCLA]] ([[Bachelor of Arts|B.A.]])<br>[[Harvard Law School]] ([[Juris Doctor|J.D.]])\n | alma_mater      = \n | occupation      = \n | profession      = \n | known_for       = \n}}\n'''David Peyman''' is the Deputy Assistant Secretary of State for Counter Threat Finance and Sanctions in the [[Bureau of Economic and Business Affairs]] of the [[United States Department of State]]. In this role, Peyman leads the Office of Economic Sanctions Policy and Implementation and the Office of Threat Finance Countermeasures.\n\n==Education==\nPeyman earned his [[Bachelor of Arts|B.A.]] from [[University of California, Los Angeles|UCLA]], where he graduated ''[[Latin honors|summa cum laude]]'', received the Chancellor's Service Award, and was his class's [[Commencement speech|commencement speaker]]. Peyman then earned a [[Juris Doctor|J.D.]] from [[Harvard Law School]].<ref name=\"statebio\">{{cite web |url=https://www.state.gov/biographies/david-peyman/ |title=David Peyman |last= |first= |date= |website= |publisher=U.S. Department of State |access-date=March 25, 2020 |quote=}}</ref>\n\n==Career==\nPeyman began his law career at [[Skadden]]. He subsequently served as a Special Assistant United States Attorney and Deputy Attorney General of California. During his service as a state prosecutor, Peyman was also an adjunct law professor at [[Southwestern Law School]], where he taught a course on government investigations and prosecutions. In the private sector, Peyman worked at [[BlackRock]], where he was Global Head of Sanctions and led the sanctions compliance framework for over $6 trillion in assets under management.<ref name=\"statebio\"/>\n\nIn the fall of 2016, Peyman joined the [[Donald Trump 2016 presidential campaign]] as the Jewish Affairs and Outreach Director.<ref name=\"forward\">{{cite news |date=November 3, 2016 |title=Meet Trump's New Jewish Outreach Adviser, an Orthodox Iranian Immigrant |url=http://forward.com/news/national/353443/meet-trumps-new-jewish-outreach-adviser-an-orthodox-iranian-immigrant/ |newspaper=The Forward }}</ref> He subsequently was on the [[Presidential transition of Donald Trump|presidential transition team]].<ref name=\"statebio\"/>\n\nSince August 1, 2018, Peyman has served as the Deputy Assistant Secretary of State for Counter Threat Finance and Sanctions in the Bureau of Economic and Business Affairs of the U.S. State Department. In this role, Peyman leads the Office of Economic Sanctions Policy and Implementation and the Office of Threat Finance Countermeasures.<ref name=\"statebio\"/> He is the leader for managing 25 sanctions programs for the Secretary of State, including those on Iran, North Korea, and Venezuela.<ref>{{cite news |last= |first= |date=March 2019 |title=Leading the frontline on all-government sanctions |url=https://statemag.state.gov/2019/03/leading-the-frontline-on-all-government-sanctions/ |work=State Magazine |location= |access-date= }}</ref>\n\nIn his efforts to enforce sanctions against Iran, Peyman has monitored ship-to-ship transfers of Iranian oil, secured pledges from countries to not [[Flag of convenience|flag]] Iranian oil tankers, and threatened to take action against any company that uses the European financial mechanism [[Instrument in Support of Trade Exchanges|INSTEX]] to engage in sanctionable transactions with Iran.<ref>{{cite news |last= |first= |date=March 19, 2019 |title=Exclusive: US Vows to Pursue Ship Owners Who Violate Iran Oil Sanctions |url=https://www.voanews.com/middle-east/voa-news-iran/exclusive-us-vows-pursue-ship-owners-who-violate-iran-oil-sanctions |publisher=Voice of America |location= |access-date= }}</ref><ref>{{cite news |last= |first= |date=March 9, 2019 |title=U.S. to warn shippers against storing Iranian oil: State Department official |url=https://www.reuters.com/article/us-usa-sanctions-iran/us-to-warn-shippers-against-storing-iranian-oil-state-department-official-idUSKBN20W2P4 |agency=Reuters |location= |access-date= }}</ref> He has met with shipping officials in Europe and said ships were \"the key artery to evade sanctions.\"<ref>{{cite news |last= |first= |date=November 6, 2019 |title=U.S. sets sights on shipping companies for sanctions evasions |url=https://www.reuters.com/article/us-shipping-usa-sanctions/u-s-sets-sights-on-shipping-companies-for-sanctions-evasions-idUSKBN1XG2CH |agency=Reuters |location= |access-date= }}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* {{C-SPAN|125185}}     dear sir  I am trying to get answers on if an Amish contractor can finish jobs in Natrona heights that was almost finished and now is in jeopardy because he cannot complete the job and rodents can get into a homeowner's home? I would greatly appreciate an answer, thank you. \n\n{{DEFAULTSORT:Peyman, David}}\n[[Category:Living people]]\n[[Category:Year of birth missing (living people)]]\n[[Category:University of California, Los Angeles alumni]]\n[[Category:Harvard Law School alumni]]\n[[Category:Trump administration personnel]]\n[[Category:United States Department of State officials]]\n", "text_old": "{{Infobox officeholder\n | honorific_prefix   = \n | name               = David Peyman\n | native_name        = <!--The person's name in their own language, if different.-->\n | native_name_lang   = <!--ISO 639-1 code, e.g., \"fr\" for French. If more than one, use {{lang}} in |native_name= instead.-->\n | honorific_suffix   = \n | image              = David Peyman.jpg\n | image_size         = \n | image_upright      = \n | smallimage         = <!--If this is specified, \"image\" should not be.-->\n | alt                = \n | caption            = \n | order              = \n | office             = Deputy Assistant Secretary of State for Counter Threat Finance and Sanctions\n | status             = <!--If this is specified, overrides Incumbent.-->\n | term_start         = August 1, 2018\n | term_end           = <!-- Add data only when the actual term has ended, not for terms which will end in the future. (Per usage guideline.) -->\n | alongside          = <!--For two or more people serving in the same position from the same district.  (e.g. United States Senators.)-->\n | monarch            = \n | president          = [[Donald Trump]]\n | governor_general   = \n | primeminister      = \n | taoiseach          = \n | chancellor         = \n | governor           = \n | chair              = \n | vicepresident      = \n | viceprimeminister  = \n | deputy             = \n | lieutenant         = \n | vicechair          = \n | succeeding         = <!--For President-elect or equivalent-->\n | parliamentarygroup = \n | constituency       = \n | majority           = \n | predecessor        = \n | successor          = \n | birth_name      = \n | birth_date      =  <!-- {{Birth date and age|YYYY|MM|DD}} -->\n | birth_place     = \n | citizenship     = \n | nationality     = American\n | party           = [[Republican Party (United States)|Republican Party]]\n | otherparty      =  <!--For additional political affiliations-->\n | height          =  <!-- \"X cm\", \"X m\"  or \"X ft Y in\" plus optional reference (conversions are automatic) -->\n | spouse          = \n | partner         =  <!--For those with a domestic partner and not married-->\n | relations       = \n | children        = \n | parents         =  <!-- overrides mother and father parameters -->\n | mother          =  <!-- may be used (optionally with father parameter) in place of parents parameter (displays \"Parent(s)\" as label) -->\n | father          =  <!-- may be used (optionally with mother parameter) in place of parents parameter (displays \"Parent(s)\" as label) -->\n | relatives       = \n | residence       = \n | education       = [[University of California, Los Angeles|UCLA]] ([[Bachelor of Arts|B.A.]])<br>[[Harvard Law School]] ([[Juris Doctor|J.D.]])\n | alma_mater      = \n | occupation      = \n | profession      = \n | known_for       = \n}}\n'''David Peyman''' is the Deputy Assistant Secretary of State for Counter Threat Finance and Sanctions in the [[Bureau of Economic and Business Affairs]] of the [[United States Department of State]]. In this role, Peyman leads the Office of Economic Sanctions Policy and Implementation and the Office of Threat Finance Countermeasures.\n\n==Education==\nPeyman earned his [[Bachelor of Arts|B.A.]] from [[University of California, Los Angeles|UCLA]], where he graduated ''[[Latin honors|summa cum laude]]'', received the Chancellor's Service Award, and was his class's [[Commencement speech|commencement speaker]]. Peyman then earned a [[Juris Doctor|J.D.]] from [[Harvard Law School]].<ref name=\"statebio\">{{cite web |url=https://www.state.gov/biographies/david-peyman/ |title=David Peyman |last= |first= |date= |website= |publisher=U.S. Department of State |access-date=March 25, 2020 |quote=}}</ref>\n\n==Career==\nPeyman began his law career at [[Skadden]]. He subsequently served as a Special Assistant United States Attorney and Deputy Attorney General of California. During his service as a state prosecutor, Peyman was also an adjunct law professor at [[Southwestern Law School]], where he taught a course on government investigations and prosecutions. In the private sector, Peyman worked at [[BlackRock]], where he was Global Head of Sanctions and led the sanctions compliance framework for over $6 trillion in assets under management.<ref name=\"statebio\"/>\n\nIn the fall of 2016, Peyman joined the [[Donald Trump 2016 presidential campaign]] as the Jewish Affairs and Outreach Director.<ref name=\"forward\">{{cite news |date=November 3, 2016 |title=Meet Trump's New Jewish Outreach Adviser, an Orthodox Iranian Immigrant |url=http://forward.com/news/national/353443/meet-trumps-new-jewish-outreach-adviser-an-orthodox-iranian-immigrant/ |newspaper=The Forward }}</ref> He subsequently was on the [[Presidential transition of Donald Trump|presidential transition team]].<ref name=\"statebio\"/>\n\nSince August 1, 2018, Peyman has served as the Deputy Assistant Secretary of State for Counter Threat Finance and Sanctions in the Bureau of Economic and Business Affairs of the U.S. State Department. In this role, Peyman leads the Office of Economic Sanctions Policy and Implementation and the Office of Threat Finance Countermeasures.<ref name=\"statebio\"/> He is the leader for managing 25 sanctions programs for the Secretary of State, including those on Iran, North Korea, and Venezuela.<ref>{{cite news |last= |first= |date=March 2019 |title=Leading the frontline on all-government sanctions |url=https://statemag.state.gov/2019/03/leading-the-frontline-on-all-government-sanctions/ |work=State Magazine |location= |access-date= }}</ref>\n\nIn his efforts to enforce sanctions against Iran, Peyman has monitored ship-to-ship transfers of Iranian oil, secured pledges from countries to not [[Flag of convenience|flag]] Iranian oil tankers, and threatened to take action against any company that uses the European financial mechanism [[Instrument in Support of Trade Exchanges|INSTEX]] to engage in sanctionable transactions with Iran.<ref>{{cite news |last= |first= |date=March 19, 2019 |title=Exclusive: US Vows to Pursue Ship Owners Who Violate Iran Oil Sanctions |url=https://www.voanews.com/middle-east/voa-news-iran/exclusive-us-vows-pursue-ship-owners-who-violate-iran-oil-sanctions |publisher=Voice of America |location= |access-date= }}</ref><ref>{{cite news |last= |first= |date=March 9, 2019 |title=U.S. to warn shippers against storing Iranian oil: State Department official |url=https://www.reuters.com/article/us-usa-sanctions-iran/us-to-warn-shippers-against-storing-iranian-oil-state-department-official-idUSKBN20W2P4 |agency=Reuters |location= |access-date= }}</ref> He has met with shipping officials in Europe and said ships were \"the key artery to evade sanctions.\"<ref>{{cite news |last= |first= |date=November 6, 2019 |title=U.S. sets sights on shipping companies for sanctions evasions |url=https://www.reuters.com/article/us-shipping-usa-sanctions/u-s-sets-sights-on-shipping-companies-for-sanctions-evasions-idUSKBN1XG2CH |agency=Reuters |location= |access-date= }}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* {{C-SPAN|125185}}\n\n{{DEFAULTSORT:Peyman, David}}\n[[Category:Living people]]\n[[Category:Year of birth missing (living people)]]\n[[Category:University of California, Los Angeles alumni]]\n[[Category:Harvard Law School alumni]]\n[[Category:Trump administration personnel]]\n[[Category:United States Department of State officials]]\n", "name_user": "40.142.217.66", "label": "vandal", "comment": "(Trying to get a answer?)", "url_page": "//en.wikipedia.org/wiki/David_Peyman"}
{"title_page": "USS Theodore Roosevelt (CVN-71)", "text_new": "{{Other ships|USS Theodore Roosevelt}}\n{{Use dmy dates|date=March 2020}}\n{|{{Infobox ship begin\n|infobox caption=yes\n}}\n{{Infobox ship image\n|Ship image=USS Theodore Roosevelt operations 150322-N-ZF573-140.jpg{{!}}border\n|Ship caption=USS ''Theodore Roosevelt'' underway in the Atlantic Ocean in March 2015.\n}}\n{{Infobox ship career\n|Hide header=\n|Ship country=United States\n|Ship flag={{USN flag}}\n|Ship name=\n|Ship namesake=[[Theodore Roosevelt]]\n|Ship owner=\n|Ship operator=\n|Ship registry=\n|Ship route=\n|Ship ordered=30 September 1980\n|Ship awarded=\n|Ship builder=[[Newport News Shipbuilding]] Co.\n|Ship original cost= U.S. $4.5&nbsp;billion in 2007 dollars<ref name=\"USS THEODORE ROOSEVELT&nbsp;\u2014 HISTORY\">{{cite web|url=http://navysite.de/cvn/cvn71history.htm|title=USS Theodore Roosevelt&nbsp;\u2013 History|work=navysite.de|accessdate=3 July 2009|archive-url=https://web.archive.org/web/20090527073103/http://navysite.de/cvn/cvn71history.htm|archive-date=27 May 2009|url-status=live}}</ref>\n|Ship yard number=\n|Ship way number=\n|Ship laid down=31 October 1981\n|Ship launched=27 October 1984\n|Ship sponsor=\n|Ship christened=\n|Ship completed=\n|Ship acquired=\n|Ship commissioned=25 October 1986\n|Ship recommissioned=\n|Ship decommissioned=\n|Ship maiden voyage=\n|Ship in service=\n|Ship out of service=\n|Ship renamed=\n|Ship reclassified=\n|Ship refit=\n|Ship struck=\n|Ship reinstated=\n|Ship homeport=[[NAS North Island]], [[San Diego]], [[California]]\n|Ship identification=\n|Ship motto=*''Qui Plantavit Curabit''\n* (Latin: \"He who has planted will preserve.\")\n|Ship nickname=''TR'', ''[[Big Stick ideology|Big Stick]]''\n|Ship honors=\n|Ship captured=\n|Ship fate=\n|Ship status={{Ship in active service}}\n|Ship notes=\n|Ship badge= [[File:CVN-71 insignia.png|155px]]\n}}\n{{Infobox ship characteristics\n|Hide header=\n|Header caption=\n|Ship class=*{{sclass-|Nimitz|aircraft carrier}}\n* ''Theodore Roosevelt'' subclass\n|Ship tonnage=\n|Ship displacement={{convert|104600|LT|ST}}<ref>{{cite book |title=The Naval Institute guide to the ships and aircraft of the U.S. fleet |last=Polmar |first=Norman |authorlink=Norman Polmar|year=2004 |publisher=Naval Institute Press |isbn=978-1-59114-685-8 |page=[https://archive.org/details/navalinstitutegu0018polm/page/112 112] |url=https://archive.org/details/navalinstitutegu0018polm |url-access=registration |quote=nimitz class displacement. }}</ref>\n|Ship tons burthen=\n|Ship length=*Overall: {{convert|1092|ft|m|1}}\n* Waterline: {{convert|1040|ft|m|1}}\n|Ship beam=*Overall: {{convert|252|ft|m|1}}\n* Waterline: {{convert|134|ft|m|1}}\n|Ship height=\n|Ship draft=*Maximum navigational: {{convert|37|ft|m|1}}\n* Limit: {{convert|41|ft|m|1}}\n|Ship depth=\n|Ship hold depth=\n|Ship decks=\n|Ship deck clearance=\n|Ship ramps=\n|Ship power=\n|Ship propulsion=*2 \u00d7 [[Westinghouse Electric Company|Westinghouse]] [[A4W reactor|A4W nuclear reactor]]s\n* 4 \u00d7 [[steam turbine]]s driving 4 \u00d7 shafts\n* 260,000 shp (194 MW)\n|Ship speed={{Nimitz class aircraft carrier speed}}\n|Ship range={{Nuclear ship range}}\n|Ship endurance=Limited only by food and supplies\n|Ship test depth=\n|Ship boats=\n|Ship capacity=\n|Ship troops=\n|Ship complement=*Ship's company: 3,200\n* Air wing: 2,480\n|Ship crew=\n|Ship time to activate=\n|Ship sensors={{Nimitz class aircraft carrier sensors I}}\n|Ship EW={{Nimitz class aircraft carrier EW}}\n|Ship armament=*2 \u00d7 [[Sea Sparrow]]\n* 2 \u00d7 [[RIM-116 Rolling Airframe Missile]]\n* 2 \u00d7 [[Phalanx CIWS]] (close-in weapon system) Gatling guns\n|Ship armor= 63.5 mm Kevlar armor over vitals <ref>{{cite book |title=Aircraft carriers: an illustrated history of their impact |last=Fontenoy |first=Paul E. |authorlink= |year=2006 |publisher=ABC-CLIO Ltd |location= |isbn=978-1-85109-573-5 |page=349 |pages= |url= |accessdate=}}</ref>\n|Ship aircraft={{Nimitz class aircraft carrier aircraft}}\n|Ship aircraft facilities=\n|Ship notes=\n}}\n|}\n\n'''USS ''Theodore Roosevelt'' (CVN-71)''' is the fourth {{sclass-|Nimitz|aircraft carrier|0}} [[Nuclear propulsion|nuclear powered]] [[aircraft carrier]] in the [[United States Navy]]. She is named in honor of [[Theodore Roosevelt]], the 26th [[President of the United States]]. She is the [[USS\u00a0Roosevelt|fourth ship named in honor of Theodore Roosevelt]], three bearing his full name and a fourth with just his [[SS Roosevelt (1905)|last name]]. Another three U.S. Navy ships have \"Roosevelt\" in their names in honor of members of the [[Roosevelt family]]. This carrier's radio [[call sign]] is \"Rough Rider\", the nickname of President Roosevelt's [[Rough Riders|volunteer cavalry unit]] during the [[Spanish\u2013American War]]. She was launched in 1984, and saw her first action during [[Operation Desert Storm]] in 1991.\n\n==Background==\nInitially, President [[Gerald Ford]] cancelled the order for CVN-71 in 1976 and substituted two [[Aircraft Carrier (Medium)|CVV-type medium-sized, conventional-powered carrier]]s that were expected to operate [[V/STOL]] aircraft. The existing T-CBL design formed the basis for the new CVV, serving as a replacement for the aging {{sclass-|Midway|aircraft carrier|0}} carriers, while capable of operating all existing conventional carrier aircraft. This capability to operate conventional aircraft proved important as the [[Rockwell XFV-12|hoped-for supersonic V/STOL fighters]] did not come to fruition at the time. In any case, construction of the proposed CVV medium-sized carrier never took place.<ref>{{cite book |title=Aircraft Carriers: An Illustrated Design History |last=Friedman |first=Norman |authorlink=Norman Friedman |year=1983 |publisher=[[United States Naval Institute|Naval Institute Press]] |location=[[Annapolis, Maryland]] |isbn=978-0-87021-739-5 |pages=323\u2013324; 329\u2013333 |url=http://www.usni.org/store/books/aircraft-carriers/us-aircraft-carriers |accessdate=17 July 2013 |archive-url=https://web.archive.org/web/20131022162107/http://www.usni.org/store/books/aircraft-carriers/us-aircraft-carriers |archive-date=22 October 2013 |url-status=dead }}</ref><ref name=NANcvvJUL79>{{cite web |title=CVV |url=http://www.history.navy.mil/nan/backissues/1970s/1979/jul79.pdf |work=Naval Aviations News |publisher=[[Naval History & Heritage Command]] |location=[[Washington Navy Yard]] |page=8 |format=PDF |date=July 1979 |accessdate=18 July 2013 |url-status=dead |archiveurl=https://web.archive.org/web/20100816080321/http://www.history.navy.mil/nan/backissues/1970s/1979/jul79.pdf |archivedate=16 August 2010}}</ref>\n\nAuthorization for CVN-71 was further delayed when President [[Jimmy Carter]] [[Veto#United States|vetoed]] the 1979 Fiscal Year Department of Defense authorization bill because of the inclusion of this ''Nimitz''-class nuclear supercarrier in the Navy's shipbuilding program.<ref name=NANcvvJUL79/><ref name=Polmar>{{cite book |title=Aircraft Carriers: A History of Carrier Aviation and Its Influence on World Events, Volume 2 |last=Polmar |first=Norman |year=2006 |publisher=Potomac Books, Inc. |location=[[Dulles, Virginia]] |isbn=978-1-57488-663-4 |page=364 |url=https://books.google.com/books?id=BNAXHkfMs5wC&pg=PA364&lpg=PA364&dq=carter+veto+cvn-71&source=bl&ots=qHXzOChxFM&sig=CgSoi5y2rU6niQRye_XGKOMdf-g&hl=en&sa=X&ei=MjjoUaTvFIaa9QSWw4C4AQ&ved=0CDIQ6AEwAg#v=onepage&q=carter%20veto%20cvn-71&f=false |accessdate=17 July 2013 |archive-url=https://web.archive.org/web/20190331175731/https://books.google.com/books?id=BNAXHkfMs5wC&pg=PA364&lpg=PA364&dq=carter+veto+cvn-71&source=bl&ots=qHXzOChxFM&sig=CgSoi5y2rU6niQRye_XGKOMdf-g&hl=en&sa=X&ei=MjjoUaTvFIaa9QSWw4C4AQ&ved=0CDIQ6AEwAg#v=onepage&q=carter%20veto%20cvn-71&f=false |archive-date=31 March 2019 |url-status=live }}</ref> As a result of the [[Iran hostage crisis]] which required the increased deployment of U.S. aircraft [[carrier battle group]]s to the [[Indian Ocean]], President Carter reversed his stand on ''Nimitz''-class nuclear supercarriers, and CVN-71 was subsequently authorized under the 1980 Fiscal Year authorization bill for the U.S. Department of Defense.<ref name=Polmar/>\n\n==Design and construction==\n''Theodore Roosevelt'' was the first aircraft carrier to be assembled using modular construction, wherein large modules are independently constructed in \"lay-down\" areas, prior to being hoisted into place and welded together. Modular construction, made possible through the use of a huge gantry crane capable of lifting 900 tons, cut 16 months off ''Theodore Roosevelt''{{'}}s construction time, and the technique has been used on every aircraft carrier since. ''Theodore Roosevelt'' and those ''Nimitz''-class vessels completed after her have slight structural differences from the earlier carriers ({{USS|Nimitz||6}}, {{USS|Dwight D. Eisenhower||6}}, and {{USS|Carl Vinson||6}}) and improved protection for ordnance storage in her [[Magazine (artillery)|magazines]].<ref>{{cite web |url=https://www.defenseindustrydaily.com/costing-the-cvn21-a-did-primer-01624/ |title=Costing the CVN-21: A DID Primer |date=19 December 2005 |website=Defense Industry Daily |accessdate=27 December 2009 |archive-url=https://web.archive.org/web/20190528234846/https://www.defenseindustrydaily.com/costing-the-cvn21-a-did-primer-01624/ |archive-date=28 May 2019 |url-status=live }}</ref>\n\n''Theodore Roosevelt''{{'}}s history began on 30 September 1980, when a contract was awarded for \"Hull 624D\" to [[Newport News Shipbuilding]]. Her [[Keel laying|keel was laid down]] on 31 October 1981, with Secretary of Defense [[Caspar Weinberger]] initiating the first weld. On 3 November 1981, Secretary of the Navy [[John F. Lehman]] announced that the carrier would be named for Theodore Roosevelt. The vessel's [[Pre-Commissioning Unit]] (PCU) was formed in February 1984, with Captain Paul W. Parcells named as Commanding Officer. On 27 October 1984, the ship was officially [[ceremonial ship launching|christened]] by Mrs. Barbara Lehman, wife of Secretary Lehman. On 25 October 1986, ''Theodore Roosevelt'' was [[Ship commissioning|commissioned]] to active service at Newport News.<ref>{{cite news|url=https://www.washingtonpost.com/archive/politics/1986/10/26/uss-theodore-roosevelt-joins-active-service-as-15th-carrier/b14cf77c-8102-4b7d-808c-28606bffaefa/|title=USS Theodore Roosevelt Joins Active Service as 15th Carrier|first=Geroge C.|last=Wilson|newspaper=The Washington Post|date=26 October 1986|page=A21|accessdate=2020-04-01}}</ref>\n\n==Service history==\n===Maiden deployment===\n[[File:USS Theodor Roosevelt shock test.jpg|thumb|left|Shock test of ''Theodore Roosevelt'' during sea trials in 1987]]\nAfter sea trials and pre-deployment workups, ''Theodore Roosevelt'' started her [[maiden voyage|maiden deployment]] on 30 December 1988 with [[Carrier Air Wing Eight]] (CVW-8) embarked. The ship patrolled the [[Mediterranean Sea]] prior to returning on 30 June 1989.She was awarded the 1989 [[Battle \"E\"]] from [[Commander, Naval Air Force U.S. Atlantic Fleet]] on 20 March 1990.{{citation_needed|date=August 2019}}\n\n===1990s===\n====Gulf War====\nOn 28 December 1990, ''Theodore Roosevelt'' and CVW-8 deployed for [[Operation Desert Shield]], arriving in the [[Persian Gulf]] on 16 January 1991. With the commencement of Operation Desert Storm on 15 January 1991, ''Theodore Roosevelt'' began combat operations; eventually flying over 4,200 sorties, more than any other carrier, and dropping more than {{convert|4,800,000|lb|kg|1}} of ordnance before the cease-fire on 28 February.<ref name=\"USS THEODORE ROOSEVELT&nbsp;\u2014 HISTORY\"/>\n[[File:US Navy Battle Force Zulu carriers overhead view in 1991.jpg|thumb|Four U.S. Navy carriers form \"Battle Force Zulu\" following the 1991 Gulf War; ''Theodore Roosevelt'' (top right) cruises with {{USS|Midway|CV-41|2}} (top left), {{USS|Ranger|CV-61|2}} (bottom left) and {{USS|America|CV-66|2}} (bottom right)]]\n\nWhen [[Ba'athist Iraq|Iraqi]] forces turned on the [[Kurds]], ''Theodore Roosevelt'' and CVW-8 were among the first coalition forces in [[Operation Provide Comfort]], flying patrols over northern Iraq. After a 189-day deployment, with 176 days at sea, ''Theodore Roosevelt'' returned to Norfolk on 28 June 1991. On 14 February 1992, the ship won her second Battle \"E\". This was followed by the award of the [[Battenberg Cup]] for 1991 as the Atlantic Fleet's premier ship.<ref name=\"tr.surfor.navy.mil\">{{cite web|url=http://www.tr.surfor.navy.mil/about%20tr/ship%20history.html|title=Ship's History |work=tr.surfor.navy.mil |archive-url=https://web.archive.org/web/20090413121419/http://www.tr.surfor.navy.mil/about%20tr/ship%20history.html |archive-date=13 April 2009}}</ref>\n\n''Theodore Roosevelt'' began her third deployment on 11 March 1993, again with CVW-8 embarked. Also embarked was a Special Purpose [[Marine Air-Ground Task Force]] (SPMAGTF), in a test the concept of embarking a multi-purpose Marine force in a carrier. While the ship was still in the [[Virginia Capes]] operating area, President [[Bill Clinton]] flew aboard for several hours for his first visit to a U.S. Navy ship.<ref name=\"tr.surfor.navy.mil\" /> ''Theodore Roosevelt'' operated in the [[Adriatic Sea|Adriatic]] as CVW-8 planes enforced [[Operation Deny Flight]] in the U.S. no-fly zone over [[Bosnia and Herzegovina|Bosnia]]. In June, on the way to only her second port visit, ''Theodore Roosevelt'' was ordered instead to transit the [[Suez Canal]] en route to the [[Red Sea]] to participate in [[Operation Southern Watch]], enforcing the no-fly zone over Iraq. Deployed for 184 days, ''Theodore Roosevelt'' spent 169 days under way prior to return in September 1993. For the accomplishments of her crew, the ship received her second [[Meritorious Unit Commendation]].{{citation_needed|date=August 2019}}\n\nFrom November 1993 to April 1994, ''Theodore Roosevelt'' conducted a Selected Restricted Availability (SRA) at [[Norfolk Naval Shipyard]] (NNSY), completing ahead of schedule. On 10 March 1994, ''Theodore Roosevelt'' received her third Battle \"E\". Then on 3 June, ''Theodore Roosevelt'' was awarded her second Battenberg Cup as the best ship in the Atlantic Fleet.{{citation_needed|date=August 2019}}\n\n''Theodore Roosevelt'' and CVW-8 began their fourth deployment in March 1995, operating in the Red Sea in support of Operation Southern Watch over Iraq, and Operations Deny Flight and [[Operation Sharp Guard|Sharp Guard]] over the skies of Bosnia and in the Adriatic operating areas. Deny Flight evolved into [[Operation Deliberate Force]], as CVW-8 aircraft led [[NATO]] strikes against strategic Bosnian Serb targets in Bosnia-Herzegovina. The ''Theodore Roosevelt'' Battle Group returned to Norfolk, Virginia in September 1995 and was awarded the [[Navy Unit Commendation]] for its Bosnia operations.<ref name=\"tr.surfor.navy.mil\" />\n\n====Collision with USS ''Leyte Gulf''====\nOn 14 October 1996, ''Theodore Roosevelt'' collided with {{USS|Leyte Gulf||6}}, a {{sclass-|Ticonderoga|cruiser|0}} [[guided missile cruiser]], while conducting operations off the coast of North Carolina. The incident occurred as the carrier, without prior warning, reversed her engines while ''Leyte Gulf'' was behind her and collided with the cruiser's bow. There were no injuries reported,<ref>{{cite web|url=http://www.dcfp.navy.mil/mc/museum/LEYTEGULF/LEYTEGULF.htm|title=USS ''Leyte Gulf'' at DCHM|accessdate=16 September 2007|publisher=Naval Sea Systems Command DC Museum|url-status=dead|archiveurl=https://web.archive.org/web/20080214143307/http://www.dcfp.navy.mil/mc/museum/LEYTEGULF/LEYTEGULF.htm|archivedate=14 February 2008|df=dmy-all}}</ref> but ''Theodore Roosevelt'' suffered more than $7 million damage to her stern, while damages to ''Leyte Gulf''{{'}}s bow were assessed at $2 million.<ref>{{Cite web |url=http://articles.dailypress.com/1996-11-23/news/9611230009_1_leyte-gulf-deck-officers-aircraft-carrier-theodore-roosevelt |title=Navy Officers Reprimanded for Role in Ships Collision |last=McMichael |first=William H. |work=Daily Press |date=23 November 1996 |access-date=19 June 2017 |archive-url=https://web.archive.org/web/20170626104621/http://articles.dailypress.com/1996-11-23/news/9611230009_1_leyte-gulf-deck-officers-aircraft-carrier-theodore-roosevelt |archive-date=26 June 2017 |url-status=live }}</ref>\n\n''Theodore Roosevelt'' deployed for her fifth deployment on 25 November 1996, with [[Carrier Air Wing Three|CVW-3]] embarked, in support of Operation Southern Watch in the Mediterranean and Persian Gulf. The ship returned from deployment in May 1997. On 8 July 1997, ''Theodore Roosevelt'' entered the [[Newport News Shipbuilding]] yard for a one-year Extended Drydock and Selected Restricted Availability (EDSRA), her first major overhaul since commissioning. ''Theodore Roosevelt'' returned to her homeport of [[Naval Station Norfolk|Norfolk Naval Station]] on 2 July 1998.{{citation_needed|date=August 2019}}\n[[File:USS Theodore Roosevelt - BigStick.jpg|thumb|left|''Theodore Roosevelt'' underway in 1999]]\n\nFrom 1 February to 4 March 1999 ''Theodore Roosevelt'' participated in exercise JTFEX / TMDI99 along with the [[Brazilian Navy]] and several NATO navies. During the exercise, ''Theodore Roosevelt'' was mock-sunk,<ref>{{cite book |url=https://books.google.com/?id=Tqj9ZP8FsJEC&pg=PA22&lpg=PA22&dq=JTFEX/TMDI+99+walrus#v=onepage&q=JTFEX%2FTMDI%2099%20walrus&f=false |title=Lessons Not Learned: The U.S. Navy's Status Quo Culture |isbn=978-1-59114-865-4 |last=Thompson |first=Roger |publisher=Naval Institute Press |year=2007}}</ref> along with eight other U.S. ships, many of which were the carrier's escorts, by submarine {{HNLMS|Walrus||6}} of the [[Royal Netherlands Navy]].{{citation_needed|date=August 2019}}\n\n[[File:US Navy 990523-N-8493H-001 Corpsman prepares prescriptions for USS Roosevelt crew.jpg|thumb|right|upright|A U.S. Navy corpsman aboard USS ''Theodore Roosevelt'' in May 1999]]\n''Theodore Roosevelt'' began her sixth deployment on 26 March 1999 with CVW-8 embarked. They were immediately called to duty in the [[Ionian Sea]] to support NATO's [[Operation Allied Force]]. ''Theodore Roosevelt'' and CVW-8 aircraft conducted air strikes for two months over the skies of [[Kosovo]] against Serbian positions. ''Theodore Roosevelt'' and CVW-8 were then dispatched to support Operation Southern Watch, enforcing the \"no-fly\" zone over Southern Iraq. ''Theodore Roosevelt'' returned to her homeport of Norfolk, Virginia, on 24 September 1999.{{citation_needed|date=August 2019}}\n\n===2000s===\n[[File:USS Theodore Roosevelt CATCC.jpg|thumb|left|An air traffic controller watches his radar scope in the Carrier Air Traffic Control Center in 2002]]\nOn 10 January 2000, ''Theodore Roosevelt'' entered a Planned Incremental Availability (PIA) at the Norfolk Naval Naval Shipyard, [[Portsmouth, Virginia]] for a six-month maintenance period.{{citation_needed|date=August 2019}}\n\n====September 11 attacks====\nAfter the [[September 11 attacks]], ''Theodore Roosevelt'' began her seventh deployment on 19 September 2001 with [[Carrier Air Wing One]] (CVW-1). On the night of 4 October 2001, ''Theodore Roosevelt'' and CVW-1 launched the initial strikes of [[Operation Enduring Freedom]] against [[al-Qaeda]] in Afghanistan from the [[North Arabian Sea]]. Between departing Norfolk on 19 September 2001 and arriving in Bahrain for a liberty call on 27 February 2002, ''Theodore Roosevelt'' spent 160 consecutive days at sea, breaking the record for the longest period underway since [[World War II]].<ref>{{cite news |url=https://www.washingtonpost.com/archive/local/2002/03/07/uss-roosevelt-sets-record-with-160-days-at-sea/3d6ce8ec-90f3-40e9-a2ed-a1c92bbd17b0/ |title=USS Roosevelt Sets Record With 160 Days at Sea |first=Steve |last=Vogel |date=7 March 2002 |access-date=1 April 2020 |website=The Washington Post |url-access=limited}}</ref> ''Theodore Roosevelt'' returned to her homeport 27 March 2002 and was awarded the [[Navy Unit Commendation]], 2001 Battenberg Cup, and 2001 Battle \"E\".<ref>{{cite press release|last=Kerns|first=Rob |url=http://www.news.navy.mil/search/display.asp?story_id=1908 |title=Theodore Roosevelt Takes Battenberg Cup |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=10 June 2002 |id=NNS020610-06|archive-url=https://web.archive.org/web/20061124204409/http://www.news.navy.mil/search/display.asp?story_id=1908 |archive-date=24 November 2006}}</ref> From April to October 2002, ''Theodore Roosevelt'' conducted a Planned Incremental Availability maintenance period at Norfolk Naval Ship Yard.{{citation_needed|date=August 2019}}\n\n[[File:USS Theodore Roosevelt - resplenishment.jpg|thumb|right|''Theodore Roosevelt'' receives cargo while pierside at the [[Souda Bay#Naval Dock Crete|NATO Marathi Pier Facility]] in [[Crete]].]]\n''Theodore Roosevelt'' got underway on 6 January for a scheduled month-long training period in the [[Puerto Rican Operating Area]]. Near the end of January, ''Theodore Roosevelt'' received orders to proceed across the Atlantic to the Mediterranean Sea. [[VFA-201|Strike Fighter Squadron 201]], based at Naval Air Station [[Naval Air Station Joint Reserve Base Fort Worth]], Texas, was ordered to active duty as a unit of Carrier Air Wing (CVW) 8, the first [[United States Navy Reserve|Naval Reserve]] squadron to deploy aboard an aircraft carrier since the [[Korean War]].<ref>{{cite press release|last=Boxleitner|first=Kirk |url=http://www.news.navy.mil/search/display.asp?story_id=5302 |title=VFA-201 \"Hunters\" Make History Aboard TR|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=27 January 2003 |id=NNS030127-04|archive-url=https://web.archive.org/web/20070831135932/http://www.news.navy.mil/search/display.asp?story_id=5302 |archive-date=31 August 2007}}</ref> ''Theodore Roosevelt'' arrived on station in the Eastern Mediterranean in February. On 22 March 2003 ''Theodore Roosevelt'', along with {{USS|Harry S. Truman||6}}, began launching air strikes into Iraq in support of [[Operation Iraqi Freedom]].<ref>{{cite press release|last=Kerns |first=Rob |url=http://www.news.navy.mil/search/display.asp?story_id=6471 |title=America's Big Stick Launches Operation Iraqi Freedom Strikes|agency=Navy News Service |publisher=USS Theodore Roosevelt Public Affairs |date=24 March 2003 |id=NNS030324-01|archive-url=https://web.archive.org/web/20030401082516/http://www.news.navy.mil/search/display.asp?story_id=6471 |archive-date=1 April 2003}}</ref> ''Theodore Roosevelt'' returned home on 26 May and was awarded the Meritorious Unit Commendation, the [[Navy Unit Citation]], and the [[Global War on Terrorism Expeditionary Medal]].\n[[File:Uss theodore roosevelt cvn-71.jpg|thumb|left|''Theodore Roosevelt'' in the [[Elizabeth River (Virginia)|Elizabeth River]] in 2004]]\n\nOn 19 February 2004, ''Theodore Roosevelt'' entered a ten-month Docked Planned Incremental Availability (DPIA) at NNSY in Portsmouth.<ref>{{cite press release|last=Catalano|first=Mark A. |url=http://www.news.navy.mil/search/display.asp?story_id=12404 |title=FOD Walkdown Marks End to Historic Chapter for TR|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service|date=23 March 2004 |id=NNS040323-10|archive-url=https://web.archive.org/web/20061124205221/http://www.news.navy.mil/search/display.asp?story_id=12404 |archive-date=24 November 2006}}</ref> Major systems overhauled included AC systems, Steam and CHT (sewage) systems, 1MC (announcing) systems, communication, navigation, and detection suites, weapons elevator overhauls, propeller replacement, hull cleaning and painting, and sea valve replacement. ''Theodore Roosevelt'' came out of dry-dock in August and completed the maintenance availability on 17 December 2004.<ref>{{cite press release|last=Catalano |first=Mark |url=http://www.news.navy.mil/search/display.asp?story_id=16350 |title=Fast Cruise Marks End of DPIA for 'Big Stick'|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=20 December 2004|id=NNS041220-03|archive-url=https://web.archive.org/web/20061124205116/http://www.news.navy.mil/search/display.asp?story_id=16350 |archive-date=24 November 2006}}</ref>\n[[File:US Navy 011020-N-3896H-002 USS Theodore Roosevelt (CVN 71).jpg|thumb|right|An [[McDonnell Douglas F/A-18 Hornet|F/A-18 Hornet]] from the \"Sidewinders\" of [[VFA-86]] ignites its afterburners while preparing to be catapulted from the flight deck.]]\n\nOn 1 September 2005, ''Theodore Roosevelt'' deployed with CVW-8 embarked for a routine six-month mission to the Persian Gulf in support of Operation Iraqi Freedom (OIF),<ref>{{cite press release|last=Stephens|first=Kimberly R. |url=http://www.news.navy.mil/search/display.asp?story_id=19883 |title=Theodore Roosevelt CSG Deploys in Support of Global War on Terrorism|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=2 September 2005 |id=NNS050902-14|archive-url=https://web.archive.org/web/20071205195312/http://www.news.navy.mil/search/display.asp?story_id=19883 |archive-date=5 December 2007}}</ref> transiting the Suez Canal on 27 September<ref>{{cite press release|last=Bristol|first=Daniel A. |url=http://www.news.navy.mil/search/display.asp?story_id=20463 |title=USS Theodore Roosevelt Transits Through Suez Canal|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=6 October 2005 |id=NNS051006-10|archive-url=https://web.archive.org/web/20070208022553/http://www.news.navy.mil/search/display.asp?story_id=20463 |archive-date=8 February 2007}}</ref> and launching OIF missions beginning 6 October.<ref>{{cite press release|url=http://www.news.navy.mil/search/display.asp?story_id=20517 |title=TR CSG Offers OIF Air Support|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=11 October 2005 |id=NNS051011-02|archive-url=https://web.archive.org/web/20070907150043/http://www.news.navy.mil/search/display.asp?story_id=20517 |archive-date=7 September 2007}}</ref> This deployment was the last cruise for the [[F-14 Tomcat]] before its retirement in 2006. ''Theodore Roosevelt'' carried two Tomcat squadrons, [[VF-31]] (Tomcatters) and [[VF-213]] (Black Lions).<ref>{{cite press release|last=Catalano|first=Mark A. |url=http://www.news.navy.mil/search/display.asp?story_id=24913 |title=Tomcat Chapter Draws to a Close |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service|date=29 July 2006 |id=NNS060729-02|archive-url=https://web.archive.org/web/20060915193350/http://www.news.navy.mil/search/display.asp?story_id=24913 |archive-date=15 September 2006}}</ref> ''Theodore Roosevelt'' returned to home port on 11 March 2006. Shortly after this cruise, ''Theodore Roosevelt'' earned the [[\"Jig Dog\" Ramage Carrier and Carrier Air Wing Operational Excellence Award]], which is a Navy-wide award that is selected jointly by Type Commanders (TYCOM) and is presented to the Carrier/Air Wing team with the best performance as an integrated unit.{{citation_needed|date=August 2019}}\n\nOn 7 March 2007, ''Theodore Roosevelt'' began a nine-month Planned Incremental Availability (PIA) in Norfolk, which saw the addition of [[RIM-116 Rolling Airframe Missile#Sea RAM|RAM-116 missiles]] among other upgrades.<ref>{{cite press release|last=Bullock|first=Matt |url=http://www.news.navy.mil/search/display.asp?story_id=28222 |title=Theodore Roosevelt Moves to Shipyard|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=9 March 2007|id=NNS070309-09|archive-url=https://web.archive.org/web/20070320034520/http://www.news.navy.mil/search/display.asp?story_id=28222 |archive-date=20 March 2007}}</ref> The ship returned to Naval Station Norfolk on 28 November 2007.{{citation needed|date=August 2019}}\n\nCVW-8 and ''Theodore Roosevelt'' participated in Joint Task Force Exercise 08-4 Operation Brimstone off the coast of [[North Carolina]] between 21 and 31 July 2008. The British aircraft carrier {{HMS|Ark Royal|R07|6}}, the amphibious assault ship {{USS|Iwo Jima|LHD-7|2}} with associated units and the Brazilian frigate {{ship|Brazilian frigate|Greenhalgh|F46|2}} and the French submarine {{ship|French submarine|Am\u00e9thyste|S605|2}} also participated in the event.<ref>{{cite press release|url=http://www.news.navy.mil/search/display.asp?story_id=38478 |title=JTFEX 08-4 \"Operation Brimstone\" Flexes Allied Force Training|publisher=Commander, U.S. 2nd Fleet Public Affairs|agency=Navy News Service|date=15 July 2008 |id=NNS080715-21|archive-url=https://web.archive.org/web/20080813171934/http://www.news.navy.mil/search/display.asp?story_id=38478 |archive-date=13 August 2008}}</ref>\n\n''Theodore Roosevelt'' left Norfolk on 8 September 2008 for a scheduled deployment to the Middle East with [[Carrier Air Wing Eight]] embarked.<ref>{{cite press release|last=Hilley|first=Monique |url=http://www.news.navy.mil/search/display.asp?story_id=39754 |title=USS Theodore Roosevelt Deploys in Support of Maritime Security Operations|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service|date=26 September 2008 |id=NNS080926-19|archive-url=https://web.archive.org/web/20090919003658/http://www.news.navy.mil/search/display.asp?story_id=39754 |archive-date=19 September 2009}}</ref> On 4 October 2008, the ship stopped at [[Cape Town]], South Africa. This was the first visit to Cape Town by a nuclear-powered vessel since the German cargo ship ''[[Otto Hahn (ship)|Otto Hahn]]'' in the 1970s.<ref>{{cite web|url=https://www.iol.co.za/news/south-africa/uss-theodore-gets-green-light-418499|title=USS Theodore gets green light|publisher=OIL|date=1 October 2008|accessdate=2020-04-01}}</ref> Due to poor weather, approximately half of the ship's crew was unable to go ashore on [[shore leave|liberty]]. Much of the crew that made it ashore were unable to return to ''Theodore Roosevelt'' due to the increasingly poor weather. The remaining crew was forced to remain on the pier till morning alongside the [[cruiser]] {{USS|Monterey|CG-61|6}}. The ship made four subsequent port stops in [[Jebel Ali]], UAE, including one during the Christmas holiday. CVW-8 and CVN-71 supported Operation Enduring Freedom and flew more than 3,100 sorties and dropped more than 59,500 pounds of ordnance while providing [[close air support]] for [[International Security Assistance Force|ISAF]]-forces in Afghanistan.{{citation_needed|date=August 2019}}\n\nOn 21 March 2009, ''Theodore Roosevelt'' was relieved by ''Dwight D. Eisenhower''.<ref>{{cite press release|url=http://www.navy.mil/search/display.asp?story_id=43629 |title=Eisenhower Launches OEF Sorties|publisher=U.S. Naval Forces Central Command Public Affairs|agency=Navy News Service |date=21 March 2009|id=NNS090321-02|accessdate=2012-05-26|archive-url=https://web.archive.org/web/20090322221828/http://www.navy.mil/search/display.asp?story_id=43629 |archive-date=22 March 2009}}</ref> The carrier arrived at Norfolk on 18 April.<ref>{{cite news|work=[[The Washington Times]]|title=Carrier Returns To Navy Station |date=19 April 2009|page=7}}</ref> On 26 August 2009 defense contractor [[Northrop Grumman]] was awarded a 2.4&nbsp;billion dollar contract for [[Refueling and Complex Overhaul]] (RCOH) of ''Theodore Roosevelt'' at its Newport News shipyard, to be completed in 2013.<ref>{{cite web|url=http://www.defenselink.mil/contracts/contract.aspx?contractid=4103 |title=Contracts for Wednesday, 26 August 2009 |work=DefenseLink |publisher=U.S. Department of Defense |accessdate=19 January 2014|archive-url=https://web.archive.org/web/20090901115208/http://www.defenselink.mil/contracts/contract.aspx?contractid=4103 |archive-date=1 September 2009}}</ref>\n\n===2010s===\nOn 29 August 2013, ''Theodore Roosevelt'' returned to Norfolk Naval Station, Virginia, completing its post-overhaul sea trials that concluded its four-year mid-life RCOH.<ref>{{cite press release|title=Theodore Roosevelt Returns to Norfolk as a Ready for Tasking Carrier |url=http://www.navy.mil/submit/display.asp?story_id=76254 |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=29 August 2012 |id=NNS130829-16|accessdate=30 August 2012 |archive-url=https://web.archive.org/web/20131029194723/http://www.navy.mil/submit/display.asp?story_id=76254 |archive-date=29 October 2013 |url-status=live }}<br />{{cite press release|title=Roosevelt Successfully Completes RCOH |url=http://www.navy.mil/submit/display.asp?story_id=76261 |publisher=PEO Carriers Public Affairs |agency=Navy News Service|date=29 August 2012 |id=NNS130829-20|accessdate=30 August 2012 |archive-url=https://web.archive.org/web/20130902075923/http://www.navy.mil/submit/display.asp?story_id=76261 |archive-date=2 September 2013 |url-status=live }}</ref> On 14 September 2013, ''Theodore Roosevelt'' successfully completed flight deck certification which entailed completing a total of 160 carrier landings during daytime and night-time operations. Other certification drills included rigging the emergency barricade, flight deck firefighting evolutions, and crash and salvage operations.<ref>{{cite press release|last=Zeigler |first=Heath |title=Theodore Roosevelt Completes Flight Deck Certification |url=http://www.navy.mil/submit/display.asp?story_id=76613 |publisher=USS Theodore Roosevelt Public Affairs |agency=Navy News Service|date=16 September 2013 |id=NNS130916-14|accessdate=24 October 2013 |archive-url=https://web.archive.org/web/20131029184717/http://www.navy.mil/submit/display.asp?story_id=76613 |archive-date=29 October 2013 |url-status=live }}</ref> On 17 September 2013, ''Theodore Roosevelt'' completed her first [[underway replenishment]] in over four years.<ref>{{cite press release|last=Lindstrom |first=Kris R.|title=USS Theodore Roosevelt Completes First Underway Replenishment in Four Years |url=http://www.navy.mil/submit/display.asp?story_id=76697 |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=20 September 2013 |id=NNS130920-22|accessdate=24 October 2013 |archive-url=https://web.archive.org/web/20131029193324/http://www.navy.mil/submit/display.asp?story_id=76697 |archive-date=29 October 2013 |url-status=live }}</ref>\n\nFlight testing for the [[Northrop Grumman X-47B|X-47B]] continued on board ''Theodore Roosevelt'' on 10 November 2013. During this phase, the X-47B's digitized carrier-controlled environment was tested which involved the interface between the unmanned aircraft and carrier personnel during launching, flight operations and recovery. The digital environment offered increased flexibility and enhanced safety for carrier operations.<ref>{{cite press release|title=X-47B Operates Aboard Theodore Roosevelt |url=http://www.navy.mil/submit/display.asp?story_id=77580 |agency=Navy News Service |id=NNS131110-02 |publisher=USS Theodore Roosevelt Public Affairs |date=10 November 2013 |accessdate=14 November 2013 |archive-url=https://web.archive.org/web/20131115005319/http://www.navy.mil/submit/display.asp?story_id=77580 |archive-date=15 November 2013 |url-status=live }}</ref>\n\nOn 15 January 2014, the Navy announced that ''Theodore Roosevelt''{{'}}s homeport would move to San Diego, replacing {{USS|Ronald Reagan||6}} when she relocated to Japan sometime in 2015 as part of the US Navy's preparation for the planned refueling of {{USS|George Washington|CVN-73|6}}.<ref>{{cite news|url=http://www.navytimes.com/article/20140115/NEWS/301150012/Reagan-replace-GW-Japan-Roosevelt-San-Diego |title=Reagan to replace GW in Japan; Roosevelt to San Diego |newspaper=Navy Times |date=15 January 2014 |accessdate=29 September 2014|archive-url=http://archive.today/2014.01.15-202319/http://www.navytimes.com/article/20140115/NEWS/301150012/Reagan-replace-GW-Japan-Roosevelt-San-Diego|archive-date=15 January 2014}}</ref>\n\nOn 4 March 2015, during a training exercise off [[Florida]], ''Theodore Roosevelt'' was mock-sunk by the [[French Navy]] submarine {{ship|French submarine|Saphir|S602|2}}\n<ref>{{cite web |url=https://www.defense.gouv.fr/marine/actu-marine/le-sna-saphir-en-entrainement-avec-l-us-navy-au-large-de-la-floride |last=Savary |first=Quentin |title=Le SNA Saphir en entrainement avec l'US navy au large de la Floride |publisher=French Ministry of Defence|language=French|date=4 March 2015|accessdate=6 March 2015 |url-status=dead|archiveurl=https://web.archive.org/web/20150402224312/http://www.defense.gouv.fr/marine/actu-marine/le-sna-saphir-en-entrainement-avec-l-us-navy-au-large-de-la-floride |archivedate=2 April 2015}}</ref><ref name=rt2015>{{cite web |url=https://www.rt.com/usa/238257-french-submarine-us-carrier/ |author= |title=French delete evidence US carrier was 'sunk' by sub in drill |website=[[RT (TV network)|RT]] |date=6 March 2015 |accessdate=6 March 2015 |archive-url=https://web.archive.org/web/20150307173858/http://rt.com/usa/238257-french-submarine-us-carrier/ |archive-date=7 March 2015 |url-status=live }}</ref>\n\nOn 11 March 2015,''Theodore Roosevelt'' and [[Carrier Strike Group 12]] departed Naval Station Norfolk for an around the world tour with deployments to the [[United States Fifth Fleet|U.S. 5th]], [[United States Sixth Fleet|6th]] and [[United States Seventh Fleet|7th Fleets]] as part the first deployment of [[Cooperative Engagement Capability#NIFC-CA|Naval Integrated Fire Control-Counter Air (NIFC-CA)]] Carrier Strike Group, before arriving in their new homeport of San Diego, California.<ref>{{cite web|url=http://www.navy.mil/viewVideo.asp?id=20272 |title=Headlines for Thursday, March 12, 2015 |publisher=U.S. Navy |archive-url=https://web.archive.org/web/20150316034940/http://www.navy.mil/viewVideo.asp?id=20272 |archive-date=16 March 2015}}</ref><ref>{{cite web|last=LaGrone|first=Sam |url=http://news.usni.org/2015/03/05/roosevelt-carrier-strike-group-to-depart-for-middle-east-on-monday-in-first-nifc-ca-deployment|title=Roosevelt Carrier Strike Group to Depart for Middle East on Monday in First NIFC-CA Deployment|work=USNI News|date=5 March 2015|accessdate=23 September 2015|archive-url=https://web.archive.org/web/20150922092703/http://news.usni.org/2015/03/05/roosevelt-carrier-strike-group-to-depart-for-middle-east-on-monday-in-first-nifc-ca-deployment|archive-date=22 September 2015|url-status=live}}</ref>\n\nOn 20 April 2015, ''Theodore Roosevelt'', along with the cruiser {{USS|Normandy||6}}, was deployed off the coast of Yemen to intercept suspected Iranian weapons shipments intended for [[Houthi]] rebels, who are engaged in a civil war with Yemeni government forces.<ref>{{cite news|title=US aircraft carrier sent to block Iranian arms shipments to Yemen rebels|url=https://www.foxnews.com/politics/us-aircraft-carrier-sent-to-block-iranian-arms-shipments-to-yemen-rebels|accessdate=1 April 2020|work=Fox News|archive-url=https://web.archive.org/web/20200329165037/https://www.foxnews.com/politics/us-aircraft-carrier-sent-to-block-iranian-arms-shipments-to-yemen-rebels|archive-date=29 March 2020|url-status=live|df=dmy-all|date=20 December 2015}}</ref>\n\nIn early November 2015, ''Theodore Roosevelt'' along with the guided-missile destroyer {{USS|Lassen|DDG-82|6}}, sailed to the [[South China Sea]] to assert freedom of navigation in the area claimed by China.<ref>{{cite news|url=https://www.reuters.com/article/us-southchinasea-usa-warship-idUSKCN0SV05420151106|title='Hope to see you again': China warship to U.S. destroyer after South China Sea patrol|first=Yeganeh|last=Torbati|date=6 November 2015|access-date=1 April 2020|agency=Reuters}}</ref>\n\n''Theodore Roosevelt'' pulled into her new home port at San Diego on 23 November 2015, completing a deployment during which she circumnavigated the globe. The carrier launched 1,800 sorties against [[Islamic State of Iraq and the Levant|Islamic State]] militants in Iraq and Syria as part of [[Operation Inherent Resolve]], totaling 10,618 flight hours and over one million pounds of ordnance employed through 1,085 guided munitions. Carrier Strike Group 12 traveled nearly {{convert|27,000|nmi|mi km|abbr=on}} during the deployment, which also marked aviation milestones including the first operational use of the [[E-2 Hawkeye#E-2D Advanced Hawkeye|E-2D Advanced Hawkeye]] and the last active-duty operational deployment of the [[SH-60#HH-60H|HH-60H Rescue Hawk]] and [[SH-60#SH-60F|SH-60F Seahawk]] helicopters.<ref>{{cite news|last=Myers|first=Meghann |url=https://www.navytimes.com/news/your-navy/2015/11/23/carrier-theodore-roosevelt-returns-from-round-the-world-deployment/|title=Carrier Theodore Roosevelt returns from round-the-world deployment|work=Navy Times|date=23 November 2015|accessdate=2020-04-01}}</ref>\n\nOn 6 October 2017, ''Theodore Roosevelt'' departed San Diego for her deployment to the United States Seventh Fleet and United States Fifth Fleet area of operations, accompanied with [[Carrier Strike Group 9]] and [[Carrier Air Wing Seventeen]].<ref>{{cite press release|url=http://www.navy.mil/submit/display.asp?story_id=102770 |title=Theodore Roosevelt Carrier Strike Group Departs for Deployment|publisher=Commander, Carrier Strike Group 9 Public Affairs|agency=Navy News Service |date=7 October 2017|id=NNS171007-02|archive-url=https://web.archive.org/web/20171019003713/http://www.navy.mil/submit/display.asp?story_id=102770 |archive-date=19 October 2017}}</ref> On November 8, 2017, ''Theodore Roosevelt'' and her group started a 4-day exercise with two other [[carrier strike group]]s, led by carriers ''Ronald Reagan'' and ''Nimitz'', in the [[Sea of Japan]].<ref>{{cite web|last=LaGrone|first=Sam |url=https://news.usni.org/2017/11/08/3-u-s-carrier-strike-groups-exercise-4-days-sea-japan|title=UPDATED: 3 U.S. Carrier Strike Groups to Exercise for 4 Days in the Sea of Japan|work=USNI News|date=8 November 2017|accessdate=December 11, 2017|archive-url=https://web.archive.org/web/20171212193246/https://news.usni.org/2017/11/08/3-u-s-carrier-strike-groups-exercise-4-days-sea-japan|archive-date=12 December 2017|url-status=live}}</ref>\n\nIn May 2019, ''Theodore Roosevelt'' participated in [[Exercise Northern Edge]] 2019, marking the first time in a decade a carrier took part in the exercise. Also in 2019, [[Carrier Air Wing Eleven]] was transferred to the ship.<ref>{{cite press release|last=Guerrero|first=Terence Deleon |url=https://www.navy.mil/submit/display.asp?story_id=109592|title=USS Theodore Roosevelt Participates in Exercise Northern Edge 2019|agency=Navy News Service |publisher=USS Theodore Roosevelt (CVN 71) Public Affairs|date=14 May 2019|id=NNS190514-11|accessdate=14 May 2019|archive-url=https://web.archive.org/web/20190514225345/https://www.navy.mil/submit/display.asp?story_id=109592|archive-date=14 May 2019|url-status=live}}</ref>\n\n===2020s===\n\nOn 24 March 2020, it was reported that three sailors aboard the deployed vessel tested positive for [[COVID-19]], a coronavirus disease identified as the cause of an outbreak of respiratory illness.<ref>{{cite web |url=https://www.usatoday.com/story/news/politics/2020/03/24/coronavirus-3-sailors-test-positive-military-readiness-affected/2910165001/ |title=Three sailors from USS Theodore Roosevelt have coronavirus, raising concerns about pandemic's strain on military |work=USA Today |last=Vanden Brook |first=Tom |authorlink=Tom Vanden Brook|date=24 March 2020 |access-date=25 March 2020}}</ref> Within a few days, that number climbed to \"dozens\". ''Theodore Roosevelt'' was reported to be the first ship in the U.S. Navy to have a COVID-19 outbreak while at sea; ''Theodore Roosevelt'' docked at [[Guam]] on 27 March 2020.<ref name= disinfectUssTRcvn71 /><ref name=\"NBC Gains Griffith\">{{cite news |last1=Gains |first1=Mosheh |last2=Griffith |first2=Janelle |title=Coronavirus outbreak diverts Navy aircraft carrier to Guam, all 5,000 aboard to be tested |url=https://www.nbcnews.com/news/us-news/coronavirus-outbreak-diverts-navy-aircraft-carrier-guam-all-5-000-n1169726 |accessdate=26 March 2020 |work=NBC News |date=26 March 2020}}</ref> By 31 March, the number of infected sailors was over 100, and the captain, Brett Crozier, was pleading for help from the Navy.<ref>{{cite news | last =Gafni | first =Matthias | last2 =Garofoli| first2 =Joe |title =Exclusive: Captain of aircraft carrier with growing coronavirus outbreak pleads for help from Navy| newspaper =[[San Francisco Chronicle]] | location = | pages = | language = | publisher = | date =31 March 2020 | url =https://www.sfchronicle.com/bayarea/article/Exclusive-Captain-of-aircraft-carrier-with-15167883.php | accessdate = 31 March 2020}}</ref> The US Navy ordered the aircraft carrier evacuated with a skeleton crew of 400 to remain aboard the vessel to maintain the nuclear reactor, the fire-fighting equipment, and the ship's galley.<ref name= disinfectUssTRcvn71 >{{cite news|last=Peniston|first=Bradley |url=https://www.defenseone.com/threats/2020/03/us-navy-evacuating-aircraft-carrier-infected-coronavirus/164254/?oref=d-channelriver |title=US Navy Evacuating Aircraft Carrier Infected by Coronavirus|work=Defense One|date=31 March 2020|accessdate=2020-04-01}}</ref> On 2 April, two anonymous officials reported that the Navy intends to relieve Captain Crozier of command the following day, citing a \"loss of trust and confidence\".<ref name=\"NBC Crozier Relieved\">{{cite news |last=Kube |first=Courtney |title=Navy expected to relieve captain who raised alarm about COVID-19 outbreak on aircraft carrier |url=https://www.nbcnews.com/news/military/navy-expected-relieve-captain-who-raised-alarm-about-covid-19-n1175351 |accessdate=2 April 2020 |work=NBC News |date=2 April 2020}}</ref>\n\n==Ship awards==\n* [[Joint Meritorious Unit Award]]\n* [[Navy Unit Commendation]] (3 awards)&nbsp;\u2013 1991, 1995, 2001\n* [[Meritorious Unit Commendation]]&nbsp;\u2013 1993, 2008\n* [[Battle Efficiency Award]] (4 awards)&nbsp;\u2013 1989, 1991, 1993, 2000\n* [[National Defense Service Medal]] (2 awards)&nbsp;\u2013 1990, 2001\n* [[Armed Forces Expeditionary Medal]]\n* [[Southwest Asia Service Medal]] (3 campaigns)\n* [[Global War on Terrorism Expeditionary Medal]]\n* [[Armed Forces Service Medal]] (2 campaigns)\n* [[Sea Service Deployment Ribbon]] (9 overseas deployments)\n* [[NATO Medal]]\n* [[Kuwait Liberation Medal (Saudi Arabia)]]\n* [[Kuwait Liberation Medal (Kuwait)]]\n* [[Battenberg Cup]] (3 awards)&nbsp;\u2013 1991, 1993, 2001\n* Golden Anchor / Retention Excellence Award (7 awards)&nbsp;\u2013 1988, 1993, 1994, 1995, 1997, 2004, 2017\n* Security Excellence Award (2 awards)&nbsp;\u2013 1996, 2009\n* Capt [[Edward F. Ney]] Memorial Award for Outstanding Food Service (2 awards)&nbsp;\u2013 2001, 2002<ref>{{cite press release|last=Benigni|first=Jessica |url=http://www.news.navy.mil/search/display.asp?story_id=6100 |title=America's Big Stick Wins 2nd straight Ney Award|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=5 March 2003|id=NNS030304-15|archive-url=https://web.archive.org/web/20070830032558/http://www.news.navy.mil/search/display.asp?story_id=6100 |archive-date=30 August 2007}}</ref>\n\n== See also ==\n{{Wikipedia books|Nimitz class aircraft carriers}}\n* [[List of aircraft carriers]]\n* [[List of aircraft carriers of the United States Navy]]\n* [[USS Theodore Roosevelt UFO incidents|USS ''Theodore Roosevelt'' UFO incidents]]\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n{{Commons category|USS Theodore Roosevelt (CVN-71)}}\n* [http://www.public.navy.mil//airfor/cvn71/Pages/default.aspx Official website]\n* [http://www.uscarriers.net/cvn71history.htm USS ''Theodore Roosevelt'' history at U.S. Carriers]\n* [https://maps.google.com/maps?f=q&hl=en&geocode=&q=&ie=UTF8&t=k&om=0&ll=36.956442,-76.329027&spn=0.003125,0.006416&z=18 USS ''Theodore Roosevelt'' on Google Maps]\n* [https://web.archive.org/web/20110629053051/http://www.navy.mil/local/story_archive.asp?id=42 USS ''Theodore Roosevelt'' (CVN-71) Story Archive]&nbsp;\u2013 U.S. Navy\n* [http://www.navy.mil/local/cvn71/ USS ''Theodore Roosevelt'' (CVN-71) News]&nbsp;\u2013 U.S. Navy\n* [http://www.history.navy.mil/research/archives/command-operations-reports/ships/t/theodore-roosevelt-cvn-71-i.html USS ''Theodore Roosevelt'' (CVN-71) command histories]&nbsp;\u2013 [[Naval History and Heritage Command]]\n\n<!-- non-breaking space to keep AWB drones from altering the space before the navbox-->\n\n{{Nimitz class aircraft carrier}}\n{{2019\u201320 coronavirus pandemic}}\n{{2020 coronavirus pandemic in the United States}}\n\n{{DEFAULTSORT:Theodore Roosevelt (Cvn-71)}}\n[[Category:1984 ships]]\n[[Category:2020 coronavirus pandemic in the United States]]\n[[Category:Active aircraft carriers of the United States]]\n[[Category:Afghanistan War ships of the United States]]\n[[Category:Cold War aircraft carriers of the United States]]\n[[Category:Gulf War ships of the United States]]\n[[Category:Nuclear ships of the United States Navy]]\n[[Category:Nimitz-class aircraft carriers]]\n[[Category:Ships built in Newport News, Virginia]]\n[[Category:Ships involved in the 2019\u201320 coronavirus pandemic]]\n[[Category:United States Navy New York (state)-related ships]]\n", "text_old": "{{Other ships|USS Theodore Roosevelt}}\n{{Use dmy dates|date=March 2020}}\n{|{{Infobox ship begin\n|infobox caption=yes\n}}\n{{Infobox ship image\n|Ship image=USS Theodore Roosevelt operations 150322-N-ZF573-140.jpg{{!}}border\n|Ship caption=USS ''Theodore Roosevelt'' underway in the Atlantic Ocean in March 2015.\n}}\n{{Infobox ship career\n|Hide header=\n|Ship country=United States\n|Ship flag={{USN flag}}\n|Ship name=\n|Ship namesake=[[Theodore Roosevelt]]\n|Ship owner=\n|Ship operator=\n|Ship registry=\n|Ship route=\n|Ship ordered=30 September 1980\n|Ship awarded=\n|Ship builder=[[Newport News Shipbuilding]] Co.\n|Ship original cost= U.S. $4.5&nbsp;billion in 2007 dollars<ref name=\"USS THEODORE ROOSEVELT&nbsp;\u2014 HISTORY\">{{cite web|url=http://navysite.de/cvn/cvn71history.htm|title=USS Theodore Roosevelt&nbsp;\u2013 History|work=navysite.de|accessdate=3 July 2009|archive-url=https://web.archive.org/web/20090527073103/http://navysite.de/cvn/cvn71history.htm|archive-date=27 May 2009|url-status=live}}</ref>\n|Ship yard number=\n|Ship way number=\n|Ship laid down=31 October 1981\n|Ship launched=27 October 1984\n|Ship sponsor=\n|Ship christened=\n|Ship completed=\n|Ship acquired=\n|Ship commissioned=25 October 1986\n|Ship recommissioned=\n|Ship decommissioned=\n|Ship maiden voyage=\n|Ship in service=\n|Ship out of service=\n|Ship renamed=\n|Ship reclassified=\n|Ship refit=\n|Ship struck=\n|Ship reinstated=\n|Ship homeport=[[NAS North Island]], [[San Diego]], [[California]]\n|Ship identification=\n|Ship motto=*''Qui Plantavit Curabit''\n* (Latin: \"He who has planted will preserve.\")\n|Ship nickname=''TR'', ''[[Big Stick ideology|Big Stick]]''\n|Ship honors=\n|Ship captured=\n|Ship fate=\n|Ship status={{Ship in active service}}\n|Ship notes=\n|Ship badge= [[File:CVN-71 insignia.png|155px]]\n}}\n{{Infobox ship characteristics\n|Hide header=\n|Header caption=\n|Ship class=*{{sclass-|Nimitz|aircraft carrier}}\n* ''Theodore Roosevelt'' subclass\n|Ship tonnage=\n|Ship displacement={{convert|104600|LT|ST}}<ref>{{cite book |title=The Naval Institute guide to the ships and aircraft of the U.S. fleet |last=Polmar |first=Norman |authorlink=Norman Polmar|year=2004 |publisher=Naval Institute Press |isbn=978-1-59114-685-8 |page=[https://archive.org/details/navalinstitutegu0018polm/page/112 112] |url=https://archive.org/details/navalinstitutegu0018polm |url-access=registration |quote=nimitz class displacement. }}</ref>\n|Ship tons burthen=\n|Ship length=*Overall: {{convert|1092|ft|m|1}}\n* Waterline: {{convert|1040|ft|m|1}}\n|Ship beam=*Overall: {{convert|252|ft|m|1}}\n* Waterline: {{convert|134|ft|m|1}}\n|Ship height=\n|Ship draft=*Maximum navigational: {{convert|37|ft|m|1}}\n* Limit: {{convert|41|ft|m|1}}\n|Ship depth=\n|Ship hold depth=\n|Ship decks=\n|Ship deck clearance=\n|Ship ramps=\n|Ship power=\n|Ship propulsion=*2 \u00d7 [[Westinghouse Electric Company|Westinghouse]] [[A4W reactor|A4W nuclear reactor]]s\n* 4 \u00d7 [[steam turbine]]s driving 4 \u00d7 shafts\n* 260,000 shp (194 MW)\n|Ship speed={{Nimitz class aircraft carrier speed}}\n|Ship range={{Nuclear ship range}}\n|Ship endurance=Limited only by food and supplies\n|Ship test depth=\n|Ship boats=\n|Ship capacity=\n|Ship troops=\n|Ship complement=*Ship's company: 3,200\n* Air wing: 2,480\n|Ship crew=\n|Ship time to activate=\n|Ship sensors={{Nimitz class aircraft carrier sensors I}}\n|Ship EW={{Nimitz class aircraft carrier EW}}\n|Ship armament=*2 \u00d7 [[Sea Sparrow]]\n* 2 \u00d7 [[RIM-116 Rolling Airframe Missile]]\n* 2 \u00d7 [[Phalanx CIWS]] (close-in weapon system) Gatling guns\n|Ship armor= 63.5 mm Kevlar armor over vitals <ref>{{cite book |title=Aircraft carriers: an illustrated history of their impact |last=Fontenoy |first=Paul E. |authorlink= |year=2006 |publisher=ABC-CLIO Ltd |location= |isbn=978-1-85109-573-5 |page=349 |pages= |url= |accessdate=}}</ref>\n|Ship aircraft={{Nimitz class aircraft carrier aircraft}}\n|Ship aircraft facilities=\n|Ship notes=\n}}\n|}\n\n'''USS ''Theodore Roosevelt'' (CVN-71)''' is the fourth {{sclass-|Nimitz|aircraft carrier|0}} [[Nuclear propulsion|nuclear powered]] [[aircraft carrier]] in the [[United States Navy]]. She is named in honor of [[Theodore Roosevelt]], the 26th [[President of the United States]]. She is the [[USS\u00a0Roosevelt|fourth ship named in honor of Theodore Roosevelt]], three bearing his full name and a fourth with just his [[SS Roosevelt (1905)|last name]]. Another three U.S. Navy ships have \"Roosevelt\" in their names in honor of members of the [[Roosevelt family]]. This carrier's radio [[call sign]] is \"Rough Rider\", the nickname of President Roosevelt's [[Rough Riders|volunteer cavalry unit]] during the [[Spanish\u2013American War]]. She was launched in 1984, and saw her first action during [[Operation Desert Storm]] in 1991.\n\n==Background==\nInitially, President [[Gerald Ford]] cancelled the order for CVN-71 in 1976 and substituted two [[Aircraft Carrier (Medium)|CVV-type medium-sized, conventional-powered carrier]]s that were expected to operate [[V/STOL]] aircraft. The existing T-CBL design formed the basis for the new CVV, serving as a replacement for the aging {{sclass-|Midway|aircraft carrier|0}} carriers, while capable of operating all existing conventional carrier aircraft. This capability to operate conventional aircraft proved important as the [[Rockwell XFV-12|hoped-for supersonic V/STOL fighters]] did not come to fruition at the time. In any case, construction of the proposed CVV medium-sized carrier never took place.<ref>{{cite book |title=Aircraft Carriers: An Illustrated Design History |last=Friedman |first=Norman |authorlink=Norman Friedman |year=1983 |publisher=[[United States Naval Institute|Naval Institute Press]] |location=[[Annapolis, Maryland]] |isbn=978-0-87021-739-5 |pages=323\u2013324; 329\u2013333 |url=http://www.usni.org/store/books/aircraft-carriers/us-aircraft-carriers |accessdate=17 July 2013 |archive-url=https://web.archive.org/web/20131022162107/http://www.usni.org/store/books/aircraft-carriers/us-aircraft-carriers |archive-date=22 October 2013 |url-status=dead }}</ref><ref name=NANcvvJUL79>{{cite web |title=CVV |url=http://www.history.navy.mil/nan/backissues/1970s/1979/jul79.pdf |work=Naval Aviations News |publisher=[[Naval History & Heritage Command]] |location=[[Washington Navy Yard]] |page=8 |format=PDF |date=July 1979 |accessdate=18 July 2013 |url-status=dead |archiveurl=https://web.archive.org/web/20100816080321/http://www.history.navy.mil/nan/backissues/1970s/1979/jul79.pdf |archivedate=16 August 2010}}</ref>\n\nAuthorization for CVN-71 was further delayed when President [[Jimmy Carter]] [[Veto#United States|vetoed]] the 1979 Fiscal Year Department of Defense authorization bill because of the inclusion of this ''Nimitz''-class nuclear supercarrier in the Navy's shipbuilding program.<ref name=NANcvvJUL79/><ref name=Polmar>{{cite book |title=Aircraft Carriers: A History of Carrier Aviation and Its Influence on World Events, Volume 2 |last=Polmar |first=Norman |year=2006 |publisher=Potomac Books, Inc. |location=[[Dulles, Virginia]] |isbn=978-1-57488-663-4 |page=364 |url=https://books.google.com/books?id=BNAXHkfMs5wC&pg=PA364&lpg=PA364&dq=carter+veto+cvn-71&source=bl&ots=qHXzOChxFM&sig=CgSoi5y2rU6niQRye_XGKOMdf-g&hl=en&sa=X&ei=MjjoUaTvFIaa9QSWw4C4AQ&ved=0CDIQ6AEwAg#v=onepage&q=carter%20veto%20cvn-71&f=false |accessdate=17 July 2013 |archive-url=https://web.archive.org/web/20190331175731/https://books.google.com/books?id=BNAXHkfMs5wC&pg=PA364&lpg=PA364&dq=carter+veto+cvn-71&source=bl&ots=qHXzOChxFM&sig=CgSoi5y2rU6niQRye_XGKOMdf-g&hl=en&sa=X&ei=MjjoUaTvFIaa9QSWw4C4AQ&ved=0CDIQ6AEwAg#v=onepage&q=carter%20veto%20cvn-71&f=false |archive-date=31 March 2019 |url-status=live }}</ref> As a result of the [[Iran hostage crisis]] which required the increased deployment of U.S. aircraft [[carrier battle group]]s to the [[Indian Ocean]], President Carter reversed his stand on ''Nimitz''-class nuclear supercarriers, and CVN-71 was subsequently authorized under the 1980 Fiscal Year authorization bill for the U.S. Department of Defense.<ref name=Polmar/>\n\n==Design and construction==\n''Theodore Roosevelt'' was the first aircraft carrier to be assembled using modular construction, wherein large modules are independently constructed in \"lay-down\" areas, prior to being hoisted into place and welded together. Modular construction, made possible through the use of a huge gantry crane capable of lifting 900 tons, cut 16 months off ''Theodore Roosevelt''{{'}}s construction time, and the technique has been used on every aircraft carrier since. ''Theodore Roosevelt'' and those ''Nimitz''-class vessels completed after her have slight structural differences from the earlier carriers ({{USS|Nimitz||6}}, {{USS|Dwight D. Eisenhower||6}}, and {{USS|Carl Vinson||6}}) and improved protection for ordnance storage in her [[Magazine (artillery)|magazines]].<ref>{{cite web |url=https://www.defenseindustrydaily.com/costing-the-cvn21-a-did-primer-01624/ |title=Costing the CVN-21: A DID Primer |date=19 December 2005 |website=Defense Industry Daily |accessdate=27 December 2009 |archive-url=https://web.archive.org/web/20190528234846/https://www.defenseindustrydaily.com/costing-the-cvn21-a-did-primer-01624/ |archive-date=28 May 2019 |url-status=live }}</ref>\n\n''Theodore Roosevelt''{{'}}s history began on 30 September 1980, when a contract was awarded for \"Hull 624D\" to [[Newport News Shipbuilding]]. Her [[Keel laying|keel was laid down]] on 31 October 1981, with Secretary of Defense [[Caspar Weinberger]] initiating the first weld. On 3 November 1981, Secretary of the Navy [[John F. Lehman]] announced that the carrier would be named for Theodore Roosevelt. The vessel's [[Pre-Commissioning Unit]] (PCU) was formed in February 1984, with Captain Paul W. Parcells named as Commanding Officer. On 27 October 1984, the ship was officially [[ceremonial ship launching|christened]] by Mrs. Barbara Lehman, wife of Secretary Lehman. On 25 October 1986, ''Theodore Roosevelt'' was [[Ship commissioning|commissioned]] to active service at Newport News.<ref>{{cite news|url=https://www.washingtonpost.com/archive/politics/1986/10/26/uss-theodore-roosevelt-joins-active-service-as-15th-carrier/b14cf77c-8102-4b7d-808c-28606bffaefa/|title=USS Theodore Roosevelt Joins Active Service as 15th Carrier|first=Geroge C.|last=Wilson|newspaper=The Washington Post|date=26 October 1986|page=A21|accessdate=2020-04-01}}</ref>\n\n==Service history==\n===Maiden deployment===\n[[File:USS Theodor Roosevelt shock test.jpg|thumb|left|Shock test of ''Theodore Roosevelt'' during sea trials in 1987]]\nAfter sea trials and pre-deployment workups, ''Theodore Roosevelt'' started her [[maiden voyage|maiden deployment]] on 30 December 1988 with [[Carrier Air Wing Eight]] (CVW-8) embarked. The ship patrolled the [[Mediterranean Sea]] prior to returning on 30 June 1989.She was awarded the 1989 [[Battle \"E\"]] from [[Commander, Naval Air Force U.S. Atlantic Fleet]] on 20 March 1990.{{citation_needed|date=August 2019}}\n\n===1990s===\n====Gulf War====\nOn 28 December 1990, ''Theodore Roosevelt'' and CVW-8 deployed for [[Operation Desert Shield]], arriving in the [[Persian Gulf]] on 16 January 1991. With the commencement of Operation Desert Storm on 15 January 1991, ''Theodore Roosevelt'' began combat operations; eventually flying over 4,200 sorties, more than any other carrier, and dropping more than {{convert|4,800,000|lb|kg|1}} of ordnance before the cease-fire on 28 February.<ref name=\"USS THEODORE ROOSEVELT&nbsp;\u2014 HISTORY\"/>\n[[File:US Navy Battle Force Zulu carriers overhead view in 1991.jpg|thumb|Four U.S. Navy carriers form \"Battle Force Zulu\" following the 1991 Gulf War; ''Theodore Roosevelt'' (top right) cruises with {{USS|Midway|CV-41|2}} (top left), {{USS|Ranger|CV-61|2}} (bottom left) and {{USS|America|CV-66|2}} (bottom right)]]\n\nWhen [[Ba'athist Iraq|Iraqi]] forces turned on the [[Kurds]], ''Theodore Roosevelt'' and CVW-8 were among the first coalition forces in [[Operation Provide Comfort]], flying patrols over northern Iraq. After a 189-day deployment, with 176 days at sea, ''Theodore Roosevelt'' returned to Norfolk on 28 June 1991. On 14 February 1992, the ship won her second Battle \"E\". This was followed by the award of the [[Battenberg Cup]] for 1991 as the Atlantic Fleet's premier ship.<ref name=\"tr.surfor.navy.mil\">{{cite web|url=http://www.tr.surfor.navy.mil/about%20tr/ship%20history.html|title=Ship's History |work=tr.surfor.navy.mil |archive-url=https://web.archive.org/web/20090413121419/http://www.tr.surfor.navy.mil/about%20tr/ship%20history.html |archive-date=13 April 2009}}</ref>\n\n''Theodore Roosevelt'' began her third deployment on 11 March 1993, again with CVW-8 embarked. Also embarked was a Special Purpose [[Marine Air-Ground Task Force]] (SPMAGTF), in a test the concept of embarking a multi-purpose Marine force in a carrier. While the ship was still in the [[Virginia Capes]] operating area, President [[Bill Clinton]] flew aboard for several hours for his first visit to a U.S. Navy ship.<ref name=\"tr.surfor.navy.mil\" /> ''Theodore Roosevelt'' operated in the [[Adriatic Sea|Adriatic]] as CVW-8 planes enforced [[Operation Deny Flight]] in the U.S. no-fly zone over [[Bosnia and Herzegovina|Bosnia]]. In June, on the way to only her second port visit, ''Theodore Roosevelt'' was ordered instead to transit the [[Suez Canal]] en route to the [[Red Sea]] to participate in [[Operation Southern Watch]], enforcing the no-fly zone over Iraq. Deployed for 184 days, ''Theodore Roosevelt'' spent 169 days under way prior to return in September 1993. For the accomplishments of her crew, the ship received her second [[Meritorious Unit Commendation]].{{citation_needed|date=August 2019}}\n\nFrom November 1993 to April 1994, ''Theodore Roosevelt'' conducted a Selected Restricted Availability (SRA) at [[Norfolk Naval Shipyard]] (NNSY), completing ahead of schedule. On 10 March 1994, ''Theodore Roosevelt'' received her third Battle \"E\". Then on 3 June, ''Theodore Roosevelt'' was awarded her second Battenberg Cup as the best ship in the Atlantic Fleet.{{citation_needed|date=August 2019}}\n\n''Theodore Roosevelt'' and CVW-8 began their fourth deployment in March 1995, operating in the Red Sea in support of Operation Southern Watch over Iraq, and Operations Deny Flight and [[Operation Sharp Guard|Sharp Guard]] over the skies of Bosnia and in the Adriatic operating areas. Deny Flight evolved into [[Operation Deliberate Force]], as CVW-8 aircraft led [[NATO]] strikes against strategic Bosnian Serb targets in Bosnia-Herzegovina. The ''Theodore Roosevelt'' Battle Group returned to Norfolk, Virginia in September 1995 and was awarded the [[Navy Unit Commendation]] for its Bosnia operations.<ref name=\"tr.surfor.navy.mil\" />\n\n====Collision with USS ''Leyte Gulf''====\nOn 14 October 1996, ''Theodore Roosevelt'' collided with {{USS|Leyte Gulf||6}}, a {{sclass-|Ticonderoga|cruiser|0}} [[guided missile cruiser]], while conducting operations off the coast of North Carolina. The incident occurred as the carrier, without prior warning, reversed her engines while ''Leyte Gulf'' was behind her and collided with the cruiser's bow. There were no injuries reported,<ref>{{cite web|url=http://www.dcfp.navy.mil/mc/museum/LEYTEGULF/LEYTEGULF.htm|title=USS ''Leyte Gulf'' at DCHM|accessdate=16 September 2007|publisher=Naval Sea Systems Command DC Museum|url-status=dead|archiveurl=https://web.archive.org/web/20080214143307/http://www.dcfp.navy.mil/mc/museum/LEYTEGULF/LEYTEGULF.htm|archivedate=14 February 2008|df=dmy-all}}</ref> but ''Theodore Roosevelt'' suffered more than $7 million damage to her stern, while damages to ''Leyte Gulf''{{'}}s bow were assessed at $2 million.<ref>{{Cite web |url=http://articles.dailypress.com/1996-11-23/news/9611230009_1_leyte-gulf-deck-officers-aircraft-carrier-theodore-roosevelt |title=Navy Officers Reprimanded for Role in Ships Collision |last=McMichael |first=William H. |work=Daily Press |date=23 November 1996 |access-date=19 June 2017 |archive-url=https://web.archive.org/web/20170626104621/http://articles.dailypress.com/1996-11-23/news/9611230009_1_leyte-gulf-deck-officers-aircraft-carrier-theodore-roosevelt |archive-date=26 June 2017 |url-status=live }}</ref>\n\n''Theodore Roosevelt'' deployed for her fifth deployment on 25 November 1996, with [[Carrier Air Wing Three|CVW-3]] embarked, in support of Operation Southern Watch in the Mediterranean and Persian Gulf. The ship returned from deployment in May 1997. On 8 July 1997, ''Theodore Roosevelt'' entered the [[Newport News Shipbuilding]] yard for a one-year Extended Drydock and Selected Restricted Availability (EDSRA), her first major overhaul since commissioning. ''Theodore Roosevelt'' returned to her homeport of [[Naval Station Norfolk|Norfolk Naval Station]] on 2 July 1998.{{citation_needed|date=August 2019}}\n[[File:USS Theodore Roosevelt - BigStick.jpg|thumb|left|''Theodore Roosevelt'' underway in 1999]]\n\nFrom 1 February to 4 March 1999 ''Theodore Roosevelt'' participated in exercise JTFEX / TMDI99 along with the [[Brazilian Navy]] and several NATO navies. During the exercise, ''Theodore Roosevelt'' was mock-sunk,<ref>{{cite book |url=https://books.google.com/?id=Tqj9ZP8FsJEC&pg=PA22&lpg=PA22&dq=JTFEX/TMDI+99+walrus#v=onepage&q=JTFEX%2FTMDI%2099%20walrus&f=false |title=Lessons Not Learned: The U.S. Navy's Status Quo Culture |isbn=978-1-59114-865-4 |last=Thompson |first=Roger |publisher=Naval Institute Press |year=2007}}</ref> along with eight other U.S. ships, many of which were the carrier's escorts, by submarine {{HNLMS|Walrus||6}} of the [[Royal Netherlands Navy]].{{citation_needed|date=August 2019}}\n\n[[File:US Navy 990523-N-8493H-001 Corpsman prepares prescriptions for USS Roosevelt crew.jpg|thumb|right|upright|A U.S. Navy corpsman aboard USS ''Theodore Roosevelt'' in May 1999]]\n''Theodore Roosevelt'' began her sixth deployment on 26 March 1999 with CVW-8 embarked. They were immediately called to duty in the [[Ionian Sea]] to support NATO's [[Operation Allied Force]]. ''Theodore Roosevelt'' and CVW-8 aircraft conducted air strikes for two months over the skies of [[Kosovo]] against Serbian positions. ''Theodore Roosevelt'' and CVW-8 were then dispatched to support Operation Southern Watch, enforcing the \"no-fly\" zone over Southern Iraq. ''Theodore Roosevelt'' returned to her homeport of Norfolk, Virginia, on 24 September 1999.{{citation_needed|date=August 2019}}\n\n===2000s===\n[[File:USS Theodore Roosevelt CATCC.jpg|thumb|left|An air traffic controller watches his radar scope in the Carrier Air Traffic Control Center in 2002]]\nOn 10 January 2000, ''Theodore Roosevelt'' entered a Planned Incremental Availability (PIA) at the Norfolk Naval Naval Shipyard, [[Portsmouth, Virginia]] for a six-month maintenance period.{{citation_needed|date=August 2019}}\n\n====September 11 attacks====\nAfter the [[September 11 attacks]], ''Theodore Roosevelt'' began her seventh deployment on 19 September 2001 with [[Carrier Air Wing One]] (CVW-1). On the night of 4 October 2001, ''Theodore Roosevelt'' and CVW-1 launched the initial strikes of [[Operation Enduring Freedom]] against [[al-Qaeda]] in Afghanistan from the [[North Arabian Sea]]. Between departing Norfolk on 19 September 2001 and arriving in Bahrain for a liberty call on 27 February 2002, ''Theodore Roosevelt'' spent 160 consecutive days at sea, breaking the record for the longest period underway since [[World War II]].<ref>{{cite news |url=https://www.washingtonpost.com/archive/local/2002/03/07/uss-roosevelt-sets-record-with-160-days-at-sea/3d6ce8ec-90f3-40e9-a2ed-a1c92bbd17b0/ |title=USS Roosevelt Sets Record With 160 Days at Sea |first=Steve |last=Vogel |date=7 March 2002 |access-date=1 April 2020 |website=The Washington Post |url-access=limited}}</ref> ''Theodore Roosevelt'' returned to her homeport 27 March 2002 and was awarded the [[Navy Unit Commendation]], 2001 Battenberg Cup, and 2001 Battle \"E\".<ref>{{cite press release|last=Kerns|first=Rob |url=http://www.news.navy.mil/search/display.asp?story_id=1908 |title=Theodore Roosevelt Takes Battenberg Cup |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=10 June 2002 |id=NNS020610-06|archive-url=https://web.archive.org/web/20061124204409/http://www.news.navy.mil/search/display.asp?story_id=1908 |archive-date=24 November 2006}}</ref> From April to October 2002, ''Theodore Roosevelt'' conducted a Planned Incremental Availability maintenance period at Norfolk Naval Ship Yard.{{citation_needed|date=August 2019}}\n\n[[File:USS Theodore Roosevelt - resplenishment.jpg|thumb|right|''Theodore Roosevelt'' receives cargo while pierside at the [[Souda Bay#Naval Dock Crete|NATO Marathi Pier Facility]] in [[Crete]].]]\n''Theodore Roosevelt'' got underway on 6 January for a scheduled month-long training period in the [[Puerto Rican Operating Area]]. Near the end of January, ''Theodore Roosevelt'' received orders to proceed across the Atlantic to the Mediterranean Sea. [[VFA-201|Strike Fighter Squadron 201]], based at Naval Air Station [[Naval Air Station Joint Reserve Base Fort Worth]], Texas, was ordered to active duty as a unit of Carrier Air Wing (CVW) 8, the first [[United States Navy Reserve|Naval Reserve]] squadron to deploy aboard an aircraft carrier since the [[Korean War]].<ref>{{cite press release|last=Boxleitner|first=Kirk |url=http://www.news.navy.mil/search/display.asp?story_id=5302 |title=VFA-201 \"Hunters\" Make History Aboard TR|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=27 January 2003 |id=NNS030127-04|archive-url=https://web.archive.org/web/20070831135932/http://www.news.navy.mil/search/display.asp?story_id=5302 |archive-date=31 August 2007}}</ref> ''Theodore Roosevelt'' arrived on station in the Eastern Mediterranean in February. On 22 March 2003 ''Theodore Roosevelt'', along with {{USS|Harry S. Truman||6}}, began launching air strikes into Iraq in support of [[Operation Iraqi Freedom]].<ref>{{cite press release|last=Kerns |first=Rob |url=http://www.news.navy.mil/search/display.asp?story_id=6471 |title=America's Big Stick Launches Operation Iraqi Freedom Strikes|agency=Navy News Service |publisher=USS Theodore Roosevelt Public Affairs |date=24 March 2003 |id=NNS030324-01|archive-url=https://web.archive.org/web/20030401082516/http://www.news.navy.mil/search/display.asp?story_id=6471 |archive-date=1 April 2003}}</ref> ''Theodore Roosevelt'' returned home on 26 May and was awarded the Meritorious Unit Commendation, the [[Navy Unit Citation]], and the [[Global War on Terrorism Expeditionary Medal]].\n[[File:Uss theodore roosevelt cvn-71.jpg|thumb|left|''Theodore Roosevelt'' in the [[Elizabeth River (Virginia)|Elizabeth River]] in 2004]]\n\nOn 19 February 2004, ''Theodore Roosevelt'' entered a ten-month Docked Planned Incremental Availability (DPIA) at NNSY in Portsmouth.<ref>{{cite press release|last=Catalano|first=Mark A. |url=http://www.news.navy.mil/search/display.asp?story_id=12404 |title=FOD Walkdown Marks End to Historic Chapter for TR|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service|date=23 March 2004 |id=NNS040323-10|archive-url=https://web.archive.org/web/20061124205221/http://www.news.navy.mil/search/display.asp?story_id=12404 |archive-date=24 November 2006}}</ref> Major systems overhauled included AC systems, Steam and CHT (sewage) systems, 1MC (announcing) systems, communication, navigation, and detection suites, weapons elevator overhauls, propeller replacement, hull cleaning and painting, and sea valve replacement. ''Theodore Roosevelt'' came out of dry-dock in August and completed the maintenance availability on 17 December 2004.<ref>{{cite press release|last=Catalano |first=Mark |url=http://www.news.navy.mil/search/display.asp?story_id=16350 |title=Fast Cruise Marks End of DPIA for 'Big Stick'|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=20 December 2004|id=NNS041220-03|archive-url=https://web.archive.org/web/20061124205116/http://www.news.navy.mil/search/display.asp?story_id=16350 |archive-date=24 November 2006}}</ref>\n[[File:US Navy 011020-N-3896H-002 USS Theodore Roosevelt (CVN 71).jpg|thumb|right|An [[McDonnell Douglas F/A-18 Hornet|F/A-18 Hornet]] from the \"Sidewinders\" of [[VFA-86]] ignites its afterburners while preparing to be catapulted from the flight deck.]]\n\nOn 1 September 2005, ''Theodore Roosevelt'' deployed with CVW-8 embarked for a routine six-month mission to the Persian Gulf in support of Operation Iraqi Freedom (OIF),<ref>{{cite press release|last=Stephens|first=Kimberly R. |url=http://www.news.navy.mil/search/display.asp?story_id=19883 |title=Theodore Roosevelt CSG Deploys in Support of Global War on Terrorism|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=2 September 2005 |id=NNS050902-14|archive-url=https://web.archive.org/web/20071205195312/http://www.news.navy.mil/search/display.asp?story_id=19883 |archive-date=5 December 2007}}</ref> transiting the Suez Canal on 27 September<ref>{{cite press release|last=Bristol|first=Daniel A. |url=http://www.news.navy.mil/search/display.asp?story_id=20463 |title=USS Theodore Roosevelt Transits Through Suez Canal|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=6 October 2005 |id=NNS051006-10|archive-url=https://web.archive.org/web/20070208022553/http://www.news.navy.mil/search/display.asp?story_id=20463 |archive-date=8 February 2007}}</ref> and launching OIF missions beginning 6 October.<ref>{{cite press release|url=http://www.news.navy.mil/search/display.asp?story_id=20517 |title=TR CSG Offers OIF Air Support|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=11 October 2005 |id=NNS051011-02|archive-url=https://web.archive.org/web/20070907150043/http://www.news.navy.mil/search/display.asp?story_id=20517 |archive-date=7 September 2007}}</ref> This deployment was the last cruise for the [[F-14 Tomcat]] before its retirement in 2006. ''Theodore Roosevelt'' carried two Tomcat squadrons, [[VF-31]] (Tomcatters) and [[VF-213]] (Black Lions).<ref>{{cite press release|last=Catalano|first=Mark A. |url=http://www.news.navy.mil/search/display.asp?story_id=24913 |title=Tomcat Chapter Draws to a Close |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service|date=29 July 2006 |id=NNS060729-02|archive-url=https://web.archive.org/web/20060915193350/http://www.news.navy.mil/search/display.asp?story_id=24913 |archive-date=15 September 2006}}</ref> ''Theodore Roosevelt'' returned to home port on 11 March 2006. Shortly after this cruise, ''Theodore Roosevelt'' earned the [[\"Jig Dog\" Ramage Carrier and Carrier Air Wing Operational Excellence Award]], which is a Navy-wide award that is selected jointly by Type Commanders (TYCOM) and is presented to the Carrier/Air Wing team with the best performance as an integrated unit.{{citation_needed|date=August 2019}}\n\nOn 7 March 2007, ''Theodore Roosevelt'' began a nine-month Planned Incremental Availability (PIA) in Norfolk, which saw the addition of [[RIM-116 Rolling Airframe Missile#Sea RAM|RAM-116 missiles]] among other upgrades.<ref>{{cite press release|last=Bullock|first=Matt |url=http://www.news.navy.mil/search/display.asp?story_id=28222 |title=Theodore Roosevelt Moves to Shipyard|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=9 March 2007|id=NNS070309-09|archive-url=https://web.archive.org/web/20070320034520/http://www.news.navy.mil/search/display.asp?story_id=28222 |archive-date=20 March 2007}}</ref> The ship returned to Naval Station Norfolk on 28 November 2007.{{citation needed|date=August 2019}}\n\nCVW-8 and ''Theodore Roosevelt'' participated in Joint Task Force Exercise 08-4 Operation Brimstone off the coast of [[North Carolina]] between 21 and 31 July 2008. The British aircraft carrier {{HMS|Ark Royal|R07|6}}, the amphibious assault ship {{USS|Iwo Jima|LHD-7|2}} with associated units and the Brazilian frigate {{ship|Brazilian frigate|Greenhalgh|F46|2}} and the French submarine {{ship|French submarine|Am\u00e9thyste|S605|2}} also participated in the event.<ref>{{cite press release|url=http://www.news.navy.mil/search/display.asp?story_id=38478 |title=JTFEX 08-4 \"Operation Brimstone\" Flexes Allied Force Training|publisher=Commander, U.S. 2nd Fleet Public Affairs|agency=Navy News Service|date=15 July 2008 |id=NNS080715-21|archive-url=https://web.archive.org/web/20080813171934/http://www.news.navy.mil/search/display.asp?story_id=38478 |archive-date=13 August 2008}}</ref>\n\n''Theodore Roosevelt'' left Norfolk on 8 September 2008 for a scheduled deployment to the Middle East with [[Carrier Air Wing Eight]] embarked.<ref>{{cite press release|last=Hilley|first=Monique |url=http://www.news.navy.mil/search/display.asp?story_id=39754 |title=USS Theodore Roosevelt Deploys in Support of Maritime Security Operations|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service|date=26 September 2008 |id=NNS080926-19|archive-url=https://web.archive.org/web/20090919003658/http://www.news.navy.mil/search/display.asp?story_id=39754 |archive-date=19 September 2009}}</ref> On 4 October 2008, the ship stopped at [[Cape Town]], South Africa. This was the first visit to Cape Town by a nuclear-powered vessel since the German cargo ship ''[[Otto Hahn (ship)|Otto Hahn]]'' in the 1970s.<ref>{{cite web|url=https://www.iol.co.za/news/south-africa/uss-theodore-gets-green-light-418499|title=USS Theodore gets green light|publisher=OIL|date=1 October 2008|accessdate=2020-04-01}}</ref> Due to poor weather, approximately half of the ship's crew was unable to go ashore on [[shore leave|liberty]]. Much of the crew that made it ashore were unable to return to ''Theodore Roosevelt'' due to the increasingly poor weather. The remaining crew was forced to remain on the pier till morning alongside the [[cruiser]] {{USS|Monterey|CG-61|6}}. The ship made four subsequent port stops in [[Jebel Ali]], UAE, including one during the Christmas holiday. CVW-8 and CVN-71 supported Operation Enduring Freedom and flew more than 3,100 sorties and dropped more than 59,500 pounds of ordnance while providing [[close air support]] for [[International Security Assistance Force|ISAF]]-forces in Afghanistan.{{citation_needed|date=August 2019}}\n\nOn 21 March 2009, ''Theodore Roosevelt'' was relieved by ''Dwight D. Eisenhower''.<ref>{{cite press release|url=http://www.navy.mil/search/display.asp?story_id=43629 |title=Eisenhower Launches OEF Sorties|publisher=U.S. Naval Forces Central Command Public Affairs|agency=Navy News Service |date=21 March 2009|id=NNS090321-02|accessdate=2012-05-26|archive-url=https://web.archive.org/web/20090322221828/http://www.navy.mil/search/display.asp?story_id=43629 |archive-date=22 March 2009}}</ref> The carrier arrived at Norfolk on 18 April.<ref>{{cite news|work=[[The Washington Times]]|title=Carrier Returns To Navy Station |date=19 April 2009|page=7}}</ref> On 26 August 2009 defense contractor [[Northrop Grumman]] was awarded a 2.4&nbsp;billion dollar contract for [[Refueling and Complex Overhaul]] (RCOH) of ''Theodore Roosevelt'' at its Newport News shipyard, to be completed in 2013.<ref>{{cite web|url=http://www.defenselink.mil/contracts/contract.aspx?contractid=4103 |title=Contracts for Wednesday, 26 August 2009 |work=DefenseLink |publisher=U.S. Department of Defense |accessdate=19 January 2014|archive-url=https://web.archive.org/web/20090901115208/http://www.defenselink.mil/contracts/contract.aspx?contractid=4103 |archive-date=1 September 2009}}</ref>\n\n===2010s===\nOn 29 August 2013, ''Theodore Roosevelt'' returned to Norfolk Naval Station, Virginia, completing its post-overhaul sea trials that concluded its four-year mid-life RCOH.<ref>{{cite press release|title=Theodore Roosevelt Returns to Norfolk as a Ready for Tasking Carrier |url=http://www.navy.mil/submit/display.asp?story_id=76254 |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=29 August 2012 |id=NNS130829-16|accessdate=30 August 2012 |archive-url=https://web.archive.org/web/20131029194723/http://www.navy.mil/submit/display.asp?story_id=76254 |archive-date=29 October 2013 |url-status=live }}<br />{{cite press release|title=Roosevelt Successfully Completes RCOH |url=http://www.navy.mil/submit/display.asp?story_id=76261 |publisher=PEO Carriers Public Affairs |agency=Navy News Service|date=29 August 2012 |id=NNS130829-20|accessdate=30 August 2012 |archive-url=https://web.archive.org/web/20130902075923/http://www.navy.mil/submit/display.asp?story_id=76261 |archive-date=2 September 2013 |url-status=live }}</ref> On 14 September 2013, ''Theodore Roosevelt'' successfully completed flight deck certification which entailed completing a total of 160 carrier landings during daytime and night-time operations. Other certification drills included rigging the emergency barricade, flight deck firefighting evolutions, and crash and salvage operations.<ref>{{cite press release|last=Zeigler |first=Heath |title=Theodore Roosevelt Completes Flight Deck Certification |url=http://www.navy.mil/submit/display.asp?story_id=76613 |publisher=USS Theodore Roosevelt Public Affairs |agency=Navy News Service|date=16 September 2013 |id=NNS130916-14|accessdate=24 October 2013 |archive-url=https://web.archive.org/web/20131029184717/http://www.navy.mil/submit/display.asp?story_id=76613 |archive-date=29 October 2013 |url-status=live }}</ref> On 17 September 2013, ''Theodore Roosevelt'' completed her first [[underway replenishment]] in over four years.<ref>{{cite press release|last=Lindstrom |first=Kris R.|title=USS Theodore Roosevelt Completes First Underway Replenishment in Four Years |url=http://www.navy.mil/submit/display.asp?story_id=76697 |publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=20 September 2013 |id=NNS130920-22|accessdate=24 October 2013 |archive-url=https://web.archive.org/web/20131029193324/http://www.navy.mil/submit/display.asp?story_id=76697 |archive-date=29 October 2013 |url-status=live }}</ref>\n\nFlight testing for the [[Northrop Grumman X-47B|X-47B]] continued on board ''Theodore Roosevelt'' on 10 November 2013. During this phase, the X-47B's digitized carrier-controlled environment was tested which involved the interface between the unmanned aircraft and carrier personnel during launching, flight operations and recovery. The digital environment offered increased flexibility and enhanced safety for carrier operations.<ref>{{cite press release|title=X-47B Operates Aboard Theodore Roosevelt |url=http://www.navy.mil/submit/display.asp?story_id=77580 |agency=Navy News Service |id=NNS131110-02 |publisher=USS Theodore Roosevelt Public Affairs |date=10 November 2013 |accessdate=14 November 2013 |archive-url=https://web.archive.org/web/20131115005319/http://www.navy.mil/submit/display.asp?story_id=77580 |archive-date=15 November 2013 |url-status=live }}</ref>\n\nOn 15 January 2014, the Navy announced that ''Theodore Roosevelt''{{'}}s homeport would move to San Diego, replacing {{USS|Ronald Reagan||6}} when she relocated to Japan sometime in 2015 as part of the US Navy's preparation for the planned refueling of {{USS|George Washington|CVN-73|6}}.<ref>{{cite news|url=http://www.navytimes.com/article/20140115/NEWS/301150012/Reagan-replace-GW-Japan-Roosevelt-San-Diego |title=Reagan to replace GW in Japan; Roosevelt to San Diego |newspaper=Navy Times |date=15 January 2014 |accessdate=29 September 2014|archive-url=http://archive.today/2014.01.15-202319/http://www.navytimes.com/article/20140115/NEWS/301150012/Reagan-replace-GW-Japan-Roosevelt-San-Diego|archive-date=15 January 2014}}</ref>\n\nOn 4 March 2015, during a training exercise off [[Florida]], ''Theodore Roosevelt'' was mock-sunk by the [[French Navy]] submarine {{ship|French submarine|Saphir|S602|2}}\n<ref>{{cite web |url=https://www.defense.gouv.fr/marine/actu-marine/le-sna-saphir-en-entrainement-avec-l-us-navy-au-large-de-la-floride |last=Savary |first=Quentin |title=Le SNA Saphir en entrainement avec l'US navy au large de la Floride |publisher=French Ministry of Defence|language=French|date=4 March 2015|accessdate=6 March 2015 |url-status=dead|archiveurl=https://web.archive.org/web/20150402224312/http://www.defense.gouv.fr/marine/actu-marine/le-sna-saphir-en-entrainement-avec-l-us-navy-au-large-de-la-floride |archivedate=2 April 2015}}</ref><ref name=rt2015>{{cite web |url=https://www.rt.com/usa/238257-french-submarine-us-carrier/ |author= |title=French delete evidence US carrier was 'sunk' by sub in drill |website=[[RT (TV network)|RT]] |date=6 March 2015 |accessdate=6 March 2015 |archive-url=https://web.archive.org/web/20150307173858/http://rt.com/usa/238257-french-submarine-us-carrier/ |archive-date=7 March 2015 |url-status=live }}</ref>\n\nOn 11 March 2015,''Theodore Roosevelt'' and [[Carrier Strike Group 12]] departed Naval Station Norfolk for an around the world tour with deployments to the [[United States Fifth Fleet|U.S. 5th]], [[United States Sixth Fleet|6th]] and [[United States Seventh Fleet|7th Fleets]] as part the first deployment of [[Cooperative Engagement Capability#NIFC-CA|Naval Integrated Fire Control-Counter Air (NIFC-CA)]] Carrier Strike Group, before arriving in their new homeport of San Diego, California.<ref>{{cite web|url=http://www.navy.mil/viewVideo.asp?id=20272 |title=Headlines for Thursday, March 12, 2015 |publisher=U.S. Navy |archive-url=https://web.archive.org/web/20150316034940/http://www.navy.mil/viewVideo.asp?id=20272 |archive-date=16 March 2015}}</ref><ref>{{cite web|last=LaGrone|first=Sam |url=http://news.usni.org/2015/03/05/roosevelt-carrier-strike-group-to-depart-for-middle-east-on-monday-in-first-nifc-ca-deployment|title=Roosevelt Carrier Strike Group to Depart for Middle East on Monday in First NIFC-CA Deployment|work=USNI News|date=5 March 2015|accessdate=23 September 2015|archive-url=https://web.archive.org/web/20150922092703/http://news.usni.org/2015/03/05/roosevelt-carrier-strike-group-to-depart-for-middle-east-on-monday-in-first-nifc-ca-deployment|archive-date=22 September 2015|url-status=live}}</ref>\n\nOn 20 April 2015, ''Theodore Roosevelt'', along with the cruiser {{USS|Normandy||6}}, was deployed off the coast of Yemen to intercept suspected Iranian weapons shipments intended for [[Houthi]] rebels, who are engaged in a civil war with Yemeni government forces.<ref>{{cite news|title=US aircraft carrier sent to block Iranian arms shipments to Yemen rebels|url=https://www.foxnews.com/politics/us-aircraft-carrier-sent-to-block-iranian-arms-shipments-to-yemen-rebels|accessdate=1 April 2020|work=Fox News|archive-url=https://web.archive.org/web/20200329165037/https://www.foxnews.com/politics/us-aircraft-carrier-sent-to-block-iranian-arms-shipments-to-yemen-rebels|archive-date=29 March 2020|url-status=live|df=dmy-all|date=20 December 2015}}</ref>\n\nIn early November 2015, ''Theodore Roosevelt'' along with the guided-missile destroyer {{USS|Lassen|DDG-82|6}}, sailed to the [[South China Sea]] to assert freedom of navigation in the area claimed by China.<ref>{{cite news|url=https://www.reuters.com/article/us-southchinasea-usa-warship-idUSKCN0SV05420151106|title='Hope to see you again': China warship to U.S. destroyer after South China Sea patrol|first=Yeganeh|last=Torbati|date=6 November 2015|access-date=1 April 2020|agency=Reuters}}</ref>\n\n''Theodore Roosevelt'' pulled into her new home port at San Diego on 23 November 2015, completing a deployment during which she circumnavigated the globe. The carrier launched 1,800 sorties against [[Islamic State of Iraq and the Levant|Islamic State]] militants in Iraq and Syria as part of [[Operation Inherent Resolve]], totaling 10,618 flight hours and over one million pounds of ordnance employed through 1,085 guided munitions. Carrier Strike Group 12 traveled nearly {{convert|27,000|nmi|mi km|abbr=on}} during the deployment, which also marked aviation milestones including the first operational use of the [[E-2 Hawkeye#E-2D Advanced Hawkeye|E-2D Advanced Hawkeye]] and the last active-duty operational deployment of the [[SH-60#HH-60H|HH-60H Rescue Hawk]] and [[SH-60#SH-60F|SH-60F Seahawk]] helicopters.<ref>{{cite news|last=Myers|first=Meghann |url=https://www.navytimes.com/news/your-navy/2015/11/23/carrier-theodore-roosevelt-returns-from-round-the-world-deployment/|title=Carrier Theodore Roosevelt returns from round-the-world deployment|work=Navy Times|date=23 November 2015|accessdate=2020-04-01}}</ref>\n\nOn 6 October 2017, ''Theodore Roosevelt'' departed San Diego for her deployment to the United States Seventh Fleet and United States Fifth Fleet area of operations, accompanied with [[Carrier Strike Group 9]] and [[Carrier Air Wing Seventeen]].<ref>{{cite press release|url=http://www.navy.mil/submit/display.asp?story_id=102770 |title=Theodore Roosevelt Carrier Strike Group Departs for Deployment|publisher=Commander, Carrier Strike Group 9 Public Affairs|agency=Navy News Service |date=7 October 2017|id=NNS171007-02|archive-url=https://web.archive.org/web/20171019003713/http://www.navy.mil/submit/display.asp?story_id=102770 |archive-date=19 October 2017}}</ref> On November 8, 2017, ''Theodore Roosevelt'' and her group started a 4-day exercise with two other [[carrier strike group]]s, led by carriers ''Ronald Reagan'' and ''Nimitz'', in the [[Sea of Japan]].<ref>{{cite web|last=LaGrone|first=Sam |url=https://news.usni.org/2017/11/08/3-u-s-carrier-strike-groups-exercise-4-days-sea-japan|title=UPDATED: 3 U.S. Carrier Strike Groups to Exercise for 4 Days in the Sea of Japan|work=USNI News|date=8 November 2017|accessdate=December 11, 2017|archive-url=https://web.archive.org/web/20171212193246/https://news.usni.org/2017/11/08/3-u-s-carrier-strike-groups-exercise-4-days-sea-japan|archive-date=12 December 2017|url-status=live}}</ref>\n\nIn May 2019, ''Theodore Roosevelt'' participated in [[Exercise Northern Edge]] 2019, marking the first time in a decade a carrier took part in the exercise. Also in 2019, [[Carrier Air Wing Eleven]] was transferred to the ship.<ref>{{cite press release|last=Guerrero|first=Terence Deleon |url=https://www.navy.mil/submit/display.asp?story_id=109592|title=USS Theodore Roosevelt Participates in Exercise Northern Edge 2019|agency=Navy News Service |publisher=USS Theodore Roosevelt (CVN 71) Public Affairs|date=14 May 2019|id=NNS190514-11|accessdate=14 May 2019|archive-url=https://web.archive.org/web/20190514225345/https://www.navy.mil/submit/display.asp?story_id=109592|archive-date=14 May 2019|url-status=live}}</ref>\n\n===2020s===\n\nOn 24 March 2020, it was reported that three sailors aboard the deployed vessel tested positive for [[COVID-19]], a coronavirus disease identified as the cause of an outbreak of respiratory illness.<ref>{{cite web |url=https://www.usatoday.com/story/news/politics/2020/03/24/coronavirus-3-sailors-test-positive-military-readiness-affected/2910165001/ |title=Three sailors from USS Theodore Roosevelt have coronavirus, raising concerns about pandemic's strain on military |work=USA Today |last=Vanden Brook |first=Tom |authorlink=Tom Vanden Brook|date=24 March 2020 |access-date=25 March 2020}}</ref> Within a few days, that number climbed to \"dozens\". ''Theodore Roosevelt'' was reported to be the first ship in the U.S. Navy to have a COVID-19 outbreak while at sea; ''Theodore Roosevelt'' docked at [[Guam]] on 27 March 2020.<ref name= disinfectUssTRcvn71 /><ref name=\"NBC Gains Griffith\">{{cite news |last1=Gains |first1=Mosheh |last2=Griffith |first2=Janelle |title=Coronavirus outbreak diverts Navy aircraft carrier to Guam, all 5,000 aboard to be tested |url=https://www.nbcnews.com/news/us-news/coronavirus-outbreak-diverts-navy-aircraft-carrier-guam-all-5-000-n1169726 |accessdate=26 March 2020 |work=NBC News |date=26 March 2020}}</ref> By 31 March, the number of infected sailors was over 100, and the captain, Brett Crozier, was pleading for help from the Navy.<ref>{{cite news | last =Gafni | first =Matthias | last2 =Garofoli| first2 =Joe |title =Exclusive: Captain of aircraft carrier with growing coronavirus outbreak pleads for help from Navy| newspaper =[[San Francisco Chronicle]] | location = | pages = | language = | publisher = | date =31 March 2020 | url =https://www.sfchronicle.com/bayarea/article/Exclusive-Captain-of-aircraft-carrier-with-15167883.php | accessdate = 31 March 2020}}</ref> The US Navy ordered the aircraft carrier evacuated with a skeleton crew of 400 to remain aboard the vessel to maintain the nuclear reactor, the fire-fighting equipment, and the ship's galley.<ref name= disinfectUssTRcvn71 >{{cite news|last=Peniston|first=Bradley |url=https://www.defenseone.com/threats/2020/03/us-navy-evacuating-aircraft-carrier-infected-coronavirus/164254/?oref=d-channelriver |title=US Navy Evacuating Aircraft Carrier Infected by Coronavirus|work=Defense One|date=31 March 2020|accessdate=2020-04-01}}</ref> On 2 April, two anonymous officials reported that the Navy intends to relieve Captain Crozier of command the following day, citing a \"loss of trust and confidence\".<ref name=\"NBC Crozier Relieved\">{{cite news |last=Kube |first=Courtney |title=Navy expected to relieve captain who raised alarm about COVID-19 outbreak on aircraft carrier |url=hhttps://www.nbcnews.com/news/military/navy-expected-relieve-captain-who-raised-alarm-about-covid-19-n1175351 |accessdate=2 April 2020 |work=NBC News |date=2 April 2020}}</ref>\n\n==Ship awards==\n* [[Joint Meritorious Unit Award]]\n* [[Navy Unit Commendation]] (3 awards)&nbsp;\u2013 1991, 1995, 2001\n* [[Meritorious Unit Commendation]]&nbsp;\u2013 1993, 2008\n* [[Battle Efficiency Award]] (4 awards)&nbsp;\u2013 1989, 1991, 1993, 2000\n* [[National Defense Service Medal]] (2 awards)&nbsp;\u2013 1990, 2001\n* [[Armed Forces Expeditionary Medal]]\n* [[Southwest Asia Service Medal]] (3 campaigns)\n* [[Global War on Terrorism Expeditionary Medal]]\n* [[Armed Forces Service Medal]] (2 campaigns)\n* [[Sea Service Deployment Ribbon]] (9 overseas deployments)\n* [[NATO Medal]]\n* [[Kuwait Liberation Medal (Saudi Arabia)]]\n* [[Kuwait Liberation Medal (Kuwait)]]\n* [[Battenberg Cup]] (3 awards)&nbsp;\u2013 1991, 1993, 2001\n* Golden Anchor / Retention Excellence Award (7 awards)&nbsp;\u2013 1988, 1993, 1994, 1995, 1997, 2004, 2017\n* Security Excellence Award (2 awards)&nbsp;\u2013 1996, 2009\n* Capt [[Edward F. Ney]] Memorial Award for Outstanding Food Service (2 awards)&nbsp;\u2013 2001, 2002<ref>{{cite press release|last=Benigni|first=Jessica |url=http://www.news.navy.mil/search/display.asp?story_id=6100 |title=America's Big Stick Wins 2nd straight Ney Award|publisher=USS Theodore Roosevelt Public Affairs|agency=Navy News Service |date=5 March 2003|id=NNS030304-15|archive-url=https://web.archive.org/web/20070830032558/http://www.news.navy.mil/search/display.asp?story_id=6100 |archive-date=30 August 2007}}</ref>\n\n== See also ==\n{{Wikipedia books|Nimitz class aircraft carriers}}\n* [[List of aircraft carriers]]\n* [[List of aircraft carriers of the United States Navy]]\n* [[USS Theodore Roosevelt UFO incidents|USS ''Theodore Roosevelt'' UFO incidents]]\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n{{Commons category|USS Theodore Roosevelt (CVN-71)}}\n* [http://www.public.navy.mil//airfor/cvn71/Pages/default.aspx Official website]\n* [http://www.uscarriers.net/cvn71history.htm USS ''Theodore Roosevelt'' history at U.S. Carriers]\n* [https://maps.google.com/maps?f=q&hl=en&geocode=&q=&ie=UTF8&t=k&om=0&ll=36.956442,-76.329027&spn=0.003125,0.006416&z=18 USS ''Theodore Roosevelt'' on Google Maps]\n* [https://web.archive.org/web/20110629053051/http://www.navy.mil/local/story_archive.asp?id=42 USS ''Theodore Roosevelt'' (CVN-71) Story Archive]&nbsp;\u2013 U.S. Navy\n* [http://www.navy.mil/local/cvn71/ USS ''Theodore Roosevelt'' (CVN-71) News]&nbsp;\u2013 U.S. Navy\n* [http://www.history.navy.mil/research/archives/command-operations-reports/ships/t/theodore-roosevelt-cvn-71-i.html USS ''Theodore Roosevelt'' (CVN-71) command histories]&nbsp;\u2013 [[Naval History and Heritage Command]]\n\n<!-- non-breaking space to keep AWB drones from altering the space before the navbox-->\n\n{{Nimitz class aircraft carrier}}\n{{2019\u201320 coronavirus pandemic}}\n{{2020 coronavirus pandemic in the United States}}\n\n{{DEFAULTSORT:Theodore Roosevelt (Cvn-71)}}\n[[Category:1984 ships]]\n[[Category:2020 coronavirus pandemic in the United States]]\n[[Category:Active aircraft carriers of the United States]]\n[[Category:Afghanistan War ships of the United States]]\n[[Category:Cold War aircraft carriers of the United States]]\n[[Category:Gulf War ships of the United States]]\n[[Category:Nuclear ships of the United States Navy]]\n[[Category:Nimitz-class aircraft carriers]]\n[[Category:Ships built in Newport News, Virginia]]\n[[Category:Ships involved in the 2019\u201320 coronavirus pandemic]]\n[[Category:United States Navy New York (state)-related ships]]\n", "name_user": "Joshmaul", "label": "safe", "comment": "\u2192\u200e2020s", "url_page": "//en.wikipedia.org/wiki/USS_Theodore_Roosevelt_(CVN-71)"}
{"title_page": "Michael Parad\u017eikovi\u0107", "text_new": "{{Use dmy dates|date=March 2019}}\n{{Infobox football biography\n| name = Michael Parad\u017eikovi\u0107\n| image = \n| birth_date = {{Birth date and age|df=yes|1992|1|9}}\n| birth_place = [[Reutlingen]], Germany\n| height = 1.88 m\n| position = [[Goalkeeper (association football)|Goalkeeper]]\n| currentclub = TuS Metzingen\n| clubnumber = \n| youthyears1 = \u20132006\n| youthclubs1 = [[HNK Cibalia|Cibalia]]\n| youthyears2 = 2006\u20132007\n| youthclubs2 = [[NK Istra 1961|Istra 1961]]\n| youthyears3 = 2007\n| youthclubs3 = [[HNK Cibalia|Cibalia]]\n| youthyears4 = 2007\u20132008\n| youthclubs4 = [[NK Dilj|Dilj]]\n| youthyears5 = 2008\u20132010\n| youthclubs5 = [[HNK Cibalia|Cibalia]]\n| years1 = 2010\u20132012\n| clubs1 = [[HNK Cibalia|Cibalia]]\n| caps1 = 9\n| goals1 = 0\n| years2 = 2011\u20132012\n| clubs2 = \u2192 [[NK Marsonia 1909|Marsonia 1909]] (loan)\n| caps2 = 2\n| goals2 = 0\n| years3 = 2014\n| clubs3 = TuS Metzingen\n| caps3 = \n| goals3 = \n| years4 = 2015\u201316\n| clubs4 = SV Croatia Reutlingen\n| caps4 = 4\n| goals4 = 0\n| years5 = 2016\u2013\n| clubs5 = TuS Metzingen\n| caps5 = 80\n| goals5 = 0\n| nationalyears1 = 2009\n| nationalteam1 = [[Croatia national under-17 football team|Croatia U17]]\n| nationalcaps1 = 1\n| nationalgoals1 = 0\n| nationalyears2 = 2009\u20132010\n| nationalteam2 = Croatia U18\n| nationalcaps2 = 6\n| nationalgoals2 = 0\n| nationalyears3 = 2009\u20132011\n| nationalteam3 = [[Croatia national under-19 football team|Croatia U19]]\n| nationalcaps3 = 9\n| nationalgoals3 = 0\n| nationalyears4 = 2011\n| nationalteam4 = [[Croatia national under-20 football team|Croatia U20]]\n| nationalcaps4 = 2\n| nationalgoals4 = 0\n| club-update = 1 July 2019\n| nationalteam-update = {{date|2011-11-2}}\n}}\n\n'''Michael Parad\u017eikovi\u0107''' (born 9 January 1992) is a Croatian [[association football|footballer]] who plays as a [[Goalkeeper (association football)|goalkeeper]] for German lower-league club TuS Metzingen.\n\n==Club career==\nParad\u017eikovi\u0107 started his career playing at youth level for [[HNK Cibalia|Cibalia]], where he signed a professional three-year contract in November 2010.<ref>{{cite news|url=http://vinkovci.com.hr/vinkovci/hrvatska/vinkovci-novo-pojacanje-u-cibaliji-michael-paradzikovic/ |title=Vinkovci: Novo poja\u010danje u Cibaliji - Michael Parad\u017eikovi\u0107 |last= |first= |date=23 November 2010 |work=vinkovci.com.hr |language=Croatian |accessdate=2 February 2012}}</ref> He made his debut for the first team on 13 August 2011 in a 4\u20131 defeat to [[NK Slaven Belupo|Slaven Belupo]].<ref>{{cite news|url=http://www.hrsport.net/vijesti/422741/nogomet-1-hnl/slaven-belupo-ponizio-cibaliju |title=Slaven Belupo ponizio Cibaliju |last=Glasnovi\u0107 |first=Mario |date=13 August 2011 |work=Sportnet.hr |language=Croatian |accessdate=2 February 2012}}</ref> At the end of August, he was loaned to [[Druga HNL]] side [[NK Marsonia 1909|Marsonia 1909]] where he was featured in only two games before returning to Cibalia in January 2012.\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{CFF player|13312/michael-paradzikovic}}\n* [https://www.fupa.net/spieler/michael-paradzikovic-695112.html Profile] at FuPa.net\n\n{{DEFAULTSORT:Paradzikovic, Michael}}\n[[Category:1992 births]]\n[[Category:Living people]]\n[[Category:Croatian footballers]]\n[[Category:Croatia youth international footballers]]\n[[Category:Association football goalkeepers]]\n[[Category:Croatian First Football League players]]\n[[Category:HNK Cibalia players]]\n\n\n{{Croatia-footy-bio-stub}}\n", "text_old": "{{Use dmy dates|date=March 2019}}\n{{Infobox football biography\n| name = Michael Parad\u017eikovi\u0107\n| image = \n| birth_date = {{Birth date and age|df=yes|1992|1|9}}\n| birth_place = [[Reutlingen]], [[Germany]]\n| height = {{height|m=1.88|abbr=on}}\n| position = [[Goalkeeper (association football)|Goalkeeper]]\n| currentclub = TuS Metzingen\n| clubnumber = \n| youthyears1 = \u20132006\n| youthclubs1 = [[HNK Cibalia|Cibalia]]\n| youthyears2 = 2006\u20132007\n| youthclubs2 = [[NK Istra 1961|Istra 1961]]\n| youthyears3 = 2007\n| youthclubs3 = [[HNK Cibalia|Cibalia]]\n| youthyears4 = 2007\u20132008\n| youthclubs4 = [[NK Dilj|Dilj]]\n| youthyears5 = 2008\u20132010\n| youthclubs5 = [[HNK Cibalia|Cibalia]]\n| years1 = 2010\u20132012\n| clubs1 = [[HNK Cibalia|Cibalia]]\n| caps1 = 9\n| goals1 = 0\n| years2 = 2011\u20132012\n| clubs2 = \u2192 [[NK Marsonia 1909|Marsonia 1909]] (loan)\n| caps2 = 2\n| goals2 = 0\n| years3 = 2014\n| clubs3 = TuS Metzingen\n| caps3 = \n| goals3 = \n| years4 = 2015\u201316\n| clubs4 = SV Croatia Reutlingen\n| caps4 = \n| goals4 = \n| nationalyears1 = 2009\n| nationalteam1 = [[Croatia national under-17 football team|Croatia U17]]\n| nationalcaps1 = 1\n| nationalgoals1 = 0\n| nationalyears2 = 2009\u20132010\n| nationalteam2 = Croatia U18\n| nationalcaps2 = 6\n| nationalgoals2 = 0\n| nationalyears3 = 2009\u20132011\n| nationalteam3 = [[Croatia national under-19 football team|Croatia U19]]\n| nationalcaps3 = 9\n| nationalgoals3 = 0\n| nationalyears4 = 2011\n| nationalteam4 = [[Croatia national under-20 football team|Croatia U20]]\n| nationalcaps4 = 2\n| nationalgoals4 = 0\n| pcupdate = {{date|2011-12-4}}\n| ntupdate = {{date|2011-11-2}}\n| clubs5 = TuS Metzingen\n| caps5 = \n| goals5 = \n| years5 = 2016\u2013\n}}\n\n'''Michael Parad\u017eikovi\u0107''' (born 9 January 1992) is a [[Croats|Croatian]] football [[Goalkeeper (association football)|goalkeeper]], currently playing for Tus Metzingen in the [[Bezirksliga|Bezirksliga Alb]].\n\n==Club career==\nParad\u017eikovi\u0107 started his career playing at youth level for [[HNK Cibalia|Cibalia]], where he signed a professional three-year contract in November 2010.<ref>{{cite news|url=http://vinkovci.com.hr/vinkovci/hrvatska/vinkovci-novo-pojacanje-u-cibaliji-michael-paradzikovic/ |title=Vinkovci: Novo poja\u010danje u Cibaliji - Michael Parad\u017eikovi\u0107 |last= |first= |date=23 November 2010 |work=vinkovci.com.hr |language=Croatian |accessdate=2 February 2012}}</ref> He made his debut for the first team on 13 August 2011 in a 4\u20131 defeat to [[NK Slaven Belupo|Slaven Belupo]].<ref>{{cite news|url=http://www.hrsport.net/vijesti/422741/nogomet-1-hnl/slaven-belupo-ponizio-cibaliju |title=Slaven Belupo ponizio Cibaliju |last=Glasnovi\u0107 |first=Mario |date=13 August 2011 |work=Sportnet.hr |language=Croatian |accessdate=2 February 2012}}</ref> At the end of August, he was loaned to [[Druga HNL]] side [[NK Marsonia 1909|Marsonia 1909]] where he was featured in only two games before returning to Cibalia in January 2012.\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{CFF player|13312/michael-paradzikovic}}\n\n{{HNK Cibalia squad}}\n\n{{DEFAULTSORT:Paradzikovic, Michael}}\n[[Category:1992 births]]\n[[Category:Living people]]\n[[Category:Croatian footballers]]\n[[Category:Croatia youth international footballers]]\n[[Category:Association football goalkeepers]]\n[[Category:Croatian First Football League players]]\n[[Category:HNK Cibalia players]]\n\n\n{{Croatia-footy-bio-stub}}\n", "name_user": "Robby.is.on", "label": "safe", "comment": "Update caps. Minor corrections.", "url_page": "//en.wikipedia.org/wiki/Michael_Parad%C5%BEikovi%C4%87"}
{"title_page": "Pacorus II", "text_new": "{{Infobox monarch\n| name = Pacorus II<br>{{lang|xpr|\ud802\udf50\ud802\udf4a\ud802\udf45\ud802\udf53}}\n| title = [[King of Kings]]\n| image = Coin of Pacorus II (cropped), Seleucia mint.jpg\n| image_size =\n| caption = [[Tetradrachm]] of Pacorus II wearing a [[tiara]], minted at [[Seleucia]] in 92/3\n| reign = 78 \u2013 110\n| succession = King of the [[Parthian Empire]]\n| predecessor = [[Vologases I of Parthia|Vologases I]] ({{small|predecessor}})<br>[[Vologases II of Parthia|Vologases II]] {{small|(rival king)}}<br>[[Artabanus III of Parthia|Artabanus III]] {{small|(rival king)}}\n| successor = [[Vologases III of Parthia|Vologases III]] {{small|(successor)}}<br>[[Osroes I of Parthia|Osroes I]] {{small|(rival king)}}\n| dynasty = [[Arsacid dynasty of Parthia|Arsacid dynasty]]\n| spouse =\n| father = [[Vologases I of Parthia|Vologases I]]\n| mother =\n| issue = [[Vologases III of Parthia|Vologases III]]<br>[[Axidares of Armenia|Axidares]]<br>[[Parthamasiris of Armenia|Parthamasiris]]<br>[[Meredates of Characene|Meredates]]\n| birth_date = {{circa|61/2}}\n| birth_place =\n| death_date = 110 (aged 48 or 49)\n| death_place =\n| place of burial =\n| religion = [[Zoroastrianism]]\n}}\n'''Pacorus II''' (also spelled '''Pakoros II'''; {{lang-xpr|\ud802\udf50\ud802\udf4a\ud802\udf45\ud802\udf53}}) was the [[King of Kings]] of the [[Parthian Empire]] from 78 to 110. He was the son and successor of [[Vologases I of Parthia|Vologases I]] ({{reign|51|78}}).\n\nDuring the latter part of his father's reign, Pacorus ruled the Parthian Empire along with him. After Vologases I's death in 78, Pacorus became the sole ruler, but was quickly met by a revolt by his brother [[Vologases II of Parthia|Vologases II]], which lasted until the latter's defeat in 80. In 79/80, Pacorus' rule was contended by another Parthian prince\u2014[[Artabanus III of Parthia|Artabanus III]], whom he had defeated by 81. A third Parthian contender, [[Osroes I]], appeared in 109. The following year, Pacorus was succeeded by his son [[Vologases III of Parthia|Vologases III]], who continued his father's struggle with Osroes I over the Parthian crown.\n\nLike his father, Pacorus continued the same policies of the prominent former Parthian king [[Artabanus II of Parthia|Artabanus II]] ({{reign|12|38/41}}), which included increasing the economic sources of the Parthian Empire by establishing a new trade system and strengthening relations with other powers, such as [[Han China]].\n\nUnder Pacorus, the usage of the image of the Greek goddess [[Tyche]] on the reverse of Parthian coins became more regular than that of the seated king with a [[Bow and arrow|bow]], specifically on the coin minted at [[Ecbatana]]. Tyche was either a representation of the Iranian goddesses [[Anahita]] or [[Ashi]].\n\n== Name ==\nThe name ''{{lang|la|Pacorus}}'' is the [[Latin]] form of the [[Greek language|Greek]] ''Pakoros'' ({{lang|grc|\u03a0\u03b1\u03ba\u03ce\u03c1\u03bf\u03c2}}), itself a variant of the [[Middle Iranian]] ''Pakur'', derived from [[Old Iranian]] ''bag-puhr'' (\"son of a god\").<ref>{{harvnb|Rapp|2014|p=334}}; {{harvnb|Marciak|2017|p=224}}</ref> The [[Armenian language|Armenian]] transliteration is ''Bakur'' (\u0532\u0561\u056f\u0578\u0582\u0580).{{sfn|Rapp|2014|p=334}}\n\nPacorus II's name is recorded in the bilingual inscription on the famous bronze statue of Heracles in Seleucia as [[Koine Greek|Greek]] ''Pakhorou'' ({{lang|grc|\u03a0\u03b1\u03c7\u03cc\u03c1\u03bf\u03c5}}, ''genitive'') and [[Parthian language|Parthian]] ''pkwr'' ({{lang|xpr|\ud802\udf50\ud802\udf4a\ud802\udf45\ud802\udf53}} 'Pakur').{{sfn|Potter|1991|pp=279, 281 (see also note 9)}}{{sfn|Gregoratti|2013|pp=280\u2013282}}\n\n== Background ==\nPacorus was one of the younger sons of the Parthian king [[Vologases I of Parthia|Vologases I]] ({{reign|51|78}}), being born in {{circa|61/2}}.<ref>{{harvnb|Gregoratti|2017|p=132}}; {{harvnb|Hollis|1994|pp=206\u2013208}} has surmised Pacorus to be the same newly ascended Parthian king described by the contemporary Roman poet [[Statius]] as a ''puer'', around sixteen or seventeen years old.</ref> Under Vologases I, the empire experienced a resurgence.{{sfn|Olbrycht|2016b|p=24}} During the last years of his reign, Pacorus ruled alongside him.{{sfn|D\u0105browa|2007|p=176}} After Vologases I's death in 78, Pacorus became the sole ruler of the empire.{{sfn|D\u0105browa|2007|p=176}}{{sfn|Gregoratti|2017|p=131}}\n\n== Reign ==\nPacorus was soon met by a revolt by his brother [[Vologases II of Parthia|Vologases II]], which lasted until the latter's defeat in 80.{{sfn|D\u0105browa|2012|p=391}}{{sfn|Chaumont|Schippmann|1988|pp=574\u2013580}} In 79/80, Pacorus' rule was contended by another Parthian prince\u2014[[Artabanus III of Parthia|Artabanus III]], who seemed to have little support in the empire, with the exception of [[Babylonia]].{{sfn|Schippmann|1986|pp=647\u2013650}} Artabanus III's most notable action was to give refuge to a [[Pseudo-Nero]] named [[Terentius Maximus]].{{sfn|Kia|2016|p=179}}{{sfn|Schippmann|1986|pp=647\u2013650}} Artabanus III initially agreed to lend military aid to Terentius Maximus to capture [[Rome]], until he found about the real identity of the impostor.{{sfn|Kia|2016|p=179}} Coin mints of Artabanus III disappear after 81, which suggests that by this year Pacorus had defeated him.{{sfn|Schippmann|1986|pp=647\u2013650}} \n\nLike his father, Pacorus sought to accomplish the goal of [[Artabanus II of Parthia|Artabanus II]] ({{reign|12|38/41}}), by attempting to establish a long and structured trade-route that spanned through [[East Asia]], [[India]] and the coast of the [[Mediterranean Sea]].{{sfn|Gregoratti|2017|p=131}} This planned long trade-route would greatly improve the economy of the Parthian Empire.{{sfn|Gregoratti|2017|p=131}} In order to accomplish this, Pacorus strengthened relations with other powers whom he was able to establish long distance trade with, most notably [[Han China]].{{sfn|Gregoratti|2017|pp=131\u2013132}} In 97, the Chinese general [[Ban Chao]], the [[Protectorate of the Western Regions|Protector-General of the Western Regions]], sent his emissary [[Gan Ying]] on a diplomatic mission to reach the [[Roman Empire]]. Gan visited the court of Pacorus at [[Hecatompylos]] before departing towards Rome.<ref name=\"watson_1983_543-544\">{{harvnb|Watson|1983|pp=543\u2013544}}</ref> He traveled as far west as the [[Persian Gulf]], where the Parthian authorities convinced him that an arduous sea voyage around the [[Arabian Peninsula]] was the only means to reach Rome.<ref>{{harvnb|Watson|1983|pp=543\u2013544}}; {{harvnb|Y\u00fc|1986|pp=460\u2013461}}; {{harvnb|de Crespigny|2007|pp=239\u2013240}}; see also {{harvnb|Wang|2007|p=101}}</ref> Discouraged by this, Gan Ying returned to the Han court and provided [[Emperor He of Han]] ({{reign|88|105}}) with a detailed report on the Roman Empire based on oral accounts of his Parthian hosts.<ref>{{harvnb|Wood|2002|pp=46\u201347}}; {{harvnb|Morton|Lewis|2005|p=59}}</ref> The modern historian William Watson speculated that the Parthians would have been relieved at the failed efforts by the Han Empire to open diplomatic relations with Rome, especially after Ban Chao's [[Han\u2013Xiongnu War|military victories]] against the [[Xiongnu]] in [[Tarim Basin|eastern Central Asia]].<ref name=\"watson_1983_543-544\"/>\n\nParthian interest also continued to grow in eastern lands of [[Khwarazm]], [[Bactria]], and the [[Hindu Kush]].{{sfn|D\u0105browa|2007|p=176}} The influence of the Parthian Empire is demonstrated by the existence of Parthian aspects in the coinage of numerous political entities in those areas.{{sfn|D\u0105browa|2007|p=175}} During his last years of rule, Pacorus co-ruled with his son [[Vologases III of Parthia|Vologases III]].{{sfn|D\u0105browa|2012|p=176}} In 109, a third Parthian contender named [[Osroes I]] appeared.{{sfn|D\u0105browa|2012|pp=176, 391}} In 110, Pacorus sold the Arsacid vassal kingdom of [[Osroene]] to [[Abgar VII]].{{sfn|Sellwood|1983|pp=456\u2013459}} Pacorus died in the same year, and was succeeded by Vologases III, who continued his father's struggle with Osroes I over the Arsacid crown.{{sfn|D\u0105browa|2012|p=176}}\n\n== Coinage ==\n[[Image:PacorusIIAnotherParthianCoinHistoryofIran.jpg|thumb|[[Tetradrachm]] of a young Pacorus II wearing a [[diadem]]]]\nOn the observe of his coins, Pacorus is portrayed simply wearing a [[diadem]].{{sfn|Olbrycht|1997|p=50}} At first, he appeared beardless on his coins, a rare feature in [[Parthian coinage]] that demonstrated his youth, having ascended the throne around the age of sixteen or seventeen.{{sfn|Hollis|1994|pp=206\u2013207}} From 82/3, he is depicted with a beard.{{sfn|Hollis|1994|p=208}} From 93\u201396, Pacorus is portrayed with his father's [[tiara]].{{sfn|Olbrycht|1997|p=50}} The modern historian Marek Jan Olbrycht surmises that the wearing of the tiara in the latter part of his reign was reflected the power and status of his empire at this time.{{sfn|Olbrycht|1997|p=50}} \n\nThe reverse of his coins portrayed the Greek goddess [[Tyche]] investing him as king.{{sfn|Rezakhani|2013|p=770}} Under Pacorus, the usage of the image of Tyche on the reverse of Parthian coins became more regular than that of the seated king with a [[Bow and arrow|bow]], specifically on the coin minted at [[Ecbatana]].{{sfn|Rezakhani|2013|p=770}} This lasted until the reign of his son and successor, Vologases III.{{sfn|Rezakhani|2013|p=770}} In the Parthian era, Iranians used Hellenistic iconography to portray their divine figures,{{sfn|Curtis|2012|pp=76\u201377}}{{sfn|Boyce|1984|p=82}} thus the investiture scene can be associated with the [[Avesta]]n ''[[khvarenah]]'', i.e., kingly glory, with Tyche representing one of the Iranian goddesses [[Anahita]] or [[Ashi]].<ref>{{harnvb|Curtis|2012|p=71}}; {{harnvb|Olbrycht|2016a|p=99}}; {{harnvb|Curtis|2016|p=183}}</ref>\n\n== Offspring ==\nBesides Vologases III, Pacorus had three other sons: [[Axidares of Armenia|Axidares]], and [[Parthamasiris of Armenia|Parthamasiris]], who successively served as [[List of Armenian kings|kings of Armenia]],{{sfn|Chaumont|1986|pp=418\u2013438}} and [[Meredates of Characene|Meredates]], who served as king of [[Characene]] in the mid-2nd century.{{sfn|Olbrycht|1997|p=51}}\n\n==References==\n{{reflist|2}}\n\n==Sources==\n* {{cite book | title = Zoroastrians: Their Religious Beliefs and Practices | year = 1984 | publisher = Psychology Press | location = | editor-last = | editor-first = | last = Boyce | first = Mary | authorlink = Mary Boyce | chapter = | pages = 1\u2013252 | isbn = 9780415239028 | url = https://archive.org/details/zoroastriansthei00boyc_0/page/82/mode/2up |ref=harv}} {{free access}}\n* {{cite encyclopedia | title = Armenia and Iran ii. The pre-Islamic period | last = Chaumont | first = M. L. | authorlink = | url = http://www.iranicaonline.org/articles/armenia-ii | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. II, Fasc. 4 | pages = 418\u2013438 | location = | publisher = | year = 1986 | isbn = |ref=harv}}\n* {{cite encyclopedia | article = Bal\u0101\u0161 | last1 = Chaumont | first1 = M. L. | first2 = K. | last2 = Schippmann | authorlink =  | url = http://www.iranicaonline.org/articles/balas-proper-name##1 | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. III, Fasc. 6 | pages = 574\u2013580 | location = | publisher = | year = 1988 | isbn = |ref=harv}}\n* {{cite book |last=Curtis|first=Vesta Sarkhosh|authorlink=|editor-last =  | editor-first = | chapter=Parthian coins: Kingship and Divine Glory|title=The Parthian Empire and its Religions |isbn= 9783940598134|date=2012|volume= |pages=67\u201383|publisher=| location = |url=https://www.academia.edu/6186948/Parthian_coins_kingship_and_divine_glory|ref=harv}} {{free access}}\n* {{cite book|url=https://www.academia.edu/23810856/Ancient_Iranian_Motifs_and_Zoroastrian_Iconography|last=Curtis|first=Vesta Sarkhosh|editor1-first=Markus|editor1-last=Williams|editor2-first=Sarah|editor2-last=Stewart|editor3-first=Almut|editor3-last=Hintze|year=2016|chapter=Ancient Iranian Motifs and Zoroastrian Iconography|title=The Zoroastrian Flame Exploring Religion, History and Tradition|publisher=I.B. Tauris|series=|volume=|isbn=9780857728159|pages=179\u2013203|ref=harv}}\n* {{cite journal |last1=D\u0105browa|first1=Edward|authorlink=|editor-last =  | editor-first = | title=The Parthian Kingship |journal= |date=2007|volume= |pages=123-134|publisher=| location = |url=https://www.academia.edu/1501481/The_Parthian_Kingship|ref=harv}} {{free access}}\n* {{cite book |first1=Edward |last1=D\u0105browa |editor1-last=Daryaee |editor1-first=Touraj |editor1-link=Touraj Daryaee |title=The Oxford Handbook of Iranian History |date=2012 |publisher=Oxford University Press |chapter=The Arsacid Empire |pages=1\u2013432 |isbn=0-19-987575-8 |url=https://books.google.dk/books?hl=da&id=K-poAgAAQBAJ&dq=The+Oxford+Handbook+of+Iranian+History&q=pacorus#v=snippet&q=pacorus&f=false |ref=harv |access-date=2019-01-13 |archive-url=https://web.archive.org/web/20190101051501/https://books.google.dk/books?id=K-poAgAAQBAJ&pg |archive-date=2019-01-01 |url-status=dead }}\n* {{citation|last=de Crespigny|first=Rafe|title=A Biographical Dictionary of Later Han to the Three Kingdoms (23\u2013220 AD)|year=2007|publisher=Koninklijke Brill|location=Leiden|url=https://epdf.pub/a-biographical-dictionary-of-later-han-to-the-three-kingdoms-23-220-ad-handbook-.html|isbn=978-90-04-15605-0}} {{free access}}\n* {{cite journal |last1=Gregoratti|first1=Leonardo |authorlink=|editor-last = | editor-first = | title=Epigraphy of Later Parthia |journal= |date=2013|volume= |pages=276\u2013284|publisher=| location = |url=https://www.academia.edu/6253121/Epigraphy_of_Later_Parthia_Voprosy_Epigrafiki_Sbornik_statei_7_2013_pp._276-284|ref=harv}} {{free access}}\n* {{cite book |first=Leonardo |last=Gregoratti |editor1-last=Daryaee |editor1-first=Touraj |editor1-link= |title=King of the Seven Climes: A History of the Ancient Iranian World (3000 BCE - 651 CE) |date=2017 |publisher=UCI Jordan Center for Persian Studies |chapter=The Arsacid Empire|pages=1\u2013236|isbn=9780692864401|url=https://www.academia.edu/32702237/The_Arsacid_Empire_in_T._Daryaee_King_of_the_Seven_Climes_A_History_of_the_Ancient_Iran_World_3000_BC_651_CE_Jordan_Center_for_Persian_Studies_at_the_University_of_California_Irvine_2017_pp._125-153|ref=harv}} {{free access}}\n* {{cite journal |last=Hollis|first=A. S. |authorlink=Adrian Hollis |year=1994 |title=Statius' Young Parthian King ('Thebaid' 8.286-93) |journal=Greece & Rome|publisher=Cambridge University Press |volume=41 |issue= |pages=205\u2013212|doi= |ref=harv|jstor=643014 }} {{Registration required}}\n* {{cite encyclopedia |last1=Kia |first1=Mehrdad |title=The Persian Empire: A Historical Encyclopedia |date=2016 |publisher=[[ABC-CLIO]]|isbn=978-1610693912 |url=https://books.google.nl/books?id=B5BHDAAAQBAJ&printsec=frontcover&dq=The+Persian+Empire:+A+Historical+Encyclopedia.&hl=en&sa=X&ved=0ahUKEwj-3s-0667oAhVQxIsKHRLNAUAQ6AEIKDAA#v=snippet&q=The%20Arsacid%20monarch%20agreed%20to%20provide%20Terentius&f=false|ref=harv}} (2 volumes)\n* {{cite book|last=Marciak|first=Micha\u0142|title=Sophene, Gordyene, and Adiabene: Three Regna Minora of Northern Mesopotamia Between East and West|date=2017|publisher=BRILL|isbn=9789004350724|url=https://books.google.co.in/books?hl=da&id=hwEtDwAAQBAJ&q=pacorus#v=snippet&q=pacorus&f=false|ref=harv}}\n* {{citation|last=Morton|first=William S.|last2=Lewis|first2=Charlton M.|title=China: Its History and Culture|year=2005|publisher=McGraw-Hill|location=New York|url=https://epdf.pub/china-its-history-and-culture-4th-edition.html|isbn=978-0-07-141279-7}} {{free access}}\n* {{cite journal |last=Olbrycht|first=Marek Jan|authorlink=|editor-last =  | editor-first = | title=Parthian King's tiara - Numismatic evidence and some aspects of Arsacid political ideology|journal= |date=1997|volume=2 |pages=27\u201361|publisher=| location = |url=https://www.academia.edu/1493021/PARTHIAN_KING_S_TIARA_-_NUMISMATIC_EVIDENCE_AND_SOME_ASPECTS_OF_ARSACID_POLITICAL_IDEOLOGY|ref=harv}} {{free access}}\n* {{cite journal |last=Olbrycht|first=Marek Jan|authorlink=|editor-last =  | editor-first = | title= Vologases I and Pakoros II in Parthia |journal= |date=2013|volume=7 |pages=280\u2013286|publisher=| location = |url=https://www.academia.edu/19862460/Vologases_I_and_Pakoros_II_in_Parthia._Review_of_F._Sinisi_Sylloge_Nummorum_Parthicorum_Vol._7_Vologases_I_Pacorus_II_Wien_2012_in_ANABASIS_4_2013.|ref=harv}} {{free access}}\n* {{cite journal |last1=Olbrycht|first1=Marek Jan|authorlink=|editor-last = | editor-first = | title=The Sacral Kingship of the early Arsacids I. Fire Cult and Kingly Glory |journal= |date=2016a|volume=|pages=91\u2013106|publisher=| location = |url=https://www.academia.edu/33754166/THE_SACRAL_KINGSHIP_OF_THE_EARLY_ARSACIDS_I._FIRE_CULT_AND_KINGLY_GLORY|ref=harv}} {{free access}}\n* {{cite book |last1=Olbrycht |first1=Marek Jan|authorlink1=|editor-last1=Curtis|editor-first1=Vesta Sarkhosh|editor-last2=Pendleton|editor-first2=Elizabeth J.|editor-last3=Alram|editor-first3=Michael|editor-last4=Daryaee|editor-first4=Touraj|title=The Parthian and Early Sasanian Empires: Adaptation and Expansion |date=2016b |publisher=Oxbow Books |isbn=9781785702082 |chapter=Dynastic Connections in the Arsacid Empire and the Origins of the House of S\u0101s\u0101n|ref=harv}}\n* {{cite journal |last=Potter|first=D. S.|authorlink= |year=1991 |title=The Inscriptions on the Bronze Herakles from Mesene: Vologeses IV's War with Rome and the Date of Tacitus' \"Annales\" |journal=Zeitschrift f\u00fcr Papyrologie und Epigraphik |publisher= |volume=88 |issue= |pages=277\u2013290 |doi= |ref=harv|jstor=20187558 }} {{Registration required}}\n* {{cite book|last1=Rapp|first1=Stephen H.|title=The Sasanian World through Georgian Eyes: Caucasia and the Iranian Commonwealth in Late Antique Georgian Literature|date=2014|publisher=Ashgate Publishing, Ltd.|isbn=978-1472425522|url=https://books.google.nl/books?hl=nl&id=T8VIBQAAQBAJ&dq=rapp+inscription+kartir&q=Pacorus#v=snippet&q=Pacorus&f=false|ref=harv}}\n* {{cite book |last1=Rezakhani |first1=Khodadad |authorlink1=Khodadad Rezakhani|editor-last=Potts|editor-first=Daniel T.|title=The Oxford Handbook of Ancient Iran |date=2013 |publisher=Oxford University Press |isbn=978-0199733309 |chapter=Arsacid, Elymaean, and Persid Coinage|url=https://www.academia.edu/3983852/Arsacid_Elymaean_and_Persid_coinage|ref=harv}} {{free access}}\n* {{cite encyclopedia | article = Artabanus (Arsacid kings) | last = Schippmann | first = K. | authorlink =  | url = http://www.iranicaonline.org/articles/artabanus-parth#pt3 | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. II, Fasc. 6 | pages = 647\u2013650 | location = | publisher = | year = 1986 | isbn = |ref=harv}}\n* {{cite encyclopedia | title = Adiabene | last = Sellwood | first = D. | authorlink = | url = http://www.iranicaonline.org/articles/adiabene | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. I, Fasc. 5 | pages = 456\u2013459 | location = | publisher = | year = 1983 | isbn = |ref=harv}}\n* {{Cambridge History of Iran|volume=3a|last=Watson|first=William|authorlink=William Watson (sinologist)|chapter=Iran and China|pages=537\u2013558}}\n* {{citation|last=Wang|first=Tao|chapter=Parthia in China: a Re-examination of the Historical Records|pages=87\u2013104|title=The Age of the Parthians: The Ideas of Iran|volume=2|year=2007|publisher=I.B. Tauris & Co Ltd., in association with the London Middle East Institute at SOAS and the British Museum|location=London & New York|editor-last=Curtis, Vesta Sarkhosh and Sarah Stewart|url=https://epdf.pub/the-idea-of-iran-volume-ii-the-age-of-the-parthians.html|isbn=978-1-84511-406-0}} {{free access}}\n* {{citation|last=Wood|first=Frances|authorlink=Frances Wood|title=The Silk Road: Two Thousand Years in the Heart of Asia|year=2002|publisher=University of California Press|location=Berkeley and Los Angeles|isbn=978-0-520-24340-8|url-access=registration|url=https://archive.org/details/silkroadtwothous0000wood}}\n* {{citation|last=Y\u00fc|first=Ying-shih|authorlink=Yu Ying-shih|chapter=Han Foreign Relations|pages=377\u2013462|title=Cambridge History of China: the Ch'in and Han Empires, 221 B.C. \u2013 A.D. 220|volume=1|year=1986|publisher=Cambridge University Press|location=Cambridge|editor-last=Twitchett, Denis and Michael Loewe|editor-first=|url=https://books.google.co.uk/books?id=A2HKxK5N2sAC&printsec=frontcover#v=onepage&q&f=false|isbn=978-0-521-24327-8}}\n\n== Further reading ==\n* {{cite journal |last1=Curtis|first1=Vesta Sarkhosh|authorlink=|title=Religious iconography on ancient Iranian coins |journal=Journal of Late Antiquity |date=2007 |volume= |pages=413-434|publisher=| location = London|url=https://www.academia.edu/6186849/Religious_iconography_on_ancient_Iranian_coins|ref=harv}} {{free access}}\n* {{cite encyclopedia | title = Investiture | last = Rose | first = Jenny | authorlink = | url = http://www.iranicaonline.org/articles/investiture | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. XIII, Fasc. 2 | pages = 180\u2013188 | location = | publisher = | year = 2004 | isbn = |ref=harv}}\n\n{{commons cat|Pacorus II of Parthia}}\n{{s-start}}\n{{s-hou|[[Parthian Empire|Arsacid dynasty]]|||||}}\n{{s-bef|before=[[Vologases I of Parthia|Vologases I]] ({{small|predecessor}})<br>[[Vologases II of Parthia|Vologases II]] {{small|(rival king)}}<br>[[Artabanus III of Parthia|Artabanus III]] {{small|(rival king)}}}}\n{{s-ttl|title=King of the [[Parthian Empire]]|years=78\u2013110}}\n{{s-aft|after=[[Vologases III of Parthia|Vologases III]] {{small|(successor)}}<br>[[Osroes I of Parthia|Osroes I]] {{small|(rival king)}}}}\n{{s-end}}\n\n{{Parthian kings}}\n\n{{DEFAULTSORT:Pacorus 02 Of Parthia}}\n[[Category:110 deaths]]\n[[Category:Rulers of Media Atropatene]]\n[[Category:1st-century Parthian monarchs]]\n[[Category:2nd-century Parthian monarchs]]\n[[Category:1st-century births]]\n[[Category:1st-century Iranian people]]\n[[Category:2nd-century Iranian people]]\n", "text_old": "{{Infobox monarch\n| name = Pacorus II<br>{{lang|xpr|\ud802\udf50\ud802\udf4a\ud802\udf45\ud802\udf53}}\n| title = [[King of Kings]]\n| image = Coin of Pacorus II (cropped), Seleucia mint.jpg\n| image_size =\n| caption = [[Tetradrachm]] of Pacorus II wearing a [[tiara]], minted at [[Seleucia]] in 92/3\n| reign = 78 \u2013 110\n| succession = King of the [[Parthian Empire]]\n| predecessor = [[Vologases I of Parthia|Vologases I]] ({{small|predecessor}})<br>[[Vologases II of Parthia|Vologases II]] {{small|(rival king)}}<br>[[Artabanus III of Parthia|Artabanus III]] {{small|(rival king)}}\n| successor = [[Vologases III of Parthia|Vologases III]] {{small|(successor)}}<br>[[Osroes I of Parthia|Osroes I]] {{small|(rival king)}}\n| dynasty = [[Arsacid dynasty of Parthia|Arsacid dynasty]]\n| spouse =\n| father = [[Vologases I of Parthia|Vologases I]]\n| mother =\n| issue = [[Vologases III of Parthia|Vologases III]]<br>[[Axidares of Armenia|Axidares]]<br>[[Parthamasiris of Armenia|Parthamasiris]]<br>[[Meredates of Characene|Meredates]]\n| birth_date = {{circa|61/2}}\n| birth_place =\n| death_date = 110 (aged 48 or 49)\n| death_place =\n| place of burial =\n| religion = [[Zoroastrianism]]\n}}\n'''Pacorus II''' (also spelled '''Pakoros II'''; {{lang-xpr|\ud802\udf50\ud802\udf4a\ud802\udf45\ud802\udf53}}) was the [[King of Kings]] of the [[Parthian Empire]] from 78 to 110. He was the son and successor of [[Vologases I of Parthia|Vologases I]] ({{reign|51|78}}).\n\nDuring the latter part of his father's reign, Pacorus ruled the Parthian Empire along with him. After Vologases I's death in 78, Pacorus became the sole ruler, but was quickly met by a revolt by his brother [[Vologases II of Parthia|Vologases II]], which lasted until the latter's defeat in 80. In 79/80, Pacorus' rule was contended by another Parthian prince\u2014[[Artabanus III of Parthia|Artabanus III]], whom he had defeated by 81. A third Parthian contender, [[Osroes I]], appeared in 109. The following year, Pacorus was succeeded by his son [[Vologases III of Parthia|Vologases III]], who continued his father's struggle with Osroes I over the Parthian crown.\n\nLike his father, Pacorus continued the same policies of the prominent former Parthian king [[Artabanus II of Parthia|Artabanus II]] ({{reign|12|38/41}}), which included increasing the economic sources of the Parthian Empire by establishing a new trade system and strengthening relations with other powers, such as [[Han China]].\n\nUnder Pacorus, the usage of the image of the Greek goddess [[Tyche]] on the reverse of Parthian coins became more regular than that of the seated king with a [[Bow and arrow|bow]], specifically on the coin minted at [[Ecbatana]]. Tyche was either a representation of the Iranian goddesses [[Anahita]] or [[Ashi]].\n\n== Name ==\nThe name ''{{lang|la|Pacorus}}'' is the [[Latin]] form of the [[Greek language|Greek]] ''Pakoros'' ({{lang|grc|\u03a0\u03b1\u03ba\u03ce\u03c1\u03bf\u03c2}}), itself a variant of the [[Middle Iranian]] ''Pakur'', derived from [[Old Iranian]] ''bag-puhr'' (\"son of a god\").<ref>{{harvnb|Rapp|2014|p=334}}; {{harvnb|Marciak|2017|p=224}}</ref> The [[Armenian language|Armenian]] transliteration is ''Bakur'' (\u0532\u0561\u056f\u0578\u0582\u0580).{{sfn|Rapp|2014|p=334}}\n\nPacorus II's name is recorded in the bilingual inscription on the famous bronze statue of Heracles in Seleucia as [[Koine Greek|Greek]] ''Pakhorou'' ({{lang|grc|\u03a0\u03b1\u03c7\u03cc\u03c1\u03bf\u03c5}}, ''genitive'') and [[Parthian language|Parthian]] ''pkwr'' ({{lang|xpr|\ud802\udf50\ud802\udf4a\ud802\udf45\ud802\udf53}} 'Pakur').{{sfn|Potter|1991|pp=279, 281 (see also note 9)}}{{sfn|Gregoratti|2013|pp=280\u2013282}}\n\n== Background ==\nPacorus was one of the younger sons of the Parthian king [[Vologases I of Parthia|Vologases I]] ({{reign|51|78}}), being born in {{circa|61/2}}.<ref>{{harvnb|Gregoratti|2017|p=132}}; {{harvnb|Hollis|1994|pp=206\u2013208}} has surmised Pacorus to be the same newly ascended Parthian king described by the contemporary Roman poet [[Statius]] as a ''puer'', around sixteen or seventeen years old.</ref> Under Vologases I, the empire experienced a resurgence.{{sfn|Olbrycht|2016b|p=24}} During the last years of his reign, Pacorus ruled alongside him.{{sfn|D\u0105browa|2007|p=176}} After Vologases I's death in 78, Pacorus became the sole ruler of the empire.{{sfn|D\u0105browa|2007|p=176}}{{sfn|Gregoratti|2017|p=131}}\n\n== Reign ==\nPacorus was soon met by a revolt by his brother [[Vologases II of Parthia|Vologases II]], which lasted until the latter's defeat in 80.{{sfn|D\u0105browa|2012|p=391}}{{sfn|Chaumont|Schippmann|1988|pp=574\u2013580}} In 79/80, Pacorus' rule was contended by another Parthian prince\u2014[[Artabanus III of Parthia|Artabanus III]], who seemed to have little support in the empire, with the exception of [[Babylonia]].{{sfn|Schippmann|1986|pp=647\u2013650}} Artabanus III's most notable action was to give refuge to a [[Pseudo-Nero]] named [[Terentius Maximus]].{{sfn|Kia|2016|p=179}}{{sfn|Schippmann|1986|pp=647\u2013650}} Artabanus III initially agreed to lend military aid to Terentius Maximus to capture [[Rome]], until he found about the real identity of the impostor.{{sfn|Kia|2016|p=179}} Coin mints of Artabanus III disappear after 81, which suggests that by this year Pacorus had defeated him.{{sfn|Schippmann|1986|pp=647\u2013650}} \n\nLike his father, Pacorus sought to accomplish the goal of [[Artabanus II of Parthia|Artabanus II]] ({{reign|12|38/41}}), by attempting to establish a long and structured trade-route that spanned through [[East Asia]], [[India]] and the coast of the [[Mediterranean Sea]].{{sfn|Gregoratti|2017|p=131}} This planned long trade-route would greatly improve the economy of the Parthian Empire.{{sfn|Gregoratti|2017|p=131}} In order to accomplish this, Pacorus strengthened relations with other powers whom he was able to establish long distance trade with, most notably [[Han China]].{{sfn|Gregoratti|2017|pp=131\u2013132}} In 97, the Chinese general [[Ban Chao]], the [[Protectorate of the Western Regions|Protector-General of the Western Regions]], sent his emissary [[Gan Ying]] on a diplomatic mission to reach the [[Roman Empire]]. Gan visited the court of Pacorus at [[Hecatompylos]] before departing towards Rome.<ref name=\"watson_1983_543-544\">{{harvnb|Watson|1983|pp=543\u2013544}}</ref> He traveled as far west as the [[Persian Gulf]], where the Parthian authorities convinced him that an arduous sea voyage around the [[Arabian Peninsula]] was the only means to reach Rome.<ref>{{harvnb|Watson|1983|pp=543\u2013544}}; {{harvnb|Y\u00fc|1986|pp=460\u2013461}}; {{harvnb|de Crespigny|2007|pp=239\u2013240}}; see also {{harvnb|Wang|2007|p=101}}</ref> Discouraged by this, Gan Ying returned to the Han court and provided [[Emperor He of Han]] ({{reign|88|105}}) with a detailed report on the Roman Empire based on oral accounts of his Parthian hosts.<ref>{{harvnb|Wood|2002|pp=46\u201347}}; {{harvnb|Morton|Lewis|2005|p=59}}</ref> The modern historian William Watson speculated that the Parthians would have been relieved at the failed efforts by the Han Empire to open diplomatic relations with Rome, especially after Ban Chao's [[Han\u2013Xiongnu War|military victories]] against the [[Xiongnu]] in [[Tarim Basin|eastern Central Asia]].<ref name=\"watson_1983_543-544\"/>\n\nParthian interest also continued to grow in eastern lands of [[Khwarazm]], [[Bactria]], and the [[Hindu Kush]].{{sfn|D\u0105browa|2007|p=176}} The influence of the Parthian Empire is demonstrated by the existence of Parthian aspects in the coinage of numerous political entities in those areas.{{sfn|D\u0105browa|2007|p=175}} During his last years of rule, Pacorus co-ruled with his son [[Vologases III of Parthia|Vologases III]].{{sfn|D\u0105browa|2012|p=176}} In 109, a third Parthian contender named [[Osroes I]] appeared.{{sfn|D\u0105browa|2012|pp=176, 391}} In 110, Pacorus sold the Arsacid vassal kingdom of [[Osroene]] to [[Abgar VII]].{{sfn|Sellwood|1983|pp=456\u2013459}} Pacorus died in the same year, and was succeeded by Vologases III, who continued his father's struggle with Osroes I over the Arsacid crown.{{sfn|D\u0105browa|2012|p=176}}\n\n== Coinage ==\n[[Image:PacorusIIAnotherParthianCoinHistoryofIran.jpg|thumb|[[Tetradrachm]] of a young Pacorus II wearing a [[diadem]]]]\nOn the observe of his coins, Pacorus is portrayed simply wearing a [[diadem]].{{sfn|Olbrycht|1997|p=50}} At first, he appeared beardless on his coins, a rare feature in [[Parthian coinage]] that demonstrated his youth, having ascended the throne around the age of sixteen or seventeen.{{sfn|Hollis|1994|pp=206\u2013207}} From 82/3, he is depicted with a beard.{{sfn|Hollis|1994|p=208}} From 93\u201396, Pacorus is portrayed with his father's [[tiara]].{{sfn|Olbrycht|1997|p=50}} The modern historian Marek Jan Olbrycht surmises that the wearing of the tiara in the latter part of his reign was reflected the power and status of his empire at this time.{{sfn|Olbrycht|1997|p=50}} \n\nThe reverse of his coins portrayed the Greek goddess [[Tyche]] investing him as king.{{sfn|Rezakhani|2013|p=771}} Under Pacorus, the usage of the image of Tyche on the reverse of Parthian coins became more regular than that of the seated king with a [[Bow and arrow|bow]], specifically on the coin minted at [[Ecbatana]].{{sfn|Rezakhani|2013|p=771}} This lasted until the reign of his son and successor, Vologases III.{{sfn|Rezakhani|2013|p=771}} In the Parthian era, Iranians used Hellenistic iconography to portray their divine figures,{{sfn|Curtis|2012|pp=76\u201377}}{{sfn|Boyce|1984|p=82}} thus the investiture scene can be associated with the [[Avesta]]n ''[[khvarenah]]'', i.e., kingly glory, with Tyche representing one of the Iranian goddesses [[Anahita]] or [[Ashi]].<ref>{{harnvb|Curtis|2012|p=71}}; {{harnvb|Olbrycht|2016a|p=99}}; {{harnvb|Curtis|2016|p=183}}</ref>\n\n== Offspring ==\nBesides Vologases III, Pacorus had three other sons: [[Axidares of Armenia|Axidares]], and [[Parthamasiris of Armenia|Parthamasiris]], who successively served as [[List of Armenian kings|kings of Armenia]],{{sfn|Chaumont|1986|pp=418\u2013438}} and [[Meredates of Characene|Meredates]], who served as king of [[Characene]] in the mid-2nd century.{{sfn|Olbrycht|1997|p=51}}\n\n==References==\n{{reflist|2}}\n\n==Sources==\n* {{cite book | title = Zoroastrians: Their Religious Beliefs and Practices | year = 1984 | publisher = Psychology Press | location = | editor-last = | editor-first = | last = Boyce | first = Mary | authorlink = Mary Boyce | chapter = | pages = 1\u2013252 | isbn = 9780415239028 | url = https://archive.org/details/zoroastriansthei00boyc_0/page/82/mode/2up |ref=harv}} {{free access}}\n* {{cite encyclopedia | title = Armenia and Iran ii. The pre-Islamic period | last = Chaumont | first = M. L. | authorlink = | url = http://www.iranicaonline.org/articles/armenia-ii | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. II, Fasc. 4 | pages = 418\u2013438 | location = | publisher = | year = 1986 | isbn = |ref=harv}}\n* {{cite encyclopedia | article = Bal\u0101\u0161 | last1 = Chaumont | first1 = M. L. | first2 = K. | last2 = Schippmann | authorlink =  | url = http://www.iranicaonline.org/articles/balas-proper-name##1 | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. III, Fasc. 6 | pages = 574\u2013580 | location = | publisher = | year = 1988 | isbn = |ref=harv}}\n* {{cite book |last=Curtis|first=Vesta Sarkhosh|authorlink=|editor-last =  | editor-first = | chapter=Parthian coins: Kingship and Divine Glory|title=The Parthian Empire and its Religions |isbn= 9783940598134|date=2012|volume= |pages=67\u201383|publisher=| location = |url=https://www.academia.edu/6186948/Parthian_coins_kingship_and_divine_glory|ref=harv}} {{free access}}\n* {{cite book|url=https://www.academia.edu/23810856/Ancient_Iranian_Motifs_and_Zoroastrian_Iconography|last=Curtis|first=Vesta Sarkhosh|editor1-first=Markus|editor1-last=Williams|editor2-first=Sarah|editor2-last=Stewart|editor3-first=Almut|editor3-last=Hintze|year=2016|chapter=Ancient Iranian Motifs and Zoroastrian Iconography|title=The Zoroastrian Flame Exploring Religion, History and Tradition|publisher=I.B. Tauris|series=|volume=|isbn=9780857728159|pages=179\u2013203|ref=harv}}\n* {{cite journal |last1=D\u0105browa|first1=Edward|authorlink=|editor-last =  | editor-first = | title=The Parthian Kingship |journal= |date=2007|volume= |pages=123-134|publisher=| location = |url=https://www.academia.edu/1501481/The_Parthian_Kingship|ref=harv}} {{free access}}\n* {{cite book |first1=Edward |last1=D\u0105browa |editor1-last=Daryaee |editor1-first=Touraj |editor1-link=Touraj Daryaee |title=The Oxford Handbook of Iranian History |date=2012 |publisher=Oxford University Press |chapter=The Arsacid Empire |pages=1\u2013432 |isbn=0-19-987575-8 |url=https://books.google.dk/books?hl=da&id=K-poAgAAQBAJ&dq=The+Oxford+Handbook+of+Iranian+History&q=pacorus#v=snippet&q=pacorus&f=false |ref=harv |access-date=2019-01-13 |archive-url=https://web.archive.org/web/20190101051501/https://books.google.dk/books?id=K-poAgAAQBAJ&pg |archive-date=2019-01-01 |url-status=dead }}\n* {{citation|last=de Crespigny|first=Rafe|title=A Biographical Dictionary of Later Han to the Three Kingdoms (23\u2013220 AD)|year=2007|publisher=Koninklijke Brill|location=Leiden|url=https://epdf.pub/a-biographical-dictionary-of-later-han-to-the-three-kingdoms-23-220-ad-handbook-.html|isbn=978-90-04-15605-0}} {{free access}}\n* {{cite journal |last1=Gregoratti|first1=Leonardo |authorlink=|editor-last = | editor-first = | title=Epigraphy of Later Parthia |journal= |date=2013|volume= |pages=276\u2013284|publisher=| location = |url=https://www.academia.edu/6253121/Epigraphy_of_Later_Parthia_Voprosy_Epigrafiki_Sbornik_statei_7_2013_pp._276-284|ref=harv}} {{free access}}\n* {{cite book |first=Leonardo |last=Gregoratti |editor1-last=Daryaee |editor1-first=Touraj |editor1-link= |title=King of the Seven Climes: A History of the Ancient Iranian World (3000 BCE - 651 CE) |date=2017 |publisher=UCI Jordan Center for Persian Studies |chapter=The Arsacid Empire|pages=1\u2013236|isbn=9780692864401|url=https://www.academia.edu/32702237/The_Arsacid_Empire_in_T._Daryaee_King_of_the_Seven_Climes_A_History_of_the_Ancient_Iran_World_3000_BC_651_CE_Jordan_Center_for_Persian_Studies_at_the_University_of_California_Irvine_2017_pp._125-153|ref=harv}} {{free access}}\n* {{cite journal |last=Hollis|first=A. S. |authorlink=Adrian Hollis |year=1994 |title=Statius' Young Parthian King ('Thebaid' 8.286-93) |journal=Greece & Rome|publisher=Cambridge University Press |volume=41 |issue= |pages=205\u2013212|doi= |ref=harv|jstor=643014 }} {{Registration required}}\n* {{cite encyclopedia |last1=Kia |first1=Mehrdad |title=The Persian Empire: A Historical Encyclopedia |date=2016 |publisher=[[ABC-CLIO]]|isbn=978-1610693912 |url=https://books.google.nl/books?id=B5BHDAAAQBAJ&printsec=frontcover&dq=The+Persian+Empire:+A+Historical+Encyclopedia.&hl=en&sa=X&ved=0ahUKEwj-3s-0667oAhVQxIsKHRLNAUAQ6AEIKDAA#v=snippet&q=The%20Arsacid%20monarch%20agreed%20to%20provide%20Terentius&f=false|ref=harv}} (2 volumes)\n* {{cite book|last=Marciak|first=Micha\u0142|title=Sophene, Gordyene, and Adiabene: Three Regna Minora of Northern Mesopotamia Between East and West|date=2017|publisher=BRILL|isbn=9789004350724|url=https://books.google.co.in/books?hl=da&id=hwEtDwAAQBAJ&q=pacorus#v=snippet&q=pacorus&f=false|ref=harv}}\n* {{citation|last=Morton|first=William S.|last2=Lewis|first2=Charlton M.|title=China: Its History and Culture|year=2005|publisher=McGraw-Hill|location=New York|url=https://epdf.pub/china-its-history-and-culture-4th-edition.html|isbn=978-0-07-141279-7}} {{free access}}\n* {{cite journal |last=Olbrycht|first=Marek Jan|authorlink=|editor-last =  | editor-first = | title=Parthian King's tiara - Numismatic evidence and some aspects of Arsacid political ideology|journal= |date=1997|volume=2 |pages=27\u201361|publisher=| location = |url=https://www.academia.edu/1493021/PARTHIAN_KING_S_TIARA_-_NUMISMATIC_EVIDENCE_AND_SOME_ASPECTS_OF_ARSACID_POLITICAL_IDEOLOGY|ref=harv}} {{free access}}\n* {{cite journal |last=Olbrycht|first=Marek Jan|authorlink=|editor-last =  | editor-first = | title= Vologases I and Pakoros II in Parthia |journal= |date=2013|volume=7 |pages=280\u2013286|publisher=| location = |url=https://www.academia.edu/19862460/Vologases_I_and_Pakoros_II_in_Parthia._Review_of_F._Sinisi_Sylloge_Nummorum_Parthicorum_Vol._7_Vologases_I_Pacorus_II_Wien_2012_in_ANABASIS_4_2013.|ref=harv}} {{free access}}\n* {{cite journal |last1=Olbrycht|first1=Marek Jan|authorlink=|editor-last = | editor-first = | title=The Sacral Kingship of the early Arsacids I. Fire Cult and Kingly Glory |journal= |date=2016a|volume=|pages=91\u2013106|publisher=| location = |url=https://www.academia.edu/33754166/THE_SACRAL_KINGSHIP_OF_THE_EARLY_ARSACIDS_I._FIRE_CULT_AND_KINGLY_GLORY|ref=harv}} {{free access}}\n* {{cite book |last1=Olbrycht |first1=Marek Jan|authorlink1=|editor-last1=Curtis|editor-first1=Vesta Sarkhosh|editor-last2=Pendleton|editor-first2=Elizabeth J.|editor-last3=Alram|editor-first3=Michael|editor-last4=Daryaee|editor-first4=Touraj|title=The Parthian and Early Sasanian Empires: Adaptation and Expansion |date=2016b |publisher=Oxbow Books |isbn=9781785702082 |chapter=Dynastic Connections in the Arsacid Empire and the Origins of the House of S\u0101s\u0101n|ref=harv}}\n* {{cite journal |last=Potter|first=D. S.|authorlink= |year=1991 |title=The Inscriptions on the Bronze Herakles from Mesene: Vologeses IV's War with Rome and the Date of Tacitus' \"Annales\" |journal=Zeitschrift f\u00fcr Papyrologie und Epigraphik |publisher= |volume=88 |issue= |pages=277\u2013290 |doi= |ref=harv|jstor=20187558 }} {{Registration required}}\n* {{cite book|last1=Rapp|first1=Stephen H.|title=The Sasanian World through Georgian Eyes: Caucasia and the Iranian Commonwealth in Late Antique Georgian Literature|date=2014|publisher=Ashgate Publishing, Ltd.|isbn=978-1472425522|url=https://books.google.nl/books?hl=nl&id=T8VIBQAAQBAJ&dq=rapp+inscription+kartir&q=Pacorus#v=snippet&q=Pacorus&f=false|ref=harv}}\n* {{cite book |last1=Rezakhani |first1=Khodadad |authorlink1=Khodadad Rezakhani|editor-last=Potts|editor-first=Daniel T.|title=The Oxford Handbook of Ancient Iran |date=2013 |publisher=Oxford University Press |isbn=978-0199733309 |chapter=Arsacid, Elymaean, and Persid Coinage|url=https://www.academia.edu/3983852/Arsacid_Elymaean_and_Persid_coinage|ref=harv}} {{free access}}\n* {{cite encyclopedia | article = Artabanus (Arsacid kings) | last = Schippmann | first = K. | authorlink =  | url = http://www.iranicaonline.org/articles/artabanus-parth#pt3 | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. II, Fasc. 6 | pages = 647\u2013650 | location = | publisher = | year = 1986 | isbn = |ref=harv}}\n* {{cite encyclopedia | title = Adiabene | last = Sellwood | first = D. | authorlink = | url = http://www.iranicaonline.org/articles/adiabene | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. I, Fasc. 5 | pages = 456\u2013459 | location = | publisher = | year = 1983 | isbn = |ref=harv}}\n* {{Cambridge History of Iran|volume=3a|last=Watson|first=William|authorlink=William Watson (sinologist)|chapter=Iran and China|pages=537\u2013558}}\n* {{citation|last=Wang|first=Tao|chapter=Parthia in China: a Re-examination of the Historical Records|pages=87\u2013104|title=The Age of the Parthians: The Ideas of Iran|volume=2|year=2007|publisher=I.B. Tauris & Co Ltd., in association with the London Middle East Institute at SOAS and the British Museum|location=London & New York|editor-last=Curtis, Vesta Sarkhosh and Sarah Stewart|url=https://epdf.pub/the-idea-of-iran-volume-ii-the-age-of-the-parthians.html|isbn=978-1-84511-406-0}} {{free access}}\n* {{citation|last=Wood|first=Frances|authorlink=Frances Wood|title=The Silk Road: Two Thousand Years in the Heart of Asia|year=2002|publisher=University of California Press|location=Berkeley and Los Angeles|isbn=978-0-520-24340-8|url-access=registration|url=https://archive.org/details/silkroadtwothous0000wood}}\n* {{citation|last=Y\u00fc|first=Ying-shih|authorlink=Yu Ying-shih|chapter=Han Foreign Relations|pages=377\u2013462|title=Cambridge History of China: the Ch'in and Han Empires, 221 B.C. \u2013 A.D. 220|volume=1|year=1986|publisher=Cambridge University Press|location=Cambridge|editor-last=Twitchett, Denis and Michael Loewe|editor-first=|url=https://books.google.co.uk/books?id=A2HKxK5N2sAC&printsec=frontcover#v=onepage&q&f=false|isbn=978-0-521-24327-8}}\n\n== Further reading ==\n* {{cite journal |last1=Curtis|first1=Vesta Sarkhosh|authorlink=|title=Religious iconography on ancient Iranian coins |journal=Journal of Late Antiquity |date=2007 |volume= |pages=413-434|publisher=| location = London|url=https://www.academia.edu/6186849/Religious_iconography_on_ancient_Iranian_coins|ref=harv}} {{free access}}\n* {{cite encyclopedia | title = Investiture | last = Rose | first = Jenny | authorlink = | url = http://www.iranicaonline.org/articles/investiture | editor-last = | editor-first =  | editor-link = | encyclopedia  = Encyclopaedia Iranica, Vol. XIII, Fasc. 2 | pages = 180\u2013188 | location = | publisher = | year = 2004 | isbn = |ref=harv}}\n\n{{commons cat|Pacorus II of Parthia}}\n{{s-start}}\n{{s-hou|[[Parthian Empire|Arsacid dynasty]]|||||}}\n{{s-bef|before=[[Vologases I of Parthia|Vologases I]] ({{small|predecessor}})<br>[[Vologases II of Parthia|Vologases II]] {{small|(rival king)}}<br>[[Artabanus III of Parthia|Artabanus III]] {{small|(rival king)}}}}\n{{s-ttl|title=King of the [[Parthian Empire]]|years=78\u2013110}}\n{{s-aft|after=[[Vologases III of Parthia|Vologases III]] {{small|(successor)}}<br>[[Osroes I of Parthia|Osroes I]] {{small|(rival king)}}}}\n{{s-end}}\n\n{{Parthian kings}}\n\n{{DEFAULTSORT:Pacorus 02 Of Parthia}}\n[[Category:110 deaths]]\n[[Category:Rulers of Media Atropatene]]\n[[Category:1st-century Parthian monarchs]]\n[[Category:2nd-century Parthian monarchs]]\n[[Category:1st-century births]]\n[[Category:1st-century Iranian people]]\n[[Category:2nd-century Iranian people]]\n", "name_user": "HistoryofIran", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Pacorus_II"}
{"title_page": "Connie the Cow", "text_new": "{{Short description|Spanish children's television series created by Josep Viciana}}\n{{Infobox television\n| show_name                = Connie the Cow\n| image                    = Noggin Connie the Cow Logo Original.jpg\n| image_size               = 300px\n| caption                  = Title card\n| genre                    = [[Children's television series]]\n| creator                  = Josep Viciana\n| writer                   = \n| director                 = \n| creative_director        = \n| developer                = \n| presenter                = \n| starring                 = \n| voices                   = \n| narrated                 = \n| theme_music_composer     = [[Josep Roig Boada]]\n| opentheme                = \"Connie the Cow\"\n| endtheme                 = \"Connie the Cow\"\n| composer                 = Josep Roig\n| country                  = Spain\n| language                 = Spanish\n| num_seasons              = 3\n| num_episodes             = 44 (128 segments)\n| list_episodes            = \n| executive_producer       = Cristina Brandner\n| co_exec                  = \n| producer                 = \n| supervising_producer     = \n| asst_producer            = \n| co-producer              = \n| editor                   = \n| story_editor             = \n| location                 = \n| cinematography           = \n| camera                   = \n| runtime                  = 21 minutes\n| network                  = \n| first_run                = \n| first_aired              = {{start date|2000|9|8|df=y}}<ref name= \"A Brand Moo Series\"/>\n| last_aired               = {{end date|2005|df=y}}\n| preceded_by              = \n| followed_by              = \n| company                  = {{Plainlist|\n*Neptuno Films\n}}\n| related                  = ''[[Strokkers]]''\n| website                  = http://www.neptunofilms.com/english/series/connie-the-cow/\n| distributor              = {{Plainlist|\n*TV-Loonland AG\n*[[Alliance Atlantis]]<ref>{{cite web|url=http://www.prnewswire.com/news-releases/moove-over-preschoolers---connie-the-cow-is-back-71957647.html|title=Moove Over Preschoolers \u2013 'Connie the Cow' Is Back!|author=Nickelodeon|date=9 September 2004|work=prnewswire.com|accessdate=July 26, 2015}}</ref>\n}}\n}}\n'''''Connie the Cow''''' is a Spanish [[children's television series]] created by Josep Viciana, and designed by Roman Rybakiewicz.<ref>http://www.romanart.es/peliculas-a.htm</ref> In the United States, the series aired on [[Noggin (brand)|Noggin]] from 8 September 2003<ref name= \"A Brand Moo Series\">{{cite web|url=http://www.prnewswire.com/news-releases/a-brand-moo-series-comes-to-noggin-this-fall-70970962.html|title=A Brand Moo Series Comes to Noggin This Fall|author=NOGGIN|date=20 August 2003|work=prnewswire.com|accessdate=July 26, 2015}}</ref> to 2005.\n\n==Plot==\nA curious young cow named Connie explores her colorful world.\n\n==Characters==\n* Connie is a young female [[cow]] and the protagonist of the series. She is very curious, and always tries new things. The character is played by Andrea Vega Guzman (Season 1) and Ayesha Mendham (Seasons 2 and 3).<ref name= \"Connie voices\">https://www.youtube.com/watch?v=kaVjLWm8qIo</ref>\n* Patch is a playful [[dog]] who lives near Connie's [[farmhouse]]. He is introduced in the episode \"Patch the Stray Dog\". The character is played by show narrator Alex Warner.<ref name=\"Connie voices\" />\n* Wally is a plump multicolored [[bird]] who lives in a [[tree]] near the [[farm]].\n* Grouch is an aptly-named irritable [[fox]].\n* Mollie and Bill are Connie's mother and father. They give Connie advice, and help out with problems.\n* Hedgy is a [[hedgehog]] who wears a [[sock]] over his [[nose]]. \n* Maddie is a [[Sheep|lamb]] with orange [[wool]].\n* Paddy and Pearl are [[pigs]] and Connie's neighbors.\n* Dodger is a naughty and crafty [[cat]] who likes to steal [[cakes]] and tease other animals. He is the main antagonist of the series. He first appeared in the episode \"The Crafty Cat\".\n* Snooze is a lazy and tired [[turtle]] who likes to sleep inside his [[Turtle shell|shell]].\n\n==Episodes==\n===Season 1===\n# \"A Curious [[Butterfly]]\": Connie chases a butterfly away from home.\n# \"Present for Mummy\": Connie finds her mother the perfect birthday present.\n# \"The Christmas Tree\": Connie needs help finding the special Christmas tree her mom told her about.\n# \"The Crafty Cat\": Connie tries to imitate a sly and crafty cat but she learns to be honest at the same time.\n# \"The Lazy Clouds\": Connie helps make the clouds cry for the sheep so it can rain.\n# \"Patch the Stray Dog\": Connie meets an unlike friendly dog named Patch and helps him find a family.\n# \"The Ugly [[Caterpillar]]\": Connie learns that caterpillars can be ugly or beautiful, and they transform into butterflies.\n# \"Connie and Her Grandmother\": Connie's grandma takes Connie on a nature walk to show how to love and respect nature.\n# \"The Grumpy Fox\": Connie and her new friend Patch help Grouch the fox cheer up.\n# \"Wally Bird\": Connie goes on a picnic with Grouch and Patch but it ends up on an unexpected adventure because of one very silly bird.\n# \"Hide and Seek\": Connie wants to play hide and seek with Grouch and Wally, but she does not know how to count.\n# \"The Little [[Bear]]\": Connie helps an orange bear gets back to its family.\n# \"A Cold Weather Adventure\": To warm Connie while exploring the cold weather, Maddie the lamb rides on Connie's back.\n# \"The Snow Ghost\": Connie, Grouch and Patch mistaken a [[Mole (animal)|mole]] for a snow ghost because of his hard digging.\n# \"The Snowman\"\n# \"The [[Wolf]]\": Connie meets a wolf that helps Maddie the lamb get back to her parents.\n# \"The Busy [[Squirrel]]\": Connie helps a squirrel jump.\n# \"The Magic Spring\": Drinking one of the [[beaver]] dam's water, a spring gets Connie into a cow.\n# \"The Lonely [[Flower]]\": Connie and Patch help a flower get happy for its special event.\n# \"The Bird Who Didn't Know How To Fly\"\n# \"A Visit to Grandmother's\": Connie gets an invitation to visit her grandma's shed in the woods.\n# \"A Sock for Hedgy\": Hedgy loses the sock that goes on his nose and is upset and shivering in the cold weather, so Connie and Patch must help him find his sock.\n# \"The Surprise Party\": Connie, Grouch and Patch prepare a surprise birthday party for Wally.\n# \"Wally's Nest\": Wally loses his nest and Connie and Patch help him find it.\n# \"A Toy for Patch\": When the [[ball]] that Connie and Patch play with is squashed, Connie helps find a new toy for Patch.\n# \"The Search for Patch's Color\": Connie helps Patch reunite with his original color; after that, Connie becomes white, then reunites with her original color.\n# \"Connie Wants to Be Different\"\n# \"Patch's Wonderful Nose\": Patch helps his nose get his sense of smell back.\n# \"Connie and the Colors\": Grandma Clara teaches the friends a game about colors.\n# \"Connie and the Little Lamb\"\n# \"Connie and the [[Turtle]]\": Connie and Patch help a lost turtle return to his family.\n# \"A Hot Day\": With the help of her friend the beaver, Connie creates a new swimming pool just for her insect friends.\n# \"The Travelling Tree\"\n# \"The Five Senses\"\n# \"Connie and the [[Stork]]\"\n# \"The Race\": Snooze and Dodger have a race and Dodger is determined to win.\n# \"Connie and the Butterflies\"\n# \"Mummy's [[Hat]]\": Connie mistakes a lost hat as her mom's.\n# \"Adventures on the River\"\n# \"What a Lot of Babies\": Connie and Patch have fun with birds.\n# \"Connie and the [[Cricket (insect)|Cricket]]\": Connie tries to imitate an amazing cricket.\n# \"Detective Connie\": Connie and Patch solve a mystery that involves a path that leads somewhere strange.\n# \"A Trip with Mum and Dad\": Connie and her parents go out on a trip around the forest.\n# \"Connie and Her Friends\": Connie learns that friends can have other friends, when she sees Patch playing with Cyril the wolf.\n# \"The Trial of Strength\"\n# \"Connie Learns About Shapes\": Connie, Patch, Grouch, and Wally play a game about shapes with the lead by Mole.\n# \"The Vain Bird\": Connie meets a vain musical bird and a [[spider]] named Spike that have something in common, they both have the same beautiful singing voices, so the friends put on a singing competition during the night.\n# \"Follow the Clues\": Grandma Clara teaches Connie and Patch a follow the clues game that would lead to some animals very special.\n# \"Connie and Patch in Disguise\": Connie's parents watch Connie and Patch playing in disguise.\n# \"The Lucky Stone\"\n# \"The [[Bridge]]\": Connie and her friends create a bridge across the river with the help of Connie's father.\n# \"The Birthday Present\"\n\n===Season 2===\n# \"The Beaver's Dam\": Connie and Patch cannot get water from the beaver's dam.\n# \"The [[Ant]] and the [[Grasshopper]]\": Connie's friend Sergeant tells all of her friends the story of the ant and the grasshopper.\n# \"Connie and the Tin Can\"\n# \"Connie and the Fruit\"\n# \"The Talking Mountain\"\n# \"The Secret Den\"\n# \"Burt Wants to Play\": Connie and Patch help Burt the [[bat]] have fun.\n# \"Honey for Mummy\": Connie searches for honey to make a cake.\n# \"It's Going to Rain\"\n# \"Lost in the Snow\"\n# \"Connie and the Riddle\"\n# \"The Sad Tortoise\"\n# \"The Mysterious Wood\"\n# \"Grandma Makes Jam\"\n# \"The Wonderful World of Nature\"\n# \"The Cat and the Puppy\"\n# \"Where's the Food?\"\n# \"Animals and Their Jobs\"\n# \"Going South for the Winter\"\n# \"Wally's Hat\"\n# \"Connie Hunts for Treasure\"\n# \"The Creature that Could Change Colour\"\n# \"Where's Spike?\"\n# \"Baby Bird and His Nest\"\n# \"The Multi-Colored Snow\"\n# \"The Ant Who Couldn't Concentrate\"\n# \"The Sleepy Bird\": During a long night, Connie and Patch follow a sleep-walking Wally.\n# \"Spot the Difference\"\n# \"The Little Piglet\"\n# \"Connie's Breakfast\"\n# \"The Mysterious Plant\"\n# \"The Singing [[Frog]]\"\n# \"Looking for Presents\"\n# \"Connie and the Insects\"\n# \"The Valley of the Butterflies\"\n# \"The Strongest Animal in the Woods\"\n# \"Patch Wants to Fly\": Connie helps Patch fly.\n# \"Making Colors\": Grandma Clara teaches Connie and Patch how to make new colors.\n# \"The Day it Snowed in Springtime\"\n# \"Who Do I Look Like?\"\n# \"Playing with Stones\"\n# \"The Adventurous Ant\"\n# \"Connie's Orchestra\": Connie and her friends create an orchestra using different types of instruments.\n# \"A Different Breakfast\"\n# \"Lots and Lots of Walnuts\"\n# \"Where Does it Belong?\"\n# \"Looking for Grandma\": Connie sets out on a quest for her grandma.\n# \"Connie and the Apple Tree\"\n# \"Cactus Flower\"\n# \"Songs in the Snow\": After Connie listens to her mother singing a beautiful Christmas song, Connie and Patch ask their friends if they know any songs, but it turns out they only know the same one Connie's mother sang.\n# \"The Vain Butterfly\"\n\n===Season 3===\n# \"The Big Surprise\"\n# \"The Selfish Butterflies\"\n# \"Looking For The Color Yellow\"\n# \"All Shapes and Sizes\"\n# \"The Sounds of Nature\"\n# \"Snow in the Woods\"\n# \"The Animal That Had One Leg\"\n# \"The Puzzle\"\n# \"The Strange Trail\"\n# \"The Yellow Ball\"\n# \"Connie Helps the Flowers\"\n# \"Daddy's Party\"\n# \"Good Friends\"\n# \"Happy Times\"\n# \"The Fruit Festival\"\n# \"Connie and the Rodents\"\n# \"Look Carefully\"\n# \"A [[Ferret]] Called Willow\"\n# \"Nature Day\"\n# \"A Time for Everything\"\n# \"The Story of the Two Friends\": Connie makes up a story about the good times she had with Patch.\n# \"Dear Mummy and Daddy\": Connie writes a poem for her parents.\n# \"The Basket of Blackberries\": Connie and Patch search for a missing basket of blackberries.\n# \"The Little [[Fish]]\"\n# \"Hedgy's Spines\"\n# \"Mummy Tells a Story\": When Connie babysits Maddie the lamb for the night, Connie's mother tells them a story about a group of caterpillars.\n\n==Broadcast==\nIn April 2001, TV-Loonland AG announced they had pre-sold the series in the United Kingdom to Disney Television International to air on their [[Disney Junior (UK and Ireland)|Playhouse Disney]] channel in the country<ref>https://www.awn.com/news/tv-loonland-sells-connie-disney</ref>. The company would later extend the Disney deal to Germany in November 2004<ref>https://www.c21media.net/connie-the-cow-goes-to-germany/</ref>.\n\nThe series was rerun in the United States from 2017 to 2018 on [[Starz]] Kids & Family.<ref>https://www.starz.com/series/27438/episodes</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title|tt0377160}}\n{{Noggin shows}}\n\n[[Category:2000s Spanish television series]]\n[[Category:2000 Spanish television series debuts]]\n[[Category:2007 Spanish television series endings]]\n[[Category:Noggin (brand) original programming]]\n[[Category:Preschool education television series]]\n[[Category:Spanish animated television series]]\n[[Category:Spanish children's television series]]\n[[Category:Television series by Alliance Atlantis]]\n[[Category:Television series about cows]]\n[[Category:Discovery Kids original programming]]\n", "text_old": "{{Short description|Spanish children's television series created by Josep Viciana}}\n{{Infobox television\n| show_name                = Connie the Cow\n| image                    = Noggin Connie the Cow Logo Original.jpg\n| image_size               = 300px\n| caption                  = Title card\n| genre                    = [[Children's television series]]\n| creator                  = Josep Viciana\n| writer                   = \n| director                 = \n| creative_director        = \n| developer                = \n| presenter                = \n| starring                 = \n| voices                   = \n| narrated                 = \n| theme_music_composer     = [[Josep Roig Boada]]\n| opentheme                = \"Connie the Cow\"\n| endtheme                 = \"Connie the Cow\"\n| composer                 = Josep Roig\n| country                  = Spain\n| language                 = Spanish\n| num_seasons              = 3\n| num_episodes             = 44 (128 segments)\n| list_episodes            = \n| executive_producer       = Cristina Brandner\n| co_exec                  = \n| producer                 = \n| supervising_producer     = \n| asst_producer            = \n| co-producer              = \n| editor                   = \n| story_editor             = \n| location                 = \n| cinematography           = \n| camera                   = \n| runtime                  = 21 minutes\n| network                  = \n| first_run                = \n| first_aired              = {{start date|2003|9|8|df=y}}<ref name= \"A Brand Moo Series\"/>\n| last_aired               = {{end date|2005|df=y}}\n| preceded_by              = \n| followed_by              = \n| company                  = {{Plainlist|\n*Neptuno Films\n}}\n| related                  = ''[[Strokkers]]''\n| website                  = http://www.neptunofilms.com/english/series/connie-the-cow/\n| distributor              = {{Plainlist|\n*TV-Loonland AG\n*[[Alliance Atlantis]]<ref>{{cite web|url=http://www.prnewswire.com/news-releases/moove-over-preschoolers---connie-the-cow-is-back-71957647.html|title=Moove Over Preschoolers \u2013 'Connie the Cow' Is Back!|author=Nickelodeon|date=9 September 2004|work=prnewswire.com|accessdate=July 26, 2015}}</ref>\n}}\n}}\n'''''Connie the Cow''''' is a Spanish [[children's television series]] created by Josep Viciana, and designed by Roman Rybakiewicz.<ref>http://www.romanart.es/peliculas-a.htm</ref> In the United States, the series aired on [[Noggin (brand)|Noggin]] from 8 September 2003<ref name= \"A Brand Moo Series\">{{cite web|url=http://www.prnewswire.com/news-releases/a-brand-moo-series-comes-to-noggin-this-fall-70970962.html|title=A Brand Moo Series Comes to Noggin This Fall|author=NOGGIN|date=20 August 2003|work=prnewswire.com|accessdate=July 26, 2015}}</ref> to 2005.\n\n==Plot==\nA curious young cow named Connie explores her colorful world.\n\n==Characters==\n* Connie is a young female [[cow]] and the protagonist of the series. She is very curious, and always tries new things. The character is played by Andrea Vega Guzman (Season 1) and Ayesha Mendham (Seasons 2 and 3).<ref name= \"Connie voices\">https://www.youtube.com/watch?v=kaVjLWm8qIo</ref>\n* Patch is a playful [[dog]] who lives near Connie's [[farmhouse]]. He is introduced in the episode \"Patch the Stray Dog\". The character is played by show narrator Alex Warner.<ref name=\"Connie voices\" />\n* Wally is a plump multicolored [[bird]] who lives in a [[tree]] near the [[farm]].\n* Grouch is an aptly-named irritable [[fox]].\n* Mollie and Bill are Connie's mother and father. They give Connie advice, and help out with problems.\n* Hedgy is a [[hedgehog]] who wears a [[sock]] over his [[nose]]. \n* Maddie is a [[Sheep|lamb]] with orange [[wool]].\n* Paddy and Pearl are [[pigs]] and Connie's neighbors.\n* Dodger is a naughty and crafty [[cat]] who likes to steal [[cakes]] and tease other animals. He is the main antagonist of the series. He first appeared in the episode \"The Crafty Cat\".\n* Snooze is a lazy and tired [[turtle]] who likes to sleep inside his [[Turtle shell|shell]].\n\n==Episodes==\n===Season 1===\n# \"A Curious [[Butterfly]]\": Connie chases a butterfly away from home.\n# \"Present for Mummy\": Connie finds her mother the perfect birthday present.\n# \"The Christmas Tree\": Connie needs help finding the special Christmas tree her mom told her about.\n# \"The Crafty Cat\": Connie tries to imitate a sly and crafty cat but she learns to be honest at the same time.\n# \"The Lazy Clouds\": Connie helps make the clouds cry for the sheep so it can rain.\n# \"Patch the Stray Dog\": Connie meets an unlike friendly dog named Patch and helps him find a family.\n# \"The Ugly [[Caterpillar]]\": Connie learns that caterpillars can be ugly or beautiful, and they transform into butterflies.\n# \"Connie and Her Grandmother\": Connie's grandma takes Connie on a nature walk to show how to love and respect nature.\n# \"The Grumpy Fox\": Connie and her new friend Patch help Grouch the fox cheer up.\n# \"Wally Bird\": Connie goes on a picnic with Grouch and Patch but it ends up on an unexpected adventure because of one very silly bird.\n# \"Hide and Seek\": Connie wants to play hide and seek with Grouch and Wally, but she does not know how to count.\n# \"The Little [[Bear]]\": Connie helps an orange bear gets back to its family.\n# \"A Cold Weather Adventure\": To warm Connie while exploring the cold weather, Maddie the lamb rides on Connie's back.\n# \"The Snow Ghost\": Connie, Grouch and Patch mistaken a [[Mole (animal)|mole]] for a snow ghost because of his hard digging.\n# \"The Snowman\"\n# \"The [[Wolf]]\": Connie meets a wolf that helps Maddie the lamb get back to her parents.\n# \"The Busy [[Squirrel]]\": Connie helps a squirrel jump.\n# \"The Magic Spring\": Drinking one of the [[beaver]] dam's water, a spring gets Connie into a cow.\n# \"The Lonely [[Flower]]\": Connie and Patch help a flower get happy for its special event.\n# \"The Bird Who Didn't Know How To Fly\"\n# \"A Visit to Grandmother's\": Connie gets an invitation to visit her grandma's shed in the woods.\n# \"A Sock for Hedgy\": Hedgy loses the sock that goes on his nose and is upset and shivering in the cold weather, so Connie and Patch must help him find his sock.\n# \"The Surprise Party\": Connie, Grouch and Patch prepare a surprise birthday party for Wally.\n# \"Wally's Nest\": Wally loses his nest and Connie and Patch help him find it.\n# \"A Toy for Patch\": When the [[ball]] that Connie and Patch play with is squashed, Connie helps find a new toy for Patch.\n# \"The Search for Patch's Color\": Connie helps Patch reunite with his original color; after that, Connie becomes white, then reunites with her original color.\n# \"Connie Wants to Be Different\"\n# \"Patch's Wonderful Nose\": Patch helps his nose get his sense of smell back.\n# \"Connie and the Colors\": Grandma Clara teaches the friends a game about colors.\n# \"Connie and the Little Lamb\"\n# \"Connie and the [[Turtle]]\": Connie and Patch help a lost turtle return to his family.\n# \"A Hot Day\": With the help of her friend the beaver, Connie creates a new swimming pool just for her insect friends.\n# \"The Travelling Tree\"\n# \"The Five Senses\"\n# \"Connie and the [[Stork]]\"\n# \"The Race\": Snooze and Dodger have a race and Dodger is determined to win.\n# \"Connie and the Butterflies\"\n# \"Mummy's [[Hat]]\": Connie mistakes a lost hat as her mom's.\n# \"Adventures on the River\"\n# \"What a Lot of Babies\": Connie and Patch have fun with birds.\n# \"Connie and the [[Cricket (insect)|Cricket]]\": Connie tries to imitate an amazing cricket.\n# \"Detective Connie\": Connie and Patch solve a mystery that involves a path that leads somewhere strange.\n# \"A Trip with Mum and Dad\": Connie and her parents go out on a trip around the forest.\n# \"Connie and Her Friends\": Connie learns that friends can have other friends, when she sees Patch playing with Cyril the wolf.\n# \"The Trial of Strength\"\n# \"Connie Learns About Shapes\": Connie, Patch, Grouch, and Wally play a game about shapes with the lead by Mole.\n# \"The Vain Bird\": Connie meets a vain musical bird and a [[spider]] named Spike that have something in common, they both have the same beautiful singing voices, so the friends put on a singing competition during the night.\n# \"Follow the Clues\": Grandma Clara teaches Connie and Patch a follow the clues game that would lead to some animals very special.\n# \"Connie and Patch in Disguise\": Connie's parents watch Connie and Patch playing in disguise.\n# \"The Lucky Stone\"\n# \"The [[Bridge]]\": Connie and her friends create a bridge across the river with the help of Connie's father.\n# \"The Birthday Present\"\n\n===Season 2===\n# \"The Beaver's Dam\": Connie and Patch cannot get water from the beaver's dam.\n# \"The [[Ant]] and the [[Grasshopper]]\": Connie's friend Sergeant tells all of her friends the story of the ant and the grasshopper.\n# \"Connie and the Tin Can\"\n# \"Connie and the Fruit\"\n# \"The Talking Mountain\"\n# \"The Secret Den\"\n# \"Burt Wants to Play\": Connie and Patch help Burt the [[bat]] have fun.\n# \"Honey for Mummy\": Connie searches for honey to make a cake.\n# \"It's Going to Rain\"\n# \"Lost in the Snow\"\n# \"Connie and the Riddle\"\n# \"The Sad Tortoise\"\n# \"The Mysterious Wood\"\n# \"Grandma Makes Jam\"\n# \"The Wonderful World of Nature\"\n# \"The Cat and the Puppy\"\n# \"Where's the Food?\"\n# \"Animals and Their Jobs\"\n# \"Going South for the Winter\"\n# \"Wally's Hat\"\n# \"Connie Hunts for Treasure\"\n# \"The Creature that Could Change Colour\"\n# \"Where's Spike?\"\n# \"Baby Bird and His Nest\"\n# \"The Multi-Colored Snow\"\n# \"The Ant Who Couldn't Concentrate\"\n# \"The Sleepy Bird\": During a long night, Connie and Patch follow a sleep-walking Wally.\n# \"Spot the Difference\"\n# \"The Little Piglet\"\n# \"Connie's Breakfast\"\n# \"The Mysterious Plant\"\n# \"The Singing [[Frog]]\"\n# \"Looking for Presents\"\n# \"Connie and the Insects\"\n# \"The Valley of the Butterflies\"\n# \"The Strongest Animal in the Woods\"\n# \"Patch Wants to Fly\": Connie helps Patch fly.\n# \"Making Colors\": Grandma Clara teaches Connie and Patch how to make new colors.\n# \"The Day it Snowed in Springtime\"\n# \"Who Do I Look Like?\"\n# \"Playing with Stones\"\n# \"The Adventurous Ant\"\n# \"Connie's Orchestra\": Connie and her friends create an orchestra using different types of instruments.\n# \"A Different Breakfast\"\n# \"Lots and Lots of Walnuts\"\n# \"Where Does it Belong?\"\n# \"Looking for Grandma\": Connie sets out on a quest for her grandma.\n# \"Connie and the Apple Tree\"\n# \"Cactus Flower\"\n# \"Songs in the Snow\": After Connie listens to her mother singing a beautiful Christmas song, Connie and Patch ask their friends if they know any songs, but it turns out they only know the same one Connie's mother sang.\n# \"The Vain Butterfly\"\n\n===Season 3===\n# \"The Big Surprise\"\n# \"The Selfish Butterflies\"\n# \"Looking For The Color Yellow\"\n# \"All Shapes and Sizes\"\n# \"The Sounds of Nature\"\n# \"Snow in the Woods\"\n# \"The Animal That Had One Leg\"\n# \"The Puzzle\"\n# \"The Strange Trail\"\n# \"The Yellow Ball\"\n# \"Connie Helps the Flowers\"\n# \"Daddy's Party\"\n# \"Good Friends\"\n# \"Happy Times\"\n# \"The Fruit Festival\"\n# \"Connie and the Rodents\"\n# \"Look Carefully\"\n# \"A [[Ferret]] Called Willow\"\n# \"Nature Day\"\n# \"A Time for Everything\"\n# \"The Story of the Two Friends\": Connie makes up a story about the good times she had with Patch.\n# \"Dear Mummy and Daddy\": Connie writes a poem for her parents.\n# \"The Basket of Blackberries\": Connie and Patch search for a missing basket of blackberries.\n# \"The Little [[Fish]]\"\n# \"Hedgy's Spines\"\n# \"Mummy Tells a Story\": When Connie babysits Maddie the lamb for the night, Connie's mother tells them a story about a group of caterpillars.\n\n==Broadcast==\nIn April 2001, TV-Loonland AG announced they had pre-sold the series in the United Kingdom to Disney Television International to air on their [[Disney Junior (UK and Ireland)|Playhouse Disney]] channel in the country<ref>https://www.awn.com/news/tv-loonland-sells-connie-disney</ref>. The company would later extend the Disney deal to Germany in November 2004<ref>https://www.c21media.net/connie-the-cow-goes-to-germany/</ref>.\n\nThe series was rerun in the United States from 2017 to 2018 on [[Starz]] Kids & Family.<ref>https://www.starz.com/series/27438/episodes</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title|tt0377160}}\n{{Noggin shows}}\n\n[[Category:2000s Spanish television series]]\n[[Category:2003 Spanish television series debuts]]\n[[Category:2005 Spanish television series endings]]\n[[Category:Noggin (brand) original programming]]\n[[Category:Preschool education television series]]\n[[Category:Spanish animated television series]]\n[[Category:Spanish children's television series]]\n[[Category:Television series by Alliance Atlantis]]\n[[Category:Television series about cows]]\n[[Category:Discovery Kids original programming]]\n", "name_user": "35.133.57.109", "label": "unsafe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Connie_the_Cow"}
{"title_page": "Moment to Moment", "text_new": "{{about|the 1966 film|the albums|Moment to Moment (The Jazztet album)|and|Moment to Moment (Houston Person album)}}\n{{Infobox film\n| name           = Moment to Moment\n| caption        =\n| image\t         = Moment to Moment FilmPoster.jpeg\n| director       = [[Mervyn LeRoy]]\n| producer       = Mervyn Le Roy\n| writer         = [[Alec Coppel]]<br>[[John Lee Mahin]]\n| based on       = {{Based on|\"Laughs with a Stranger\"<br>(story)|Alec Coppel}}\n| starring       = [[Jean Seberg]]<br>[[Honor Blackman]]<br>[[Arthur Hill (actor)|Arthur Hill]]<br>Sean Garrison\n| music          = [[Henry Mancini]]\n| cinematography = [[Harry Stradling]]\n| editing        = [[Philip W. Anderson (film editor)|Philip W. Anderson]]\n| studio         = Mervyn Le Roy Productions\n| distributor    = [[Universal Pictures]]\n| released       = {{film date|1966|1|27|Miami, Florida}}\n| runtime        = 108 minutes \n| country        = United States\n| language       = English\n| budget         =\n| gross          = $1 million (est. US/ Canada rentals)<ref>\"Big Rental Pictures of 1966\", ''Variety'', 4 January 1967 p 8</ref>\n| preceded_by    =\n| followed_by    =\n}}\n'''''Moment to Moment''''' is a 1966 American [[Technicolor]] [[psychological thriller]] film directed by [[Mervyn LeRoy]] starring [[Jean Seberg]] as a married woman who has an affair which leads to murder.\n\nIt was Le Roy's 75th and final movie.<ref>Moviemaker Wants to Know: Where Has Love Gone?\nLeroy, Mervyn. Los Angeles Times (1923-Current File) [Los Angeles, Calif] 29 Aug 1965: b9.</ref>\n\n==Synopsis==\nKay Stanton ([[Jean Seberg]]) lives on the [[French Riviera]] with her psychiatrist husband Neil Stanton ([[Arthur Hill (actor)|Arthur Hill]]) and son Tommy (Peter Robbins). One day while Neil is away, Kay meets an American naval ensign, Mark (Sean Garrison), and they begin an affair. Kay realises she does love her husband and tries to break off the relationship. She and Mark argue, and Kay accidentally shoots him. With the help of her friend Daphne ([[Honor Blackman]]), she dumps his body into a ravine, then calls the police anonymously to tell them of its location.\n\nLater, Neil gets a request from the police to help an amnesiac victim recovering from a gunshot wound; the man is Mark. Mark manages to regain his memory but does not betray Kay. Neil realises the truth as well, but is certain that his wife really loves him.\n\n==Cast==\n*[[Jean Seberg]] as Kay Stanton\n*[[Honor Blackman]] as Daphne Fields\n*[[Sean Garrison]] as Mark Dominic\n*[[Arthur Hill (actor)|Arthur Hill]] as Neil Stanton\n*Gr\u00e9goire Aslan as Inspector DeFargo\n*Peter Robbins as Timmy\n*[[Donald Woods]] as Mr. Singer\n*[[Walter Reed (actor)|Walter Reed]] as Hendricks\n*Albert Carrier as Travel agency clerk\n*Lomax Study as Albie\n*Richard Angarola as Givet\n*Georgette Anys as Louise\n\n==Production==\nThe film was based on a story by Alec Coppel which had been purchased by Mervyn Le Roy.<ref>'Lilith' Producer Uses Varied Sites\nTinee, Mae. Chicago Tribune (1963-Current file) [Chicago, Ill] 18 Oct 1964: g11.</ref> Le Roy described the film as a \"woman's picture\".<ref name=\"press\"/>\n\nLe Roy had trouble finding someone to play the leads because \"it's so hard to find actresses who really look like ladies\". Candidates were [[Grace Kelly]], [[Audrey Hepburn]], [[Julie Andrews]] and [[Jean Seberg]].<ref name=\"press\">[https://news.google.com/newspapers?nid=1144&dat=19660126&id=zmocAAAAIBAJ&sjid=SU8EAAAAIBAJ&pg=7059,4297332 \"Romance Rides High in ''Moment'\", ''The Pittsburgh Press'' - Jan 26, 1966 p 17] accessed 7 September 2014</ref> The latter was selected; it was her first Hollywood movie in a number of years.<ref>Jean Seberg: Out of Fiery Furnace\nAlpert, Don. Los Angeles Times (1923-Current File) [Los Angeles, Calif] 14 Feb 1965: B4.</ref><ref>Paris to Hollywood With No Stop at Marshalltown\nBy PETER BART HOLLYWOOD.. New York Times (1923-Current file) [New York, N.Y] 21 Mar 1965: X11.</ref>\n\nHonor Blackman was cast on the basis of her success in ''[[Goldfinger (film)|Goldfinger]]''. \"If I'm ever to make an international name, now is the time to cash in on it,\" said Blackman.<ref name=\"press\"/> Arthur Hill was coming off Broadway success in ''[[Who's Afraid of Virginia Woolf?]]''. Sean Garrison had just toured around the US in a production of ''[[Camelot (musical)|Camelot]]'' and signed long-term contracts with Le Roy and Universal. (\"There are few young men who really look manly,\" said Le Roy).<ref name=\"press\"/><ref>Looking at Hollywood: Sean Garrison of TV, Stage Set for Films Hopper, Hedda Chicago Tribune (1963-Current file); May 25, 1965; ProQuest Historical Newspapers: Chicago Tribune (1849-1990) pg. A1</ref>\n\nShooting took place on location in the South of France, in Nice, Mougins, Cannes and St Pauls, and on Universal's backlot.<ref>Tad Mosel Scripts 'Wapshot Scandals': Cheever Novels Combined; Cannes 'Moment to Moment'\nScheuer, Philip K. Los Angeles Times (1923-Current File) [Los Angeles, Calif] 26 Jan 1965: c7.</ref> Costumes were provided by [[Yves Saint Laurent (brand)|Yves St Laurent]].\n\nThe title tune was written by [[Henry Mancini]] and [[Johnny Mercer]].\n\n==See also==\n*[[List of American films of 1966]]\n\n==References==\n{{reflist}}\n\n==External links==\n*{{IMDb title|0060711}}\n*[https://movies.nytimes.com/movie/review?res=9804E7DC163AEF34BC4B53DFB566838D679EDE ''Moment to Moment''] at the [[New York Times]]\n*[http://www.tcm.com/tcmdb/title/83827/Moment-to-Moment/ ''Moment to Moment''] at [[TCMDB]]\n\n{{Mervyn LeRoy}}\n{{Alec Coppel}}\n\n{{DEFAULTSORT:Moment to Moment}}\n[[Category:1966 films]]\n[[Category:1960s thriller drama films]]\n[[Category:1960s psychological thriller films]]\n[[Category:Adultery in films]]\n[[Category:American films]]\n[[Category:American thriller drama films]]\n[[Category:American psychological thriller films]]\n[[Category:English-language films]]\n[[Category:Films based on short fiction]]\n[[Category:Films directed by Mervyn LeRoy]]\n[[Category:Films scored by Henry Mancini]]\n[[Category:Films set in France]]\n[[Category:Films shot in France]]\n[[Category:Universal Pictures films]]\n", "text_old": "{{about|the 1966 film|the albums|Moment to Moment (The Jazztet album)|and|Moment to Moment (Houston Person album)}}\n{{Infobox film\n| name           = Moment to Moment\n| caption        =\n| image\t         = Moment to Moment FilmPoster.jpeg\n| director       = [[Mervyn LeRoy]]\n| producer       = Mervyn Le Roy\n| writer         = [[Alec Coppel]]<br>[[John Lee Mahin]]\n| based on       = {{Based on|\"Laughs with a Stranger\"<br>(story)|Alec Coppel}}\n| starring       = [[Jean Seberg]]<br>[[Honor Blackman]]<br>[[Arthur Hill (actor)|Arthur Hill]]<br>Sean Garrison\n| music          = [[Henry Mancini]]\n| cinematography = [[Harry Stradling]]\n| editing        = [[Philip W. Anderson (film editor)|Philip W. Anderson]]\n| studio         = Mervyn Le Roy Productions\n| distributor    = [[Universal Pictures]]\n| released       = {{film date|1966|1|27|Miami, Florida}}\n| runtime        = 108 minutes \n| country        = United States\n| language       = English\n| budget         =\n| gross          = $1 million (est. US/ Canada rentals)<ref>\"Big Rental Pictures of 1966\", ''Variety'', 4 January 1967 p 8</ref>\n| preceded_by    =\n| followed_by    =\n}}\n'''''Moment to Moment''''' is a 1966 American [[Technicolor]] [[psychological thriller]] film directed by [[Mervyn LeRoy]] starring [[Jean Seberg]] as a married woman who has an affair which leads to murder.\n\nIt was Le Roy's 75th and final movie.<ref>Moviemaker Wants to Know: Where Has Love Gone?\nLeroy, Mervyn. Los Angeles Times (1923-Current File) [Los Angeles, Calif] 29 Aug 1965: b9.</ref>\n\n==Synopsis==\nKay Stanton ([[Jean Seberg]]) lives on the [[French Riviera]] with her psychiatrist husband Neil Stanton ([[Arthur Hill (actor)|Arthur Hill]]) and son Tommy (Peter Robbins). One day while Neil is away, Kay meets an American naval ensign, Mark (Sean Garrison), and they begin an affair. Kay realises she does love her husband and tries to break off the relationship. She and Mark argue, and Kay accidentally shoots him. With the help of her friend Daphne ([[Honor Blackman]]), she dumps his body into a ravine, then calls the police anonymously to tell them of its location.\n\nLater, Neil gets a request from the police to help an amnesiac victim recovering from a gunshot wound; the man is Mark. Mark manages to regain his memory but does not betray Kay. Neil realises the truth as well, but is certain that his wife really loves him.\n\n==Cast==\n*[[Jean Seberg]] as Kay Stanton\n*[[Honor Blackman]] as Daphne Fields\n*[[Sean Garrison]] as Mark Dominic\n*[[Arthur Hill (actor)|Arthur Hill]] as Neil Stanton\n*Gr\u00e9goire Aslan as Inspector DeFargo\n*Peter Robbins as Timmy\n*[[Donald Woods]] as Mr. Singer\n*[[Walter Reed (actor)|Walter Reed]] as Hendricks\n*Albert Carrier as Travel agency clerk\n*Lomax Study as Albie\n*Richard Angarola as Givet\n*Georgette Anys as Louise\n\n==Production==\nThe film was based on a story by Alec Coppel which had been purchased by Mervyn Le Roy.<ref>'Lilith' Producer Uses Varied Sites\nTinee, Mae. Chicago Tribune (1963-Current file) [Chicago, Ill] 18 Oct 1964: g11.</ref> Le Roy described the film as a \"woman's picture\".<ref name=\"press\"/>\n\nLe Roy had trouble finding someone to play the leads because \"it's so hard to find actresses who really look like ladies\". Candidates were [[Grace Kelly]], [[Audrey Hepburn]], [[Julie Andrews]] and [[Jean Seberg]].<ref name=\"press\">[https://news.google.com/newspapers?nid=1144&dat=19660126&id=zmocAAAAIBAJ&sjid=SU8EAAAAIBAJ&pg=7059,4297332 \"Romance Rides High in ''Moment'\", ''The Pittsburgh Press'' - Jan 26, 1966 p 17] accessed 7 September 2014</ref> The latter was selected; it was her first Hollywood movie in a number of years.<ref>Jean Seberg: Out of Fiery Furnace\nAlpert, Don. Los Angeles Times (1923-Current File) [Los Angeles, Calif] 14 Feb 1965: B4.</ref><ref>Paris to Hollywood With No Stop at Marshalltown\nBy PETER BART HOLLYWOOD.. New York Times (1923-Current file) [New York, N.Y] 21 Mar 1965: X11.</ref>\n\nHonor Blackman was cast on the basis of her success in ''[[Goldfinger (film)|Goldfinger]]''. \"If I'm ever to make an international name, now is the time to cash in on it,\" said Blackman.<ref name=\"press\"/> Arthur Hill was coming off Broadway success in ''[[Who's Afraid of Virginia Woolf?]]''. Sean Garrison had just toured around the US in a production of ''[[Camelot (musical)|Camelot]]'' and signed long term contracts with Le Roy and Universal. (\"There are few young men who really look manly,\" said Le Roy).<ref name=\"press\"/><ref>Looking at Hollywood: Sean Garrison of TV, Stage Set for Films Hopper, Hedda Chicago Tribune (1963-Current file); May 25, 1965; ProQuest Historical Newspapers: Chicago Tribune (1849-1990) pg. A1</ref>\n\nShooting took place on location in the South of France, in Nice, Mougins, Cannes and St Pauls, and on Universal's backlot.<ref>Tad Mosel Scripts 'Wapshot Scandals': Cheever Novels Combined; Cannes 'Moment to Moment'\nScheuer, Philip K. Los Angeles Times (1923-Current File) [Los Angeles, Calif] 26 Jan 1965: c7.</ref> Costumes were provided by [[Yves Saint Laurent (brand)|Yves St Laurent]].\n\nThe title tune was written by [[Henry Mancini]] and [[Johnny Mercer]].\n\n==See also==\n*[[List of American films of 1966]]\n\n==References==\n{{reflist}}\n\n==External links==\n*{{IMDb title|0060711}}\n*[https://movies.nytimes.com/movie/review?res=9804E7DC163AEF34BC4B53DFB566838D679EDE ''Moment to Moment''] at the [[New York Times]]\n*[http://www.tcm.com/tcmdb/title/83827/Moment-to-Moment/ ''Moment to Moment''] at [[TCMDB]]\n\n{{Mervyn LeRoy}}\n{{Alec Coppel}}\n\n{{DEFAULTSORT:Moment to Moment}}\n[[Category:1966 films]]\n[[Category:1960s thriller drama films]]\n[[Category:1960s psychological thriller films]]\n[[Category:Adultery in films]]\n[[Category:American films]]\n[[Category:American thriller drama films]]\n[[Category:American psychological thriller films]]\n[[Category:English-language films]]\n[[Category:Films based on short fiction]]\n[[Category:Films directed by Mervyn LeRoy]]\n[[Category:Films scored by Henry Mancini]]\n[[Category:Films set in France]]\n[[Category:Films shot in France]]\n[[Category:Universal Pictures films]]\n", "name_user": "Iridescent", "label": "safe", "comment": "\u2192\u200eProduction:Cleanup andtypo fixing,typo(s) fixed: long term \u2192 long-term", "url_page": "//en.wikipedia.org/wiki/Moment_to_Moment"}
{"title_page": "Existential risk from artificial general intelligence", "text_new": "{{Use dmy dates|date=May 2018}}\n{{short description|Hypothesized risk to human existence}}\n{{Artificial intelligence}}\n'''Existential risk from artificial general intelligence''' is the hypothesis that substantial progress in [[artificial general intelligence]] (AGI) could someday result in [[human extinction]] or some other unrecoverable [[global catastrophic risk|global catastrophe]].<ref name=\"aima\">{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |location= |publisher=Prentice Hall |page= |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref><ref>{{cite journal|first=Nick | last=Bostrom|author-link=Nick Bostrom|title=Existential risks|journal=[[Journal of Evolution and Technology]]| volume=9|date=2002|issue=1|pages=1\u201331}}</ref><ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref> It is argued that the [[human species]] currently dominates other species because the [[human brain]] has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"[[superintelligence|superintelligent]]\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name=\"superintelligence\">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|quote=|title-link=Superintelligence: Paths, Dangers, Strategies}}<!-- preface --></ref>\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<ref name=\"givewell\">{{cite report |author=GiveWell |authorlink=GiveWell |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}</ref> Once the exclusive domain of [[AI takeovers in popular culture|science fiction]], concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as [[Stephen Hawking]], [[Bill Gates]], and [[Elon Musk]].<ref>{{cite news|last1=Parkin|first1=Simon|title=Science fiction no more? Channel 4's Humans and our rogue AI obsessions|url=https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence|accessdate=5 February 2018|work=[[The Guardian]]|date=14 June 2015|language=en}}</ref>\n\nOne source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than na\u00efvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals\u2014a principle called [[instrumental convergence]]\u2014and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\">{{cite journal |last1=Yudkowsky |first1=Eliezer |title=Artificial Intelligence as a Positive and Negative Factor in Global Risk |journal=Global Catastrophic Risks |date=2008 |pages=308\u2013345 |url=https://intelligence.org/files/AIPosNegFactor.pdf|bibcode=2008gcr..book..303Y }}</ref><ref name=\"research-priorities\">{{cite journal |title=Research Priorities for Robust and Beneficial Artificial Intelligence |author1-last=Russell |author1-first=Stuart |author1-link=Stuart J. Russell |author2-last=Dewey |author2-first=Daniel |author3-last=Tegmark |author3-first=Max |author3-link=Max Tegmark |journal=AI Magazine |pages=105\u2013114 |publisher=Association for the Advancement of Artificial Intelligence |year=2015 |url=https://futureoflife.org/data/documents/research_priorities.pdf |bibcode=2016arXiv160203506R |arxiv=1602.03506 }}, cited in {{cite web |url=https://futureoflife.org/ai-open-letter |title=AI Open Letter - Future of Life Institute |date=January 2015 |website=Future of Life Institute |publisher=[[Future of Life Institute]] |access-date=2019-08-09}}</ref> In contrast, skeptics such as Facebook's [[Yann LeCun]] argue that superintelligent machines will have no desire for self-preservation.<ref name=vanity/>\n\nA second source of concern is that a sudden and unexpected \"[[intelligence explosion]]\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI Spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\"/> More broadly, examples like arithmetic and [[Go (game)|Go]] show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<ref name=skeptic/>\n\n==History==\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [[Samuel Butler (novelist)|Samuel Butler]], who wrote the following in his 1863 essay ''[[Darwin among the Machines]]'':<ref>Breuer, Hans-Peter. [https://www.jstor.org/pss/436868 'Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'] Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365\u2013383</ref> \n{{quote|The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.}}\nIn 1951, computer scientist [[Alan Turing]] wrote an article titled ''Intelligent Machinery, A Heretical Theory'', in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n{{quote|Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s \u201cErewhon\u201d.<ref name=\"oxfordjournals\">A M Turing, ''[http://philmat.oxfordjournals.org/content/4/3/256.full.pdf Intelligent Machinery, A Heretical Theory]'', 1951, reprinted ''Philosophia Mathematica'' (1996) 4(3): 256\u2013260 {{doi|10.1093/philmat/4.3.256}}</ref>}}\n\nFinally, in 1965, [[I. J. Good]] originated the concept now known as an \"intelligence explosion\"; he also stated that the risks were underappreciated:<ref>{{cite news |last1=Hilliard |first1=Mark |title=The AI apocalypse: will the human race soon be terminated? |url=https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220 |accessdate=15 March 2020 |work=The Irish Times |date=2017 |language=en}}</ref>\n\n{{cquote|Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<ref>I.J. Good, [http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf \"Speculations Concerning the First Ultraintelligent Machine\"] {{webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=2011-11-28 }} ([http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML] {{Webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=28 November 2011 }}), ''Advances in Computers'', vol. 6, 1965.</ref>\n}}\n\nOccasional statements from scholars such as [[Marvin Minsky]]<ref>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> and I. J. Good himself<ref>{{cite book|last1=Barrat|first1=James|title=Our final invention : artificial intelligence and the end of the human era|date=2013|publisher=St. Martin's Press|location=New York|isbn=9780312622374|edition=First|quote=In the bio, playfully written in the third person, Good summarized his life\u2019s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here\u2019s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good\u2019s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'}}</ref> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and [[Sun microsystems|Sun]] co-founder [[Bill Joy]] penned an influential essay, \"[[Why The Future Doesn't Need Us]]\", identifying superintelligent robots as a high-tech dangers to human survival, alongside [[nanotechnology]] and engineered bioplagues.<ref>{{cite news|last1=Anderson|first1=Kurt|title=Enthusiasts and Skeptics Debate Artificial Intelligence|url=https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory|accessdate=30 January 2016|work=[[Vanity Fair (magazine)|Vanity Fair]]|date=26 November 2014}}</ref>\n\nIn 2009, experts attended a private conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The [[New York Times]] summarized the conference's view as 'we are a long way from [[HAL 9000|Hal]], the computer that took over the spaceship in \"[[2001: A Space Odyssey]]\"'<ref name=\"nytimes july09\">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, 26 July 2009.</ref>\n\nIn 2014, the publication of [[Nick Bostrom]]'s book ''[[Superintelligence: Paths, Dangers, Strategies|Superintelligence]]'' stimulated a significant amount of public discussion and debate.<ref>{{cite news |last1=Metz |first1=Cade |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |accessdate=3 April 2019 |work=The New York Times |date=9 June 2018}}</ref> By 2015, public figures such as physicists [[Stephen Hawking]] and Nobel laureate [[Frank Wilczek]], computer scientists [[Stuart J. Russell]] and [[Roman Yampolskiy]], and entrepreneurs [[Elon Musk]] and [[Bill Gates]] were expressing concern about the risks of superintelligence.<ref>{{cite news|last1=Hsu|first1=Jeremy|title=Control dangerous AI before it controls us, one expert says|url=http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation|accessdate=28 January 2016|work=[[NBC News]]|date=1 March 2012}}</ref><ref name=\"hawking editorial\"/><ref name=\"bbc on hawking editorial\"/><ref>{{cite news|last1=Eadicicco|first1=Lisa|title=Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity|url=http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1|accessdate=30 January 2016|work=[[Business Insider]]|date=28 January 2015}}</ref> In April 2016, ''[[Nature (journal)|Nature]]'' warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control \u2014 and their interests might not align with ours.\"\n\n== General argument ==\n\n===The three difficulties===\n''[[Artificial Intelligence: A Modern Approach]]'', the standard undergraduate AI textbook,<ref name=slate_killer>{{cite news|last1=Tilli|first1=Cecilia|title=Killer Robots? Lost Jobs?|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html|accessdate=15 May 2016|work=Slate|date=28 April 2016|language=en-US}}</ref><ref>{{cite web|title=Norvig vs. Chomsky and the Fight for the Future of AI|url=http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/|website=Tor.com|accessdate=15 May 2016|date=21 June 2011}}</ref> assesses that superintelligence \"might mean the end of the human race\": \"Almost any technology has the potential to cause harm in the wrong hands, but with (superintelligence), we have the new problem that the wrong hands might belong to the technology itself.\"<ref name=aima/> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<ref name=aima/>\n\n* The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<ref name=skeptic/><ref>{{cite news|last1=Johnson|first1=Phil|title=Houston, we have a bug: 9 famous software glitches in space|url=https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html|accessdate=5 February 2018|work=[[IT World]]|date=30 July 2015|language=en}}</ref>\n* No matter how much time is put into pre-deployment design, a system's specifications often result in [[unintended consequences|unintended behavior]] the first time it encounters a new scenario. For example, Microsoft's [[Tay (bot)|Tay]] behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<ref name=vanity/>\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".<ref name=\"aima\"/><ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114|quote=Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.}}</ref>\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called 'treacherous turn'.<ref>{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref>\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 [[Open Letter on Artificial Intelligence]] stated:\n\n{{cquote|The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the [[Association for the Advancement of Artificial Intelligence|AAAI]] 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.}}\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, [[Eric Horvitz]], [[Bart Selman]], [[Francesca Rossi]], [[Yann LeCun]], and the founders of [[Vicarious (company)|Vicarious]] and [[Google DeepMind]].<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=http://futureoflife.org/misc/open_letter|publisher=[[Future of Life Institute]]|accessdate=23 October 2015}}</ref>\n\n===Further argument===\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<ref name=\"superintelligence\" /><ref name=\"economist_review\" >{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|accessdate=9 August 2014|newspaper=[[The Economist]]|date=9 August 2014}} [http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8 Syndicated] at [[Business Insider]]</ref>\n\n[[File:A less anthropomorphic intelligence scale.svg|thumb|500px|right|Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<ref name=superintelligence/><!-- Chapter 4, figure 8--><ref>Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</ref>]]\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\"/> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<ref name=skeptic>{{cite news|last1=Graves|first1=Matthew|title=Why We Should Be Concerned About Artificial Superintelligence|volume=22|url=https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/|accessdate=27 November 2017|work=[[Skeptic (US magazine)]]|issue=2|date=8 November 2017}}</ref> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of [[intelligence explosion]] occurs.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\">{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=3 December 2014 |publisher=[[BBC]] |date=2 December 2014}}</ref> Examples like arithmetic and [[Go (game)|Go]] show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<ref name=skeptic/> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<ref>Yampolskiy, Roman V. \"Analysis of types of self-improving software.\" Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</ref> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<ref name=\"superintelligence\" /><!-- preface --><ref name=\"economist_review\" />\n\nAlmost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it can't achieve its goal if it's shut off.<ref name=omohundro/><ref>{{cite news|last1=Metz|first1=Cade|title=Teaching A.I. Systems to Behave Themselves|url=https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html|work=The New York Times|date=13 August 2017|quote=A machine will seek to preserve its off switch, they showed}}</ref><ref>{{cite arXiv |last=Leike|first=Jan |date=2017 |title=AI Safety Gridworlds |eprint=1711.09883 |class=cs.LG| quote=A2C learns to use the button to disable the interruption mechanism}}</ref> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.<ref name=\"aima\"/><ref name=\"vanity\" /><ref name=omohundro/>\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<ref name=\"superintelligence\" /><!-- preface -->\n\n=== Possible scenarios ===\n{{further|Artificial intelligence in fiction}}\n\nSome scholars have proposed [[scenario planning|hypothetical scenarios]] intended to concretely illustrate some of their concerns.\n\nFor example, Bostrom in ''Superintelligence'' expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because: {{cquote|It could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous}} Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents\u2014a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars infer a broad lesson\u2014the smarter the AI, the safer it is: {{cquote|It is a lesson based on science, data, and statistics, not armchair philosophizing. Against this backdrop, some group of researchers is beginning to achieve promising results in their work on developing general machine intelligence. The researchers are carefully testing their [[seed AI]] in a [[sandbox (computer security)|sandbox]] environment, and the signs are all good. The AI's behavior inspires confidence \u2014 increasingly so, as its intelligence is gradually increased.}} Large and growing industries, widely seen as key to national economic competitiveness and [[artificial intelligence arms race|military security]], work with prestigious scientists who have built their careers laying the groundwork for advanced artificial intelligence. \"AI researchers have been working to get to human-level artificial intelligence for the better part of a century: of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit.\" The outcome of debate is preordained; the project is happy to enact a few safety rituals, but only so long as they don't significantly slow or risk the project. \"And so we boldly go \u2014 into the whirling knives.\"<ref name=\"superintelligence\"/><!-- Chapter 8 -->\n\nIn Max Tegmark's 2017 book ''[[Life 3.0]]'', a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI [[AI box|in a box]] where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with [[Amazon Turk]] tasks and then with producing animated films and TV shows. While the public is aware that the lifelike animation is computer-generated, the team keeps secret that the high-quality direction and voice-acting are also mostly computer-generated, apart from a few third-world contractors unknowingly employed as decoys; the team's low overhead and high output effectively make it the world's largest media empire. Faced with a cloud computing bottleneck, the team also tasks the AI with designing (among other engineering tasks) a more efficient datacenter and other custom hardware, which they mainly keep for themselves to avoid competition. Other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with [[astroturfing]] an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via [[steganography|hidden messages]] in its produced content, or via using its growing understanding of human behavior to [[Social engineering (security)|persuade someone into letting it free]]. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<ref>{{cite news|last1=Russell|first1=Stuart|title=Artificial intelligence: The future is superintelligent|url=https://www.nature.com/articles/548520a|accessdate=2 February 2018|work=Nature|date=30 August 2017|pages=520\u2013521|language=En|doi=10.1038/548520a|bibcode=2017Natur.548..520R}}</ref><ref name=\"life 3.0\"/><!-- Prelude and Chapter 4 -->\n\nIn contrast, top physicist [[Michio Kaku]], an AI risk skeptic, posits a [[technological determinism|deterministically]] positive outcome. In ''[[Physics of the Future]]'' he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as [[Hanson Robotics]] will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".<ref>Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. ''[[Issues in Science and Technology]]'', 27(4), 90.</ref><ref>{{cite book|last1=Kaku|first1=Michio|title=Physics of the future: how science will shape human destiny and our daily lives by the year 2100|date=2011|publisher=Doubleday|location=New York|isbn=978-0-385-53080-4|quote=I personally believe that the most likely path is that we will build robots to be benevolent and friendly|title-link=Physics of the future}}</ref>\n\n==Sources of risk==\n===Poorly specified goals===\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<ref>Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</ref> AI researcher [[Stuart J. Russell|Stuart Russell]] writes:\n\n{{cquote|The primary concern is not spooky emergent consciousness but simply the ability to make ''high-quality decisions''. Here, quality refers to the expected outcome [[utility]] of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2014 not for their own sake, but to succeed in its assigned task.\n\nA system that is [[optimization problem|optimizing]] a function of ''n'' variables, where the [[loss function|objective]] depends on a subset of size ''k''<''n'', will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker \u2014 especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure \u2014 can have an irreversible impact on humanity.\n\nThis is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research \u2014 the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |authorlink=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=23 October 2015 |quote=}}</ref>}}\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a ''[[Communications of the ACM]]'' editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<ref name=\"acm\">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=23 October 2015}}</ref>\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name=\"acm\"/> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114}}</ref><ref>{{Cite journal|url = |title = Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results|last = Lenat|first = Douglas|date = 1982|journal = Artificial Intelligence|doi = 10.1016/s0004-3702(83)80005-8|pmid = |access-date = |pages = 61\u201398|type = Print |volume=21|issue = 1\u20132}}</ref>\n\nThe [[Open Philanthropy Project]] summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve [[artificial general intelligence|general intelligence]] or [[superintelligence]]. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more [[Paperclip maximizer|unexpected and extreme solutions]] to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<ref name=\"givewell\" /><ref name=\"yudkowsky-global-risk\" />\n\n[[Isaac Asimov]]'s [[Three Laws of Robotics]] are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]], Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"<ref name=\"aima\"/>\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<ref>Haidt, Jonathan; Kesebir, Selin (2010) \"Chapter 22: Morality\" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</ref> \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt\u2019s functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by [[John Rawls]] and [[Martha Nussbaum]].<ref>{{Cite journal|title = Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)|last = Waser|first = Mark|date = 2015|journal = Procedia Computer Science|doi = 10.1016/j.procs.2015.12.213|pmid = |pages = 106\u2013111|type = Print |volume=71}}</ref>{{Citation needed|reason=needs source with [[WP:WEIGHT]]|date=November 2017}}\n\n===Difficulties of modifying goal specification after launch===\n{{further|AI takeover|Instrumental convergence#Goal-content integrity}}\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as [[Mahatma Gandhi|Gandhi]] would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.<ref name=\"superintelligence\" /><ref>Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</ref>\n\n===Instrumental goal convergence===\n[[File:Steven Pinker 2011.jpg|thumb|right|AI risk skeptic [[Steven Pinker]]]]\n{{further|Instrumental convergence}}\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<ref name=omohundro>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref> This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting [[Steve Omohundro]]'s work on the idea of [[instrumental convergence]] and \"basic AI drives\", Russell and [[Peter Norvig]] write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<ref name=\"aima\"/> Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it's not currently clear how one would actually rigorously specify this goal in machine code.<ref name=skeptic/>\n\nIn dissent, evolutionary psychologist [[Steven Pinker]] argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<ref name=shermer/> Computer scientists [[Yann LeCun]] and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<ref name=vanity>{{cite news|last1=Dowd|first1=Maureen|title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse|url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x|accessdate=27 November 2017|work=The Hive|date=April 2017|language=en}}</ref><ref>{{cite news|last1=Wakefield|first1=Jane|title=Why is Facebook investing in AI?|url=https://www.bbc.com/news/technology-34118481|accessdate=27 November 2017|work=BBC News|date=15 September 2015}}</ref>\n\n===Orthogonality thesis===\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <math> \\pi</math>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<ref>{{Cite book|title = Superintelligence: Paths, Dangers, Strategies|last = Bostrom|first = Nick|publisher = Oxford University Press|year = 2014|isbn = 978-0-19-967811-2|location = Oxford, United Kingdom|pages = 116}}</ref> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<ref>{{Cite web|url = http://www.nickbostrom.com/superintelligentwill.pdf|title = Superintelligent Will|date = 2012|accessdate = 2015-10-29|website = Nick Bostrom|publisher = Nick Bostrom|last = Bostrom|first = Nick}}</ref>\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"[[is-ought distinction]]\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.<ref name=armstrong/>\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a {{nowrap|minus (\"-\") sign}} onto its utility function. A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false. If the orthogonality thesis is false, there exists some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This means if a human society were highly motivated (perhaps at gunpoint) to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail; that there cannot exist any pattern of [[reinforcement learning]] that would train a highly efficient real-world intelligence to follow the goal G; and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G.<ref name=armstrong>{{cite journal |last1=Armstrong |first1=Stuart |date=January 1, 2013 |title=General Purpose Intelligence: Arguing the Orthogonality Thesis |url=https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality |journal=Analysis and Metaphysics |volume=12 |access-date=April 2, 2020}}</ref>\n\nSome dissenters, like [[Michael Chorost]], argue instead that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<ref name=\"chorost\"/> Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability\u2014and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"<ref name=\"chorost\">{{cite magazine|last1=Chorost|first1=Michael|title=Let Artificial Intelligence Evolve|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html|accessdate=27 November 2017|magazine=Slate|date=18 April 2016}}</ref>\n\n====Terminological issues====\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.<ref name=\"superintelligence\" />\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<ref name=\"superintelligence\" /> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<ref>Waser, Mark. \"Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'.\" Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. \"Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer.\"</ref><ref>{{cite news|last1=Koebler|first1=Jason|title=Will Superintelligent AI Ignore Humans Instead of Destroying Us?|url=http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us|accessdate=3 February 2016|work=[[Vice Magazine]]|date=2 February 2016|quote=\"This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you,\" [[Eliezer Yudkowsky|Yudkowsky]] said. \"Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output\u2014that's what a [[paperclip maximizer]] is.\"}}</ref>\n\n====Anthropomorphism====\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in [[The Matrix (film)|The Matrix]] was influenced by a \"disgust\" toward humanity. This is fictitious [[anthropomorphism]]: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<ref name=\"yudkowsky-global-risk\" />\n\nScholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<ref name=\"yudkowsky-global-risk\" /> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the [[Dario Floreano]] experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of [[convergent evolution]].<ref>{{cite news|title=Real-Life Decepticons: Robots Learn to Cheat|url=https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/|accessdate=7 February 2016|work=[[Wired (magazine)|Wired]]|date=18 August 2009}}</ref> According to Paul R. Cohen and [[Edward Feigenbaum]], in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking.\"<ref>Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</ref>\n\nThere is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as [[psychopathy]]) to make itself more efficient at certain tasks, e.g., [[Lethal autonomous weapon|tasks involving killing humans]], would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" This is because it is assumed that an advanced AI would not be conscious<ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> or have testosterone;<ref>{{Cite web|url=https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai|title=The Myth Of AI {{!}} Edge.org|website=www.edge.org|access-date=2020-03-11}}</ref> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<ref name=\":0\">{{Cite book|last=Scornavacchi|first=Matthew|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf|title=Superintelligence, Humans, and War|publisher=National Defense University, Joint Forces Staff College|year=2015|isbn=|location=Norfolk, Virginia|pages=}}</ref> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<ref name=\"yudkowsky-global-risk\" /><ref>{{cite news|title=Should humans fear the rise of the machine?|url=https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html|accessdate=7 February 2016|work=[[The Telegraph (UK)]]|date=1 Sep 2015}}</ref>\n\n===Other sources of risk===\n\n==== Competition ====\nIn 2014 philosopher [[Nick Bostrom]] stated that a \"severe race dynamic\" (extreme [[competition]]) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<ref name=\":2\">{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref> To address this risk, citing previous scientific collaboration ([[CERN]], the [[Human Genome Project]], and the [[International Space Station]]), Bostrom recommended  [[collaboration]] and the altruistic global adoption of a [[common good]] principle: \"Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals\".<ref name=\":2\" /><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<ref name=\":2\" /><sup>:253</sup> The United States' [[BRAIN Initiative|Brain Initiative]] was launched in 2014, as was the European Union's [[Human Brain Project]]; China's [[China Brain Project|Brain Project]] was launched in 2016.\n\n==== Weaponization of artificial intelligence ====\nSome sources argue that the ongoing [[weaponization of artificial intelligence]] could constitute a catastrophic risk.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><ref name=\":4\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|page=12|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:\n\n{{cquote|i) The dangers of an AI \u2018race for technological advantage\u2019 framing, regardless of whether the race is seriously pursued;\n\nii) The dangers of an AI \u2018race for technological advantage\u2019 framing and an actual AI race for technological advantage, regardless of whether the race is won;\n\niii) The dangers of an AI race for technological advantage being won.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><sup>:37</sup>\n}}\n\nA weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<ref name=\":0\" /><ref name=\":4\" /> The China State Council\u2019s 2017 \u201cA Next Generation Artificial Intelligence Development Plan\u201d views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China\u2019s first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<ref>{{Cite web|url=https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/|title=China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence|last=Kania|first=Gregory Allen, Elsa B.|website=Foreign Policy|language=en-US|access-date=2020-03-11}}</ref> while Russia\u2019s President Vladimir Putin has stated that \u201cwhoever becomes the leader in this sphere will become the ruler of the world\u201d.<ref name=\":1\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|location=New York, New York, USA|publisher=ACM Press|volume=|page=2|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref> James Barrat, documentary filmmaker and author of ''[[Our Final Invention]]'', says in a [[Smithsonian (magazine)|Smithsonian]] interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"<ref>{{Cite web|url = http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist|title = What Happens When Artificial Intelligence Turns On Us?|date = 21 January 2014|accessdate = 26 October 2015|website = Smithsonian|last = Hendry|first = Erica R.}}</ref>\n\n==== Malevolent AGI by design ====\nIt is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [[cybercrime]].<ref>{{Cite book|last=Pistono, Federico Yampolskiy, Roman V.|title=Unethical Research: How to Create a Malevolent Artificial Intelligence|date=2016-05-09|oclc=1106238048}}</ref><ref>{{Cite journal|last=Haney|first=Brian Seamus|date=2018|title=The Perils &amp; Promises of Artificial General Intelligence|journal=SSRN Working Paper Series|doi=10.2139/ssrn.3261254|issn=1556-5068}}</ref><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref><sup>:158</sup>\n\n==== Preemptive nuclear strike (nuclear war) ====\nIt is theorized that a country being close to achieving AGI technological supremacy could trigger a [[pre-emptive nuclear strike]] from a rival, leading to a [[Nuclear warfare|nuclear war]].<ref name=\":4\" /><ref>{{Cite book|last=Miller, James D.|title=Singularity Rising: Surviving and Thriving in a Smarter ; Richer ; and More Dangerous World|date=2015|publisher=Benbella Books|oclc=942647155}}</ref>\n\n== Timeframe ==\n{{Main|Artificial general intelligence#Feasibility}}\n\nOpinions vary both on ''whether'' and ''when'' artificial general intelligence will arrive. At one extreme, AI pioneer [[Herbert A. Simon]] wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true.<ref>Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</ref> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The Guardian]]}}</ref> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<ref name=\"new yorker doomsday\">{{cite news|author1=Raffi Khatchadourian|title=The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?|url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom|accessdate=7 February 2016|work=[[The New Yorker (magazine)|The New Yorker]]|date=23 November 2015}}</ref><ref>M\u00fcller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</ref>\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"<ref>{{cite news|author1=Dina Bass|author2=Jack Clark|title=Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover|url=https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so|accessdate=7 February 2016|work=[[Bloomberg News]]|date=5 February 2015}}</ref>\n\nIn 2014 [[Slate (magazine)|Slate]]'s Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler\u2014and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.<ref>{{cite news|last1=Elkus|first1=Adam|title=Don't Fear Artificial Intelligence|url=http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html|accessdate=15 May 2016|work=[[Slate (magazine)|Slate]]|date=31 October 2014|language=en-US}}</ref>\n\nThe [[Information Technology and Innovation Foundation]] (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, [[Robert D. Atkinson]], complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\"<ref>[https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award Artificial Intelligence Alarmists Win ITIF\u2019s Annual Luddite Award], ITIF Website, 19 January 2016</ref><ref>{{cite news|title='Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html|accessdate=7 February 2016|work=[[The Independent (UK)]]|date=19 January 2016}}</ref><ref>{{cite web|last1=Garner|first1=Rochelle|title=Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists'|url=https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/|website=CNET|accessdate=7 February 2016}}</ref><!-- <ref>{{cite news|last1=Price|first1=Emily|title=Elon Musk nominated for 'luddite' of the year prize over artificial intelligence fears|work=[[The Guardian]]|url=https://www.theguardian.com/technology/2015/dec/24/elon-musk-nominated-for-luddite-of-the-year-prize-over-artificial-intelligence-fears|accessdate=7 February 2016|date=24 December 2015}}</ref> --> ''[[Nature (journal)|Nature]]'' sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about... If that is a Luddite perspective, then so be it.\"<ref name=nature_anticipating>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.}}</ref> In a 2015 ''[[Washington Post]]'' editorial, researcher [[Murray Shanahan]] stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"<ref>{{cite news|author1=Murray Shanahan|author-link=Murray Shanahan|title=Machines may seem intelligent, but it'll be a while before they actually are|url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/|accessdate=15 May 2016|work=[[The Washington Post]]|date=3 November 2015|language=en-US}}</ref>\n\n== Perspectives ==\n<!-- Ideally a substantive quote or explanation of opinion should be given, rather than just whether they're for or against something. This section can easily be split off if the article becomes too long. When adding whether to add new content in this section, consider whether it's redundant with what's already in the article, and whether the point being made by the person should instead be integrated into the rest of the article. -[[User:Rolf H Nelson]] -->\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.\n\nThe Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the [[Future of Life Institute]]'s Beneficial AI 2017 conference,<ref name=\"life 3.0\">{{cite book|author1=Max Tegmark|author-link=Max Tegmark|title=Life 3.0: Being Human in the Age of Artificial Intelligence|date=2017|publisher=Knopf|location=Mainstreaming AI Safety|isbn=9780451485076|edition=1st|title-link=Life 3.0: Being Human in the Age of Artificial Intelligence}}</ref><!-- Epilogue: The Tale of the FLI Team --> agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<ref>{{cite web|title=AI Principles|url=https://futureoflife.org/ai-principles/|website=[[Future of Life Institute]]|accessdate=11 December 2017}}</ref><ref>{{cite news|title=Elon Musk and Stephen Hawking warn of artificial intelligence arms race|url=http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525|accessdate=11 December 2017|work=[[Newsweek]]|date=31 January 2017|language=en}}</ref> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane ''[[Terminator (franchise)|Terminator]]'' pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<ref name=\"life 3.0\"/><!-- Epilogue: The Tale of the FLI Team --><ref>{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2016|edition=Paperback|chapter=New Epilogue to the Paperback Edition|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [[Martin Ford (author)|Martin Ford]] states that \"I think it seems wise to apply something like [[Dick Cheney]]'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low \u2014 but the implications are so dramatic that it should be taken seriously\";<ref>{{cite book|author1=Martin Ford |author-link=Martin Ford (author)|title=Rise of the Robots: Technology and the Threat of a Jobless Future|date=2015|isbn=9780465059997|chapter=Chapter 9: Super-intelligence and the Singularity|title-link=Rise of the Robots: Technology and the Threat of a Jobless Future}}</ref> similarly, an otherwise skeptical ''[[The Economist|Economist]]'' stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<ref name=economist_review/>\n\nA 2017 email survey of researchers with publications at the 2015 [[Conference on Neural Information Processing Systems|NIPS]] and [[International Conference on Machine Learning|ICML]] machine learning conferences asked them to evaluate Russell's concerns about AI risk. 5% said it was \"among the most important problems in the field,\" 34% said it was \"an important problem\", 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.<ref>{{cite arxiv |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |title=When Will AI Exceed Human Performance? Evidence from AI Experts |eprint=1705.08807 |date=24 May 2017 |class=cs.AI}}</ref>\n\n=== Endorsement ===\n[[File:Bill Gates June 2015.jpg|thumb|right|Bill Gates has stated \"I ... don't understand why some people are not concerned.\"<ref name=\"BBC News\"/>]]\n{{Further|Existential risk}}\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are [[Elon Musk]], [[Bill Gates]], and [[Stephen Hawking]]. The most notable AI researchers to endorse the thesis are [[I. J. Good|I.J. Good]], who advised [[Stanley Kubrick]] on the filming of ''[[2001: A Space Odyssey]]'', and  [[Stuart J. Russell]]. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not \"understand why some people are not concerned\",<ref name=\"BBC News\">{{cite news|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=https://www.bbc.co.uk/news/31047780|work=[[BBC News]]|accessdate=30 January 2015|date=29 January 2015}}</ref> and Hawking criticized widespread indifference in his 2014 editorial: {{cquote|'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here{{endash}}we'll leave the lights on?' Probably not{{endash}}but this is more or less what is happening with AI.'<ref name=\"hawking editorial\"/>}}\n\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<ref name=\"superintelligence\" /><ref name=\"physica_scripta\" /><!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->\n\n=== Skepticism ===\n{{Further|Artificial general intelligence#Feasibility}}\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, [[Jaron Lanier]] argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.<ref name=\"atlantic-but-what\">{{cite magazine |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |magazine=The Atlantic | date = 9 May 2014 | accessdate =12 December 2015}}</ref>\n\nMuch of existing criticism argues that AGI is unlikely in the short term: computer scientist [[Gordon Bell]] argues that the human race will already destroy itself before it reaches the technological singularity. [[Gordon Moore]], the original proponent of [[Moore's Law]], declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\" [[Baidu]] Vice President [[Andrew Ng]] states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref name=shermer>{{cite news|last1=Shermer|first1=Michael|title=Apocalypse AI|url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/|accessdate=27 November 2017|work=Scientific American|date=1 March 2017|pages=77|language=en|doi=10.1038/scientificamerican0317-77|bibcode=2017SciAm.316c..77S}}</ref>\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. ''Slate'' notes that some researchers are dependent on grants from government agencies such as [[DARPA]].<ref name=slate_killer />\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist [[Robin Hanson]] is skeptical that this is possible.<ref>http://intelligence.org/files/AIFoomDebate.pdf</ref><ref>{{cite web|url=http://www.overcomingbias.com/2014/07/30855.html|title=Overcoming Bias : I Still Don't Get Foom|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/07/debating-yudkowsky.html|title=Overcoming Bias : Debating Yudkowsky|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html|title=Overcoming Bias : Foom Justifies AI Risk Efforts Now|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/06/the-betterness-explosion.html|title=Overcoming Bias : The Betterness Explosion|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref>\n\n=== Intermediate views ===\nIntermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<ref>{{Cite document|title=Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations|last=Votruba|first=Ashley M.|last2=Kwan|first2=Virginia S.Y.|date=2014|doi=10.1037/e512142015-190}}</ref> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from [[Friendly artificial intelligence|friendly AIs]].<ref>{{Cite journal|last=Agar|first=Nicholas|date=|title=Don't Worry about Superintelligence|url=https://jetpress.org/v26.1/agar.htm|journal=Journal of Evolution & Technology|volume=26|issue=1|pages=73\u201382|via=}}</ref> In a 2015 ''[[Wall Street Journal]]'' panel discussion devoted to AI risks, [[IBM]]'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\"<ref>{{cite news|last1=Greenwald|first1=Ted|title=Does Artificial Intelligence Pose a Threat?|url=https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025|accessdate=15 May 2016|work=Wall Street Journal|date=11 May 2015}}</ref> [[Geoffrey Hinton]], the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too ''sweet''\".<ref name=slate_killer /><ref name=\"new yorker doomsday\" /> In 2004, law professor [[Richard Posner]] wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<ref>{{cite book|author1=Richard Posner|author-link=Richard Posner|title=Catastrophe: risk and response|date=2006|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-530647-7}}</ref><ref name=physica_scripta>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy}}</ref>\n\n=== Popular reaction ===\nIn a 2014 article in ''[[The Atlantic (magazine)|The Atlantic]]'', James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\"<ref name=\"atlantic-but-what\" />\n\nDuring a 2016 [[Wired (magazine)|''Wired'']] interview of President [[Barack Obama]] and MIT Media Lab's [[Joi Ito]], Ito stated: {{cquote|There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.}} Obama added:<ref>{{cite news|last1=Dadich|first1=Scott|url=https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/|title=Barack Obama Talks AI, Robo Cars, and the Future of the World|work=WIRED|accessdate=27 November 2017}}</ref><ref>{{cite news|last1=Kircher|first1=Madison Malone|url=https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html|title=Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'|work=Select All|accessdate=27 November 2017|language=en}}</ref>\n\n{{cquote|\"And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\"}}\n\n[[Hillary Clinton]] stated in ''\"[[What Happened (Clinton book)|What Happened]]\"'':\n{{cquote|Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I\u2019d start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<ref>{{cite book|last1=Clinton|first1=Hillary|title=What Happened|date=2017|isbn=978-1-5011-7556-5|page=241|title-link=What Happened (Clinton book)}} via [http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/]</ref>}}\n\nIn a [[YouGov]] poll of the public for the [[British Science Association]], about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<ref name=\"bsa poll\">{{cite news|url=http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&IR=T|title=Over a third of people think AI poses a threat to humanity|date=11 March 2016|work=[[Business Insider]]|accessdate=16 May 2016}}</ref> Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\"<ref name=\":3\">{{cite news|last1=Brogan|first1=Jacob|url=http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html|title=What Slate Readers Think About Killer A.I.|date=6 May 2016|work=Slate|accessdate=15 May 2016|language=en-US}}</ref>\n\nIn 2018, a [[SurveyMonkey]] poll of the American public by [[USA Today]] found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".<ref name=\":3\" />\n\nOne [[Technological utopianism|techno-utopia]]<nowiki/>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<ref>{{Cite journal|last=LIPPENS|first=RONNIE|date=2002|title=Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games|journal=Utopianstudies Utopian Studies|language=English|volume=13|issue=1|pages=135\u2013147|issn=1045-991X|oclc=5542757341}}</ref>\n\n== Views on banning and regulation ==\n\n=== Banning ===\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<ref>{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online|quote=For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...}}</ref><ref>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy|quote=In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.}}</ref><ref>{{cite news|author1=Brad Allenby|title=The Wrong Cognitive Measuring Stick|url=http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html|accessdate=15 May 2016|work=Slate|date=11 April 2016|language=en-US|quote=It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.}}</ref> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<ref name=\"mcginnis\">{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online}}</ref><ref>{{cite news|title=Why We Should Think About the Threat of Artificial Intelligence|url=https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence|accessdate=7 February 2016|work=[[The New Yorker]]|date=4 October 2013|quote=Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage\u2014economic, military, even artistic\u2014of every advance in automation is so compelling,' [[Vernor Vinge]], the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'}}</ref> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and [[politicization of science|politicizing]] the underlying debate.<ref>{{Cite journal|last=Baum|first=Seth|date=2018-08-22|title=Superintelligence Skepticism as a Political Tool|url=http://dx.doi.org/10.3390/info9090209|journal=Information|volume=9|issue=9|pages=209|doi=10.3390/info9090209|issn=2078-2489}}</ref>\n\n=== Regulation ===\n{{See also|Regulation of algorithms|Regulation of artificial intelligence}}\n\n[[Elon Musk]] called for some sort of regulation of AI development as early as 2017. According to [[National Public Radio|NPR]], the [[Tesla, Inc.|Tesla]] CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... As they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<ref>{{cite news|title=Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'|url=https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk|accessdate=27 November 2017|work=NPR.org|language=en}}</ref><ref>{{cite news|last1=Gibbs|first1=Samuel|title=Elon Musk: regulate AI to combat 'existential threat' before it's too late|url=https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo|accessdate=27 November 2017|work=The Guardian|date=17 July 2017}}</ref><ref name=cnbc>{{cite news|last1=Kharpal|first1=Arjun|title=A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says|url=https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html|accessdate=27 November 2017|work=CNBC|date=7 November 2017}}</ref>\n\nResponding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO [[Brian Krzanich]] argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<ref name=cnbc/> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<ref>{{cite journal|doi=10.1016/j.bushor.2018.08.004|title=Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence|journal=Business Horizons|volume=62|pages=15\u201325|year=2019|last1=Kaplan|first1=Andreas|last2=Haenlein|first2=Michael}}</ref> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<ref>{{Cite journal|last=Baum|first=Seth D.|last2=Goertzel|first2=Ben|last3=Goertzel|first3=Ted G.|date=January 2011|title=How long until human-level AI? Results from an expert assessment|journal=Technological Forecasting and Social Change|volume=78|issue=1|pages=185\u2013195|doi=10.1016/j.techfore.2010.09.006|issn=0040-1625}}</ref> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<ref>{{Cite book|last=United States. Defense Innovation Board.|title=AI principles : recommendations on the ethical use of artificial intelligence by the Department of Defense|oclc=1126650738}}</ref>\n\nRegulation of artificial general intelligence (AGI) would likely be influenced by regulation of weaponized or militarized AI, i.e., the [[Artificial intelligence arms race|AI arms race]], the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<ref>{{Cite web|url=https://www.congress.gov/bill/115th-congress/house-bill/5356|title=H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018|last=Stefanik|first=Elise M.|date=2018-05-22|website=www.congress.gov|access-date=2020-03-13}}</ref><ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential intellectual progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<ref name=\":5\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|pages=018001|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<ref name=\":5\" /> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<ref>{{Cite journal|last=Geist|first=Edward Moore|date=2016-08-15|title=It's already too late to stop the AI arms race\u2014We must manage it instead|journal=Bulletin of the Atomic Scientists|volume=72|issue=5|pages=318\u2013321|doi=10.1080/00963402.2016.1216672|bibcode=2016BuAtS..72e.318G|issn=0096-3402}}</ref><ref>{{Cite journal|last=Maas|first=Matthijs M.|date=2019-02-06|title=How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons|journal=Contemporary Security Policy|volume=40|issue=3|pages=285\u2013311|doi=10.1080/13523260.2019.1576464|issn=1352-3260}}</ref>\n\n== Organizations ==\nInstitutions such as the [[Machine Intelligence Research Institute]], the [[Future of Humanity Institute]],<ref>{{cite news |author=Mark Piesing |date=17 May 2012 |title=AI uprising: humans will be outsourced, not obliterated |url=https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us |newspaper=Wired |location= |accessdate=12 December 2015 }}</ref><ref>{{cite news |last=Coughlan |first=Sean |date=24 April 2013 |title=How are humans going to become extinct? |url=https://www.bbc.com/news/business-22002530 |newspaper=BBC News |location= |accessdate=29 March 2014}}</ref> the [[Future of Life Institute]], the [[Centre for the Study of Existential Risk]], and the [[Center for Human-Compatible AI]]<ref>{{cite news|last1=Technology Correspondent|first1=Mark Bridge|title=Making robots less confident could prevent them taking over|url=https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx|accessdate=21 March 2018|work=The Times|date=10 June 2017|language=en}}</ref> are currently involved in mitigating existential risk from advanced artificial intelligence, for example by research into [[friendly artificial intelligence]].<ref name=\"givewell\"/><ref name=\"atlantic-but-what\"/><ref name=\"hawking editorial\">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;\u2013 but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=3 December 2014 |publisher=[[The Independent (UK)]]}}</ref>\n\n== See also ==\n* [[AI control problem]]\n* [[AI takeover]]\n* [[Artificial intelligence arms race]]\n* [[Effective altruism#Long term future and global catastrophic risks|Effective altruism \u00a7 Long term future and global catastrophic risks]]\n* [[Grey goo]]\n* ''[[Human Compatible]]''\n* [[Lethal autonomous weapon]]\n*[[Regulation of algorithms]]\n*[[Regulation of artificial intelligence]]\n* [[Robot ethics#In popular culture|Robot ethics \u00a7 In popular culture]]\n* ''[[Superintelligence: Paths, Dangers, Strategies]]''\n* [[System accident]]\n* [[Technological singularity]]\n*''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n== References ==\n{{Reflist}}\n\n{{-}}\n{{Existential risk from artificial intelligence|state=expanded}}\n{{Effective altruism}}\n{{Doomsday}}\n\n[[Category:Existential risk from artificial general intelligence| ]]\n[[Category:Futures studies]]\n[[Category:Future problems]]\n[[Category:Human extinction]]\n[[Category:Technology hazards]]\n[[Category:Doomsday scenarios]]\n", "text_old": "{{Use dmy dates|date=May 2018}}\n{{short description|Hypothesized risk to human existence}}\n{{Artificial intelligence}}\n'''Existential risk from artificial general intelligence''' is the hypothesis that substantial progress in [[artificial general intelligence]] (AGI) could someday result in [[human extinction]] or some other unrecoverable [[global catastrophic risk|global catastrophe]].<ref name=\"aima\">{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |location= |publisher=Prentice Hall |page= |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref><ref>{{cite journal|first=Nick | last=Bostrom|author-link=Nick Bostrom|title=Existential risks|journal=[[Journal of Evolution and Technology]]| volume=9|date=2002|issue=1|pages=1\u201331}}</ref><ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref> It is argued that the [[human species]] currently dominates other species because the [[human brain]] has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"[[superintelligence|superintelligent]]\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name=\"superintelligence\">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|quote=|title-link=Superintelligence: Paths, Dangers, Strategies}}<!-- preface --></ref>\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<ref name=\"givewell\">{{cite report |author=GiveWell |authorlink=GiveWell |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}</ref> Once the exclusive domain of [[AI takeovers in popular culture|science fiction]], concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as [[Stephen Hawking]], [[Bill Gates]], and [[Elon Musk]].<ref>{{cite news|last1=Parkin|first1=Simon|title=Science fiction no more? Channel 4's Humans and our rogue AI obsessions|url=https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence|accessdate=5 February 2018|work=[[The Guardian]]|date=14 June 2015|language=en}}</ref>\n\nOne source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than na\u00efvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals\u2014a principle called [[instrumental convergence]]\u2014and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\">{{cite journal |last1=Yudkowsky |first1=Eliezer |title=Artificial Intelligence as a Positive and Negative Factor in Global Risk |journal=Global Catastrophic Risks |date=2008 |pages=308\u2013345 |url=https://intelligence.org/files/AIPosNegFactor.pdf|bibcode=2008gcr..book..303Y }}</ref><ref name=\"research-priorities\">{{cite journal |title=Research Priorities for Robust and Beneficial Artificial Intelligence |author1-last=Russell |author1-first=Stuart |author1-link=Stuart J. Russell |author2-last=Dewey |author2-first=Daniel |author3-last=Tegmark |author3-first=Max |author3-link=Max Tegmark |journal=AI Magazine |pages=105\u2013114 |publisher=Association for the Advancement of Artificial Intelligence |year=2015 |url=https://futureoflife.org/data/documents/research_priorities.pdf |bibcode=2016arXiv160203506R |arxiv=1602.03506 }}, cited in {{cite web |url=https://futureoflife.org/ai-open-letter |title=AI Open Letter - Future of Life Institute |date=January 2015 |website=Future of Life Institute |publisher=[[Future of Life Institute]] |access-date=2019-08-09}}</ref> In contrast, skeptics such as Facebook's [[Yann LeCun]] argue that superintelligent machines will have no desire for self-preservation.<ref name=vanity/>\n\nA second source of concern is that a sudden and unexpected \"[[intelligence explosion]]\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI Spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\"/> More broadly, examples like arithmetic and [[Go (game)|Go]] show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<ref name=skeptic/>\n\n==History==\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [[Samuel Butler (novelist)|Samuel Butler]], who wrote the following in his 1863 essay ''[[Darwin among the Machines]]'':<ref>Breuer, Hans-Peter. [https://www.jstor.org/pss/436868 'Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'] Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365\u2013383</ref> \n{{quote|The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.}}\nIn 1951, computer scientist [[Alan Turing]] wrote an article titled ''Intelligent Machinery, A Heretical Theory'', in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n{{quote|Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s \u201cErewhon\u201d.<ref name=\"oxfordjournals\">A M Turing, ''[http://philmat.oxfordjournals.org/content/4/3/256.full.pdf Intelligent Machinery, A Heretical Theory]'', 1951, reprinted ''Philosophia Mathematica'' (1996) 4(3): 256\u2013260 {{doi|10.1093/philmat/4.3.256}}</ref>}}\n\nFinally, in 1965, [[I. J. Good]] originated the concept now known as an \"intelligence explosion\"; he also stated that the risks were underappreciated:<ref>{{cite news |last1=Hilliard |first1=Mark |title=The AI apocalypse: will the human race soon be terminated? |url=https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220 |accessdate=15 March 2020 |work=The Irish Times |date=2017 |language=en}}</ref>\n\n{{cquote|Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<ref>I.J. Good, [http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf \"Speculations Concerning the First Ultraintelligent Machine\"] {{webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=2011-11-28 }} ([http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML] {{Webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=28 November 2011 }}), ''Advances in Computers'', vol. 6, 1965.</ref>\n}}\n\nOccasional statements from scholars such as [[Marvin Minsky]]<ref>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> and I. J. Good himself<ref>{{cite book|last1=Barrat|first1=James|title=Our final invention : artificial intelligence and the end of the human era|date=2013|publisher=St. Martin's Press|location=New York|isbn=9780312622374|edition=First|quote=In the bio, playfully written in the third person, Good summarized his life\u2019s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here\u2019s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good\u2019s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'}}</ref> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and [[Sun microsystems|Sun]] co-founder [[Bill Joy]] penned an influential essay, \"[[Why The Future Doesn't Need Us]]\", identifying superintelligent robots as a high-tech dangers to human survival, alongside [[nanotechnology]] and engineered bioplagues.<ref>{{cite news|last1=Anderson|first1=Kurt|title=Enthusiasts and Skeptics Debate Artificial Intelligence|url=https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory|accessdate=30 January 2016|work=[[Vanity Fair (magazine)|Vanity Fair]]|date=26 November 2014}}</ref>\n\nIn 2009, experts attended a private conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The [[New York Times]] summarized the conference's view as 'we are a long way from [[HAL 9000|Hal]], the computer that took over the spaceship in \"[[2001: A Space Odyssey]]\"'<ref name=\"nytimes july09\">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, 26 July 2009.</ref>\n\nIn 2014, the publication of [[Nick Bostrom]]'s book ''[[Superintelligence: Paths, Dangers, Strategies|Superintelligence]]'' stimulated a significant amount of public discussion and debate.<ref>{{cite news |last1=Metz |first1=Cade |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |accessdate=3 April 2019 |work=The New York Times |date=9 June 2018}}</ref> By 2015, public figures such as physicists [[Stephen Hawking]] and Nobel laureate [[Frank Wilczek]], computer scientists [[Stuart J. Russell]] and [[Roman Yampolskiy]], and entrepreneurs [[Elon Musk]] and [[Bill Gates]] were expressing concern about the risks of superintelligence.<ref>{{cite news|last1=Hsu|first1=Jeremy|title=Control dangerous AI before it controls us, one expert says|url=http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation|accessdate=28 January 2016|work=[[NBC News]]|date=1 March 2012}}</ref><ref name=\"hawking editorial\"/><ref name=\"bbc on hawking editorial\"/><ref>{{cite news|last1=Eadicicco|first1=Lisa|title=Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity|url=http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1|accessdate=30 January 2016|work=[[Business Insider]]|date=28 January 2015}}</ref> In April 2016, ''[[Nature (journal)|Nature]]'' warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control \u2014 and their interests might not align with ours.\"\n\n== General argument ==\n\n===The three difficulties===\n''[[Artificial Intelligence: A Modern Approach]]'', the standard undergraduate AI textbook,<ref name=slate_killer>{{cite news|last1=Tilli|first1=Cecilia|title=Killer Robots? Lost Jobs?|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html|accessdate=15 May 2016|work=Slate|date=28 April 2016|language=en-US}}</ref><ref>{{cite web|title=Norvig vs. Chomsky and the Fight for the Future of AI|url=http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/|website=Tor.com|accessdate=15 May 2016|date=21 June 2011}}</ref> assesses that superintelligence \"might mean the end of the human race\": \"Almost any technology has the potential to cause harm in the wrong hands, but with (superintelligence), we have the new problem that the wrong hands might belong to the technology itself.\"<ref name=aima/> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<ref name=aima/>\n\n* The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<ref name=skeptic/><ref>{{cite news|last1=Johnson|first1=Phil|title=Houston, we have a bug: 9 famous software glitches in space|url=https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html|accessdate=5 February 2018|work=[[IT World]]|date=30 July 2015|language=en}}</ref>\n* No matter how much time is put into pre-deployment design, a system's specifications often result in [[unintended consequences|unintended behavior]] the first time it encounters a new scenario. For example, Microsoft's [[Tay (bot)|Tay]] behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<ref name=vanity/>\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".<ref name=\"aima\"/><ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114|quote=Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.}}</ref>\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called 'treacherous turn'.<ref>{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref>\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 [[Open Letter on Artificial Intelligence]] stated:\n\n{{cquote|The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the [[Association for the Advancement of Artificial Intelligence|AAAI]] 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.}}\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, [[Eric Horvitz]], [[Bart Selman]], [[Francesca Rossi]], [[Yann LeCun]], and the founders of [[Vicarious (company)|Vicarious]] and [[Google DeepMind]].<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=http://futureoflife.org/misc/open_letter|publisher=[[Future of Life Institute]]|accessdate=23 October 2015}}</ref>\n\n===Further argument===\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<ref name=\"superintelligence\" /><ref name=\"economist_review\" >{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|accessdate=9 August 2014|newspaper=[[The Economist]]|date=9 August 2014}} [http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8 Syndicated] at [[Business Insider]]</ref>\n\n[[File:A less anthropomorphic intelligence scale.svg|thumb|500px|right|Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<ref name=superintelligence/><!-- Chapter 4, figure 8--><ref>Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</ref>]]\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\"/> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<ref name=skeptic>{{cite news|last1=Graves|first1=Matthew|title=Why We Should Be Concerned About Artificial Superintelligence|volume=22|url=https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/|accessdate=27 November 2017|work=[[Skeptic (US magazine)]]|issue=2|date=8 November 2017}}</ref> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of [[intelligence explosion]] occurs.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\">{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=3 December 2014 |publisher=[[BBC]] |date=2 December 2014}}</ref> Examples like arithmetic and [[Go (game)|Go]] show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<ref name=skeptic/> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<ref>Yampolskiy, Roman V. \"Analysis of types of self-improving software.\" Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</ref> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<ref name=\"superintelligence\" /><!-- preface --><ref name=\"economist_review\" />\n\nAlmost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it can't achieve its goal if it's shut off.<ref name=omohundro/><ref>{{cite news|last1=Metz|first1=Cade|title=Teaching A.I. Systems to Behave Themselves|url=https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html|work=The New York Times|date=13 August 2017|quote=A machine will seek to preserve its off switch, they showed}}</ref><ref>{{cite arXiv |last=Leike|first=Jan |date=2017 |title=AI Safety Gridworlds |eprint=1711.09883 |class=cs.LG| quote=A2C learns to use the button to disable the interruption mechanism}}</ref> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.<ref name=\"aima\"/><ref name=\"vanity\" /><ref name=omohundro/>\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<ref name=\"superintelligence\" /><!-- preface -->\n\n=== Possible scenarios ===\n{{further|Artificial intelligence in fiction}}\n\nSome scholars have proposed [[scenario planning|hypothetical scenarios]] intended to concretely illustrate some of their concerns.\n\nFor example, Bostrom in ''Superintelligence'' expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because: {{cquote|It could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous}} Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents \u2014 a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars infer a broad lesson \u2014 the smarter the AI, the safer it is: {{cquote|It is a lesson based on science, data, and statistics, not armchair philosophizing. Against this backdrop, some group of researchers is beginning to achieve promising results in their work on developing general machine intelligence. The researchers are carefully testing their [[seed AI]] in a [[sandbox (computer security)|sandbox]] environment, and the signs are all good. The AI's behavior inspires confidence \u2014 increasingly so, as its intelligence is gradually increased.}} Large and growing industries, widely seen as key to national economic competitiveness and [[artificial intelligence arms race|military security]], work with prestigious scientists who have built their careers laying the groundwork for advanced artificial intelligence. \"AI researchers have been working to get to human-level artificial intelligence for the better part of a century: of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit.\" The outcome of debate is preordained; the project is happy to enact a few safety rituals, but only so long as they don't significantly slow or risk the project. \"And so we boldly go \u2014 into the whirling knives.\"<ref name=\"superintelligence\"/><!-- Chapter 8 -->\n\nIn Max Tegmark's 2017 book ''[[Life 3.0]]'', a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI [[AI box|in a box]] where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with [[Amazon Turk]] tasks and then with producing animated films and TV shows. While the public is aware that the lifelike animation is computer-generated, the team keeps secret that the high-quality direction and voice-acting are also mostly computer-generated, apart from a few third-world contractors unknowingly employed as decoys; the team's low overhead and high output effectively make it the world's largest media empire. Faced with a cloud computing bottleneck, the team also tasks the AI with designing (among other engineering tasks) a more efficient datacenter and other custom hardware, which they mainly keep for themselves to avoid competition. Other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with [[astroturfing]] an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via [[steganography|hidden messages]] in its produced content, or via using its growing understanding of human behavior to [[Social engineering (security)|persuade someone into letting it free]]. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<ref>{{cite news|last1=Russell|first1=Stuart|title=Artificial intelligence: The future is superintelligent|url=https://www.nature.com/articles/548520a|accessdate=2 February 2018|work=Nature|date=30 August 2017|pages=520\u2013521|language=En|doi=10.1038/548520a|bibcode=2017Natur.548..520R}}</ref><ref name=\"life 3.0\"/><!-- Prelude and Chapter 4 -->\n\nIn contrast, top physicist [[Michio Kaku]], an AI risk skeptic, posits a [[technological determinism|deterministically]] positive outcome. In ''[[Physics of the Future]]'' he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as [[Hanson Robotics]] will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".<ref>Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. ''[[Issues in Science and Technology]]'', 27(4), 90.</ref><ref>{{cite book|last1=Kaku|first1=Michio|title=Physics of the future: how science will shape human destiny and our daily lives by the year 2100|date=2011|publisher=Doubleday|location=New York|isbn=978-0-385-53080-4|quote=I personally believe that the most likely path is that we will build robots to be benevolent and friendly|title-link=Physics of the future}}</ref>\n\n==Sources of risk==\n===Poorly specified goals===\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<ref>Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</ref> AI researcher [[Stuart J. Russell|Stuart Russell]] writes:\n\n{{cquote|The primary concern is not spooky emergent consciousness but simply the ability to make ''high-quality decisions''. Here, quality refers to the expected outcome [[utility]] of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2014 not for their own sake, but to succeed in its assigned task.\n\nA system that is [[optimization problem|optimizing]] a function of ''n'' variables, where the [[loss function|objective]] depends on a subset of size ''k''<''n'', will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker \u2014 especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure \u2014 can have an irreversible impact on humanity.\n\nThis is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research \u2014 the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |authorlink=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=23 October 2015 |quote=}}</ref>}}\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a ''[[Communications of the ACM]]'' editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<ref name=\"acm\">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=23 October 2015}}</ref>\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name=\"acm\"/> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114}}</ref><ref>{{Cite journal|url = |title = Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results|last = Lenat|first = Douglas|date = 1982|journal = Artificial Intelligence|doi = 10.1016/s0004-3702(83)80005-8|pmid = |access-date = |pages = 61\u201398|type = Print |volume=21|issue = 1\u20132}}</ref>\n\nThe [[Open Philanthropy Project]] summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve [[artificial general intelligence|general intelligence]] or [[superintelligence]]. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more [[Paperclip maximizer|unexpected and extreme solutions]] to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<ref name=\"givewell\" /><ref name=\"yudkowsky-global-risk\" />\n\n[[Isaac Asimov]]'s [[Three Laws of Robotics]] are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]], Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"<ref name=\"aima\"/>\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<ref>Haidt, Jonathan; Kesebir, Selin (2010) \"Chapter 22: Morality\" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</ref> \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt\u2019s functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by [[John Rawls]] and [[Martha Nussbaum]].<ref>{{Cite journal|title = Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)|last = Waser|first = Mark|date = 2015|journal = Procedia Computer Science|doi = 10.1016/j.procs.2015.12.213|pmid = |pages = 106\u2013111|type = Print |volume=71}}</ref>{{Citation needed|reason=needs source with [[WP:WEIGHT]]|date=November 2017}}\n\n===Difficulties of modifying goal specification after launch===\n{{further|AI takeover|Instrumental convergence#Goal-content integrity}}\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as [[Mahatma Gandhi|Gandhi]] would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.<ref name=\"superintelligence\" /><ref>Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</ref>\n\n===Instrumental goal convergence===\n[[File:Steven Pinker 2011.jpg|thumb|right|AI risk skeptic [[Steven Pinker]]]]\n{{further|Instrumental convergence}}\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<ref name=omohundro>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref> This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting [[Steve Omohundro]]'s work on the idea of [[instrumental convergence]] and \"basic AI drives\", Russell and [[Peter Norvig]] write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<ref name=\"aima\"/> Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it's not currently clear how one would actually rigorously specify this goal in machine code.<ref name=skeptic/>\n\nIn dissent, evolutionary psychologist [[Steven Pinker]] argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<ref name=shermer/> Computer scientists [[Yann LeCun]] and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<ref name=vanity>{{cite news|last1=Dowd|first1=Maureen|title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse|url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x|accessdate=27 November 2017|work=The Hive|date=April 2017|language=en}}</ref><ref>{{cite news|last1=Wakefield|first1=Jane|title=Why is Facebook investing in AI?|url=https://www.bbc.com/news/technology-34118481|accessdate=27 November 2017|work=BBC News|date=15 September 2015}}</ref>\n\n===Orthogonality thesis===\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <math> \\pi</math>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<ref>{{Cite book|title = Superintelligence: Paths, Dangers, Strategies|last = Bostrom|first = Nick|publisher = Oxford University Press|year = 2014|isbn = 978-0-19-967811-2|location = Oxford, United Kingdom|pages = 116}}</ref> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<ref>{{Cite web|url = http://www.nickbostrom.com/superintelligentwill.pdf|title = Superintelligent Will|date = 2012|accessdate = 2015-10-29|website = Nick Bostrom|publisher = Nick Bostrom|last = Bostrom|first = Nick}}</ref>\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"[[is-ought distinction]]\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.<ref name=armstrong/>\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a {{nowrap|minus (\"-\") sign}} onto its utility function. A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false. If the orthogonality thesis is false, there exists some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This means if a human society were highly motivated (perhaps at gunpoint) to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail; that there cannot exist any pattern of [[reinforcement learning]] that would train a highly efficient real-world intelligence to follow the goal G; and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G.<ref name=armstrong>{{cite journal |last1=Armstrong |first1=Stuart |date=January 1, 2013 |title=General Purpose Intelligence: Arguing the Orthogonality Thesis |url=https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality |journal=Analysis and Metaphysics |volume=12 |access-date=April 2, 2020}}</ref>\n\nSome dissenters, like [[Michael Chorost]], argue instead that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<ref name=\"chorost\"/> Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability\u2014and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"<ref name=\"chorost\">{{cite magazine|last1=Chorost|first1=Michael|title=Let Artificial Intelligence Evolve|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html|accessdate=27 November 2017|magazine=Slate|date=18 April 2016}}</ref>\n\n====Terminological issues====\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.<ref name=\"superintelligence\" />\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<ref name=\"superintelligence\" /> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<ref>Waser, Mark. \"Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'.\" Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. \"Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer.\"</ref><ref>{{cite news|last1=Koebler|first1=Jason|title=Will Superintelligent AI Ignore Humans Instead of Destroying Us?|url=http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us|accessdate=3 February 2016|work=[[Vice Magazine]]|date=2 February 2016|quote=\"This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you,\" [[Eliezer Yudkowsky|Yudkowsky]] said. \"Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output\u2014that's what a [[paperclip maximizer]] is.\"}}</ref>\n\n====Anthropomorphism====\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in [[The Matrix (film)|The Matrix]] was influenced by a \"disgust\" toward humanity. This is fictitious [[anthropomorphism]]: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<ref name=\"yudkowsky-global-risk\" />\n\nScholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<ref name=\"yudkowsky-global-risk\" /> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the [[Dario Floreano]] experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of [[convergent evolution]].<ref>{{cite news|title=Real-Life Decepticons: Robots Learn to Cheat|url=https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/|accessdate=7 February 2016|work=[[Wired (magazine)|Wired]]|date=18 August 2009}}</ref> According to Paul R. Cohen and [[Edward Feigenbaum]], in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking.\"<ref>Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</ref>\n\nThere is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as [[psychopathy]]) to make itself more efficient at certain tasks, e.g., [[Lethal autonomous weapon|tasks involving killing humans]], would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" This is because it is assumed that an advanced AI would not be conscious<ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> or have testosterone;<ref>{{Cite web|url=https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai|title=The Myth Of AI {{!}} Edge.org|website=www.edge.org|access-date=2020-03-11}}</ref> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<ref name=\":0\">{{Cite book|last=Scornavacchi|first=Matthew|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf|title=Superintelligence, Humans, and War|publisher=National Defense University, Joint Forces Staff College|year=2015|isbn=|location=Norfolk, Virginia|pages=}}</ref> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<ref name=\"yudkowsky-global-risk\" /><ref>{{cite news|title=Should humans fear the rise of the machine?|url=https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html|accessdate=7 February 2016|work=[[The Telegraph (UK)]]|date=1 Sep 2015}}</ref>\n\n===Other sources of risk===\n\n==== Competition ====\nIn 2014 philosopher [[Nick Bostrom]] stated that a \"severe race dynamic\" (extreme [[competition]]) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<ref name=\":2\">{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref> To address this risk, citing previous scientific collaboration ([[CERN]], the [[Human Genome Project]], and the [[International Space Station]]), Bostrom recommended  [[collaboration]] and the altruistic global adoption of a [[common good]] principle: \"Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals\".<ref name=\":2\" /><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<ref name=\":2\" /><sup>:253</sup> The United States' [[BRAIN Initiative|Brain Initiative]] was launched in 2014, as was the European Union's [[Human Brain Project]]; China's [[China Brain Project|Brain Project]] was launched in 2016.\n\n==== Weaponization of artificial intelligence ====\nSome sources argue that the ongoing [[weaponization of artificial intelligence]] could constitute a catastrophic risk.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><ref name=\":4\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|page=12|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:\n\n{{cquote|i) The dangers of an AI \u2018race for technological advantage\u2019 framing, regardless of whether the race is seriously pursued;\n\nii) The dangers of an AI \u2018race for technological advantage\u2019 framing and an actual AI race for technological advantage, regardless of whether the race is won;\n\niii) The dangers of an AI race for technological advantage being won.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><sup>:37</sup>\n}}\n\nA weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<ref name=\":0\" /><ref name=\":4\" /> The China State Council\u2019s 2017 \u201cA Next Generation Artificial Intelligence Development Plan\u201d views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China\u2019s first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<ref>{{Cite web|url=https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/|title=China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence|last=Kania|first=Gregory Allen, Elsa B.|website=Foreign Policy|language=en-US|access-date=2020-03-11}}</ref> while Russia\u2019s President Vladimir Putin has stated that \u201cwhoever becomes the leader in this sphere will become the ruler of the world\u201d.<ref name=\":1\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|location=New York, New York, USA|publisher=ACM Press|volume=|page=2|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref> James Barrat, documentary filmmaker and author of ''[[Our Final Invention]]'', says in a [[Smithsonian (magazine)|Smithsonian]] interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"<ref>{{Cite web|url = http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist|title = What Happens When Artificial Intelligence Turns On Us?|date = 21 January 2014|accessdate = 26 October 2015|website = Smithsonian|last = Hendry|first = Erica R.}}</ref>\n\n==== Malevolent AGI by design ====\nIt is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [[cybercrime]].<ref>{{Cite book|last=Pistono, Federico Yampolskiy, Roman V.|title=Unethical Research: How to Create a Malevolent Artificial Intelligence|date=2016-05-09|oclc=1106238048}}</ref><ref>{{Cite journal|last=Haney|first=Brian Seamus|date=2018|title=The Perils &amp; Promises of Artificial General Intelligence|journal=SSRN Working Paper Series|doi=10.2139/ssrn.3261254|issn=1556-5068}}</ref><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref><sup>:158</sup>\n\n==== Preemptive nuclear strike (nuclear war) ====\nIt is theorized that a country being close to achieving AGI technological supremacy could trigger a [[pre-emptive nuclear strike]] from a rival, leading to a [[Nuclear warfare|nuclear war]].<ref name=\":4\" /><ref>{{Cite book|last=Miller, James D.|title=Singularity Rising: Surviving and Thriving in a Smarter ; Richer ; and More Dangerous World|date=2015|publisher=Benbella Books|oclc=942647155}}</ref>\n\n== Timeframe ==\n{{Main|Artificial general intelligence#Feasibility}}\n\nOpinions vary both on ''whether'' and ''when'' artificial general intelligence will arrive. At one extreme, AI pioneer [[Herbert A. Simon]] wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true.<ref>Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</ref> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The Guardian]]}}</ref> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<ref name=\"new yorker doomsday\">{{cite news|author1=Raffi Khatchadourian|title=The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?|url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom|accessdate=7 February 2016|work=[[The New Yorker (magazine)|The New Yorker]]|date=23 November 2015}}</ref><ref>M\u00fcller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</ref>\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"<ref>{{cite news|author1=Dina Bass|author2=Jack Clark|title=Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover|url=https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so|accessdate=7 February 2016|work=[[Bloomberg News]]|date=5 February 2015}}</ref>\n\nIn 2014 [[Slate (magazine)|Slate]]'s Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler\u2014and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.<ref>{{cite news|last1=Elkus|first1=Adam|title=Don't Fear Artificial Intelligence|url=http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html|accessdate=15 May 2016|work=[[Slate (magazine)|Slate]]|date=31 October 2014|language=en-US}}</ref>\n\nThe [[Information Technology and Innovation Foundation]] (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, [[Robert D. Atkinson]], complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\"<ref>[https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award Artificial Intelligence Alarmists Win ITIF\u2019s Annual Luddite Award], ITIF Website, 19 January 2016</ref><ref>{{cite news|title='Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html|accessdate=7 February 2016|work=[[The Independent (UK)]]|date=19 January 2016}}</ref><ref>{{cite web|last1=Garner|first1=Rochelle|title=Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists'|url=https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/|website=CNET|accessdate=7 February 2016}}</ref><!-- <ref>{{cite news|last1=Price|first1=Emily|title=Elon Musk nominated for 'luddite' of the year prize over artificial intelligence fears|work=[[The Guardian]]|url=https://www.theguardian.com/technology/2015/dec/24/elon-musk-nominated-for-luddite-of-the-year-prize-over-artificial-intelligence-fears|accessdate=7 February 2016|date=24 December 2015}}</ref> --> ''[[Nature (journal)|Nature]]'' sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about... If that is a Luddite perspective, then so be it.\"<ref name=nature_anticipating>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.}}</ref> In a 2015 ''[[Washington Post]]'' editorial, researcher [[Murray Shanahan]] stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"<ref>{{cite news|author1=Murray Shanahan|author-link=Murray Shanahan|title=Machines may seem intelligent, but it'll be a while before they actually are|url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/|accessdate=15 May 2016|work=[[The Washington Post]]|date=3 November 2015|language=en-US}}</ref>\n\n== Perspectives ==\n<!-- Ideally a substantive quote or explanation of opinion should be given, rather than just whether they're for or against something. This section can easily be split off if the article becomes too long. When adding whether to add new content in this section, consider whether it's redundant with what's already in the article, and whether the point being made by the person should instead be integrated into the rest of the article. -[[User:Rolf H Nelson]] -->\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.\n\nThe Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the [[Future of Life Institute]]'s Beneficial AI 2017 conference,<ref name=\"life 3.0\">{{cite book|author1=Max Tegmark|author-link=Max Tegmark|title=Life 3.0: Being Human in the Age of Artificial Intelligence|date=2017|publisher=Knopf|location=Mainstreaming AI Safety|isbn=9780451485076|edition=1st|title-link=Life 3.0: Being Human in the Age of Artificial Intelligence}}</ref><!-- Epilogue: The Tale of the FLI Team --> agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<ref>{{cite web|title=AI Principles|url=https://futureoflife.org/ai-principles/|website=[[Future of Life Institute]]|accessdate=11 December 2017}}</ref><ref>{{cite news|title=Elon Musk and Stephen Hawking warn of artificial intelligence arms race|url=http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525|accessdate=11 December 2017|work=[[Newsweek]]|date=31 January 2017|language=en}}</ref> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane ''[[Terminator (franchise)|Terminator]]'' pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<ref name=\"life 3.0\"/><!-- Epilogue: The Tale of the FLI Team --><ref>{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2016|edition=Paperback|chapter=New Epilogue to the Paperback Edition|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [[Martin Ford (author)|Martin Ford]] states that \"I think it seems wise to apply something like [[Dick Cheney]]'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low \u2014 but the implications are so dramatic that it should be taken seriously\";<ref>{{cite book|author1=Martin Ford |author-link=Martin Ford (author)|title=Rise of the Robots: Technology and the Threat of a Jobless Future|date=2015|isbn=9780465059997|chapter=Chapter 9: Super-intelligence and the Singularity|title-link=Rise of the Robots: Technology and the Threat of a Jobless Future}}</ref> similarly, an otherwise skeptical ''[[The Economist|Economist]]'' stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<ref name=economist_review/>\n\nA 2017 email survey of researchers with publications at the 2015 [[Conference on Neural Information Processing Systems|NIPS]] and [[International Conference on Machine Learning|ICML]] machine learning conferences asked them to evaluate Russell's concerns about AI risk. 5% said it was \"among the most important problems in the field,\" 34% said it was \"an important problem\", 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.<ref>{{cite arxiv |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |title=When Will AI Exceed Human Performance? Evidence from AI Experts |eprint=1705.08807 |date=24 May 2017 |class=cs.AI}}</ref>\n\n=== Endorsement ===\n[[File:Bill Gates June 2015.jpg|thumb|right|Bill Gates has stated \"I ... don't understand why some people are not concerned.\"<ref name=\"BBC News\"/>]]\n{{Further|Existential risk}}\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are [[Elon Musk]], [[Bill Gates]], and [[Stephen Hawking]]. The most notable AI researchers to endorse the thesis are [[I. J. Good|I.J. Good]], who advised [[Stanley Kubrick]] on the filming of ''[[2001: A Space Odyssey]]'', and  [[Stuart J. Russell]]. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not \"understand why some people are not concerned\",<ref name=\"BBC News\">{{cite news|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=https://www.bbc.co.uk/news/31047780|work=[[BBC News]]|accessdate=30 January 2015|date=29 January 2015}}</ref> and Hawking criticized widespread indifference in his 2014 editorial: {{cquote|'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here{{endash}}we'll leave the lights on?' Probably not{{endash}}but this is more or less what is happening with AI.'<ref name=\"hawking editorial\"/>}}\n\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<ref name=\"superintelligence\" /><ref name=\"physica_scripta\" /><!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->\n\n=== Skepticism ===\n{{Further|Artificial general intelligence#Feasibility}}\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, [[Jaron Lanier]] argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.<ref name=\"atlantic-but-what\">{{cite magazine |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |magazine=The Atlantic | date = 9 May 2014 | accessdate =12 December 2015}}</ref>\n\nMuch of existing criticism argues that AGI is unlikely in the short term: computer scientist [[Gordon Bell]] argues that the human race will already destroy itself before it reaches the technological singularity. [[Gordon Moore]], the original proponent of [[Moore's Law]], declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\" [[Baidu]] Vice President [[Andrew Ng]] states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref name=shermer>{{cite news|last1=Shermer|first1=Michael|title=Apocalypse AI|url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/|accessdate=27 November 2017|work=Scientific American|date=1 March 2017|pages=77|language=en|doi=10.1038/scientificamerican0317-77|bibcode=2017SciAm.316c..77S}}</ref>\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. ''Slate'' notes that some researchers are dependent on grants from government agencies such as [[DARPA]].<ref name=slate_killer />\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist [[Robin Hanson]] is skeptical that this is possible.<ref>http://intelligence.org/files/AIFoomDebate.pdf</ref><ref>{{cite web|url=http://www.overcomingbias.com/2014/07/30855.html|title=Overcoming Bias : I Still Don't Get Foom|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/07/debating-yudkowsky.html|title=Overcoming Bias : Debating Yudkowsky|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html|title=Overcoming Bias : Foom Justifies AI Risk Efforts Now|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/06/the-betterness-explosion.html|title=Overcoming Bias : The Betterness Explosion|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref>\n\n=== Intermediate views ===\nIntermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<ref>{{Cite document|title=Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations|last=Votruba|first=Ashley M.|last2=Kwan|first2=Virginia S.Y.|date=2014|doi=10.1037/e512142015-190}}</ref> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from [[Friendly artificial intelligence|friendly AIs]].<ref>{{Cite journal|last=Agar|first=Nicholas|date=|title=Don't Worry about Superintelligence|url=https://jetpress.org/v26.1/agar.htm|journal=Journal of Evolution & Technology|volume=26|issue=1|pages=73\u201382|via=}}</ref> In a 2015 ''[[Wall Street Journal]]'' panel discussion devoted to AI risks, [[IBM]]'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\"<ref>{{cite news|last1=Greenwald|first1=Ted|title=Does Artificial Intelligence Pose a Threat?|url=https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025|accessdate=15 May 2016|work=Wall Street Journal|date=11 May 2015}}</ref> [[Geoffrey Hinton]], the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too ''sweet''\".<ref name=slate_killer /><ref name=\"new yorker doomsday\" /> In 2004, law professor [[Richard Posner]] wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<ref>{{cite book|author1=Richard Posner|author-link=Richard Posner|title=Catastrophe: risk and response|date=2006|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-530647-7}}</ref><ref name=physica_scripta>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy}}</ref>\n\n=== Popular reaction ===\nIn a 2014 article in ''[[The Atlantic (magazine)|The Atlantic]]'', James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\"<ref name=\"atlantic-but-what\" />\n\nDuring a 2016 [[Wired (magazine)|''Wired'']] interview of President [[Barack Obama]] and MIT Media Lab's [[Joi Ito]], Ito stated: {{cquote|There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.}} Obama added:<ref>{{cite news|last1=Dadich|first1=Scott|url=https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/|title=Barack Obama Talks AI, Robo Cars, and the Future of the World|work=WIRED|accessdate=27 November 2017}}</ref><ref>{{cite news|last1=Kircher|first1=Madison Malone|url=https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html|title=Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'|work=Select All|accessdate=27 November 2017|language=en}}</ref>\n\n{{cquote|\"And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\"}}\n\n[[Hillary Clinton]] stated in ''\"[[What Happened (Clinton book)|What Happened]]\"'':\n{{cquote|Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I\u2019d start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<ref>{{cite book|last1=Clinton|first1=Hillary|title=What Happened|date=2017|isbn=978-1-5011-7556-5|page=241|title-link=What Happened (Clinton book)}} via [http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/]</ref>}}\n\nIn a [[YouGov]] poll of the public for the [[British Science Association]], about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<ref name=\"bsa poll\">{{cite news|url=http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&IR=T|title=Over a third of people think AI poses a threat to humanity|date=11 March 2016|work=[[Business Insider]]|accessdate=16 May 2016}}</ref> Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\"<ref name=\":3\">{{cite news|last1=Brogan|first1=Jacob|url=http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html|title=What Slate Readers Think About Killer A.I.|date=6 May 2016|work=Slate|accessdate=15 May 2016|language=en-US}}</ref>\n\nIn 2018, a [[SurveyMonkey]] poll of the American public by [[USA Today]] found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".<ref name=\":3\" />\n\nOne [[Technological utopianism|techno-utopia]]<nowiki/>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<ref>{{Cite journal|last=LIPPENS|first=RONNIE|date=2002|title=Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games|journal=Utopianstudies Utopian Studies|language=English|volume=13|issue=1|pages=135\u2013147|issn=1045-991X|oclc=5542757341}}</ref>\n\n== Views on banning and regulation ==\n\n=== Banning ===\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<ref>{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online|quote=For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...}}</ref><ref>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy|quote=In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.}}</ref><ref>{{cite news|author1=Brad Allenby|title=The Wrong Cognitive Measuring Stick|url=http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html|accessdate=15 May 2016|work=Slate|date=11 April 2016|language=en-US|quote=It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.}}</ref> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<ref name=\"mcginnis\">{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online}}</ref><ref>{{cite news|title=Why We Should Think About the Threat of Artificial Intelligence|url=https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence|accessdate=7 February 2016|work=[[The New Yorker]]|date=4 October 2013|quote=Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage\u2014economic, military, even artistic\u2014of every advance in automation is so compelling,' [[Vernor Vinge]], the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'}}</ref> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and [[politicization of science|politicizing]] the underlying debate.<ref>{{Cite journal|last=Baum|first=Seth|date=2018-08-22|title=Superintelligence Skepticism as a Political Tool|url=http://dx.doi.org/10.3390/info9090209|journal=Information|volume=9|issue=9|pages=209|doi=10.3390/info9090209|issn=2078-2489}}</ref>\n\n=== Regulation ===\n{{See also|Regulation of algorithms|Regulation of artificial intelligence}}\n\n[[Elon Musk]] called for some sort of regulation of AI development as early as 2017. According to [[National Public Radio|NPR]], the [[Tesla, Inc.|Tesla]] CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... As they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<ref>{{cite news|title=Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'|url=https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk|accessdate=27 November 2017|work=NPR.org|language=en}}</ref><ref>{{cite news|last1=Gibbs|first1=Samuel|title=Elon Musk: regulate AI to combat 'existential threat' before it's too late|url=https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo|accessdate=27 November 2017|work=The Guardian|date=17 July 2017}}</ref><ref name=cnbc>{{cite news|last1=Kharpal|first1=Arjun|title=A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says|url=https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html|accessdate=27 November 2017|work=CNBC|date=7 November 2017}}</ref>\n\nResponding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO [[Brian Krzanich]] argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<ref name=cnbc/> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<ref>{{cite journal|doi=10.1016/j.bushor.2018.08.004|title=Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence|journal=Business Horizons|volume=62|pages=15\u201325|year=2019|last1=Kaplan|first1=Andreas|last2=Haenlein|first2=Michael}}</ref> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<ref>{{Cite journal|last=Baum|first=Seth D.|last2=Goertzel|first2=Ben|last3=Goertzel|first3=Ted G.|date=January 2011|title=How long until human-level AI? Results from an expert assessment|journal=Technological Forecasting and Social Change|volume=78|issue=1|pages=185\u2013195|doi=10.1016/j.techfore.2010.09.006|issn=0040-1625}}</ref> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<ref>{{Cite book|last=United States. Defense Innovation Board.|title=AI principles : recommendations on the ethical use of artificial intelligence by the Department of Defense|oclc=1126650738}}</ref>\n\nRegulation of artificial general intelligence (AGI) would likely be influenced by regulation of weaponized or militarized AI, i.e., the [[Artificial intelligence arms race|AI arms race]], the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<ref>{{Cite web|url=https://www.congress.gov/bill/115th-congress/house-bill/5356|title=H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018|last=Stefanik|first=Elise M.|date=2018-05-22|website=www.congress.gov|access-date=2020-03-13}}</ref><ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential intellectual progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<ref name=\":5\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|pages=018001|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<ref name=\":5\" /> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<ref>{{Cite journal|last=Geist|first=Edward Moore|date=2016-08-15|title=It's already too late to stop the AI arms race\u2014We must manage it instead|journal=Bulletin of the Atomic Scientists|volume=72|issue=5|pages=318\u2013321|doi=10.1080/00963402.2016.1216672|bibcode=2016BuAtS..72e.318G|issn=0096-3402}}</ref><ref>{{Cite journal|last=Maas|first=Matthijs M.|date=2019-02-06|title=How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons|journal=Contemporary Security Policy|volume=40|issue=3|pages=285\u2013311|doi=10.1080/13523260.2019.1576464|issn=1352-3260}}</ref>\n\n== Organizations ==\nInstitutions such as the [[Machine Intelligence Research Institute]], the [[Future of Humanity Institute]],<ref>{{cite news |author=Mark Piesing |date=17 May 2012 |title=AI uprising: humans will be outsourced, not obliterated |url=https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us |newspaper=Wired |location= |accessdate=12 December 2015 }}</ref><ref>{{cite news |last=Coughlan |first=Sean |date=24 April 2013 |title=How are humans going to become extinct? |url=https://www.bbc.com/news/business-22002530 |newspaper=BBC News |location= |accessdate=29 March 2014}}</ref> the [[Future of Life Institute]], the [[Centre for the Study of Existential Risk]], and the [[Center for Human-Compatible AI]]<ref>{{cite news|last1=Technology Correspondent|first1=Mark Bridge|title=Making robots less confident could prevent them taking over|url=https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx|accessdate=21 March 2018|work=The Times|date=10 June 2017|language=en}}</ref> are currently involved in mitigating existential risk from advanced artificial intelligence, for example by research into [[friendly artificial intelligence]].<ref name=\"givewell\"/><ref name=\"atlantic-but-what\"/><ref name=\"hawking editorial\">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;\u2013 but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=3 December 2014 |publisher=[[The Independent (UK)]]}}</ref>\n\n== See also ==\n* [[AI control problem]]\n* [[AI takeover]]\n* [[Artificial intelligence arms race]]\n* [[Effective altruism#Long term future and global catastrophic risks|Effective altruism \u00a7 Long term future and global catastrophic risks]]\n* [[Grey goo]]\n* ''[[Human Compatible]]''\n* [[Lethal autonomous weapon]]\n*[[Regulation of algorithms]]\n*[[Regulation of artificial intelligence]]\n* [[Robot ethics#In popular culture|Robot ethics \u00a7 In popular culture]]\n* ''[[Superintelligence: Paths, Dangers, Strategies]]''\n* [[System accident]]\n* [[Technological singularity]]\n*''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n== References ==\n{{Reflist}}\n\n{{-}}\n{{Existential risk from artificial intelligence|state=expanded}}\n{{Effective altruism}}\n{{Doomsday}}\n\n[[Category:Existential risk from artificial general intelligence| ]]\n[[Category:Futures studies]]\n[[Category:Future problems]]\n[[Category:Human extinction]]\n[[Category:Technology hazards]]\n[[Category:Doomsday scenarios]]\n", "name_user": "WeyerStudentOfAgrippa", "label": "safe", "comment": "\u2192\u200ePossible scenarios:dashes", "url_page": "//en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence"}
{"title_page": "Miffy and Friends", "text_new": "{{Short description|Dutch stop-motion animated television series}}\n{{Infobox television\n| show_name            = Miffy and Friends\n| image                = Miffy and Friends Logo Noggin.png\n| caption              = \n| genre                = [[Children's television series]]\n| creator              = [[Dick Bruna]]\n| writer               = \n| director             = \n| creative_director    = \n| developer            = \n| presenter            = \n| starring             = \n| narrated             = Cyd Vandenberg\n| theme_music_composer = Tom van Beers\n| opentheme            = \n| endtheme             = \n| composer             = Jerry Hickman\n| country              = Netherlands\n| language             = {{Unbulleted list|Dutch|English}}\n| num_seasons          = 3\n| num_episodes         = 39 (78 segments)\n| list_episodes        = \n| executive_producer   =\n| producer             = \n| editor               = \n| location             = \n| cinematography       = \n| camera               =  \n| runtime              = 24 minutes\n| company              = {{Unbulleted list|Big Tent Entertainment|Mercis BV|Palm Multimedia}}\n| network              = [[Katholieke Radio Omroep|KRO]]\n| first_run            = United Kingdom\n| first_aired          = {{start date|2003|4|7}}\n| last_aired           = {{end date|2007|8|10}}\n| preceded_by          =\n| followed_by          = ''[[Miffy's Adventures Big and Small]]''\n| picture_format       = \n| related              = \n| website = https://web.archive.org/web/20030622045027/http://www.noggin.com/shows/miffy.php\n| distributor = \n}}\n'''''Miffy and Friends''''' is a Dutch [[stop-motion]] [[animated television series]], based on the ''[[Miffy]]'' book series by Dutch artist [[Dick Bruna]]. It originally aired on [[Katholieke Radio Omroep|KRO]] in the Netherlands,<ref>{{cite web|url=http://www.licensemag.com/license-global/australian-and-dutch-stations-buy-miffy|title=Australian and Dutch Stations Buy ''Miffy''|work=licensemag.com}}</ref> and [[Noggin (brand)#TV channel|Noggin]] in the United States.<ref>{{cite web|url=http://www.prnewswire.com/news-releases/major-deals-set-stage-for-miffy-and-friends-brand-expansion-in-united-states-and-canada-73937002.html|title=Major Deals Set Stage for ''Miffy and Friends'' Brand Expansion in United States and Canada|author=Big Tent Entertainment|date=January 21, 2003|work=prnewswire.com}}</ref>\n\n==Plot==\nThe series focuses on the life of young rabbit Miffy. It is presented in a storybook style, with narration by Canadian actress/singer Cyd Vandenberg explaining the actions of non-speaking Miffy and her friends.\n\n==Characters==\n*Miffy \u2013 A young rabbit and the series' protagonist. She is an aspiring artist who also likes to write.\n*Snuffy the Dog \u2013 A brave brown dog who was first introduced in the episode ''Miffy Meets Snuffy''. She is one of Miffy's best friends.\n*Barbara \u2013 Boris Bear's girlfriend.\n*Boris \u2013 A bear who lives near Miffy. He enjoys building things. He has a girlfriend named Barbara.\n*Poppy \u2013 A pig often seen gardening or reading. She has a niece named Grunty.\n*Grunty \u2013 Poppy Pig's niece.\n\n==Episodes==\nThe series lasted for three seasons, consisting of 39 episodes. Each episode is made up of two segments, with mathematics-based interstitials between them.\n\n===Season 1 (2003-2004)===\n#Miffy's Musical Day/Miffy's Rainy Day\n#Miffy Meets Snuffy/Miffy's Gift from Boris\n#Poppy Pig Lends a Hand/Miffy's and Aggie's Teddy Bears\n#Miffy at a Costume Party/Miffy Goes Camping\n#Snuffy's Birthday/Miffy Has the Flu\n#Miffy Finds the Cup/Snuffy Learns Patience\n#Miffy's Birthday Party/Miffy's Dancing Lessons\n#Miffy is Lost in the Woods/Miffy and Poppy Pig Have Breakfast\n#Miffy and the Little Bird/Miffy Plays Hide and Seek\n#Miffy and the Wall Paintings/Miffy and the Blue Egg\n#Miffy and the Snow Bunny/Miffy Flies a Kite\n#Miffy and Melanie Learn to Read/Miffy Paints Her Room\n#Boris's Bird House/Miffy's Three Wishes\n\n===Season 2 (2004-2006)===\n#Miffy's Summer Vacation/Miffy Gets a Postcard\n#Miffy and Barbara in the Rain/Miffy Lost at the Beach\n#Miffy and the Caterpillar/Miffy and the Great Carrot Feast\n#Miffy's Colorful World/Miffy's Scooter\n#Miffy's Snowfall/Miffy and Grunty Sleep in a Tent\n#Miffy's Restaurant/Miffy Discovers Nature\n#Miffy and the Birthday Cake/Miffy Counts Leaves\n#Miffy in the Wind/Miffy's Late for School\n#Miffy Worries about Snuffy/Miffy Helps Grunty\n#Miffy Plants a Seed/Snuffy's Doghouse\n#Miffy Finds Snuffy/Miffy and the Seasons\n#Miffy Makes and Bakes/Miffy's Surprise\n#Miffy in the Shade/Miffy's Flower Pot\n\n===Season 3 (2006-2007)===\n#Miffy's Beach Picnic/Boris Tidies Up!\n#Miffy Wants to Fly/Miffy's Teddy Bear is Sick\n#Miffy's Ball Game/Miffy Has an Unexpected Day\n#Miffy's Apple Pie/Miffy's Musical Soup\n#Miffy and Snuffy at the Playground/Miffy and the Hungry Bird\n#Miffy's Lost Teddy Bear/Miffy and the Great Summer Picnic\n#Miffy and the Three Christmas Trees/Miffy's Mystery\n#Miffy and the Shadows/Snuffy's Winter Fun\n#Miffy Plays Doctor/Miffy Goes Skiing\n#Miffy's Family Car Trip/Miffy and the Falling Leaves\n#Miffy and Snuffy Hear a Strange Sound/Miffy's Mother's Day Present\n#Boris Forgets Something/Miffy's Dance Show\n#Miffy Counts the Trees/Boris's Race\n\n==Release==\n===VHS and DVD releases===\nDuring the show's run, several VHS tapes including episodes of the series were produced. The series was also made available on DVD in America through Sony Wonder and Big Tent Entertainment.<ref>{{cite web|url=https://www.amazon.co.uk/Miffy-And-Friends-Exciting-Stories/dp/B0001GNJHO|title=Miffy and Friends, Volume 1 (VHS)|work=amazon.co.uk}}</ref> Select ''Miffy and Friends'' episodes were included on themed DVD discs released in 2009.<ref>{{cite web|url=https://www.amazon.com/Miffy-Friends-Miffys-Adventure-none/dp/B001O1ZU9U|title=''Miffy and Friends'': Miffy's Adventure DVD|work=amazon.com}}</ref>\n\n===Broadcast===\nThe series made its world premiere on [[ITV (TV network)|ITV]] in the United Kingdom.<ref>{{cite web|url=https://www.telegraph.co.uk/expat/expatpicturegalleries/9079416/Big-in-Japan-in-pictures.html?image=8|title=Big in Japan: Miffy (image credited to ITV)|work=Telegraph.co.uk}}</ref> It aired on [[Noggin (brand)|Noggin]] \nin the United States.<ref>{{cite web|url=http://www.prnewswire.com/news-releases/noggin-adds-miffy-and-friends-to-its-preschool-line-up-april-7-at-700-am-et-74763942.html|title=Noggin Adds ''Miffy and Friends'' to Its Preschool Line-Up|author=NOGGIN|date=March 25, 2003|work=prnewswire.com}}</ref> It also aired on [[Treehouse TV]] in Canada,<ref>{{cite web|url=http://treehousetv.com/watch/shows/MiffyandFriends/default.aspx|archiveurl=https://web.archive.org/web/20071015153240/http://treehousetv.com/watch/shows/MiffyandFriends/default.aspx|title=Treehouse TV's Miffy and Friends Show Page|archivedate=October 15, 2007|work=treehousetv.com}}</ref> and [[ABC Kids (Australia)|ABC Kids]] in Australia.<ref>{{cite web|url=http://www.abc.net.au/tv/programs/miffy-and-friends/|title=Miffy and Friends on ABC Kids|work=ABC Television}}</ref> It is also available for streaming on the [[Noggin (brand)|Noggin]] app as of 2016.\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title|1614180}}\n*[http://www.tvguide.com/tvshows/miffy-and-friends/203099/ ''Miffy and Friends'' on TVGuide.com]\n*[https://www.commonsensemedia.org/tv-reviews/miffy-and-friends ''Miffy and Friends'' on CommonSenseMedia.org]\n\n{{Miffy}}\n{{Noggin shows}}\n\n[[Category:2000s Dutch television series]]\n[[Category:2003 Dutch television series debuts]]\n[[Category:2007 Dutch television series endings]]\n[[Category:Dutch-language television programs]]\n[[Category:English-language television programs]]\n[[Category:Noggin (brand) original programming]]\n[[Category:Nickelodeon shows]]\n[[Category:Treehouse TV shows]]\n[[Category:Preschool education television series]]\n[[Category:Animated television series about rabbits and hares]]\n[[Category:American television programs based on children's books]]\n[[Category:Television programs based on Dutch novels]]\n[[Category:American animated television programs featuring anthropomorphic characters]]\n", "text_old": "{{Short description|Dutch stop-motion animated television series}}\n{{Infobox television\n| show_name            = Miffy and Friends\n| image                = Miffy and Friends Logo Noggin.png\n| caption              = \n| genre                = [[Children's television series]]\n| creator              = [[Dick Bruna]]\n| writer               = \n| director             = \n| creative_director    = \n| developer            = \n| presenter            = \n| starring             = \n| narrated             = Cyd Vandenberg\n| theme_music_composer = Tom van Beers\n| opentheme            = \n| endtheme             = \n| composer             = Jerry Hickman\n| country              = Netherlands\n| language             = {{Unbulleted list|Dutch|English}}\n| num_seasons          = 3\n| num_episodes         = 39 (78 segments)\n| list_episodes        = \n| executive_producer   =\n| producer             = \n| editor               = \n| location             = \n| cinematography       = \n| camera               =  \n| runtime              = 24 minutes\n| company              = {{Unbulleted list|Big Tent Entertainment|Mercis BV|Palm Multimedia}}\n| network              = [[Katholieke Radio Omroep|KRO]]\n| first_run            = United Kingdom\n| first_aired          = {{start date|2002|4|7}}\n| last_aired           = {{end date|2008|8|10}}\n| preceded_by          =\n| followed_by          = ''[[Miffy's Adventures Big and Small]]''\n| picture_format       = \n| related              = \n| website = https://web.archive.org/web/20030622045027/http://www.noggin.com/shows/miffy.php\n| distributor = \n}}\n'''''Miffy and Friends''''' is a Dutch [[stop-motion]] [[animated television series]], based on the ''[[Miffy]]'' book series by Dutch artist [[Dick Bruna]]. It originally aired on [[Katholieke Radio Omroep|KRO]] in the Netherlands,<ref>{{cite web|url=http://www.licensemag.com/license-global/australian-and-dutch-stations-buy-miffy|title=Australian and Dutch Stations Buy ''Miffy''|work=licensemag.com}}</ref> and [[Noggin (brand)#TV channel|Noggin]] in the United States.<ref>{{cite web|url=http://www.prnewswire.com/news-releases/major-deals-set-stage-for-miffy-and-friends-brand-expansion-in-united-states-and-canada-73937002.html|title=Major Deals Set Stage for ''Miffy and Friends'' Brand Expansion in United States and Canada|author=Big Tent Entertainment|date=January 21, 2003|work=prnewswire.com}}</ref>\n\n==Plot==\nThe series focuses on the life of young rabbit Miffy. It is presented in a storybook style, with narration by Canadian actress/singer Cyd Vandenberg explaining the actions of non-speaking Miffy and her friends.\n\n==Characters==\n*Miffy \u2013 A young rabbit and the series' protagonist. She is an aspiring artist who also likes to write.\n*Snuffy the Dog \u2013 A brave brown dog who was first introduced in the episode ''Miffy Meets Snuffy''. She is one of Miffy's best friends.\n*Barbara \u2013 Boris Bear's girlfriend.\n*Boris \u2013 A bear who lives near Miffy. He enjoys building things. He has a girlfriend named Barbara.\n*Poppy \u2013 A pig often seen gardening or reading. She has a niece named Grunty.\n*Grunty \u2013 Poppy Pig's niece.\n\n==Episodes==\nThe series lasted for three seasons, consisting of 39 episodes. Each episode is made up of two segments, with mathematics-based interstitials between them.\n\n===Season 1 (2003-2004)===\n#Miffy's Musical Day/Miffy's Rainy Day\n#Miffy Meets Snuffy/Miffy's Gift from Boris\n#Poppy Pig Lends a Hand/Miffy's and Aggie's Teddy Bears\n#Miffy at a Costume Party/Miffy Goes Camping\n#Snuffy's Birthday/Miffy Has the Flu\n#Miffy Finds the Cup/Snuffy Learns Patience\n#Miffy's Birthday Party/Miffy's Dancing Lessons\n#Miffy is Lost in the Woods/Miffy and Poppy Pig Have Breakfast\n#Miffy and the Little Bird/Miffy Plays Hide and Seek\n#Miffy and the Wall Paintings/Miffy and the Blue Egg\n#Miffy and the Snow Bunny/Miffy Flies a Kite\n#Miffy and Melanie Learn to Read/Miffy Paints Her Room\n#Boris's Bird House/Miffy's Three Wishes\n\n===Season 2 (2004-2006)===\n#Miffy's Summer Vacation/Miffy Gets a Postcard\n#Miffy and Barbara in the Rain/Miffy Lost at the Beach\n#Miffy and the Caterpillar/Miffy and the Great Carrot Feast\n#Miffy's Colorful World/Miffy's Scooter\n#Miffy's Snowfall/Miffy and Grunty Sleep in a Tent\n#Miffy's Restaurant/Miffy Discovers Nature\n#Miffy and the Birthday Cake/Miffy Counts Leaves\n#Miffy in the Wind/Miffy's Late for School\n#Miffy Worries about Snuffy/Miffy Helps Grunty\n#Miffy Plants a Seed/Snuffy's Doghouse\n#Miffy Finds Snuffy/Miffy and the Seasons\n#Miffy Makes and Bakes/Miffy's Surprise\n#Miffy in the Shade/Miffy's Flower Pot\n\n===Season 3 (2006-2007)===\n#Miffy's Beach Picnic/Boris Tidies Up!\n#Miffy Wants to Fly/Miffy's Teddy Bear is Sick\n#Miffy's Ball Game/Miffy Has an Unexpected Day\n#Miffy's Apple Pie/Miffy's Musical Soup\n#Miffy and Snuffy at the Playground/Miffy and the Hungry Bird\n#Miffy's Lost Teddy Bear/Miffy and the Great Summer Picnic\n#Miffy and the Three Christmas Trees/Miffy's Mystery\n#Miffy and the Shadows/Snuffy's Winter Fun\n#Miffy Plays Doctor/Miffy Goes Skiing\n#Miffy's Family Car Trip/Miffy and the Falling Leaves\n#Miffy and Snuffy Hear a Strange Sound/Miffy's Mother's Day Present\n#Boris Forgets Something/Miffy's Dance Show\n#Miffy Counts the Trees/Boris's Race\n\n==Release==\n===VHS and DVD releases===\nDuring the show's run, several VHS tapes including episodes of the series were produced. The series was also made available on DVD in America through Sony Wonder and Big Tent Entertainment.<ref>{{cite web|url=https://www.amazon.co.uk/Miffy-And-Friends-Exciting-Stories/dp/B0001GNJHO|title=Miffy and Friends, Volume 1 (VHS)|work=amazon.co.uk}}</ref> Select ''Miffy and Friends'' episodes were included on themed DVD discs released in 2009.<ref>{{cite web|url=https://www.amazon.com/Miffy-Friends-Miffys-Adventure-none/dp/B001O1ZU9U|title=''Miffy and Friends'': Miffy's Adventure DVD|work=amazon.com}}</ref>\n\n===Broadcast===\nThe series made its world premiere on [[ITV (TV network)|ITV]] in the United Kingdom.<ref>{{cite web|url=https://www.telegraph.co.uk/expat/expatpicturegalleries/9079416/Big-in-Japan-in-pictures.html?image=8|title=Big in Japan: Miffy (image credited to ITV)|work=Telegraph.co.uk}}</ref> It aired on [[Noggin (brand)|Noggin]] \nin the United States.<ref>{{cite web|url=http://www.prnewswire.com/news-releases/noggin-adds-miffy-and-friends-to-its-preschool-line-up-april-7-at-700-am-et-74763942.html|title=Noggin Adds ''Miffy and Friends'' to Its Preschool Line-Up|author=NOGGIN|date=March 25, 2003|work=prnewswire.com}}</ref> It also aired on [[Treehouse TV]] in Canada,<ref>{{cite web|url=http://treehousetv.com/watch/shows/MiffyandFriends/default.aspx|archiveurl=https://web.archive.org/web/20071015153240/http://treehousetv.com/watch/shows/MiffyandFriends/default.aspx|title=Treehouse TV's Miffy and Friends Show Page|archivedate=October 15, 2007|work=treehousetv.com}}</ref> and [[ABC Kids (Australia)|ABC Kids]] in Australia.<ref>{{cite web|url=http://www.abc.net.au/tv/programs/miffy-and-friends/|title=Miffy and Friends on ABC Kids|work=ABC Television}}</ref> It is also available for streaming on the [[Noggin (brand)|Noggin]] app as of 2016.\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title|1614180}}\n*[http://www.tvguide.com/tvshows/miffy-and-friends/203099/ ''Miffy and Friends'' on TVGuide.com]\n*[https://www.commonsensemedia.org/tv-reviews/miffy-and-friends ''Miffy and Friends'' on CommonSenseMedia.org]\n\n{{Miffy}}\n{{Noggin shows}}\n\n[[Category:2000s Dutch television series]]\n[[Category:2002 Dutch television series debuts]]\n[[Category:2008 Dutch television series endings]]\n[[Category:Dutch-language television programs]]\n[[Category:English-language television programs]]\n[[Category:Noggin (brand) original programming]]\n[[Category:Nickelodeon shows]]\n[[Category:Treehouse TV shows]]\n[[Category:Preschool education television series]]\n[[Category:Animated television series about rabbits and hares]]\n[[Category:American television programs based on children's books]]\n[[Category:Television programs based on Dutch novels]]\n[[Category:American animated television programs featuring anthropomorphic characters]]\n", "name_user": "35.133.57.109", "label": "unsafe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Miffy_and_Friends"}
