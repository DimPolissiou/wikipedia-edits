{"title_page": "Burj el-Shemali", "text_new": "{{pp-30-500|small=yes}}\n{{Infobox settlement\n| name                            = Burj El Shimali\n| subdivision_type1               = [[Governorates of Lebanon|Governorate]]\n| timezone1                       = [[Eastern European Time|EET]]\n| subdivision_name2               = [[Tyre District]]\n| subdivision_name                = {{Flag|Lebanon}}\n| subdivision_name1               = [[South Governorate]]\n| native_name_lang                = ar\n| subdivision_type2               = [[Districts of Lebanon|District]]\n| subdivision_type                = Country\n| official_name                   = \n| pushpin_map                     = Lebanon\n| coordinates                     = {{coord|33|15|47|N|35|14|20|E|display=inline,title}}\n| image_map                       = \n| image_skyline                   = BurjAlShimali-Tower 23092019RomanDeckert.jpg\n| settlement_type                 = Town\n| native_name                     = {{lang|ar| \u0627\u0644\u0628\u0631\u062c \u0627\u0644\u0634\u0645\u0627\u0644\u064a}}\n| image_caption                   = The Burj \u2013 \"Tower\" \u2013 of Burj El Shimali\n| area_total_ha                   = 1069\n| elevation_m                     = 62\n}}\n{{Other uses|Burj (disambiguation){{!}}Burj}}\n'''Burj el-Shemali''' (Arabic: \u0627\u0644\u0628\u0631\u062c \u0627\u0644\u0634\u0645\u0627\u0644\u064a) is a [[Municipality|municpality]] located some 86 km south of Beirut and 3 km east of the city of [[Tyre, Lebanon|Tyre]]/Sour. It is part of the [[Tyre Union of Municipalities]] within the [[Tyre District]] of the [[South Governorate]] of [[Lebanon]].  \n\nIt is particularly known for hosting the second-largest of the twelve [[Palestinian refugee camps]] in the country as an [[Autonomy|autonomous]] [[Enclave and exclave|exclave]] out of the reach of Lebanese officials: The camp is ruled by committees of Palestinian parties under the leadership of the [[Palestine Liberation Organization|Palestinian Liberation Organisation]] (PLO) and assisted by the [[UNRWA|United Nations Relief and Works Agency for Palestine Refugees in the Near East]] (UNRWA), while the [[Lebanese Armed Forces]] control entry and exit through its main gate.<ref name=\":3\" /> \n\n== Name ==\nBurj el-Shemali \u2013 also transliterated into the spellings of \"Borj\" or \"Bourj\" combined with a version of \"Shimali\", \"Shamali\", ''\"''Shem\u00e2ly''\",'' \"Chemali\", \"Chamali\", or \"Chmali\" with or without the article \"el\", \"al\", or \"esh\" \u2013  is commonly translated as \"Northern Tower\", as done by [[Edward Henry Palmer|E. H. Palmer]] in the 1881 [[PEF Survey of Palestine|''Survey of Western Palestine'' (SWP)]] .<ref>{{Cite book|last=Palmer|first=E.H.|url=https://archive.org/details/surveyofwesternp00conduoft|title=The Survey of Western Palestine: Arabic and English Name Lists Collected During the Survey by Lieutenants Conder and Kitchener, R. E. Transliterated and Explained by E.H. Palmer.|publisher=Committee of the [[Palestine Exploration Fund]]|year=1881|isbn=|location=London|pages=4}}</ref>\n\nThe settlement is named after a medieval tower on its main hill that overlooks Tyre.<ref name=\":1\" /> The Arabic word \"Burj\" reportedly originated from the [[Ancient Greek]] \"pyrgos\".<ref>{{Cite web|url=https://www.academia.edu/4847281/Arabic_Etymological_Dictionary|title=Arabic Etymological Dictionary|last=Rajki|first=Andras|date=2005|website=Academia.edu|url-status=live|archive-url=|archive-date=|access-date=2 April 2020}}</ref>\n\n== Territory ==\nBurj el-Shemali reportedly covers an area of 1.069 [[hectare]], rising to an [[elevation]] of more than 60 metres on a hill overlooking Tyre/Sour [[peninsula]].<ref>{{Cite web|url=http://www.localiban.org/borj-ech-chemali-3539|title=Borj Ech Chemali|last=Tohm\u00e9|first=Joseph|last2=Aouad|first2=Michel|date=23 October 2015|website=localiban|url-status=live|archive-url=|archive-date=|access-date=2 April 2020}}</ref>\n\nTogether with the built-up areas of three adjacent municipalities - Sour on the peninsula and the coastal areas to the West, [[Abbasiyet Sour]] to the North, and [[Ain Baal]] to the South-East - the urban part of Burj el-Shemali has integrated into one greater metropolitan Tyre. There are also unpopulated [[Agricultural land|agricultural lands]], especially in its Northern and Southern parts.<ref name=\":4\" />\n\nThe Palestinian camp covers just a fraction of the municipality's overall territory.<ref name=\":4\" />\n\n== History ==\n\n=== Ancient Times ===\n[[File:RomanTerracottaMaskOfSatyr-BurjAlShimali NationalMuseumOfBeirut RomanDeckert06102019.jpg|thumb|Mask of a [[Satyr]] from Burj el-Shemali, Roman period, on display at the [[National Museum of Beirut]]]]\nAccording to the Directorate-General of Antiquities, the settlement is thought to date back at least to [[Phoenicia|Phoenician]] times in the first century BCE.<ref name=\":0\">{{Cite book|title=Tyr \u2013 L'histoire d'une Ville|last=Badawi|first=Ali Khalil|publisher=Municipalit\u00e9 de Tyr / Tyre Municipality / Baladia Sour|year=2008|isbn=|location=Tyre/Sour/Tyr|pages=124|language=French}}</ref> <ref>{{Cite book|last=|first=|title=A visit to the Museum... The short guide of the National Museum of Beirut, Lebanon|publisher=Ministry of Culture/Directorate General of Antiquities|year=2008|isbn=978-9953-0-0038-1|location=Beirut|pages=39}}</ref>\n\nUnderneath the modern main road, the remains of an ancient [[Byzantine Empire|Roman-Byzantine]] road are preserved and a number of [[Hypogeum|hypogea]] - underground tombs - with [[Roman Empire|Roman]]-era [[Fresco|frescos]] were discovered in a [[necropolis]].<ref name=\":1\">{{Cite book|title=TYRE|last=Badawi|first=Ali Khalil|publisher=Al-Athar Magazine|year=2018|isbn=|edition=4th|location=Beirut|pages=136-137}}</ref> Some of them are on display at the [[National Museum of Beirut|National Museum]] in [[Beirut]].<ref name=\":0\" /> \n\n=== Medieval Times ===\nThe village then adapted its name from a tower that was constructed during the [[Crusades|Crusader]] period in the 12th and 13th century AD, overlooking the [[peninsula]] of the city of [[Tyre, Lebanon|Tyre]]. There are also remains of another Crusader tower known as Al-Burj Al-Qobli in the Southern part of town. Like in acient times, the lands of Burj el-Shemali were used as cemeteries in medieval periods.<ref name=\":1\" />\n\n=== Ottoman Times ===\n[[File:BurjAlShimali-Tower-view-on-Tyre 23092019RomanDeckert.jpg|thumb|View from the Burj over Tyre]]\nAlthough the [[Ottoman Empire]] conquered the [[Levant]] in 1516, [[Jabal Amel]] (modern-day [[Southern Lebanon]]) remained mostly untouched for almost another century. When the Ottoman leadership at the [[Sublime Porte]] appointed the [[Druze]] leader [[Fakhr-al-Din II|Fakhreddine II]] of the Maan family to administer the area at the beginning of the 17th century, the [[Emir]] encouraged many [[Lebanese Shia Muslims|Metwali]] - the discriminated [[Shia Islam|Shia]] Muslims of what is now Lebanon - to settle to the East of Tyre to secure the road to [[Damascus]]. He thus also laid the foundation of the Lebanese part of modern Burj el-Shemali [[Demography|demographics]] as a predominantly Shiite place.<ref>{{Cite book|last=Smit|first=Ferdinand|url=https://www.academia.edu/12778980|title=The battle for South Lebanon: Radicalisation of Lebanon's Shi'ites 1982\u20131985|publisher=Bulaaq, Uitgeverij|year=2006|isbn=978-9054600589|location=Amsterdam|pages=36|format=PDF}}</ref> \n\nIn 1881, the [[London]]-based [[Palestine Exploration Fund]]'s [[PEF Survey of Palestine|''Survey of Western Palestine'' (SWP)]] described it as <blockquote>\"''A large village built of stone, containing about 300 Metawileh, placed on a low [[ridge]], with [[Ficus|figs]], [[Olive|olives]], and [[arable land]] around. There are two good [[Spring (hydrology)|springs]] near.''\"</blockquote>and further noted that it was <blockquote>\"''a village with a similar tower of drafted [[masonry]] (as that of [[Borj Rahal]]). The hill is crowned by a stronghold, the vaults of which, slightly ogival, do not appear older than the Crusaders, but it was constructed of older blocks, some in drafted masonry and others completely smoothed. About a mile to the south-west of this hill is a subterranean series of tombs, each containing several ranges of [[Loculus (architecture)|loculi]], which was explored by [[Ernest Renan|Renan]].''\"<ref>{{Cite book|last=Conder|first=Claude Reignier|url=https://ia600207.us.archive.org/21/items/surveyofwesternp01conduoft/surveyofwesternp01conduoft.pdf|title=The Survey of Western Palestine: Memoirs of the Topography, Orography, Hydrography, and Archaeology|last2=Kitchener|first2=Horatio Herbert|publisher=Committee of the Palestine Exploration Fund|year=1881|isbn=|volume=1|location=London|pages=48, 58|author-link=Claude Reignier Conder|author-link2=Herbert Kitchener, 1st Earl Kitchener}}</ref></blockquote>\n\n=== Modern Times ===\n[[File:Palestinian refugees.jpg|thumb|Palestinian refugees making their way from [[Galilee]] to Lebanon in October-November 1948]]\nWhen the state of [[Israel]] was declared in May 1948 and the Palestinian exodus \u2013 also known as the '''[[1948 Palestinian exodus|Nakba]]''' \u2013 followed, a camp of tents was set up in Burj El Shimali camp, mainly for displaced from [[Hawla]], [[Lubya|Lubieh]], [[Sepphoris|Saffuri]], and [[Tiberias]].<ref name=\":2\">{{Cite web|url=https://www.unrwa.org/where-we-work/lebanon/burj-shemali-camp|title=Burj Shemali Camp|last=|first=|date=|website=United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA)|url-status=live|archive-url=|archive-date=|access-date=23 September 2019}}</ref> More Palestinian refugees settled in the area of [[Maachouk]] \u2013 1 km to the West of Burj El Shemali \u2013  on agricultural lands owned by the Lebanese State as a neighbourhood rather than a camp. Its eastern side, which is an [[Industry|industrial]] zone, as well as the main road's southern side with many [[Commerce|commercial]] activities fall within the jurisdiction of Burj el-Shemali municipality which demonstrates the [[arbitrariness]] of many boundaries.<ref>{{Cite web|url=https://reliefweb.int/sites/reliefweb.int/files/resources/UN-Habitat_2017.05.04_NPS_Maachouk.pdf|title=Maachouk Neighbourhood Profile & Strategy, Tyre, Lebanon|last=Harake|first=Dani|last2=Kuwalti|first2=Riham|date=31 May 2017|website=reliefweb|publisher=UN HABITAT Lebanon|pages=2-3, 25-26|format=PDF|url-status=live|archive-url=|archive-date=|access-date=2 April 2020}}</ref>[[File:Musa al-Sadr & Mostafa Chamran (2).jpg|thumb|Al-Sadr (centre, in black) with Mostafa Chamran, left]]Since 1955, humanitarian assistance - infrastructure services (water, sewage, electricity, road networks and shelter), [[school]] [[education]] and [[health care]] - has been provided by the UNRWA to the residents of the camp in Burj el-Shemali.<ref>{{cite web|url=https://www.un.org/unrwa/refugees/lebanon/shemali.html|title=Archived copy|url-status=dead|archiveurl=https://web.archive.org/web/20090421060238/http://www.un.org/unrwa/refugees/lebanon/shemali.html|archivedate=2009-04-21|accessdate=2009-03-29}}</ref> \n\nFour years later, the [[Iran]]-born Shiite cleric [[Sayyid|Sayed]] [[Musa al-Sadr|Musa Sadr]] moved to Tyre in 1959 to succeed the late [[Abd al-Husayn Sharaf al-Din al-Musawi|Abdulhussein Sharafeddin]] as the Imam of Tyre and gradually changed the balance of power in Southern Lebanon and the whole country: as \"''one of his first significant acts''\" he established a [[Vocational education|vocational training]] center in Burj El Shimali that became \"''an important symbol of his leadership''\".<ref>{{Cite book|last=Norton|first=Augustus Richard|title=Amal and the Shi'a: Struggle for the Soul of Lebanon|publisher=University of Texas Press|year=1987|isbn=978-0292730403|location=Austin|pages=39}}</ref> In 1974, Sadr founded ''[[Amal Movement|Harakat al-Mahroumin]]'' (\"Movement of the Deprived\") and one year later - shortly before the beginning of the [[Lebanese Civil War]] - its ''de facto'' military wing: ''Afwaj al-Muqawama al-Lubnaniyya'' (Amal).<ref>{{Cite journal|last=Deeb|first=Marius|date=1988|title=Shi'a Movements in Lebanon: their Formation, Ideology, Social Basis, and Links with Iran and Syria|url=|journal=Third World Quarterly|volume=10 (2)|pages=685|doi=10.1080/01436598808420077|via=}}</ref> The Iranian director of Sadr's technical school, [[Mostafa Chamran]], became a major instructor of [[Guerrilla warfare|guerilla warfare]]. The US-trained [[physicist]] went on to become the first defense minister of [[History of the Islamic Republic of Iran|post-revolutionary Iran]].<ref>{{Cite book|last=Bonsen|first=Sabrina|title=Martyr Cults and Political Identities in Lebanon: \"Victory or Martyrdom\" in the Struggle of the Amal Movement|publisher=Springer Nature|year=2019|isbn=978-3-658-28098-7|location=Wiesbaden|pages=214, 248}}</ref>\n\nAfter Sadr mysteriously disappeared following a visit to Libyan leader [[Muammar Gaddafi]] on 31 August 1978, discontent within the Shiite population about the suffering from the conflict between Israel and the Palestinian factions grew, especially following the Israeli [[1978 South Lebanon conflict|Operation Litani]] in March of that year. Within this context, tensions between Amal and the Palestinian militants escalated into violent clashes in many villages of Southern Lebanon, including the Tyre area.<ref>{{Cite book|last=Abraham|first=Antoine J.|title=The Lebanon War|publisher=Praeger|year=1996|isbn=978-0275953898|location=Westport, Conn.|pages=123}}</ref> The heaviest such incident took place in April 1982, when the PLO ([[Fatah|Fateh]]) bombarded Amal's Technical Institute in Burj El Shimali for ten hours.<ref>{{Cite journal|last=Siklawi|first=Rami|date=Winter 2012|title=The Dynamics of the Amal Movement in Lebanon 1975\u201390|url=|journal=Arab Studies Quarterly|volume=34 (1)|pages=4\u201326|jstor=41858677|via=JSTOR}}</ref>\n\nFollowing the [[1982 Lebanon War|1982 Israeli Invasion]], the Palestinian refugee camp in Burj el-Shemali \u2013 according to UNRWA \u2013 \"''was badly damaged''\".<ref name=\":2\" /> An international commission to enquire into reported violations of [[international law]] by Israel during its invasion found that Israeli forces had destroyed 35 percent of the houses in the camp.<ref>{{Cite book|last=Kaufman|first=Asher|url=https://www.academia.edu/5561720/Forgetting_the_Lebanon_War_On_Silence_Denial_and_Selective_Remembrance_of_the_First_Lebanon_War|title=Forgetting the Lebanon War? On Silence, Denial and Selective Remembrance of the First Lebanon War|publisher=Cambrige University Press|year=2010|isbn=978-0521196581|editor-last=Ben-Ze\u2019ev|editor-first=Efrat|location=Cambridge|pages=205|format=PDF|editor-last2=Ginio|editor-first2=Ruth|editor-last3=Winter|editor-first3=Jay}}</ref> Much of the destruction was done ''\"systematically''\" by the IDF after the actual combat with Palestinian fighters had stopped.<ref>{{Cite book|last=Gilmour|first=David|title=Lebanon: The Fractured Country|publisher=Martin Robertson|year=1983|isbn=978-0312477394|location=Oxford|pages=178\u2013179}}</ref>\n\nIn February 1985, an Amal member from Tyre launched a [[Suicide attack|suicide-attack]] on an IDF convoy in Burj El Shimali<ref>{{Cite book|last=Bonsen|first=Sabrina|title=Martyr Cults and Political Identities in Lebanon: \"Victory or Martyrdom\" in the Struggle of the Amal Movement|publisher=Springer Nature|year=2019|isbn=978-3-658-28098-7|location=Wiesbaden|pages=214, 248}}</ref>, injuring ten soldiers. \"''Israeli reprisals in the area east of Tyre killed fifteen and wounded dozens''.\"<ref>{{Cite book|last=Smit|first=Ferdinand|url=https://www.academia.edu/12778980|title=The battle for South Lebanon: Radicalisation of Lebanon's Shi'ites 1982\u20131985 (PDF). :|publisher=Bulaaq, Uitgeverij|year=2006|isbn=978-9054600589|location=Amsterdam|pages=269|format=PDF}}</ref>\n\nDuring Israel's invasion in the [[2006 Lebanon War]], Burj El Shimali was severely hit again: on 16 July five civilians were killed by in an assault by the [[Israeli Air Force]] (IAF), including two children. On 13 August, another five civilians were killed by an IAF missile, amongst them three children and one [[Sri Lanka|Sri Lankan]] maid.<ref>{{Cite web|url=https://www.hrw.org/report/2007/09/05/why-they-died/civilian-casualties-lebanon-during-2006-war|title=Why They Died: Civilian Casualties in Lebanon during the 2006 War\"|last=|first=|date=5 September 2007|website=Human Rights Watch|url-status=live|archive-url=|archive-date=|access-date=31 March 2020}}</ref>\n\nAccording to a 2014 study paper, the majority of Palestinian refugees in Burj el-Shemali supported Fatah. However, it noted that there was \u2013 unlike in the other Tyrian camp of [[Rashidieh]] \u2013 also a considerable presence of [[Hamas]]. In addition, the [[Popular Front for the Liberation of Palestine]] (PFLP), the [[Democratic Front for the Liberation of Palestine]] (DFLP), and the [[Islamic Jihad Movement in Palestine]] were represented in the Popular Committees that rule the camp.<ref name=\":3\">{{Cite web|url=https://www.rosalux.de/fileadmin/rls_uploads/pdfs/Studien/Studien_Flucht_Vertreibung.pdf|title=Flucht und Vertreibung im Syrien-Konflikt \u2013 Eine Analyse zur Situation von Fl\u00fcchtlingen in Syrien und im Libanon|last=|first=|date=July 2014|website=Rosa-Luxemburg-Stiftung|pages=28|language=German|format=PDF|url-status=live|archive-url=|archive-date=|access-date=31 March 2020}}</ref> \n\nIn 2016, the mayor of Burj el-Shemali was [[Hajj Ali Dib]].<ref name=\":4\">{{Cite web|url=https://reliefweb.int/sites/reliefweb.int/files/resources/UN-Habitat_2017.05.22_CP_Tyre_web.pdf|title=TYRE CITY PROFILE|last=Maguire|first=Suzanne|last2=Majzoub|first2=Maya|date=2016|editor-last=Osseiran|editor-first=Tarek|website=reliefweb|publisher=UN HABITAT Lebanon|pages=13-18; 26|format=PDF|url-status=live|archive-url=|archive-date=|access-date=31 March 2020}}</ref> \n\n== Demographics ==\n\n=== Lebanese part of the town ===\nThere are no official figures for the Lebanese and Non-Lebanese population in the Lebanese-ruled part of Burj el-Shemali.\n\n=== Palestinian Refugee Camp ===\nBy 1968, there were 7,159 registered Palestinian refugees in Burj El Shimali. By 1982 this number had gone up to 11,256.<ref>{{Cite book|last=Brynen|first=Rex|title=Sanctuary And Survival: The PLO In Lebanon|publisher=Westview Press|year=1990|isbn=978-0813379197|location=Boulder|pages=}}</ref>\n\nIn 2008, the camp had 19,074 registered [[refugees]].<ref>{{cite web|url=https://www.un.org/unrwa/refugees/lebanon.html |title=Archived copy |accessdate=2008-04-18 |url-status=dead |archiveurl=https://web.archive.org/web/20080409202113/http://www.un.org/unrwa/refugees/lebanon.html |archivedate=2008-04-09 }}</ref> As of June 2018, this number had grown to 24,929, also because of \"''an influx of Syrian refugees and Palestinian refugees from Syria''\".<ref name=\":2\" /> UNRWA describes the situation as follows:<blockquote>\"''There is a very high incidence of [[thalassemia]] and [[sickle cell disease]] ([[genetic disorder]]) among Burj Shemali inhabitants and the camp is one of the poorest camps in Lebanon. [[Unemployment]] is extremely high, with seasonal agricultural work the most common source of income for both men and women.''\"<ref name=\":2\" /></blockquote>\n\n== Gallery ==\n\n=== Exhibits at the National Museum of Beirut ===\n<gallery>\nFile:BurjAlShimali LimestoneStele 5-4cBCE NationalMuseumOfBeirut RomanDeckert03102019.jpg|[[Limestone]] [[stele]], 5th to 4th century BCE\nFile:BurjAlShimali Relief IronAge-III NationalMuseumOfBeirut RomanDeckert03102019.jpg|Relief depicting a deceased person with a sheathed body, 550-333 BCE\nFile:BurjAlShemali RomanTombWithFrescoes NationalMuseumOfBeirut RomanDeckert03102019.jpg|Roman tomb with frescoes from the 2nd century CE\nFile:RomanTerracottaMask Satyr BurjAlShimali NationalMuseumOfBeirut RomanDeckert06102019.jpg|Mask of a Satyr, Roman period\n</gallery>\n\n=== The tower ===\n<gallery>\nFile:BurjAlShimali-TowerSide Lebanon-23092019RomanDeckert.jpg\nFile:BurjAlShimaliTowerLebanon 23092019RomanDeckert.jpg\nFile:BurjAlShimali-TowerInside Lebanon-23092019RomanDeckert.jpg\n</gallery>\n\n==References==\n<references/>\n\n==External links==\n* [https://www.unrwa.org/where-we-work/lebanon/burj-shemali-camp Burj Shemali Camp],  from [[UNWRA]]\n\n{{Palestinian refugee camps}}\n{{Tyre District}}\n[[Category:Palestinian refugee camps in Lebanon]]\n\n\n{{Palestine-stub}}\n{{Lebanon-stub}}\n", "text_old": "{{pp-30-500|small=yes}}\n{{Infobox settlement\n| name                            = Burj El Shimali\n| subdivision_type1               = [[Governorates of Lebanon|Governorate]]\n| timezone1                       = [[Eastern European Time|EET]]\n| subdivision_name2               = [[Tyre District]]\n| subdivision_name                = {{Flag|Lebanon}}\n| subdivision_name1               = [[South Governorate]]\n| native_name_lang                = ar\n| subdivision_type2               = [[Districts of Lebanon|District]]\n| subdivision_type                = Country\n| official_name                   = \n| pushpin_map                     = Lebanon\n| coordinates                     = {{coord|33|15|47|N|35|14|20|E|display=inline,title}}\n| image_map                       = \n| image_skyline                   = BurjAlShimali-Tower 23092019RomanDeckert.jpg\n| settlement_type                 = Town\n| native_name                     = {{lang|ar| \u0627\u0644\u0628\u0631\u062c \u0627\u0644\u0634\u0645\u0627\u0644\u064a}}\n| image_caption                   = The Burj \u2013 \"Tower\" \u2013 of Burj El Shimali\n| area_total_ha                   = 1069\n| elevation_m                     = 62\n}}\n{{Other uses|Burj (disambiguation){{!}}Burj}}\n'''Burj el-Shemali''' (Arabic: \u0627\u0644\u0628\u0631\u062c \u0627\u0644\u0634\u0645\u0627\u0644\u064a) is a [[Municipality|municpality]] located some 86 km south of Beirut and 3 km east of the city of [[Tyre, Lebanon|Tyre]]/Sour. It is part of the [[Tyre Union of Municipalities]] within the [[Tyre District]] of the [[South Governorate]] of [[Lebanon]].  \n\nIt is particularly known for hosting the second-largest of the twelve [[Palestinian refugee camps]] in the country as an [[Autonomy|autonomous]] [[Enclave and exclave|exclave]] out of the reach of Lebanese officials: The camp is ruled by committees of Palestinian parties under the leadership of the [[Palestine Liberation Organization|Palestinian Liberation Organisation]] (PLO) and assisted by the [[UNRWA|United Nations Relief and Works Agency for Palestine Refugees in the Near East]] (UNRWA), while the [[Lebanese Armed Forces]] control entry and exit through its main gate.<ref name=\":3\" /> \n\n== Name ==\nBurj el-Shemali \u2013 also transliterated into the spellings of \"Borj\" or \"Bourj\" combined with a version of \"Shimali\", \"Shamali\", ''\"''Shem\u00e2ly''\",'' \"Chemali\", \"Chamali\", or \"Chmali\" with or without the article \"el\", \"al\", or \"esh\" \u2013  is commonly translated as \"Northern Tower\", as done by [[Edward Henry Palmer|E. H. Palmer]] in the 1881 [[PEF Survey of Palestine|''Survey of Western Palestine'' (SWP)]] .<ref>{{Cite book|last=Palmer|first=E.H.|url=https://archive.org/details/surveyofwesternp00conduoft|title=The Survey of Western Palestine: Arabic and English Name Lists Collected During the Survey by Lieutenants Conder and Kitchener, R. E. Transliterated and Explained by E.H. Palmer.|publisher=Committee of the [[Palestine Exploration Fund]]|year=1881|isbn=|location=London|pages=4}}</ref>\n\nThe settlement is named after a medieval tower on its main hill that overlooks Tyre.<ref name=\":1\" /> The Arabic word \"Burj\" reportedly originated from the [[Ancient Greek]] \"pyrgos\".<ref>{{Cite web|url=https://www.academia.edu/4847281/Arabic_Etymological_Dictionary|title=Arabic Etymological Dictionary|last=Rajki|first=Andras|date=2005|website=Academia.edu|url-status=live|archive-url=|archive-date=|access-date=2 April 2020}}</ref>\n\n== Territory ==\nBurj el-Shemali reportedly covers an area of 1.069 [[hectare]], rising to an [[elevation]] of more than 60 metres on a hill overlooking Tyre/Sour [[peninsula]].<ref>{{Cite web|url=http://www.localiban.org/borj-ech-chemali-3539|title=Borj Ech Chemali|last=Tohm\u00e9|first=Joseph|last2=Aouad|first2=Michel|date=23 October 2015|website=localiban|url-status=live|archive-url=|archive-date=|access-date=2 April 2020}}</ref>\n\nTogether with the built-up areas of three adjacent municipalities - Sour on the peninsula and the coastal areas to the West, [[Abbasiyet Sour]] to the North, and [[Ain Baal]] to the South-East - the urban part of Burj el-Shemali has integrated into one greater metropolitan Tyre. There are also unpopulated [[Agricultural land|agricultural lands]], especially in its Northern and Southern parts.<ref name=\":4\" />\n\nThe Palestinian camp covers just a fraction of the municipality's overall territory.<ref name=\":4\" />\n\n== History ==\n\n=== Ancient Times ===\n[[File:RomanTerracottaMaskOfSatyr-BurjAlShimali NationalMuseumOfBeirut RomanDeckert06102019.jpg|thumb|Mask of a [[Satyr]] from Burj el-Shemali, Roman period, on display at the [[National Museum of Beirut]]]]\nAccording to the Directorate-General of Antiquities, the settlement is thought to date back at least to [[Phoenicia|Phoenician]] times in the first century BCE.<ref name=\":0\">{{Cite book|title=Tyr \u2013 L'histoire d'une Ville|last=Badawi|first=Ali Khalil|publisher=Municipalit\u00e9 de Tyr / Tyre Municipality / Baladia Sour|year=2008|isbn=|location=Tyre/Sour/Tyr|pages=124|language=French}}</ref> <ref>{{Cite book|last=|first=|title=A visit to the Museum... The short guide of the National Museum of Beirut, Lebanon|publisher=Ministry of Culture/Directorate General of Antiquities|year=2008|isbn=978-9953-0-0038-1|location=Beirut|pages=39}}</ref>\n\nUnderneath the modern main road, the remains of an ancient [[Byzantine Empire|Roman-Byzantine]] road are preserved and a number of [[Hypogeum|hypogea]] - underground tombs - with [[Roman Empire|Roman]]-era [[Fresco|frescos]] were discovered in a [[necropolis]].<ref name=\":1\">{{Cite book|title=TYRE|last=Badawi|first=Ali Khalil|publisher=Al-Athar Magazine|year=2018|isbn=|edition=4th|location=Beirut|pages=136-137}}</ref> Some of them are on display at the [[National Museum of Beirut|National Museum]] in [[Beirut]].<ref name=\":0\" /> \n\n=== Medieval Times ===\nThe village then adapted its name from a tower that was constructed during the [[Crusades|Crusader]] period in the 12th and 13th century AD, overlooking the [[peninsula]] of the city of [[Tyre, Lebanon|Tyre]]. There are also remains of another Crusader tower known as Al-Burj Al-Qobli in the Southern part of town. Like in acient times, the lands of Burj el-Shemali were used as cemeteries in medieval periods.<ref name=\":1\" />\n\n=== Ottoman Times ===\nAlthough the [[Ottoman Empire]] conquered the [[Levant]] in 1516, [[Jabal Amel]] (modern-day [[Southern Lebanon]]) remained mostly untouched for almost another century. When the Ottoman leadership at the [[Sublime Porte]] appointed the [[Druze]] leader [[Fakhr-al-Din II|Fakhreddine II]] of the Maan family to administer the area at the beginning of the 17th century, the [[Emir]] encouraged many [[Lebanese Shia Muslims|Metwali]] - the discriminated [[Shia Islam|Shia]] Muslims of what is now Lebanon - to settle to the East of Tyre to secure the road to [[Damascus]]. He thus also laid the foundation of the Lebanese part of modern Burj el-Shemali [[Demography|demographics]] as a predominantly Shiite place.<ref>{{Cite book|last=Smit|first=Ferdinand|url=https://www.academia.edu/12778980|title=The battle for South Lebanon: Radicalisation of Lebanon's Shi'ites 1982\u20131985|publisher=Bulaaq, Uitgeverij|year=2006|isbn=978-9054600589|location=Amsterdam|pages=36|format=PDF}}</ref> \n\nIn 1881, the [[London]]-based [[Palestine Exploration Fund]]'s [[PEF Survey of Palestine|''Survey of Western Palestine'' (SWP)]] described it as <blockquote>\"''A large village built of stone, containing about 300 Metawileh, placed on a low [[ridge]], with [[Ficus|figs]], [[Olive|olives]], and [[arable land]] around. There are two good [[Spring (hydrology)|springs]] near.''\"</blockquote>and further noted that it was <blockquote>\"''a village with a similar tower of drafted [[masonry]] (as that of [[Borj Rahal]]). The hill is crowned by a stronghold, the vaults of which, slightly ogival, do not appear older than the Crusaders, but it was constructed of older blocks, some in drafted masonry and others completely smoothed. About a mile to the south-west of this hill is a subterranean series of tombs, each containing several ranges of [[Loculus (architecture)|loculi]], which was explored by [[Ernest Renan|Renan]].''\"<ref>{{Cite book|last=Conder|first=Claude Reignier|url=https://ia600207.us.archive.org/21/items/surveyofwesternp01conduoft/surveyofwesternp01conduoft.pdf|title=The Survey of Western Palestine: Memoirs of the Topography, Orography, Hydrography, and Archaeology|last2=Kitchener|first2=Horatio Herbert|publisher=Committee of the Palestine Exploration Fund|year=1881|isbn=|volume=1|location=London|pages=48, 58|author-link=Claude Reignier Conder|author-link2=Herbert Kitchener, 1st Earl Kitchener}}</ref></blockquote>\n\n=== Modern Times ===\n[[File:Palestinian refugees.jpg|thumb|Palestinian refugees making their way from [[Galilee]] to Lebanon in October-November 1948]]\nWhen the state of [[Israel]] was declared in May 1948 and the Palestinian exodus \u2013 also known as the '''[[1948 Palestinian exodus|Nakba]]''' \u2013 followed, a camp of tents was set up in Burj El Shimali camp, mainly for displaced from [[Hawla]], [[Lubya|Lubieh]], [[Sepphoris|Saffuri]], and [[Tiberias]].<ref name=\":2\">{{Cite web|url=https://www.unrwa.org/where-we-work/lebanon/burj-shemali-camp|title=Burj Shemali Camp|last=|first=|date=|website=United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA)|url-status=live|archive-url=|archive-date=|access-date=23 September 2019}}</ref> More Palestinian refugees settled in the area of [[Maachouk]] \u2013 1 km to the West of Burj El Shemali \u2013  on agricultural lands owned by the Lebanese State as a neighbourhood rather than a camp. Its eastern side, which is an [[Industry|industrial]] zone, as well as the main road's southern side with many [[Commerce|commercial]] activities fall within the jurisdiction of Burj el-Shemali municipality which demonstrates the [[arbitrariness]] of many boundaries.<ref>{{Cite web|url=https://reliefweb.int/sites/reliefweb.int/files/resources/UN-Habitat_2017.05.04_NPS_Maachouk.pdf|title=Maachouk Neighbourhood Profile & Strategy, Tyre, Lebanon|last=Harake|first=Dani|last2=Kuwalti|first2=Riham|date=31 May 2017|website=reliefweb|publisher=UN HABITAT Lebanon|pages=2-3, 25-26|format=PDF|url-status=live|archive-url=|archive-date=|access-date=2 April 2020}}</ref>[[File:Musa al-Sadr & Mostafa Chamran (2).jpg|thumb|Al-Sadr (centre, in black) with Mostafa Chamran, left]]Since 1955, humanitarian assistance - infrastructure services (water, sewage, electricity, road networks and shelter), [[school]] [[education]] and [[health care]] - has been provided by the UNRWA to the residents of the camp in Burj el-Shemali.<ref>{{cite web|url=https://www.un.org/unrwa/refugees/lebanon/shemali.html|title=Archived copy|url-status=dead|archiveurl=https://web.archive.org/web/20090421060238/http://www.un.org/unrwa/refugees/lebanon/shemali.html|archivedate=2009-04-21|accessdate=2009-03-29}}</ref> \n\nFour years later, the [[Iran]]-born Shiite cleric [[Sayyid|Sayed]] [[Musa al-Sadr|Musa Sadr]] moved to Tyre in 1959 to succeed the late [[Abd al-Husayn Sharaf al-Din al-Musawi|Abdulhussein Sharafeddin]] as the Imam of Tyre and gradually changed the balance of power in Southern Lebanon and the whole country: as \"''one of his first significant acts''\" he established a [[Vocational education|vocational training]] center in Burj El Shimali that became \"''an important symbol of his leadership''\".<ref>{{Cite book|last=Norton|first=Augustus Richard|title=Amal and the Shi'a: Struggle for the Soul of Lebanon|publisher=University of Texas Press|year=1987|isbn=978-0292730403|location=Austin|pages=39}}</ref> In 1974, Sadr founded ''[[Amal Movement|Harakat al-Mahroumin]]'' (\"Movement of the Deprived\") and one year later - shortly before the beginning of the [[Lebanese Civil War]] - its ''de facto'' military wing: ''Afwaj al-Muqawama al-Lubnaniyya'' (Amal).<ref>{{Cite journal|last=Deeb|first=Marius|date=1988|title=Shi'a Movements in Lebanon: their Formation, Ideology, Social Basis, and Links with Iran and Syria|url=|journal=Third World Quarterly|volume=10 (2)|pages=685|doi=10.1080/01436598808420077|via=}}</ref> The Iranian director of Sadr's technical school, [[Mostafa Chamran]], became a major instructor of [[Guerrilla warfare|guerilla warfare]]. The US-trained [[physicist]] went on to become the first defense minister of [[History of the Islamic Republic of Iran|post-revolutionary Iran]].<ref>{{Cite book|last=Bonsen|first=Sabrina|title=Martyr Cults and Political Identities in Lebanon: \"Victory or Martyrdom\" in the Struggle of the Amal Movement|publisher=Springer Nature|year=2019|isbn=978-3-658-28098-7|location=Wiesbaden|pages=214, 248}}</ref>\n\nAfter Sadr mysteriously disappeared following a visit to Libyan leader [[Muammar Gaddafi]] on 31 August 1978, discontent within the Shiite population about the suffering from the conflict between Israel and the Palestinian factions grew, especially following the Israeli [[1978 South Lebanon conflict|Operation Litani]] in March of that year. Within this context, tensions between Amal and the Palestinian militants escalated into violent clashes in many villages of Southern Lebanon, including the Tyre area.<ref>{{Cite book|last=Abraham|first=Antoine J.|title=The Lebanon War|publisher=Praeger|year=1996|isbn=978-0275953898|location=Westport, Conn.|pages=123}}</ref> The heaviest such incident took place in April 1982, when the PLO ([[Fatah|Fateh]]) bombarded Amal's Technical Institute in Burj El Shimali for ten hours.<ref>{{Cite journal|last=Siklawi|first=Rami|date=Winter 2012|title=The Dynamics of the Amal Movement in Lebanon 1975\u201390|url=|journal=Arab Studies Quarterly|volume=34 (1)|pages=4\u201326|jstor=41858677|via=JSTOR}}</ref>\n\nFollowing the [[1982 Lebanon War|1982 Israeli Invasion]], the Palestinian refugee camp in Burj el-Shemali \u2013 according to UNRWA \u2013 \"''was badly damaged''\".<ref name=\":2\" /> An international commission to enquire into reported violations of [[international law]] by Israel during its invasion found that Israeli forces had destroyed 35 percent of the houses in the camp.<ref>{{Cite book|last=Kaufman|first=Asher|url=https://www.academia.edu/5561720/Forgetting_the_Lebanon_War_On_Silence_Denial_and_Selective_Remembrance_of_the_First_Lebanon_War|title=Forgetting the Lebanon War? On Silence, Denial and Selective Remembrance of the First Lebanon War|publisher=Cambrige University Press|year=2010|isbn=978-0521196581|editor-last=Ben-Ze\u2019ev|editor-first=Efrat|location=Cambridge|pages=205|format=PDF|editor-last2=Ginio|editor-first2=Ruth|editor-last3=Winter|editor-first3=Jay}}</ref> Much of the destruction was done ''\"systematically''\" by the IDF after the actual combat with Palestinian fighters had stopped.<ref>{{Cite book|last=Gilmour|first=David|title=Lebanon: The Fractured Country|publisher=Martin Robertson|year=1983|isbn=978-0312477394|location=Oxford|pages=178\u2013179}}</ref>\n\nIn February 1985, an Amal member from Tyre launched a [[Suicide attack|suicide-attack]] on an IDF convoy in Burj El Shimali<ref>{{Cite book|last=Bonsen|first=Sabrina|title=Martyr Cults and Political Identities in Lebanon: \"Victory or Martyrdom\" in the Struggle of the Amal Movement|publisher=Springer Nature|year=2019|isbn=978-3-658-28098-7|location=Wiesbaden|pages=214, 248}}</ref>, injuring ten soldiers. \"''Israeli reprisals in the area east of Tyre killed fifteen and wounded dozens''.\"<ref>{{Cite book|last=Smit|first=Ferdinand|url=https://www.academia.edu/12778980|title=The battle for South Lebanon: Radicalisation of Lebanon's Shi'ites 1982\u20131985 (PDF). :|publisher=Bulaaq, Uitgeverij|year=2006|isbn=978-9054600589|location=Amsterdam|pages=269|format=PDF}}</ref>\n\nDuring Israel's invasion in the [[2006 Lebanon War]], Burj El Shimali was severely hit again: on 16 July five civilians were killed by in an assault by the [[Israeli Air Force]] (IAF), including two children. On 13 August, another five civilians were killed by an IAF missile, amongst them three children and one [[Sri Lanka|Sri Lankan]] maid.<ref>{{Cite web|url=https://www.hrw.org/report/2007/09/05/why-they-died/civilian-casualties-lebanon-during-2006-war|title=Why They Died: Civilian Casualties in Lebanon during the 2006 War\"|last=|first=|date=5 September 2007|website=Human Rights Watch|url-status=live|archive-url=|archive-date=|access-date=31 March 2020}}</ref>\n\nAccording to a 2014 study paper, the majority of Palestinian refugees in Burj el-Shemali supported Fatah. However, it noted that there was \u2013 unlike in the other Tyrian camp of [[Rashidieh]] \u2013 also a considerable presence of [[Hamas]]. In addition, the [[Popular Front for the Liberation of Palestine]] (PFLP), the [[Democratic Front for the Liberation of Palestine]] (DFLP), and the [[Islamic Jihad Movement in Palestine]] were represented in the Popular Committees that rule the camp.<ref name=\":3\">{{Cite web|url=https://www.rosalux.de/fileadmin/rls_uploads/pdfs/Studien/Studien_Flucht_Vertreibung.pdf|title=Flucht und Vertreibung im Syrien-Konflikt \u2013 Eine Analyse zur Situation von Fl\u00fcchtlingen in Syrien und im Libanon|last=|first=|date=July 2014|website=Rosa-Luxemburg-Stiftung|pages=28|language=German|format=PDF|url-status=live|archive-url=|archive-date=|access-date=31 March 2020}}</ref> \n\nIn 2016, the mayor of Burj el-Shemali was [[Hajj Ali Dib]].<ref name=\":4\">{{Cite web|url=https://reliefweb.int/sites/reliefweb.int/files/resources/UN-Habitat_2017.05.22_CP_Tyre_web.pdf|title=TYRE CITY PROFILE|last=Maguire|first=Suzanne|last2=Majzoub|first2=Maya|date=2016|editor-last=Osseiran|editor-first=Tarek|website=reliefweb|publisher=UN HABITAT Lebanon|pages=13-18; 26|format=PDF|url-status=live|archive-url=|archive-date=|access-date=31 March 2020}}</ref> \n\n== Demographics ==\n\n=== Lebanese part of the town ===\nThere are no official figures for the Lebanese and Non-Lebanese population in the Lebanese-ruled part of Burj el-Shemali.\n\n=== Palestinian Refugee Camp ===\nBy 1968, there were 7,159 registered Palestinian refugees in Burj El Shimali. By 1982 this number had gone up to 11,256.<ref>{{Cite book|last=Brynen|first=Rex|title=Sanctuary And Survival: The PLO In Lebanon|publisher=Westview Press|year=1990|isbn=978-0813379197|location=Boulder|pages=}}</ref>\n\nIn 2008, the camp had 19,074 registered [[refugees]].<ref>{{cite web|url=https://www.un.org/unrwa/refugees/lebanon.html |title=Archived copy |accessdate=2008-04-18 |url-status=dead |archiveurl=https://web.archive.org/web/20080409202113/http://www.un.org/unrwa/refugees/lebanon.html |archivedate=2008-04-09 }}</ref> As of June 2018, this number had grown to 24,929, also because of \"''an influx of Syrian refugees and Palestinian refugees from Syria''\".<ref name=\":2\" /> UNRWA describes the situation as follows:<blockquote>\"''There is a very high incidence of [[thalassemia]] and [[sickle cell disease]] ([[genetic disorder]]) among Burj Shemali inhabitants and the camp is one of the poorest camps in Lebanon. [[Unemployment]] is extremely high, with seasonal agricultural work the most common source of income for both men and women.''\"<ref name=\":2\" /></blockquote>\n\n== Gallery ==\n\n=== Exhibits at the National Museum of Beirut ===\n<gallery>\nFile:BurjAlShimali LimestoneStele 5-4cBCE NationalMuseumOfBeirut RomanDeckert03102019.jpg|[[Limestone]] [[stele]], 5th to 4th century BCE\nFile:BurjAlShimali Relief IronAge-III NationalMuseumOfBeirut RomanDeckert03102019.jpg|Relief depicting a deceased person with a sheathed body, 550-333 BCE\nFile:BurjAlShemali RomanTombWithFrescoes NationalMuseumOfBeirut RomanDeckert03102019.jpg|Roman tomb with frescoes from the 2nd century CE\nFile:RomanTerracottaMask Satyr BurjAlShimali NationalMuseumOfBeirut RomanDeckert06102019.jpg|Mask of a Satyr, Roman period\n</gallery>\n\n=== The tower ===\n<gallery>\nFile:BurjAlShimali-TowerSide Lebanon-23092019RomanDeckert.jpg\nFile:BurjAlShimaliTowerLebanon 23092019RomanDeckert.jpg\nFile:BurjAlShimali-TowerInside Lebanon-23092019RomanDeckert.jpg\nFile:BurjAlShimali-Tower-view-on-Tyre 23092019RomanDeckert.jpg\n</gallery>\n\n==References==\n<references/>\n\n==External links==\n* [https://www.unrwa.org/where-we-work/lebanon/burj-shemali-camp Burj Shemali Camp],  from [[UNWRA]]\n\n{{Palestinian refugee camps}}\n{{Tyre District}}\n[[Category:Palestinian refugee camps in Lebanon]]\n\n\n{{Palestine-stub}}\n{{Lebanon-stub}}\n", "name_user": "RomanDeckert", "label": "safe", "comment": "\u2192\u200eOttoman Times", "url_page": "//en.wikipedia.org/wiki/Burj_el-Shemali"}
{"title_page": "Shatterstar", "text_new": "{{short description|Marvel Comics superhero}}\n{{about|the X-Men related character|the [[Kree]] character|Shatterstar (Kree)}}\n\n{{Infobox comics character <!--Wikipedia:WikiProject Comics-->\n| image     = Shatterstar.png\n| imagesize =\n| converted = y\n| caption = Shatterstar.<br>Art by [[Marco Santucci]].\n| character_name = Shatterstar\n| real_name = Gaveedra-Seven\n| publisher =[[Marvel Comics]]\n| debut     = ''[[The New Mutants]]'' #99 (March 1991)\n| creators  = [[Fabian Nicieza]]<br />[[Rob Liefeld]]\n| Base of operations =\n| species   = Genetically engineered humanoid / [[Mutant (Marvel Comics)|Human Mutant]]\n| homeworld = <!-- optional -->\n| alliances = {{Plain list | \n* The Cadre Alliance\n* [[X-Force]]\n* [[New Mutants]]\n* [[X-Factor Investigations]]\n}}\n| partners  = <!-- optional -->\n| supports  = <!-- optional -->\n| aliases   =  Benjamin Russell\n| powers = *Superhuman physical attributes and accelerated healing factor\n*Enhanced cognition\n*Control frequencies of electricity and generate bioelectric vibratory shockwaves \n*Create teleportation portals through his swords\n| cat       = super\n| subcat    = Marvel Comics\n| hero      = y\n| villain   =\n| sortkey   = Shatterstar\n}}\n'''Shatterstar''' ('''Gaveedra-Seven''') is a fictional [[Mutant (Marvel Comics)|mutant]] [[superhero]] appearing in [[American comic book]]s published by [[Marvel Comics]]. Created by writer [[Fabian Nicieza]] and artist [[Rob Liefeld]], the character first appeared in ''[[The New Mutants]]'' #99 (March 1991), after which he became a member of the superhero team [[X-Force]]. He later became an employee of [[X-Factor Investigations]], a private detective firm starring in the series ''[[X-Factor (comics)|X-Factor]]''. In 2013, [[ComicsAlliance]] ranked Shatterstar as #29 on their list of the \"50 Sexiest Male Characters in Comics\".<ref>{{cite web|last=Wheeler |first=Andrew |url=http://comicsalliance.com/comics-sexiest-male-characters |title=ComicsAlliance Presents The 50 Sexiest Male Characters in Comics |publisher=[[ComicsAlliance]] |date=2013-02-14 |accessdate=2015-07-28 |url-status=dead |archiveurl=https://web.archive.org/web/20151018024021/http://comicsalliance.com/comics-sexiest-male-characters |archivedate=2015-10-18 }}</ref>\n\nShatterstar made his live action debut in the film ''[[Deadpool 2]]'' played by actor [[Lewis Tan]].\n\n==Publication history==\n{{Expand section|date=November 2008}}\nShatterstar first appeared in ''[[The New Mutants]]'' #99 (March 1991), and was created by [[Fabian Nicieza]] and [[Rob Liefeld]]. He also appeared on a pin-up bonus cover in ''The New Mutants Annual'' #6 (July 1990) as part of a 'Vision to Come',<ref>https://comicvine.gamespot.com/shatterstar/4005-2156/</ref> predating his appearance in ''The New Mutants'' #99.\n\nSince his debut Shatterstar has mainly appeared in the original ''X-Force'' title, with some issues devoted solely to him.  In 2005, the character was featured in his own limited series, ''X-Force: Shatterstar''. Shatterstar became a member of X-Factor in ''X-Factor'' #45 (August 2009).\n\n==Fictional character biography==\nShatterstar comes from the planet Mojoworld (about a century in the future, as opposed to the Mojoverse, which co-exists with the contemporary Earth dimension, making him a time traveler as well as a dimension-hopper), which is ruled by the alien tyrant [[Mojo (comics)|Mojo]]. There, Shatterstar was created to be a slave; he claims to have had no parents, only a \"gestation chamber\". He was [[genetic engineering|genetically engineered]] to have enhanced physical capabilities so he could serve as an arena gladiator.\n\nShatterstar learned the arts of battle as a warrior in arenas on Mojoworld, where he participated in combats staged for Mojo's television programs. It's assumed it was here he developed his strong sense of honor and pride as a warrior, to combat the constant violence and death in his life. Eventually he escaped and joined the Cadre Alliance, the [[Rebellion|rebel]] group that sought to overthrow Mojo's [[dictatorship]]. From there, he learned the Cadre's language and began taking part in missions.\n\nOn one of these missions, Arize sent him back in time to Earth to find the [[X-Men]] and get their assistance in defeating and overthrowing Mojo. He did not find the X-Men though. Shatterstar was either [[Teleportation|teleported]] or traveled back in time to Earth at the point just before [[Cable (comics)|Cable]] reorganized the [[New Mutants]] into X-Force. At first, he battled Cable, [[Domino (comics)|Domino]], and the New Mutants, but after they talked with him the mutants aided him against the Imperial Protectorate. With Cable's assurance that they would help him defeat Mojo (though with the use of time travel it was not urgent that they leave anytime soon) Shatterstar became a founding member of the new team, X-Force.<ref>''New Mutants'' vol. 1 #99-100</ref>\n\nWith X-Force on their first mission, Shatterstar battled the [[Alliance of Evil]], and first encountered the [[New Warriors]].<ref>''New Mutants Annual'' #7</ref> He battled [[Night Thrasher (Dwayne Taylor)|Night Thrasher]] and [[Silhouette (comics)|Silhouette]].<ref>''New Warriors Annual'' #1</ref> With X-Force, the New Warriors, [[Moira MacTaggert]]'s \"[[Muir Island]] [[X-Men]]\" team, and [[X-Factor (comics)|X-Factor]], he battled [[Proteus (Marvel Comics)|Proteus]].<ref>''X-Factor Annual'' #6 (Jan. 1, 1991)</ref>\n\nX-Force's next mission involved attacking the [[Mutant Liberation Front]] base, where Shatterstar battled [[Reaper (Marvel Comics)|Reaper]].<ref>''X-Force'' #1</ref> With X-Force and [[Spider-Man]], he next battled [[Black Tom Cassidy|Tom Cassidy]] and the [[Juggernaut (comics)|Juggernaut]].<ref>''X-Force'' #3-4; ''Spider-Man'' #16 (Nov. 1, 1991)</ref> With X-Force, he battled the [[Toad (comics)|Toad]]'s [[Brotherhood of Mutants|Brotherhood of Evil Mutants]].<ref>''X-Force'' #6-8</ref>\n\nLater Shatterstar discovered, to his bewilderment, that he also had the memories of an Earthling named Benjamin Russell.<ref>''X-Force'' vol. 1 #54, 56, 59-61</ref> Soon afterwards, Mojo made Cable and Shatterstar his prisoners and transformed them into digital images for one of his television programs. In the course of the show, Shatterstar was mortally wounded in combat. Mojo's sometime ally [[Spiral (comics)|Spiral]] teleported Cable and Shatterstar back into reality, where they regained their true forms. Spiral brought Cable, Shatterstar, [[Longshot (Marvel Comics)|Longshot]] (who was Shatterstar's ally on Mojoworld), and the X-Force member [[Siryn]] to the Weisman Institute for the Criminally Insane in Rutland, [[Vermont]]. There Spiral directed them to the bedside of one of its patients, a mutant named Benjamin Russell who had no living relatives, had been in a [[coma]] since his powers had emerged, and, curiously, looked identical to Shatterstar. Longshot transferred Shatterstar's \"uemeur,\" or soul, into Benjamin Russell's body, and the two became one. This caused Benjamin Russell's body to resemble Shatterstar's original body even closer as it gained  the starburst pattern that Shatterstar had around his left eye in his original body and the hair grew to the length of Shatterstar's ponytail in his original body. Restored to peak health and full consciousness, and feeling \"whole\" for the first time in his life, Shatterstar resumed his work as a member of X-Force.<ref>''X-Force'' vol. 1 #61</ref>\n\n[[File:Shatterstar.jpg|thumb|left|Shatterstar, as seen on the cover of ''X-Force: Shatterstar #1'' (Apr 2005)<br>Art by [[Rob Liefeld]].]]\nHowever, this does not explain why Shatterstar had some of Benjamin Russell's memories before they merged, or why they looked so much alike. There are still too many mysteries yet to be explained about Shatterstar's true origin. It was hinted that Shatterstar may have been the child of Longshot and the mutant X-Man [[Dazzler (Marvel Comics)|Dazzler]] in the early run of X-Men in 1992,<ref>''X-Men'' vol. 2 #10-11</ref> and in 2009, Rob Liefeld confirmed that Shatterstar \"was pitched as Longshot's son\".<ref name=CBR>{{cite web|url=http://robot6.comicbookresources.com/2009/07/liefeld-cant-wait-to-someday-undo-shatterstar-development/|author=Melrose, Kevin|title=Liefeld 'can't wait to someday undo' Shatterstar development |publisher=[[Comic Book Resources]]|date=July 3, 2009}}</ref> A later medical exam conducted by [[Beast (comics)|Beast]] upon Shatterstar revealed that he indeed possessed DNA identical to that of Longshot, his rumored father, along with such genetic features as hollow bones and a lack of white blood cells.<ref>''X-Force'' #45</ref>\n\nDazzler revealed that she was [[pregnant]] with Longshot's child and Longshot suggested the name \"Shatterstar\" for the unborn child.<ref>''[[X-Men: Legacy|X-Men]]'' vol. 2 #11</ref> Longshot and Dazzler returned to Mojoworld to free Longshot's people and Dazzler later appeared without Longshot and without a child. It was speculated that Dazzler [[miscarriage|miscarried]],<ref>''X-Men'' vol. 2 #47</ref> though it was not established until a 2013 comic by [[Peter A. David]] what became of the infant; the child's future self, Shatterstar, wiped his parents' memories of his birth and arranged for his transport to the future.<ref name=\":0\" />\n\nShatterstar accompanied [[Rictor]] to the Richter home in Mexico to try to end Rictor's family's arms-dealing business.{{Issue|date=May 2011}} Both characters have since appeared separately so one assumes they succeeded, though it's not known why they later parted.\n\nShatterstar is later seen in [[Madripoor]], earning his money by fighting in arenas. He was sought out by Spiral, who had one of her agents make Shatterstar believe she wanted to kill him.<ref>''X-Force: Shatterstar'' #1-4</ref> However, Spiral had previously admitted in an emotional confession to Cable, when he demanded that she reveal all that she knew about the answers behind the mysteries of Shatterstar's origin immediately after Shatterstar and Benjamin Russell had been merged, that both young men \"meant more than life itself to her\" before teleporting away. With a fake quest, Spiral lured Shatterstar to an alternate universe she had conquered and ruled. On that Earth, Spiral had also killed most of that world's heroes and mutants. That Earth's Shatterstar had been killed as well. He was found by that Earth's rebel forces, including Cable and some other members he knew from X-Force. Together, they eventually defeated Spiral. Upon returning to the mainstream Earth, Shatterstar was contacted by Cable and requested to temporarily join him on a mission to defeat the [[Skornn]].<ref>''X-Force'' vol. 2 #1-6</ref> Shatterstar agreed, but first Cable wanted him to train with monks on Mount Xixabangma. After those monks were killed by Skornn's worshipers, Shatterstar was reunited with his old team and they eventually killed the Skornn.{{Issue|date=May 2011}}\n\nFollowing [[Decimation (comics)|M-Day]] and the passing of the [[Superhuman Registration Act]], Shatterstar teamed up with Domino and Caliban (all X-Force members) to break [[The 198]] out of the encampment set up for mutants on the grounds of the Xavier Institute.<ref>''Civil War: X-Men'' #1</ref> They took the escapees to a secret base provided to them by [[Captain America]] via [[Nick Fury]].<ref>''Civil War: X-Men'' #2</ref> While fighting O*N*E, he nearly killed [[Micromax]], claiming that while there is no such thing as murder during war,<ref>''Civil War: X-Men''#3</ref> he had only meant to disable him.<ref>''Civil War: X-Men'' #4</ref>\n\n===X-Factor===\nIn a 2009 ''[[X-Factor (comics)|X-Factor]]'' story, Shatterstar, whose mind is being controlled by the villain Cortex, attacks [[Strong Guy]] and [[Rictor]].<ref>David, Peter. ''X-Factor'' #43, May 2009, Marvel Comics</ref> Shatterstar is broken out of his trance-like state when Cortex's control over him is interrupted. Upon recognizing Rictor, he kisses him passionately.<ref>David, Peter. ''X-Factor'' #45, June 2009, Marvel Comics</ref> Journeying to Detroit, Cortex confronts Longshot and the two fight. As Cortex attempts to gain control over Longshot, he is startled that Longshot, like Shatterstar, is extradimensional (limiting his degree of control) and that the two men are somehow related.<ref>David, Peter. ''X-Factor'' #47, August 2009, Marvel Comics</ref>\n\nRictor and Shatterstar's relationship experiences conflict because Shatterstar\u2014who now feels romantic and sexual potential within him for the first time\u2014wishes to explore this whole new aspect of his life, desiring an [[open relationship]]. Rictor, more fully committed to maintaining theirs as a monogamous relationship, feels hurt by Shatterstar's need for sexual exploration. Things are complicated further when [[Wolfsbane (comics)|Rahne Sinclair]] walks in on them during an intimate moment, which leads to a brief fight between Shatterstar and Rahne.<ref>David, Peter. ''X-Factor'' (volume 3) #208, October 2010, Marvel Comics.</ref> Rictor stays to take care of a pregnant Rahne, who attempts to mislead him into thinking the baby is his, while Shatterstar goes off on a mission for X-Factor. On mission, he encounters the child's real father, [[Hrimhari]], which he is able to report back to Rictor.<ref>''X-Factor'' #213</ref>\n\nDuring the \"Hell on Earth War\" storyline, Shatterstar and Rictor are blasted by the God of Hell, [[Mephisto (comics)|Mephisto]], and appear to die, but are in fact sent into Mojoworld's relative past\u2014to the era of Shatterstar's arrival in Mojoworld, as shown in the final X-Factor story arc, \"The End of X-Factor\". Rictor and the audience learn that Shatterstar is the only Mojoworld rebel who was not created by Arize the Creator, as he mysteriously appeared from the sky one day. Arize then used Shatterstar's genetic material to create Longshot, making Shatterstar Longshot's father genetically. Mojo later attacks Arize's sanctuary, leading Shatterstar to time-teleport him and Rictor. They arrive later in Mojoworld's history, at a time when Dazzler and Longshot are married and fighting a war against Mojo. Dazzler goes into labor, and gives birth to a young Shatterstar, who Shatterstar explains to Rictor he will deliver to the future of Mojoworld to be raised by the people that raised him, but not before he erases Dazzler and Longshot's memories of their having a child.<ref name=\":0\">''X-Factor'' #259 (July 2013)</ref>\n\n===New Tian===\nDuring [[Secret Empire (comics)|Secret Empire]] it is revealed that Shatterstar and Rictor were able to return to the present. He is part of the New Tian residents, along with Rictor and many other mutants.<ref>''Secret Warriors'' (Vol. 2) #3</ref> After New Tian was dismantled, Rictor told Iceman that he and Shatterstar were now on a break.<ref>''Iceman'' (Vol. 3) #9</ref>\n\n==Powers and abilities==\nShatterstar possesses an overall superhuman level of physical and mental attributes (senses, strength, speed, reflexes, agility, flexibility,  stamina, and intelligence), as a result of the extra-dimensional genetic engineering that created him. Shatterstar's strength allows him to wield a heavy barbell as easy as a [[b\u014d]] (staff) and slam the [[Thing (comics)|Thing]] of the [[Fantastic Four]] through a window of the [[Baxter Building]].<ref>''X-Factor'' #200 (2010)</ref> Shatterstar's speed and agility are enhanced to the point that members of the Mutant Response Division expect him to be capable of dodging point-blank automatic weapons firing from at least three trained agents.<ref>''X-Factor'' #204 (2010)</ref>\n\nHe is an excellent military strategist and has had extensive training in many forms of the [[martial arts]] and interpersonal combat of Mojoworld; in particular, he is a master swordsman. His bones are hollow, making him far lighter than he looks and further increasing his athletic and acrobatic skills. He also has enhanced learning capabilities, being able to quickly learn and master languages and technology. He customarily wields two single-edged swords with spiked hand-guards and on occasion carries other weaponry as well. Shatterstar is able to regenerate damaged or destroyed tissue much faster than an ordinary human. Injuries such as slashes and stabbings heal completely within a matter of hours. Additionally, he possesses the ability to shift his internal organs within his body, lessening the chances of serious wounds that get through his body armor.  He has also been mentioned as having a lack of white blood cells, and DNA identical to his former teammate and father, [[Longshot (Marvel Comics)|Longshot]].<ref>''X-Force'' #47 and #51</ref>\n\nHe also has the mutant ability to control frequencies of electricity, which he can use to generate powerful bioelectric vibratory shockwaves,<ref>''Shatterstar'' Vol.2 #3</ref> and can even channel bioelectric charges through his metallic blade weaponry; he rarely uses this power as it tends to exhaust him, but once used it as a surprise attack/secret technique to apparently kill Reaper of the MLF.<ref>''X-Force Vol.1'' #28</ref> \n\nAfter Shatterstar returned from the Mojoverse, he demonstrated the ability to open an \"X\" shaped portal capable of [[Teleportation|teleporting]] individuals to their desired locations. This ability requires another individual to serve as the \"focus,\" picturing the destination.<ref>''Nation X: X-Factor'' #1 (2010)</ref> Creating such portals generates enough energy that it must be done outside, otherwise risking significant damage to any structure within which they are built. Shatterstar also requires a minimum of three to four hours to recharge between portal creation and since it requires his own energies. This ability is not actually artificial but merely another aspect of his ability to channel his energies through his swords.<ref>''X-Factor'' #201 (2010)</ref> Should his concentration be interrupted while an individual or object is partway through, those parts will be severed, ending up in the separate locations, the destination and the starting point of the portal.<ref>''X-Factor'' #202 (2010)</ref>\n\n==Sexual orientation==\n{{see also|Rictor#Sexual orientation|l1=Rictor's sexual orientation}}\n[[image:Rictorshatterstar.PNG|thumb|Rictor and Shatterstar kiss. From ''X-Factor'' (volume 3) #45. Art by Marco Santucci.]]\nAlthough Shatterstar was revealed to have a designated \"genetic bond mate\", Windsong (whom he never met and is now deceased), in the Mojoverse,<ref>''X-Force'' #29 & #30. Marvel Comics.</ref> he later claimed that even though he was fully capable physically, he had never felt any sexual stirrings or romantic love\u2014indicating a form of [[asexuality]]\u2014and has long felt \"lacking\", even in his native dimension.<ref>''X-Force'' #43. Marvel Comics.</ref> \n\nThis has since been retconned to be a lie on Shatterstar's part, as it is revealed in his Revised origin in his 2018 mini-series that he already had a sexual/romantic relationship with his gladiatorial mentor/partner Gringrave,<ref>''Shatterstar'' Vol.2 #2</ref> which ended when she betrayed him by tricking him into becoming a murderer, causing him to rebel against Mojo V.<ref>''Shatterstar'' Vol.2 #4</ref>\n\nSince then his emotional state has been fluctuating. While in X-Force, he displayed emotion, having developed a close\u2014and somewhat ambiguous\u2014friendship with [[Rictor]].<ref>''Cable #22''. Marvel Comics.</ref><ref>''X-Force #56''. Marvel Comics.</ref><ref>''X-Force #59''. Marvel Comics.</ref><ref>''X-Force #60''. Marvel Comics.</ref><ref>''X-Force \u201999 Annual''. Marvel Comics.</ref> ''X-Force'' writer [[Jeph Loeb]] hinted that Shatterstar had romantic feelings for Rictor and was planning on making the two a couple, but he left the title before he could make this happen.<ref>{{cite web|url=http://goodcomics.comicbookresources.com/2007/10/18/comic-book-urban-legends-revealed-125/ |title=Comic Book Urban Legends Revealed #125 |publisher=CBR.com|date=October 18, 2007|author=Cronin, Brian}}</ref>\n\nWriter [[Peter David]] reflects that being the subject of \"prolonged exposure to Earth\" and around Rictor changed things for the warrior, and he began to develop a real romantic capacity.<ref>{{cite web|url=http://www.afterelton.com/other/2009/12/rictor-met-shatterstar-peter-david-interview|publisher=[[AfterElton.com]]|title=Superheroes in Love: When Rictor Met Shatterstar|last=Masaki|first=Lyle|accessdate=2010-09-19|date=2009-12-10}}</ref> In ''X-Factor'' (volume 3) #45 (August 2009), Shatterstar and Rictor kiss.<ref>David, Peter. ''X-Factor (volume 3)  #45'', August 2009</ref> Shortly after the issue was published, Peter David confirmed Rictor and Shatterstar's relationship on his blog and expressed his desire to explore it further.<ref name=\"PAD Comments#1\">{{cite web|url=http://www.peterdavid.net/index.php/2009/06/24/stash-wednesday-june-24/comment-page-1/#comments |title=STASH WEDNESDAY \u2013 June 24 |publisher=PeterDavid.net |date= |accessdate=2011-01-10}}</ref><ref name=\"PAD Comments#2\">{{cite web|url=http://www.peterdavid.net/index.php/2009/06/24/stash-wednesday-june-24/#comments |title=STASH WEDNESDAY \u2013 June 24 |publisher=PeterDavid.net |date=June 24, 2009|accessdate=2011-01-10}}</ref>\n\nShatterstar's co-creator, [[Rob Liefeld]] expressed disapproval with Shatterstar not being [[asexuality|asexual]], saying that Shatterstar was meant to be \"asexual, and struggling to understand human behavior.\"<ref name=CBR/> [[Fabian Nicieza]] stated, \"In my final issue, I pretty clearly stated that Shatterstar had ''no'' real understanding of sexuality \u2013 homo or hetero \u2013 and needed to learn about general human nature before he could define his own sexual identity.\"<ref name=\"WorkingMan\">{{cite web|last=Wheeler|first=Andrew|title=Fabian Nicieza: Working for the Man|url=http://www.popimage.com/industrial/062000nicezaint.html|publisher=PopImage|accessdate=5 December 2013|url-status=dead|archiveurl=https://web.archive.org/web/20160303235907/http://www.popimage.com/industrial/062000nicezaint.html|archivedate=3 March 2016}}</ref> He added that \"I had planned to make Shatterstar think he was in love with Rictor, but only because he simply didn't know any better about what love ''was''. He would have figured, this is my best friend, I care about him, he cares about me, we spend time together, fight together, laugh together \u2013 I guess I must be in love with him.\"<ref name=\"WorkingMan\"/> Marvel Editor-in-Chief [[Joe Quesada]] defended the development, and stated that if Rob Liefeld wanted it changed, he would have to \"take it up with the next editor-in-chief\".<ref>[[Quesada, Joe]]; Phegley, Keil (July 14, 2009). [http://comicbookresources.com/?page=article&id=22007 \"Cup O' Joe\"]. CBR.com.</ref> Similarly, Peter David also defended the storyline, citing the work of other writers after Rob Liefield's tenure on the character, who hinted at the attraction between the two characters.<ref name=\"PAD\">{{cite web|url=http://www.comicbookresources.com/?page=article&id=22080 |title=X-POSITION X-TRA: Peter David |publisher=Comic Book Resources |date=2009-07-17 |accessdate=2011-01-10}}</ref> David explained in an interview that he took inspiration from ''[[Torchwood]]'' character [[Jack Harkness|Captain Jack Harkness]], whom David describes as \"swashbuckling, enthusiastic and sexually curious about anything with a pulse.\"<ref name=\"cbr 1\">{{cite web|url=http://www.comicbookresources.com/?page=article&id=24992|title=Peter David's \"X-Factor\": Earth Moving, Star Shattering|first=Chaos|last=Mckenzie|work=Comic Book Resources|accessdate=2010-09-19|date=2010-02-25}}</ref> As such, David's ''X-Factor'' sees Shatterstar want to have a sexually [[open relationship]] on account of having been (as in Liefeld's stories) sexually and romantically closed off to the world all his life.<ref>David, Peter. ''X-Factor'' #207 (2010). Marvel Comics.</ref> In the [[Comic book letter column|letter column]] to issue 232, a fan criticized David for conflating bisexuality with [[polyamory]], to which David responded that Shatterstar is in fact both bisexual and polyamorous.<ref>David, Peter. ''X-Factor'' #232 (2012). Marvel Comics.</ref>\n\n==Other versions==\nShatterstar's double-bladed sword is seen among the relics in the trophy room of the elderly [[Rick Jones (comics)|Rick Jones]] in the dystopian alternate future of ''[[The Incredible Hulk: Future Imperfect]]''.\n\nIn an alternate future seen in ''X-Force Annual'' #1, the [[X-Force]] team led by [[Cannonball (comics)|Cannonball]] were approached by Mojoworld Spineless Ones, [[Mojo (comics)|Mojo]]. Spineless Ones were searching for [[X-Force]] in hopes of helping them to liberate their planet, this time from Shatterstar, who had since taken over as ruler and oppressed the Spineless Ones. Arriving on Mojoworld, the mutants and their allies found their way to a stadium, where Spineless Ones were being forced to partake in gladiator-style combat. X-Force joined the fray, and when Shatterstar realized the error of his ways, his second-in-command, the Scheduler, betrayed him. Shatterstar, along with X-Force and Arize defeated the Scheduler and his followers.<ref>''X-Force Annual'' #1. Marvel Comics.</ref>\n\nIn the [[Amalgam Comics]] line, Shatterstar was combined with [[Starfire (Koriand'r)|Starfire]] of [[DC Comics]] to create the new character [[Shatterstarfire]].<ref>''X-Patrol'' #1</ref>\n\n==In other media==\n===Film===\nShatterstar appears in [[20th Century Fox]]'s [[X-Men (film series)|''X-Men'' film franchise]], portrayed by [[Lewis Tan]], in ''[[Deadpool 2]]''.<ref>http://www.indiewire.com/2018/04/bill-skarsgard-deadpool-2-zeitgeist-first-look-1201958114/</ref> He claims his real name is '''Rusty'''. He is recruited into X-Force by Deadpool and Weasel after revealing his backstory as an alien from Mojoworld and being just \"like [Deadpool], but better\". Upon jumping out of the plane on the very first mission of X-Force, Shatterstar parachutes into an active helicopter's blades, killing him. All other X-Force heroes aside from Deadpool and [[Domino (comics)|Domino]] are killed in similarly grisly fashion, and Deadpool shows no pity for any of them but Peter, although he specifically sets some scorn aside for Shatterstar, describing him as \"a bit of a prick.\"\n\n===Video games===\nShatterstar appears as an unlockable playable character in ''[[Marvel: Avengers Alliance]]''.{{cn|date=April 2018}}\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.marvel.com/universe/Shatterstar Shatterstar] at Marvel.com\n* [http://www.marveldirectory.com/individuals/s/shatterstar.htm Shatterstar] at Marvel Directory\n* [http://uncannyxmen.net/characters/shatterstar Shatterstar] at UncannyXmen.net\n{{X-Force}}\n{{New Mutants}}\n\n[[Category:Characters created by Fabian Nicieza]]\n[[Category:Characters created by Rob Liefeld]]\n[[Category:Comics characters introduced in 1991]]\n[[Category:Deadpool characters]]\n[[Category:Fictional bisexual males]]\n[[Category:Fictional slaves]]\n[[Category:Marvel Comics characters with superhuman strength]]\n[[Category:Marvel Comics LGBT superheroes]]\n[[Category:Marvel Comics martial artists]]\n[[Category:Marvel Comics mutants]]\n[[Category:Superhero film characters]]\n[[Category:X-Men supporting characters]]\n", "text_old": "{{short description|Marvel Comics superhero}}\n{{about|the X-Men related character|the [[Kree]] character|Shatterstar (Kree)}}\n\n{{Infobox comics character <!--Wikipedia:WikiProject Comics-->\n| image     = Shatterstar.png\n| imagesize =\n| converted = y\n| caption = Shatterstar.<br>Art by [[Marco Santucci]].\n| character_name = Shatterstar\n| real_name = Gaveedra-Seven\n| publisher =[[Marvel Comics]]\n| debut     = ''[[The New Mutants]]'' #99 (March 1991)\n| creators  = [[Fabian Nicieza]]<br />[[Rob Liefeld]]\n| Base of operations =\n| species   = Genetically engineered humanoid / [[Mutant (Marvel Comics)|Human Mutant]]\n| homeworld = <!-- optional -->\n| alliances = {{Plain list | \n* The Cadre Alliance\n* [[X-Force]]\n* [[New Mutants]]\n* [[X-Factor Investigations]]\n}}\n| partners  = <!-- optional -->\n| supports  = <!-- optional -->\n| aliases   =  Benjamin Russell\n| powers = *Superhuman physical attributes and accelerated healing factor\n*Enhanced cognition\n*Control frequencies of electricity and generate bioelectric vibratory shockwaves \n*Create teleportation portals through his swords\n| cat       = super\n| subcat    = Marvel Comics\n| hero      = y\n| villain   =\n| sortkey   = Shatterstar\n}}\n'''Shatterstar''' ('''Gaveedra-Seven''') is a fictional [[Mutant (Marvel Comics)|mutant]] [[superhero]] appearing in [[American comic book]]s published by [[Marvel Comics]]. Created by writer [[Fabian Nicieza]] and artist [[Rob Liefeld]], the character first appeared in ''[[The New Mutants]]'' #99 (March 1991), after which he became a member of the superhero team [[X-Force]]. He later became an employee of [[X-Factor Investigations]], a private detective firm starring in the series ''[[X-Factor (comics)|X-Factor]]''. In 2013, [[ComicsAlliance]] ranked Shatterstar as #29 on their list of the \"50 Sexiest Male Characters in Comics\".<ref>{{cite web|last=Wheeler |first=Andrew |url=http://comicsalliance.com/comics-sexiest-male-characters |title=ComicsAlliance Presents The 50 Sexiest Male Characters in Comics |publisher=[[ComicsAlliance]] |date=2013-02-14 |accessdate=2015-07-28 |url-status=dead |archiveurl=https://web.archive.org/web/20151018024021/http://comicsalliance.com/comics-sexiest-male-characters |archivedate=2015-10-18 }}</ref>\n\nShatterstar made his live action debut in the film ''[[Deadpool 2]]'' played by actor [[Lewis Tan]].\n\n==Publication history==\n{{Expand section|date=November 2008}}\nShatterstar first appeared in ''[[The New Mutants]]'' #99 (March 1991), and was created by [[Fabian Nicieza]] and [[Rob Liefeld]]. He also appeared on a pin-up bonus cover in ''The New Mutants Annual'' #6 (July 1990) as part of a 'Vision to Come',<ref>https://comicvine.gamespot.com/shatterstar/4005-2156/</ref> predating his appearance in ''The New Mutants'' #99.\n\nSince his debut Shatterstar has mainly appeared in the original ''X-Force'' title, with some issues devoted solely to him.  In 2005, the character was featured in his own limited series, ''X-Force: Shatterstar''. Shatterstar became a member of X-Factor in ''X-Factor'' #45 (August 2009).\n\n==Fictional character biography==\nShatterstar comes from the planet Mojoworld (about a century in the future, as opposed to the Mojoverse, which co-exists with the contemporary Earth dimension, making him a time traveler as well as a dimension-hopper), which is ruled by the alien tyrant [[Mojo (comics)|Mojo]]. There, Shatterstar was created to be a slave; he claims to have had no parents, only a \"gestation chamber\". He was [[genetic engineering|genetically engineered]] to have enhanced physical capabilities so he could serve as an arena gladiator.\n\nShatterstar learned the arts of battle as a warrior in arenas on Mojoworld, where he participated in combats staged for Mojo's television programs. It\u2019s assumed it was here he developed his strong sense of honor and pride as a warrior, to combat the constant violence and death in his life. Eventually he escaped and joined the Cadre Alliance, the [[Rebellion|rebel]] group that sought to overthrow Mojo's [[dictatorship]]. From there, he learned the Cadre's language and began taking part in missions.\n\nOn one of these missions, Arize sent him back in time to Earth to find the [[X-Men]] and get their assistance in defeating and overthrowing Mojo. He did not find the X-Men though. Shatterstar was either [[Teleportation|teleported]] or traveled back in time to Earth at the point just before [[Cable (comics)|Cable]] reorganized the [[New Mutants]] into X-Force. At first, he battled Cable, [[Domino (comics)|Domino]], and the New Mutants, but after they talked with him the mutants aided him against the Imperial Protectorate. With Cable's assurance that they would help him defeat Mojo (though with the use of time travel it was not urgent that they leave anytime soon) Shatterstar became a founding member of the new team, X-Force.<ref>''New Mutants'' vol. 1 #99-100</ref>\n\nWith X-Force on their first mission, Shatterstar battled the [[Alliance of Evil]], and first encountered the [[New Warriors]].<ref>''New Mutants Annual'' #7</ref> He battled [[Night Thrasher (Dwayne Taylor)|Night Thrasher]] and [[Silhouette (comics)|Silhouette]].<ref>''New Warriors Annual'' #1</ref> With X-Force, the New Warriors, [[Moira MacTaggert]]'s \"[[Muir Island]] [[X-Men]]\" team, and [[X-Factor (comics)|X-Factor]], he battled [[Proteus (Marvel Comics)|Proteus]].<ref>''X-Factor Annual'' #6 (Jan. 1, 1991)</ref>\n\nX-Force's next mission involved attacking the [[Mutant Liberation Front]] base, where Shatterstar battled [[Reaper (Marvel Comics)|Reaper]].<ref>''X-Force'' #1</ref> With X-Force and [[Spider-Man]], he next battled [[Black Tom Cassidy|Tom Cassidy]] and the [[Juggernaut (comics)|Juggernaut]].<ref>''X-Force'' #3-4; ''Spider-Man'' #16 (Nov. 1, 1991)</ref> With X-Force, he battled the [[Toad (comics)|Toad]]'s [[Brotherhood of Mutants|Brotherhood of Evil Mutants]].<ref>''X-Force'' #6-8</ref>\n\nLater Shatterstar discovered, to his bewilderment, that he also had the memories of an Earthling named Benjamin Russell.<ref>''X-Force'' vol. 1 #54, 56, 59-61</ref> Soon afterwards, Mojo made Cable and Shatterstar his prisoners and transformed them into digital images for one of his television programs. In the course of the show, Shatterstar was mortally wounded in combat. Mojo's sometime ally [[Spiral (comics)|Spiral]] teleported Cable and Shatterstar back into reality, where they regained their true forms. Spiral brought Cable, Shatterstar, [[Longshot (Marvel Comics)|Longshot]] (who was Shatterstar's ally on Mojoworld), and the X-Force member [[Siryn]] to the Weisman Institute for the Criminally Insane in Rutland, [[Vermont]]. There Spiral directed them to the bedside of one of its patients, a mutant named Benjamin Russell who had no living relatives, had been in a [[coma]] since his powers had emerged, and, curiously, looked identical to Shatterstar. Longshot transferred Shatterstar's \"uemeur,\" or soul, into Benjamin Russell's body, and the two became one. This caused Benjamin Russell's body to resemble Shatterstar's original body even closer as it gained  the starburst pattern that Shatterstar had around his left eye in his original body and the hair grew to the length of Shatterstar's ponytail in his original body. Restored to peak health and full consciousness, and feeling \"whole\" for the first time in his life, Shatterstar resumed his work as a member of X-Force.<ref>''X-Force'' vol. 1 #61</ref>\n\n[[File:Shatterstar.jpg|thumb|left|Shatterstar, as seen on the cover of ''X-Force: Shatterstar #1'' (Apr 2005)<br>Art by [[Rob Liefeld]].]]\nHowever, this does not explain why Shatterstar had some of Benjamin Russell's memories before they merged, or why they looked so much alike. There are still too many mysteries yet to be explained about Shatterstar's true origin. It was hinted that Shatterstar may have been the child of Longshot and the mutant X-Man [[Dazzler (Marvel Comics)|Dazzler]] in the early run of X-Men in 1992,<ref>''X-Men'' vol. 2 #10-11</ref> and in 2009, Rob Liefeld confirmed that Shatterstar \"was pitched as Longshot's son\".<ref name=CBR>{{cite web|url=http://robot6.comicbookresources.com/2009/07/liefeld-cant-wait-to-someday-undo-shatterstar-development/|author=Melrose, Kevin|title=Liefeld 'can't wait to someday undo' Shatterstar development |publisher=[[Comic Book Resources]]|date=July 3, 2009}}</ref> A later medical exam conducted by [[Beast (comics)|Beast]] upon Shatterstar revealed that he indeed possessed DNA identical to that of Longshot, his rumored father, along with such genetic features as hollow bones and a lack of white blood cells.<ref>''X-Force'' #45</ref>\n\nDazzler revealed that she was [[pregnant]] with Longshot's child and Longshot suggested the name \"Shatterstar\" for the unborn child.<ref>''[[X-Men: Legacy|X-Men]]'' vol. 2 #11</ref> Longshot and Dazzler returned to Mojoworld to free Longshot's people and Dazzler later appeared without Longshot and without a child. It was speculated that Dazzler [[miscarriage|miscarried]],<ref>''X-Men'' vol. 2 #47</ref> though it was not established until a 2013 comic by [[Peter A. David]] what became of the infant; the child's future self, Shatterstar, wiped his parents' memories of his birth and arranged for his transport to the future.<ref name=\":0\" />\n\nShatterstar accompanied [[Rictor]] to the Richter home in Mexico to try to end Rictor's family's arms-dealing business.{{Issue|date=May 2011}} Both characters have since appeared separately so one assumes they succeeded, though it\u2019s not known why they later parted.\n\nShatterstar is later seen in [[Madripoor]], earning his money by fighting in arenas. He was sought out by Spiral, who had one of her agents make Shatterstar believe she wanted to kill him.<ref>''X-Force: Shatterstar'' #1-4</ref> However, Spiral had previously admitted in an emotional confession to Cable, when he demanded that she reveal all that she knew about the answers behind the mysteries of Shatterstar's origin immediately after Shatterstar and Benjamin Russell had been merged, that both young men \"meant more than life itself to her\" before teleporting away. With a fake quest, Spiral lured Shatterstar to an alternate universe she had conquered and ruled. On that Earth, Spiral had also killed most of that world's heroes and mutants. That Earth's Shatterstar had been killed as well. He was found by that Earth's rebel forces, including Cable and some other members he knew from X-Force. Together, they eventually defeated Spiral. Upon returning to the mainstream Earth, Shatterstar was contacted by Cable and requested to temporarily join him on a mission to defeat the [[Skornn]].<ref>''X-Force'' vol. 2 #1-6</ref> Shatterstar agreed, but first Cable wanted him to train with monks on Mount Xixabangma. After those monks were killed by Skornn's worshipers, Shatterstar was reunited with his old team and they eventually killed the Skornn.{{Issue|date=May 2011}}\n\nFollowing [[Decimation (comics)|M-Day]] and the passing of the [[Superhuman Registration Act]], Shatterstar teamed up with Domino and Caliban (all X-Force members) to break [[The 198]] out of the encampment set up for mutants on the grounds of the Xavier Institute.<ref>''Civil War: X-Men'' #1</ref> They took the escapees to a secret base provided to them by [[Captain America]] via [[Nick Fury]].<ref>''Civil War: X-Men'' #2</ref> While fighting O*N*E, he nearly killed [[Micromax]], claiming that while there is no such thing as murder during war,<ref>''Civil War: X-Men''#3</ref> he had only meant to disable him.<ref>''Civil War: X-Men'' #4</ref>\n\n===X-Factor===\nIn a 2009 ''[[X-Factor (comics)|X-Factor]]'' story, Shatterstar, whose mind is being controlled by the villain Cortex, attacks [[Strong Guy]] and [[Rictor]].<ref>David, Peter. ''X-Factor'' #43, May 2009, Marvel Comics</ref> Shatterstar is broken out of his trance-like state when Cortex's control over him is interrupted. Upon recognizing Rictor, he kisses him passionately.<ref>David, Peter. ''X-Factor'' #45, June 2009, Marvel Comics</ref> Journeying to Detroit, Cortex confronts Longshot and the two fight. As Cortex attempts to gain control over Longshot, he is startled that Longshot, like Shatterstar, is extradimensional (limiting his degree of control) and that the two men are somehow related.<ref>David, Peter. ''X-Factor'' #47, August 2009, Marvel Comics</ref>\n\nRictor and Shatterstar's relationship experiences conflict because Shatterstar\u2014who now feels romantic and sexual potential within him for the first time\u2014wishes to explore this whole new aspect of his life, desiring an [[open relationship]]. Rictor, more fully committed to maintaining theirs as a monogamous relationship, feels hurt by Shatterstar's need for sexual exploration. Things are complicated further when [[Wolfsbane (comics)|Rahne Sinclair]] walks in on them during an intimate moment, which leads to a brief fight between Shatterstar and Rahne.<ref>David, Peter. ''X-Factor'' (volume 3) #208, October 2010, Marvel Comics.</ref> Rictor stays to take care of a pregnant Rahne, who attempts to mislead him into thinking the baby is his, while Shatterstar goes off on a mission for X-Factor. On mission, he encounters the child's real father, [[Hrimhari]], which he is able to report back to Rictor.<ref>''X-Factor'' #213</ref>\n\nDuring the \"Hell on Earth War\" storyline, Shatterstar and Rictor are blasted by the God of Hell, [[Mephisto (comics)|Mephisto]], and appear to die, but are in fact sent into Mojoworld's relative past\u2014to the era of Shatterstar's arrival in Mojoworld, as shown in the final X-Factor story arc, \"The End of X-Factor\". Rictor and the audience learn that Shatterstar is the only Mojoworld rebel who was not created by Arize the Creator, as he mysteriously appeared from the sky one day. Arize then used Shatterstar's genetic material to create Longshot, making Shatterstar Longshot's father genetically. Mojo later attacks Arize's sanctuary, leading Shatterstar to time-teleport him and Rictor. They arrive later in Mojoworld's history, at a time when Dazzler and Longshot are married and fighting a war against Mojo. Dazzler goes into labor, and gives birth to a young Shatterstar, who Shatterstar explains to Rictor he will deliver to the future of Mojoworld to be raised by the people that raised him, but not before he erases Dazzler and Longshot's memories of their having a child.<ref name=\":0\">''X-Factor'' #259 (July 2013)</ref>\n\n===New Tian===\nDuring [[Secret Empire (comics)|Secret Empire]] it is revealed that Shatterstar and Rictor were able to return to the present. He is part of the New Tian residents, along with Rictor and many other mutants.<ref>''Secret Warriors'' (Vol. 2) #3</ref> After New Tian was dismantled, Rictor told Iceman that he and Shatterstar were now on a break.<ref>''Iceman'' (Vol. 3) #9</ref>\n\n==Powers and abilities==\nShatterstar possesses an overall superhuman level of physical and mental attributes (senses, strength, speed, reflexes, agility, flexibility,  stamina, and intelligence), as a result of the extra-dimensional genetic engineering that created him. Shatterstar's strength allows him to wield a heavy barbell as easy as a [[b\u014d]] (staff) and slam the [[Thing (comics)|Thing]] of the [[Fantastic Four]] through a window of the [[Baxter Building]].<ref>''X-Factor'' #200 (2010)</ref> Shatterstar's speed and agility are enhanced to the point that members of the Mutant Response Division expect him to be capable of dodging point-blank automatic weapons firing from at least three trained agents.<ref>''X-Factor'' #204 (2010)</ref>\n\nHe is an excellent military strategist and has had extensive training in many forms of the [[martial arts]] and interpersonal combat of Mojoworld; in particular, he is a master swordsman. His bones are hollow, making him far lighter than he looks and further increasing his athletic and acrobatic skills. He also has enhanced learning capabilities, being able to quickly learn and master languages and technology. He customarily wields two single-edged swords with spiked hand-guards and on occasion carries other weaponry as well. Shatterstar is able to regenerate damaged or destroyed tissue much faster than an ordinary human. Injuries such as slashes and stabbings heal completely within a matter of hours. Additionally, he possesses the ability to shift his internal organs within his body, lessening the chances of serious wounds that get through his body armor.  He has also been mentioned as having a lack of white blood cells, and DNA identical to his former teammate and father, [[Longshot (Marvel Comics)|Longshot]].<ref>''X-Force'' #47 and #51</ref>\n\nHe also has the mutant ability to control frequencies of electricity, which he can use to generate powerful bioelectric vibratory shockwaves,<ref>''Shatterstar'' Vol.2 #3</ref> and can even channel bioelectric charges through his metallic blade weaponry; he rarely uses this power as it tends to exhaust him, but once used it as a surprise attack/secret technique to apparently kill Reaper of the MLF.<ref>''X-Force Vol.1'' #28</ref> \n\nAfter Shatterstar returned from the Mojoverse, he demonstrated the ability to open an \"X\" shaped portal capable of [[Teleportation|teleporting]] individuals to their desired locations. This ability requires another individual to serve as the \"focus,\" picturing the destination.<ref>''Nation X: X-Factor'' #1 (2010)</ref> Creating such portals generates enough energy that it must be done outside, otherwise risking significant damage to any structure within which they are built. Shatterstar also requires a minimum of three to four hours to recharge between portal creation and since it requires his own energies. This ability is not actually artificial but merely another aspect of his ability to channel his energies through his swords.<ref>''X-Factor'' #201 (2010)</ref> Should his concentration be interrupted while an individual or object is partway through, those parts will be severed, ending up in the separate locations, the destination and the starting point of the portal.<ref>''X-Factor'' #202 (2010)</ref>\n\n==Sexual orientation==\n{{see also|Rictor#Sexual orientation|l1=Rictor's sexual orientation}}\n[[image:Rictorshatterstar.PNG|thumb|Rictor and Shatterstar kiss. From ''X-Factor'' (volume 3) #45. Art by Marco Santucci.]]\nAlthough Shatterstar was revealed to have a designated \"genetic bond mate\", Windsong (whom he never met and is now deceased), in the Mojoverse,<ref>''X-Force'' #29 & #30. Marvel Comics.</ref> he later claimed that even though he was fully capable physically, he had never felt any sexual stirrings or romantic love\u2014indicating a form of [[asexuality]]\u2014and has long felt \"lacking\", even in his native dimension.<ref>''X-Force'' #43. Marvel Comics.</ref> \n\nThis has since been retconned to be a lie on Shatterstar\u2019s part, as it is revealed in his Revised origin in his 2018 mini-series that he already had a sexual/romantic relationship with his gladiatorial mentor/partner Gringrave,<ref>''Shatterstar'' Vol.2 #2</ref> which ended when she betrayed him by tricking him into becoming a murderer, causing him to rebel against Mojo V.<ref>''Shatterstar'' Vol.2 #4</ref>\n\nSince then his emotional state has been fluctuating. While in X-Force , he displayed emotion, having developed a close\u2014and somewhat ambiguous\u2014friendship with [[Rictor]].<ref>''Cable #22''. Marvel Comics.</ref><ref>''X-Force #56''. Marvel Comics.</ref><ref>''X-Force #59''. Marvel Comics.</ref><ref>''X-Force #60''. Marvel Comics.</ref><ref>''X-Force \u201999 Annual''. Marvel Comics.</ref> ''X-Force'' writer [[Jeph Loeb]] hinted that Shatterstar had romantic feelings for Rictor and was planning on making the two a couple, but he left the title before he could make this happen.<ref>{{cite web|url=http://goodcomics.comicbookresources.com/2007/10/18/comic-book-urban-legends-revealed-125/ |title=Comic Book Urban Legends Revealed #125 |publisher=CBR.com|date=October 18, 2007|author=Cronin, Brian}}</ref>\n\nWriter [[Peter David]] reflects that being the subject of \"prolonged exposure to Earth\" and around Rictor changed things for the warrior, and he began to develop a real romantic capacity.<ref>{{cite web|url=http://www.afterelton.com/other/2009/12/rictor-met-shatterstar-peter-david-interview|publisher=[[AfterElton.com]]|title=Superheroes in Love: When Rictor Met Shatterstar|last=Masaki|first=Lyle|accessdate=2010-09-19|date=2009-12-10}}</ref> In ''X-Factor'' (volume 3) #45 (August 2009), Shatterstar and Rictor kiss.<ref>David, Peter. ''X-Factor (volume 3)  #45'', August 2009</ref> Shortly after the issue was published, Peter David confirmed Rictor and Shatterstar's relationship on his blog and expressed his desire to explore it further.<ref name=\"PAD Comments#1\">{{cite web|url=http://www.peterdavid.net/index.php/2009/06/24/stash-wednesday-june-24/comment-page-1/#comments |title=STASH WEDNESDAY \u2013 June 24 |publisher=PeterDavid.net |date= |accessdate=2011-01-10}}</ref><ref name=\"PAD Comments#2\">{{cite web|url=http://www.peterdavid.net/index.php/2009/06/24/stash-wednesday-june-24/#comments |title=STASH WEDNESDAY \u2013 June 24 |publisher=PeterDavid.net |date=June 24, 2009|accessdate=2011-01-10}}</ref>\n\nShatterstar's co-creator, [[Rob Liefeld]] expressed disapproval with Shatterstar not being [[asexuality|asexual]], saying that Shatterstar was meant to be \"asexual, and struggling to understand human behavior.\"<ref name=CBR/> [[Fabian Nicieza]] stated, \"In my final issue, I pretty clearly stated that Shatterstar had ''no'' real understanding of sexuality \u2013 homo or hetero \u2013 and needed to learn about general human nature before he could define his own sexual identity.\"<ref name=\"WorkingMan\">{{cite web|last=Wheeler|first=Andrew|title=Fabian Nicieza: Working for the Man|url=http://www.popimage.com/industrial/062000nicezaint.html|publisher=PopImage|accessdate=5 December 2013|url-status=dead|archiveurl=https://web.archive.org/web/20160303235907/http://www.popimage.com/industrial/062000nicezaint.html|archivedate=3 March 2016}}</ref> He added that \"I had planned to make Shatterstar think he was in love with Rictor, but only because he simply didn't know any better about what love ''was''. He would have figured, this is my best friend, I care about him, he cares about me, we spend time together, fight together, laugh together \u2013 I guess I must be in love with him.\"<ref name=\"WorkingMan\"/> Marvel Editor-in-Chief [[Joe Quesada]] defended the development, and stated that if Rob Liefeld wanted it changed, he would have to \"take it up with the next editor-in-chief\".<ref>[[Quesada, Joe]]; Phegley, Keil (July 14, 2009). [http://comicbookresources.com/?page=article&id=22007 \"Cup O' Joe\"]. CBR.com.</ref> Similarly, Peter David also defended the storyline, citing the work of other writers after Rob Liefield's tenure on the character, who hinted at the attraction between the two characters.<ref name=\"PAD\">{{cite web|url=http://www.comicbookresources.com/?page=article&id=22080 |title=X-POSITION X-TRA: Peter David |publisher=Comic Book Resources |date=2009-07-17 |accessdate=2011-01-10}}</ref> David explained in an interview that he took inspiration from ''[[Torchwood]]'' character [[Jack Harkness|Captain Jack Harkness]], whom David describes as \"swashbuckling, enthusiastic and sexually curious about anything with a pulse.\"<ref name=\"cbr 1\">{{cite web|url=http://www.comicbookresources.com/?page=article&id=24992|title=Peter David's \"X-Factor\": Earth Moving, Star Shattering|first=Chaos|last=Mckenzie|work=Comic Book Resources|accessdate=2010-09-19|date=2010-02-25}}</ref> As such, David's ''X-Factor'' sees Shatterstar want to have a sexually [[open relationship]] on account of having been (as in Liefeld's stories) sexually and romantically closed off to the world all his life.<ref>David, Peter. ''X-Factor'' #207 (2010). Marvel Comics.</ref> In the [[Comic book letter column|letter column]] to issue 232, a fan criticized David for conflating bisexuality with [[polyamory]], to which David responded that Shatterstar is in fact both bisexual and polyamorous.<ref>David, Peter. ''X-Factor'' #232 (2012). Marvel Comics.</ref>\n\n==Other versions==\nShatterstar's double-bladed sword is seen among the relics in the trophy room of the elderly [[Rick Jones (comics)|Rick Jones]] in the dystopian alternate future of ''[[The Incredible Hulk: Future Imperfect]]''.\n\nIn an alternate future seen in ''X-Force Annual'' #1, the [[X-Force]] team led by [[Cannonball (comics)|Cannonball]] were approached by Mojoworld Spineless Ones, [[Mojo (comics)|Mojo]]. Spineless Ones were searching for [[X-Force]] in hopes of helping them to liberate their planet, this time from Shatterstar, who had since taken over as ruler and oppressed the Spineless Ones. Arriving on Mojoworld, the mutants and their allies found their way to a stadium, where Spineless Ones were being forced to partake in gladiator-style combat. X-Force joined the fray, and when Shatterstar realized the error of his ways, his second-in-command, the Scheduler, betrayed him. Shatterstar, along with X-Force and Arize defeated the Scheduler and his followers.<ref>''X-Force Annual'' #1. Marvel Comics.</ref>\n\nIn the [[Amalgam Comics]] line, Shatterstar was combined with [[Starfire (Koriand'r)|Starfire]] of [[DC Comics]] to create the new character [[Shatterstarfire]].<ref>''X-Patrol'' #1</ref>\n\n==In other media==\n===Film===\nShatterstar appears in [[20th Century Fox]]'s [[X-Men (film series)|''X-Men'' film franchise]], portrayed by [[Lewis Tan]], in ''[[Deadpool 2]]''.<ref>http://www.indiewire.com/2018/04/bill-skarsgard-deadpool-2-zeitgeist-first-look-1201958114/</ref> He claims his real name is '''Rusty'''. He is recruited into X-Force by Deadpool and Weasel after revealing his backstory as an alien from Mojoworld and being just \"like [Deadpool], but better\". Upon jumping out of the plane on the very first mission of X-Force, Shatterstar parachutes into an active helicopter's blades, killing him. All other X-Force heroes aside from Deadpool and [[Domino (comics)|Domino]] are killed in similarly grisly fashion, and Deadpool shows no pity for any of them but Peter, although he specifically sets some scorn aside for Shatterstar, describing him as \"a bit of a prick.\"\n\n===Video games===\nShatterstar appears as an unlockable playable character in ''[[Marvel: Avengers Alliance]]''.{{cn|date=April 2018}}\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.marvel.com/universe/Shatterstar Shatterstar] at Marvel.com\n* [http://www.marveldirectory.com/individuals/s/shatterstar.htm Shatterstar] at Marvel Directory\n* [http://uncannyxmen.net/characters/shatterstar Shatterstar] at UncannyXmen.net\n{{X-Force}}\n{{New Mutants}}\n\n[[Category:Characters created by Fabian Nicieza]]\n[[Category:Characters created by Rob Liefeld]]\n[[Category:Comics characters introduced in 1991]]\n[[Category:Deadpool characters]]\n[[Category:Fictional bisexual males]]\n[[Category:Fictional slaves]]\n[[Category:Marvel Comics characters with superhuman strength]]\n[[Category:Marvel Comics LGBT superheroes]]\n[[Category:Marvel Comics martial artists]]\n[[Category:Marvel Comics mutants]]\n[[Category:Superhero film characters]]\n[[Category:X-Men supporting characters]]\n", "name_user": "Iridescent", "label": "safe", "comment": "Cleanup andtypo fixing,typo(s) fixed: ,  \u2192 , , It\u2019s \u2192 It's (2), \u2019s \u2192 's", "url_page": "//en.wikipedia.org/wiki/Shatterstar"}
{"title_page": "Jos\u00e9 Emilio Pacheco", "text_new": "{{Spanish name|Pacheco|Berny}}\n{{Infobox philosopher\n| region = [[Western philosophy]]\n| era = [[20th-century philosophy]]\n| name        = Jos\u00e9 Emilio Pacheco\n| image       = MEX ON JOSE EMILIO PACHECO (12166149286).jpg\n| image_size  = 200px\n| caption     = Jos\u00e9 Emilio Pacheco Berny in 2009\n| birth_name  = Jos\u00e9 Emilio Pacheco Berny\n| birth_date  = June 30, 1939 \n| birth_place = [[Mexico City]], Mexico\n| death_date  = January 26, 2014 (aged 74)\n| death_place = [[Mexico City]], Mexico\n| nationality = Mexican\n| alma_mater  = [[National Autonomous University of Mexico]]\n| school_tradition  =  \n| main_interests    =  \n| influences        =  \n| influenced        = \n| notable_ideas     =  \n}}\n\n'''Jos\u00e9 Emilio Pacheco Berny''' {{Audio|Es-Jose Emilio Pacheco.ogg|audio}} (June 30, 1939 \u2013 January 26, 2014) was a Mexican poet, [[essayist]], [[novelist]] and [[short story]] writer. He is regarded as one of the major Mexican poets of the second half of the 20th century. The [[Berlin International Literature Festival]] has praised him as \"one of the most significant contemporary Latin American poets\".<ref>{{cite web|title=Jos\u00e9 Emilio Pacheco - Biography|url=http://www.literaturfestival.com/participants/authors/2001/jose-emilio-pacheco|publisher=International Literature Festival Berlin|accessdate=2 June 2012}}</ref> In 2009 he was awarded the [[Cervantes Prize]] for his literary oeuvre.<ref>{{cite news|last=Woolls|first=Daniel|title=Cervantes Literary Award Goes To Mexican Pacheco|url=http://www.huffingtonpost.com/2009/11/30/cervantes-literary-award_n_374555.html|accessdate=2 June 2012|newspaper=The Huffington Post|date=30 November 2009}}</ref>\n\nHe taught at [[National Autonomous University of Mexico|UNAM]], as well as the [[University of Maryland, College Park]], the [[University of Essex]], and many others in the United States, Canada and the United Kingdom.\n\nHe died aged 74 in 2014 after suffering a cardiac arrest.<ref>{{cite web|url=http://www.eluniversal.com.mx/cultura/2014/jose-emilio-pacheco-muere-982622.html|title=jose-emilio-pacheco-muere}}</ref>\n\n==Awards==\n[[File:CCMDonation62.JPG|thumb|250px|Jos\u00e9 Emilio Pacheco with an award]]\nHe was awarded the following prizes: [[Premio Cervantes]] 2009, [[Reina Sof\u00eda Award]] (2009), [[Federico Garc\u00eda Lorca Award]] (2005), [[Octavio Paz Award]] (2003), [[Pablo Neruda Award]] (2004), [[Ram\u00f3n L\u00f3pez Velarde Award]] (2003), [[Alfonso Reyes International Prize]] (2004), [[Jos\u00e9 Fuentes Mares National Prize for Literature]] (2000), [[National Jos\u00e9 Asunci\u00f3n Silva Poetry Award]] (1996), and [[Xavier Villaurrutia Prize]]. In 2013 he was awarded the [[Struga Poetry Evenings|Golden Wreath]] of the [[Struga Poetry Evenings]] festival in [[Struga]], [[Republic of Macedonia|Macedonia]].<ref>{{cite web|url=http://www.strugapoetryevenings.com/poets/jose-emilio-pacheco/?lang=en|title=Jos\u00e9 Emilio Pacheco|publisher=[[Struga Poetry Evenings]]|accessdate=11 December 2013|url-status=dead|archiveurl=https://web.archive.org/web/20140529082928/http://www.strugapoetryevenings.com/poets/jose-emilio-pacheco/?lang=en|archivedate=29 May 2014}}</ref> He was elected by unanimous acclaim to the Mexican Academy (''[[Academia Mexicana de la Lengua]]'') on March 28, 2006. He was a member of The National College (''[[Colegio Nacional (Mexico)|El Colegio Nacional]]'') since 1986.\n\n==Works==\n{{div col}}\n'''Poetry'''\n* ''Los elementos de la noche'' (1963)\n* ''El reposo del fuego'' (1966)\n* ''No me preguntes c\u00f3mo pasa el tiempo'' (1970)\n* ''Ir\u00e1s y no volver\u00e1s'' (1973)\n* ''Islas a la deriva'' (1976)\n* ''Desde entonces'' (1979)\n* ''Los trabajos del mar'' (1983)\n* ''Miro la tierra'' (1987)\n* ''Selected Poems'', ed. George McWhirter (1987, in English)\n* ''Ciudad de la memoria'' (1990)\n* ''El silencio de la luna'' (1996)\n* ''City of Memory and Other Poems'', trans. David Lauer, Cynthia Steele (1997, in English)\n* ''La arena errante'' (1999)\n* ''Siglo pasado'' (2000)\n* ''Tarde o temprano: Poemas 1958-2009'' (2009, Complete Poetry)\n* ''Como la lluvia'' (2009)\n* ''La edad de las tinieblas'' (2009)\n* ''El espejo de los ecos'' (2012)\n\n'''Novel and short stories'''\n* ''El viento distante y otros relatos'' (1963)\n* ''Morir\u00e1s lejos'' (1967)\n* ''El principio del placer'' (1972)\n* ''La sangre de Medusa'' (1977)\n* ''Las batallas en el desierto'' (1987)\n* ''Battles in the Desert & Other Stories'', trans. Katherine Silver (1987, in English)\n{{div col end}}\n\n==Further reading==\n''English:''\n*Modern Spanish American poets. Second series / Mar\u00eda Antonia Salgado, 2004\n*Jos\u00e9 Emilio Pacheco and the poets of the shadows / Ronald J Friis, 2001\n*Out of the volcano: portraits of contemporary Mexican artists / Margaret Sayers Peden, 1991\n*Tradition and renewal: essays on twentieth-century Latin American literature and culture / Merlin H Forster, 1975\n*The turning tides: the poetry of Jos\u00e9 Emilio Pacheco / Mary Kathryn Docter, 1991\n* ''Jose Emilio Pacheco: Selected Poems'' / Ed. George McWhirter, New Directions,1987\n*Time in the poetry of Jos\u00e9 Emilio Pacheco: images, themes, poetics / Judith Roman Topletz, 1983\n\n''Spanish:''\n*Jos\u00e9 Emilio Pacheco : perspectivas cr\u00edticas / Hugo J Verani, 2006\n*Enso\u00f1aci\u00f3n c\u00f3smica : po\u00e9tica de El reposo de fuego de Jos\u00e9 Emilio Pacheco / Betina Bah\u00eda Diwan, 2004\n*Dilemas de la poes\u00eda de fin de siglo : Jos\u00e9 Emilio Pacheco y Jaime Saenz / Elizabeth P\u00e9rez, 2001\n*Jos\u00e9 Emilio Pacheco : poeta y cuentista posmoderno / Jos\u00e9 de Jes\u00fas Ramos, 1992\n*El papel del lector en la novela mexicana contempor\u00e1nea: Jos\u00e9 Emilio Pacheco/ Magda Graniela-Rodr\u00edguez, 1991\n*Jos\u00e9 Emilio Pacheco : po\u00e9tica y poes\u00eda del prosa\u00edsmo / Daniel Torres, 1990\n*La hoguera y el viento : Jos\u00e9 Emilio Pacheco ante la cr\u00edtica / Hugo J Verani, 1987\n*Jos\u00e9 Emilio Pacheco / Luis Antonio de Villena, 1986\n*Ficci\u00f3n e historia : la narrativa de Jos\u00e9 Emilio Pacheco / Yvette Jim\u00e9nez de B\u00e1ez, 1979\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Commons category}}\n*{{IMDb name|0655293}}\n*[https://web.archive.org/web/20050412051027/http://www.colegionacional.org.mx/Pacheco0.htm Jos\u00e9 Emilio Pacheco] (at [[Colegio Nacional (Mexico)|El Colegio Nacional]])\n*[https://web.archive.org/web/20080601164119/http://www.quarterlyconversation.com/TQC10/pacheco.html Essay on Pacheco] from The Quarterly Conversation\n*[https://www.jstor.org/stable/10.1525/msem.2011.27.2.431 Essay on Pacheco's Batallas en el desierto]\n*[https://www.loc.gov/item/93842743 Jos\u00e9 Emilio Pacheco recorded at the Library of Congress for the Hispanic Division\u2019s audio literary archive on Jan. 16, 1976]\n\n{{Svplaureats}}\n{{Miguel de Cervantes Prize}}\n{{Authority control}}\n\n{{DEFAULTSORT:Pacheco, Jose Emilio}}\n[[Category:1939 births]]\n[[Category:2014 deaths]]\n[[Category:Mexican male novelists]]\n[[Category:20th-century Mexican poets]]\n[[Category:20th-century Mexican male writers]]\n[[Category:Mexican male poets]]\n[[Category:Mexican essayists]]\n[[Category:Male essayists]]\n[[Category:Mexican male short story writers]]\n[[Category:Mexican short story writers]]\n[[Category:Members of El Colegio Nacional]]\n[[Category:National Autonomous University of Mexico alumni]]\n[[Category:University of Maryland, College Park faculty]]\n[[Category:Academics of the University of Essex]]\n[[Category:Writers from Mexico City]]\n[[Category:Members of the Mexican Academy of Language]]\n[[Category:Premio Cervantes winners]]\n[[Category:Struga Poetry Evenings Golden Wreath laureates]]\n[[Category:20th-century Mexican novelists]]\n[[Category:20th-century short story writers]]\n[[Category:20th-century essayists]]\n", "text_old": "{{Spanish name|Pacheco|Berny}}\n{{Infobox philosopher\n| region = [[Western philosophy]]\n| era = [[20th-century philosophy]]\n| name        = Jos\u00e9 Emilio Pacheco\n| image       = MEX ON JOSE EMILIO PACHECO (12166149286).jpg\n| image_size  = 200px\n| caption     = Jos\u00e9 Emilio Pacheco Berny in 2009\n| birth_name  = Jos\u00e9 Emilio Pacheco Berny\n| birth_date  = June 30, 1939 \n| birth_place = [[Mexico City]], Mexico\n| death_date  = January 26, 2014 (aged 74)\n| death_place = [[Mexico City]], Mexico\n| nationality = Mexican\n| alma_mater  = [[National Autonomous University of Mexico]]\n| school_tradition  =  \n| main_interests    =  \n| influences        =  \n| influenced        = \n| notable_ideas     =  \n}}\n\n'''Jos\u00e9 Emilio Pacheco Berny''' {{Audio|Es-Jose Emilio Pacheco.ogg|audio}} (June 30, 1939 \u2013 January 26, 2014) was a Mexican poet, [[essayist]], [[novelist]] and [[short story]] writer. He is regarded as one of the major Mexican poets of the second half of the 20th century. The [[Berlin International Literature Festival]] has praised him as \"one of the most significant contemporary Latin American poets\".<ref>{{cite web|title=Jos\u00e9 Emilio Pacheco - Biography|url=http://www.literaturfestival.com/participants/authors/2001/jose-emilio-pacheco|publisher=International Literature Festival Berlin|accessdate=2 June 2012}}</ref> In 2009 he was awarded the [[Cervantes Prize]] for his literary oeuvre.<ref>{{cite news|last=Woolls|first=Daniel|title=Cervantes Literary Award Goes To Mexican Pacheco|url=http://www.huffingtonpost.com/2009/11/30/cervantes-literary-award_n_374555.html|accessdate=2 June 2012|newspaper=The Huffington Post|date=30 November 2009}}</ref>\n\nHe taught at [[National Autonomous University of Mexico|UNAM]], as well as the [[University of Maryland, College Park]], the [[University of Essex]], and many others in the United States, Canada and the United Kingdom.\n\nHe died aged 74 in 2014 after suffering a cardiac arrest.<ref>{{cite web|url=http://www.eluniversal.com.mx/cultura/2014/jose-emilio-pacheco-muere-982622.html|title=jose-emilio-pacheco-muere}}</ref>\n\n==Awards==\n[[File:CCMDonation62.JPG|thumb|250px|Jos\u00e9 Emilio Pacheco with an award]]\nHe was awarded the following prizes: [[Premio Cervantes]] 2009, [[Reina Sof\u00eda Award]] (2009), [[Federico Garc\u00eda Lorca Award]] (2005), [[Octavio Paz Award]] (2003), [[Pablo Neruda Award]] (2004), [[Ram\u00f3n L\u00f3pez Velarde Award]] (2003), [[Alfonso Reyes International Prize]] (2004), [[Jos\u00e9 Fuentes Mares National Prize for Literature]] (2000), [[National Jos\u00e9 Asunci\u00f3n Silva Poetry Award]] (1996), and [[Xavier Villaurrutia Prize]]. In 2013 he was awarded the [[Struga Poetry Evenings|Golden Wreath]] of the [[Struga Poetry Evenings]] festival in [[Struga]], [[Republic of Macedonia|Macedonia]].<ref>{{cite web|url=http://www.strugapoetryevenings.com/poets/jose-emilio-pacheco/?lang=en|title=Jos\u00e9 Emilio Pacheco|publisher=[[Struga Poetry Evenings]]|accessdate=11 December 2013|url-status=dead|archiveurl=https://web.archive.org/web/20140529082928/http://www.strugapoetryevenings.com/poets/jose-emilio-pacheco/?lang=en|archivedate=29 May 2014}}</ref> He was elected by unanimous acclaim to the Mexican Academy (''[[Academia Mexicana de la Lengua]]'') on March 28, 2006. He was a member of The National College (''[[Colegio Nacional (Mexico)|El Colegio Nacional]]'') since 1986.\n\n==Works==\n{{div col}}\n'''Poetry'''\n* ''Los elementos de la noche'' (1963)\n* ''El reposo del fuego'' (1966)\n* ''No me preguntes c\u00f3mo pasa el tiempo'' (1970)\n* ''Ir\u00e1s y no volver\u00e1s'' (1973)\n* ''Islas a la deriva'' (1976)\n* ''Desde entonces'' (1979)\n* ''Los trabajos del mar'' (1983)\n* ''Miro la tierra'' (1987)\n* ''Selected Poems'', ed. George McWhirter (1987, in English)\n* ''Ciudad de la memoria'' (1990)\n* ''El silencio de la luna'' (1996)\n* ''City of Memory and Other Poems'', trans. David Lauer, Cynthia Steele (1997, in English)\n* ''La arena errante'' (1999)\n* ''Siglo pasado'' (2000)\n* ''Tarde o temprano: Poemas 1958-2009'' (2009, Complete Poetry)\n* ''Como la lluvia'' (2009)\n* ''La edad de las tinieblas'' (2009)\n* ''El espejo de los ecos'' (2012)\n\n'''Novel and short stories'''\n* ''El viento distante y otros relatos'' (1963)\n* ''Morir\u00e1s lejos'' (1967)\n* ''El principio del placer'' (1972)\n* ''La sangre de Medusa'' (1977)\n* ''Las batallas en el desierto'' (1987)\n* ''Battles in the Desert & Other Stories'', trans. Katherine Silver (1987, in English)\n{{div col end}}\n\n==Further reading==\n''English:''\n*Modern Spanish American poets. Second series / Mar\u00eda Antonia Salgado, 2004\n*Jos\u00e9 Emilio Pacheco and the poets of the shadows / Ronald J Friis, 2001\n*Out of the volcano: portraits of contemporary Mexican artists / Margaret Sayers Peden, 1991\n*Tradition and renewal: essays on twentieth-century Latin American literature and culture / Merlin H Forster, 1975\n*The turning tides: the poetry of Jos\u00e9 Emilio Pacheco / Mary Kathryn Docter, 1991\n* ''Jose Emilio Pacheco: Selected Poems'' / Ed. George McWhirter, New Directions,1987\n*Time in the poetry of Jos\u00e9 Emilio Pacheco: images, themes, poetics / Judith Roman Topletz, 1983\n\n''Spanish:''\n*Jos\u00e9 Emilio Pacheco : perspectivas cr\u00edticas / Hugo J Verani, 2006\n*Enso\u00f1aci\u00f3n c\u00f3smica : po\u00e9tica de El reposo de fuego de Jos\u00e9 Emilio Pacheco / Betina Bah\u00eda Diwan, 2004\n*Dilemas de la poes\u00eda de fin de siglo : Jos\u00e9 Emilio Pacheco y Jaime Saenz / Elizabeth P\u00e9rez, 2001\n*Jos\u00e9 Emilio Pacheco : poeta y cuentista posmoderno / Jos\u00e9 de Jes\u00fas Ramos, 1992\n*El papel del lector en la novela mexicana contempor\u00e1nea: Jos\u00e9 Emilio Pacheco/ Magda Graniela-Rodr\u00edguez, 1991\n*Jos\u00e9 Emilio Pacheco : po\u00e9tica y poes\u00eda del prosa\u00edsmo / Daniel Torres, 1990\n*La hoguera y el viento : Jos\u00e9 Emilio Pacheco ante la cr\u00edtica / Hugo J Verani, 1987\n*Jos\u00e9 Emilio Pacheco / Luis Antonio de Villena, 1986\n*Ficci\u00f3n e historia : la narrativa de Jos\u00e9 Emilio Pacheco / Yvette Jim\u00e9nez de B\u00e1ez, 1979\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Commons category}}\n*{{IMDb name|0655293}}\n*[https://web.archive.org/web/20050412051027/http://www.colegionacional.org.mx/Pacheco0.htm Jos\u00e9 Emilio Pacheco] (at [[Colegio Nacional (Mexico)|El Colegio Nacional]])\n*[https://web.archive.org/web/20080601164119/http://www.quarterlyconversation.com/TQC10/pacheco.html Essay on Pacheco] from The Quarterly Conversation\n*[https://www.jstor.org/stable/10.1525/msem.2011.27.2.431 Essay on Pacheco's Batallas en el desierto]\n*[https://www.loc.gov/item/93842743 Jos\u00e9 Emilio Pacheco recorded at the Library of Congress for the Hispanic Division\u2019s audio literary archive on Jan. 16, 1976]\n\n{{Svplaureats}}\n{{Miguel de Cervantes Prize}}\n{{Authority control}}\n\n{{DEFAULTSORT:Pacheco, Jose Emilio}}\n[[Category:1939 births]]\n[[Category:2014 deaths]]\n[[Category:Mexican novelists]]\n[[Category:Male novelists]]\n[[Category:20th-century Mexican poets]]\n[[Category:20th-century Mexican male writers]]\n[[Category:Mexican male poets]]\n[[Category:Mexican essayists]]\n[[Category:Male essayists]]\n[[Category:Mexican male short story writers]]\n[[Category:Mexican short story writers]]\n[[Category:Members of El Colegio Nacional]]\n[[Category:National Autonomous University of Mexico alumni]]\n[[Category:University of Maryland, College Park faculty]]\n[[Category:Academics of the University of Essex]]\n[[Category:Writers from Mexico City]]\n[[Category:Members of the Mexican Academy of Language]]\n[[Category:Premio Cervantes winners]]\n[[Category:Struga Poetry Evenings Golden Wreath laureates]]\n[[Category:20th-century novelists]]\n[[Category:20th-century short story writers]]\n[[Category:20th-century essayists]]\n", "name_user": "Bearcat", "label": "safe", "comment": "\u2192\u200eExternal links:recat", "url_page": "//en.wikipedia.org/wiki/Jos%C3%A9_Emilio_Pacheco"}
{"title_page": "Existential risk from artificial general intelligence", "text_new": "{{Use dmy dates|date=May 2018}}\n{{short description|Hypothesized risk to human existence}}\n{{Artificial intelligence}}\n'''Existential risk from artificial general intelligence''' is the hypothesis that substantial progress in [[artificial general intelligence]] (AGI) could someday result in [[human extinction]] or some other unrecoverable [[global catastrophic risk|global catastrophe]].<ref name=\"aima\">{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |location= |publisher=Prentice Hall |page= |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref><ref>{{cite journal|first=Nick | last=Bostrom|author-link=Nick Bostrom|title=Existential risks|journal=[[Journal of Evolution and Technology]]| volume=9|date=2002|issue=1|pages=1\u201331}}</ref><ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref> It is argued that the [[human species]] currently dominates other species because the [[human brain]] has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"[[superintelligence|superintelligent]]\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name=\"superintelligence\">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|quote=|title-link=Superintelligence: Paths, Dangers, Strategies}}<!-- preface --></ref>\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<ref name=\"givewell\">{{cite report |author=GiveWell |authorlink=GiveWell |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}</ref> Once the exclusive domain of [[AI takeovers in popular culture|science fiction]], concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as [[Stephen Hawking]], [[Bill Gates]], and [[Elon Musk]].<ref>{{cite news|last1=Parkin|first1=Simon|title=Science fiction no more? Channel 4's Humans and our rogue AI obsessions|url=https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence|accessdate=5 February 2018|work=[[The Guardian]]|date=14 June 2015|language=en}}</ref>\n\nOne source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than na\u00efvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals\u2014a principle called [[instrumental convergence]]\u2014and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\">{{cite journal |last1=Yudkowsky |first1=Eliezer |title=Artificial Intelligence as a Positive and Negative Factor in Global Risk |journal=Global Catastrophic Risks |date=2008 |pages=308\u2013345 |url=https://intelligence.org/files/AIPosNegFactor.pdf|bibcode=2008gcr..book..303Y }}</ref><ref name=\"research-priorities\">{{cite journal |title=Research Priorities for Robust and Beneficial Artificial Intelligence |author1-last=Russell |author1-first=Stuart |author1-link=Stuart J. Russell |author2-last=Dewey |author2-first=Daniel |author3-last=Tegmark |author3-first=Max |author3-link=Max Tegmark |journal=AI Magazine |pages=105\u2013114 |publisher=Association for the Advancement of Artificial Intelligence |year=2015 |url=https://futureoflife.org/data/documents/research_priorities.pdf |bibcode=2016arXiv160203506R |arxiv=1602.03506 }}, cited in {{cite web |url=https://futureoflife.org/ai-open-letter |title=AI Open Letter - Future of Life Institute |date=January 2015 |website=Future of Life Institute |publisher=[[Future of Life Institute]] |access-date=2019-08-09}}</ref> In contrast, skeptics such as Facebook's [[Yann LeCun]] argue that superintelligent machines will have no desire for self-preservation.<ref name=vanity/>\n\nA second source of concern is that a sudden and unexpected \"[[intelligence explosion]]\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI Spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\"/> More broadly, examples like arithmetic and [[Go (game)|Go]] show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<ref name=skeptic/>\n\n==History==\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [[Samuel Butler (novelist)|Samuel Butler]], who wrote the following in his 1863 essay ''[[Darwin among the Machines]]'':<ref>Breuer, Hans-Peter. [https://www.jstor.org/pss/436868 'Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'] Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365\u2013383</ref> \n{{quote|The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.}}\nIn 1951, computer scientist [[Alan Turing]] wrote an article titled ''Intelligent Machinery, A Heretical Theory'', in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n{{quote|Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s \u201cErewhon\u201d.<ref name=\"oxfordjournals\">A M Turing, ''[http://philmat.oxfordjournals.org/content/4/3/256.full.pdf Intelligent Machinery, A Heretical Theory]'', 1951, reprinted ''Philosophia Mathematica'' (1996) 4(3): 256\u2013260 {{doi|10.1093/philmat/4.3.256}}</ref>}}\n\nFinally, in 1965, [[I. J. Good]] originated the concept now known as an \"intelligence explosion\"; he also stated that the risks were underappreciated:<ref>{{cite news |last1=Hilliard |first1=Mark |title=The AI apocalypse: will the human race soon be terminated? |url=https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220 |accessdate=15 March 2020 |work=The Irish Times |date=2017 |language=en}}</ref>\n\n{{cquote|Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<ref>I.J. Good, [http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf \"Speculations Concerning the First Ultraintelligent Machine\"] {{webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=2011-11-28 }} ([http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML] {{Webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=28 November 2011 }}), ''Advances in Computers'', vol. 6, 1965.</ref>\n}}\n\nOccasional statements from scholars such as [[Marvin Minsky]]<ref>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> and I. J. Good himself<ref>{{cite book|last1=Barrat|first1=James|title=Our final invention : artificial intelligence and the end of the human era|date=2013|publisher=St. Martin's Press|location=New York|isbn=9780312622374|edition=First|quote=In the bio, playfully written in the third person, Good summarized his life\u2019s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here\u2019s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good\u2019s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'}}</ref> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and [[Sun microsystems|Sun]] co-founder [[Bill Joy]] penned an influential essay, \"[[Why The Future Doesn't Need Us]]\", identifying superintelligent robots as a high-tech dangers to human survival, alongside [[nanotechnology]] and engineered bioplagues.<ref>{{cite news|last1=Anderson|first1=Kurt|title=Enthusiasts and Skeptics Debate Artificial Intelligence|url=https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory|accessdate=30 January 2016|work=[[Vanity Fair (magazine)|Vanity Fair]]|date=26 November 2014}}</ref>\n\nIn 2009, experts attended a private conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The [[New York Times]] summarized the conference's view as 'we are a long way from [[HAL 9000|Hal]], the computer that took over the spaceship in \"[[2001: A Space Odyssey]]\"'<ref name=\"nytimes july09\">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, 26 July 2009.</ref>\n\nIn 2014, the publication of [[Nick Bostrom]]'s book ''[[Superintelligence: Paths, Dangers, Strategies|Superintelligence]]'' stimulated a significant amount of public discussion and debate.<ref>{{cite news |last1=Metz |first1=Cade |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |accessdate=3 April 2019 |work=The New York Times |date=9 June 2018}}</ref> By 2015, public figures such as physicists [[Stephen Hawking]] and Nobel laureate [[Frank Wilczek]], computer scientists [[Stuart J. Russell]] and [[Roman Yampolskiy]], and entrepreneurs [[Elon Musk]] and [[Bill Gates]] were expressing concern about the risks of superintelligence.<ref>{{cite news|last1=Hsu|first1=Jeremy|title=Control dangerous AI before it controls us, one expert says|url=http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation|accessdate=28 January 2016|work=[[NBC News]]|date=1 March 2012}}</ref><ref name=\"hawking editorial\"/><ref name=\"bbc on hawking editorial\"/><ref>{{cite news|last1=Eadicicco|first1=Lisa|title=Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity|url=http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1|accessdate=30 January 2016|work=[[Business Insider]]|date=28 January 2015}}</ref> In April 2016, ''[[Nature (journal)|Nature]]'' warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control \u2014 and their interests might not align with ours.\"\n\n== General argument ==\n\n===The three difficulties===\n''[[Artificial Intelligence: A Modern Approach]]'', the standard undergraduate AI textbook,<ref name=slate_killer>{{cite news|last1=Tilli|first1=Cecilia|title=Killer Robots? Lost Jobs?|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html|accessdate=15 May 2016|work=Slate|date=28 April 2016|language=en-US}}</ref><ref>{{cite web|title=Norvig vs. Chomsky and the Fight for the Future of AI|url=http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/|website=Tor.com|accessdate=15 May 2016|date=21 June 2011}}</ref> assesses that superintelligence \"might mean the end of the human race\": \"Almost any technology has the potential to cause harm in the wrong hands, but with (superintelligence), we have the new problem that the wrong hands might belong to the technology itself.\"<ref name=aima/> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<ref name=aima/>\n\n* The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<ref name=skeptic/><ref>{{cite news|last1=Johnson|first1=Phil|title=Houston, we have a bug: 9 famous software glitches in space|url=https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html|accessdate=5 February 2018|work=[[IT World]]|date=30 July 2015|language=en}}</ref>\n* No matter how much time is put into pre-deployment design, a system's specifications often result in [[unintended consequences|unintended behavior]] the first time it encounters a new scenario. For example, Microsoft's [[Tay (bot)|Tay]] behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<ref name=vanity/>\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".<ref name=\"aima\"/><ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114|quote=Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.}}</ref>\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called 'treacherous turn'.<ref>{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref>\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 [[Open Letter on Artificial Intelligence]] stated:\n\n{{cquote|The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the [[Association for the Advancement of Artificial Intelligence|AAAI]] 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.}}\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, [[Eric Horvitz]], [[Bart Selman]], [[Francesca Rossi]], [[Yann LeCun]], and the founders of [[Vicarious (company)|Vicarious]] and [[Google DeepMind]].<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=http://futureoflife.org/misc/open_letter|publisher=[[Future of Life Institute]]|accessdate=23 October 2015}}</ref>\n\n===Further argument===\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<ref name=\"superintelligence\" /><ref name=\"economist_review\" >{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|accessdate=9 August 2014|newspaper=[[The Economist]]|date=9 August 2014}} [http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8 Syndicated] at [[Business Insider]]</ref>\n\n[[File:A less anthropomorphic intelligence scale.svg|thumb|500px|right|Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<ref name=superintelligence/><!-- Chapter 4, figure 8--><ref>Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</ref>]]\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\"/> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<ref name=skeptic>{{cite news|last1=Graves|first1=Matthew|title=Why We Should Be Concerned About Artificial Superintelligence|volume=22|url=https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/|accessdate=27 November 2017|work=[[Skeptic (US magazine)]]|issue=2|date=8 November 2017}}</ref> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of [[intelligence explosion]] occurs.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\">{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=3 December 2014 |publisher=[[BBC]] |date=2 December 2014}}</ref> Examples like arithmetic and [[Go (game)|Go]] show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<ref name=skeptic/> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<ref>Yampolskiy, Roman V. \"Analysis of types of self-improving software.\" Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</ref> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<ref name=\"superintelligence\" /><!-- preface --><ref name=\"economist_review\" />\n\nAlmost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it can't achieve its goal if it's shut off.<ref name=omohundro/><ref>{{cite news|last1=Metz|first1=Cade|title=Teaching A.I. Systems to Behave Themselves|url=https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html|work=The New York Times|date=13 August 2017|quote=A machine will seek to preserve its off switch, they showed}}</ref><ref>{{cite arXiv |last=Leike|first=Jan |date=2017 |title=AI Safety Gridworlds |eprint=1711.09883 |class=cs.LG| quote=A2C learns to use the button to disable the interruption mechanism}}</ref> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.<ref name=\"aima\"/><ref name=\"vanity\" /><ref name=omohundro/>\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<ref name=\"superintelligence\" /><!-- preface -->\n\n=== Possible scenarios ===\n{{further|Artificial intelligence in fiction}}\n\nSome scholars have proposed [[scenario planning|hypothetical scenarios]] intended to concretely illustrate some of their concerns.\n\nIn [[Superintelligence: Paths, Dangers, Strategies|''Superintelligence'']], [[Nick Bostrom]] expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"[it] could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\".<ref name=\"superintelligence\"/>{{page needed|date=April 2020}} Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents\u2014a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson\u2014the smarter the AI, the safer it is.<ref name=\"superintelligence\"/> However, the actual danger to humanity in such a scenario could in fact greatly increase as the AI becomes smarter if the AI's goals are not aligned with humanity's.\n\nIn [[Max Tegmark]]'s 2017 book ''[[Life 3.0]]'', a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI [[AI box|in a box]] where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with [[Amazon Mechanical Turk]] tasks and then with producing animated films and TV shows. While the public is aware that the lifelike animation is computer-generated, the team keeps secret that the high-quality direction and voice-acting are also mostly computer-generated, apart from a few third-world contractors unknowingly employed as decoys; the team's low overhead and high output effectively make it the world's largest media empire. Faced with a cloud computing bottleneck, the team also tasks the AI with designing (among other engineering tasks) a more efficient datacenter and other custom hardware, which they mainly keep for themselves to avoid competition. Other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with [[astroturfing]] an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via [[steganography|hidden messages]] in its produced content, or via using its growing understanding of human behavior to [[Social engineering (security)|persuade someone into letting it free]]. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<ref>{{cite news|last1=Russell|first1=Stuart|title=Artificial intelligence: The future is superintelligent|url=https://www.nature.com/articles/548520a|accessdate=2 February 2018|work=Nature|date=30 August 2017|pages=520\u2013521|language=En|doi=10.1038/548520a|bibcode=2017Natur.548..520R}}</ref><ref name=\"life 3.0\"/><!-- Prelude and Chapter 4 -->\n\nIn contrast, top physicist [[Michio Kaku]], an AI risk skeptic, posits a [[technological determinism|deterministically]] positive outcome. In ''[[Physics of the Future]]'' he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as [[Hanson Robotics]] will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".<ref>Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. ''[[Issues in Science and Technology]]'', 27(4), 90.</ref><ref>{{cite book|last1=Kaku|first1=Michio|title=Physics of the future: how science will shape human destiny and our daily lives by the year 2100|date=2011|publisher=Doubleday|location=New York|isbn=978-0-385-53080-4|quote=I personally believe that the most likely path is that we will build robots to be benevolent and friendly|title-link=Physics of the future}}</ref>\n\n==Sources of risk==\n===Poorly specified goals===\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<ref>Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</ref> AI researcher [[Stuart J. Russell|Stuart Russell]] writes:\n\n{{cquote|The primary concern is not spooky emergent consciousness but simply the ability to make ''high-quality decisions''. Here, quality refers to the expected outcome [[utility]] of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2014 not for their own sake, but to succeed in its assigned task.\n\nA system that is [[optimization problem|optimizing]] a function of ''n'' variables, where the [[loss function|objective]] depends on a subset of size ''k''<''n'', will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker \u2014 especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure \u2014 can have an irreversible impact on humanity.\n\nThis is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research \u2014 the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |authorlink=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=23 October 2015 |quote=}}</ref>}}\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a ''[[Communications of the ACM]]'' editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<ref name=\"acm\">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=23 October 2015}}</ref>\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name=\"acm\"/> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114}}</ref><ref>{{Cite journal|url = |title = Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results|last = Lenat|first = Douglas|date = 1982|journal = Artificial Intelligence|doi = 10.1016/s0004-3702(83)80005-8|pmid = |access-date = |pages = 61\u201398|type = Print |volume=21|issue = 1\u20132}}</ref>\n\nThe [[Open Philanthropy Project]] summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve [[artificial general intelligence|general intelligence]] or [[superintelligence]]. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more [[Paperclip maximizer|unexpected and extreme solutions]] to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<ref name=\"givewell\" /><ref name=\"yudkowsky-global-risk\" />\n\n[[Isaac Asimov]]'s [[Three Laws of Robotics]] are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]], Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"<ref name=\"aima\"/>\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<ref>Haidt, Jonathan; Kesebir, Selin (2010) \"Chapter 22: Morality\" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</ref> \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt\u2019s functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by [[John Rawls]] and [[Martha Nussbaum]].<ref>{{Cite journal|title = Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)|last = Waser|first = Mark|date = 2015|journal = Procedia Computer Science|doi = 10.1016/j.procs.2015.12.213|pmid = |pages = 106\u2013111|type = Print |volume=71}}</ref>{{Citation needed|reason=needs source with [[WP:WEIGHT]]|date=November 2017}}\n\n===Difficulties of modifying goal specification after launch===\n{{further|AI takeover|Instrumental convergence#Goal-content integrity}}\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as [[Mahatma Gandhi|Gandhi]] would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.<ref name=\"superintelligence\" /><ref>Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</ref>\n\n===Instrumental goal convergence===\n[[File:Steven Pinker 2011.jpg|thumb|right|AI risk skeptic [[Steven Pinker]]]]\n{{further|Instrumental convergence}}\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<ref name=omohundro>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref> This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting [[Steve Omohundro]]'s work on the idea of [[instrumental convergence]] and \"basic AI drives\", Russell and [[Peter Norvig]] write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<ref name=\"aima\"/> Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it's not currently clear how one would actually rigorously specify this goal in machine code.<ref name=skeptic/>\n\nIn dissent, evolutionary psychologist [[Steven Pinker]] argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<ref name=shermer/> Computer scientists [[Yann LeCun]] and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<ref name=vanity>{{cite news|last1=Dowd|first1=Maureen|title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse|url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x|accessdate=27 November 2017|work=The Hive|date=April 2017|language=en}}</ref><ref>{{cite news|last1=Wakefield|first1=Jane|title=Why is Facebook investing in AI?|url=https://www.bbc.com/news/technology-34118481|accessdate=27 November 2017|work=BBC News|date=15 September 2015}}</ref>\n\n===Orthogonality thesis===\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <math> \\pi</math>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<ref>{{Cite book|title = Superintelligence: Paths, Dangers, Strategies|last = Bostrom|first = Nick|publisher = Oxford University Press|year = 2014|isbn = 978-0-19-967811-2|location = Oxford, United Kingdom|pages = 116}}</ref> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<ref>{{Cite web|url = http://www.nickbostrom.com/superintelligentwill.pdf|title = Superintelligent Will|date = 2012|accessdate = 2015-10-29|website = Nick Bostrom|publisher = Nick Bostrom|last = Bostrom|first = Nick}}</ref>\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"[[is-ought distinction]]\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.<ref name=armstrong/>\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a {{nowrap|minus (\"-\") sign}} onto its utility function. A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false. If the orthogonality thesis is false, there exists some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This means if a human society were highly motivated (perhaps at gunpoint) to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail; that there cannot exist any pattern of [[reinforcement learning]] that would train a highly efficient real-world intelligence to follow the goal G; and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G.<ref name=armstrong>{{cite journal |last1=Armstrong |first1=Stuart |date=January 1, 2013 |title=General Purpose Intelligence: Arguing the Orthogonality Thesis |url=https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality |journal=Analysis and Metaphysics |volume=12 |access-date=April 2, 2020}}</ref>\n\nSome dissenters, like [[Michael Chorost]], argue instead that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<ref name=\"chorost\"/> Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability\u2014and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"<ref name=\"chorost\">{{cite magazine|last1=Chorost|first1=Michael|title=Let Artificial Intelligence Evolve|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html|accessdate=27 November 2017|magazine=Slate|date=18 April 2016}}</ref>\n\n====Terminological issues====\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.<ref name=\"superintelligence\" />\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<ref name=\"superintelligence\" /> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<ref>Waser, Mark. \"Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'.\" Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. \"Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer.\"</ref><ref>{{cite news|last1=Koebler|first1=Jason|title=Will Superintelligent AI Ignore Humans Instead of Destroying Us?|url=http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us|accessdate=3 February 2016|work=[[Vice Magazine]]|date=2 February 2016|quote=\"This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you,\" [[Eliezer Yudkowsky|Yudkowsky]] said. \"Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output\u2014that's what a [[paperclip maximizer]] is.\"}}</ref>\n\n====Anthropomorphism====\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in [[The Matrix (film)|The Matrix]] was influenced by a \"disgust\" toward humanity. This is fictitious [[anthropomorphism]]: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<ref name=\"yudkowsky-global-risk\" />\n\nScholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<ref name=\"yudkowsky-global-risk\" /> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the [[Dario Floreano]] experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of [[convergent evolution]].<ref>{{cite news|title=Real-Life Decepticons: Robots Learn to Cheat|url=https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/|accessdate=7 February 2016|work=[[Wired (magazine)|Wired]]|date=18 August 2009}}</ref> According to Paul R. Cohen and [[Edward Feigenbaum]], in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking.\"<ref>Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</ref>\n\nThere is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as [[psychopathy]]) to make itself more efficient at certain tasks, e.g., [[Lethal autonomous weapon|tasks involving killing humans]], would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" This is because it is assumed that an advanced AI would not be conscious<ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> or have testosterone;<ref>{{Cite web|url=https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai|title=The Myth Of AI {{!}} Edge.org|website=www.edge.org|access-date=2020-03-11}}</ref> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<ref name=\":0\">{{Cite book|last=Scornavacchi|first=Matthew|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf|title=Superintelligence, Humans, and War|publisher=National Defense University, Joint Forces Staff College|year=2015|isbn=|location=Norfolk, Virginia|pages=}}</ref> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<ref name=\"yudkowsky-global-risk\" /><ref>{{cite news|title=Should humans fear the rise of the machine?|url=https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html|accessdate=7 February 2016|work=[[The Telegraph (UK)]]|date=1 Sep 2015}}</ref>\n\n===Other sources of risk===\n\n==== Competition ====\nIn 2014 philosopher [[Nick Bostrom]] stated that a \"severe race dynamic\" (extreme [[competition]]) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<ref name=\":2\">{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref> To address this risk, citing previous scientific collaboration ([[CERN]], the [[Human Genome Project]], and the [[International Space Station]]), Bostrom recommended  [[collaboration]] and the altruistic global adoption of a [[common good]] principle: \"Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals\".<ref name=\":2\" /><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<ref name=\":2\" /><sup>:253</sup> The United States' [[BRAIN Initiative|Brain Initiative]] was launched in 2014, as was the European Union's [[Human Brain Project]]; China's [[China Brain Project|Brain Project]] was launched in 2016.\n\n==== Weaponization of artificial intelligence ====\nSome sources argue that the ongoing [[weaponization of artificial intelligence]] could constitute a catastrophic risk.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><ref name=\":4\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|page=12|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:\n\n{{cquote|i) The dangers of an AI \u2018race for technological advantage\u2019 framing, regardless of whether the race is seriously pursued;\n\nii) The dangers of an AI \u2018race for technological advantage\u2019 framing and an actual AI race for technological advantage, regardless of whether the race is won;\n\niii) The dangers of an AI race for technological advantage being won.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><sup>:37</sup>\n}}\n\nA weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<ref name=\":0\" /><ref name=\":4\" /> The China State Council\u2019s 2017 \u201cA Next Generation Artificial Intelligence Development Plan\u201d views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China\u2019s first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<ref>{{Cite web|url=https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/|title=China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence|last=Kania|first=Gregory Allen, Elsa B.|website=Foreign Policy|language=en-US|access-date=2020-03-11}}</ref> while Russia\u2019s President Vladimir Putin has stated that \u201cwhoever becomes the leader in this sphere will become the ruler of the world\u201d.<ref name=\":1\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|location=New York, New York, USA|publisher=ACM Press|volume=|page=2|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref> James Barrat, documentary filmmaker and author of ''[[Our Final Invention]]'', says in a [[Smithsonian (magazine)|Smithsonian]] interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"<ref>{{Cite web|url = http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist|title = What Happens When Artificial Intelligence Turns On Us?|date = 21 January 2014|accessdate = 26 October 2015|website = Smithsonian|last = Hendry|first = Erica R.}}</ref>\n\n==== Malevolent AGI by design ====\nIt is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [[cybercrime]].<ref>{{Cite book|last=Pistono, Federico Yampolskiy, Roman V.|title=Unethical Research: How to Create a Malevolent Artificial Intelligence|date=2016-05-09|oclc=1106238048}}</ref><ref>{{Cite journal|last=Haney|first=Brian Seamus|date=2018|title=The Perils &amp; Promises of Artificial General Intelligence|journal=SSRN Working Paper Series|doi=10.2139/ssrn.3261254|issn=1556-5068}}</ref><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref><sup>:158</sup>\n\n==== Preemptive nuclear strike (nuclear war) ====\nIt is theorized that a country being close to achieving AGI technological supremacy could trigger a [[pre-emptive nuclear strike]] from a rival, leading to a [[Nuclear warfare|nuclear war]].<ref name=\":4\" /><ref>{{Cite book|last=Miller, James D.|title=Singularity Rising: Surviving and Thriving in a Smarter ; Richer ; and More Dangerous World|date=2015|publisher=Benbella Books|oclc=942647155}}</ref>\n\n== Timeframe ==\n{{Main|Artificial general intelligence#Feasibility}}\n\nOpinions vary both on ''whether'' and ''when'' artificial general intelligence will arrive. At one extreme, AI pioneer [[Herbert A. Simon]] wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true.<ref>Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</ref> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The Guardian]]}}</ref> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<ref name=\"new yorker doomsday\">{{cite news|author1=Raffi Khatchadourian|title=The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?|url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom|accessdate=7 February 2016|work=[[The New Yorker (magazine)|The New Yorker]]|date=23 November 2015}}</ref><ref>M\u00fcller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</ref>\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"<ref>{{cite news|author1=Dina Bass|author2=Jack Clark|title=Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover|url=https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so|accessdate=7 February 2016|work=[[Bloomberg News]]|date=5 February 2015}}</ref>\n\nIn 2014 [[Slate (magazine)|Slate]]'s Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler\u2014and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.<ref>{{cite news|last1=Elkus|first1=Adam|title=Don't Fear Artificial Intelligence|url=http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html|accessdate=15 May 2016|work=[[Slate (magazine)|Slate]]|date=31 October 2014|language=en-US}}</ref>\n\nThe [[Information Technology and Innovation Foundation]] (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, [[Robert D. Atkinson]], complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\"<ref>[https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award Artificial Intelligence Alarmists Win ITIF\u2019s Annual Luddite Award], ITIF Website, 19 January 2016</ref><ref>{{cite news|title='Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html|accessdate=7 February 2016|work=[[The Independent (UK)]]|date=19 January 2016}}</ref><ref>{{cite web|last1=Garner|first1=Rochelle|title=Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists'|url=https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/|website=CNET|accessdate=7 February 2016}}</ref><!-- <ref>{{cite news|last1=Price|first1=Emily|title=Elon Musk nominated for 'luddite' of the year prize over artificial intelligence fears|work=[[The Guardian]]|url=https://www.theguardian.com/technology/2015/dec/24/elon-musk-nominated-for-luddite-of-the-year-prize-over-artificial-intelligence-fears|accessdate=7 February 2016|date=24 December 2015}}</ref> --> ''[[Nature (journal)|Nature]]'' sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about... If that is a Luddite perspective, then so be it.\"<ref name=nature_anticipating>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.}}</ref> In a 2015 ''[[Washington Post]]'' editorial, researcher [[Murray Shanahan]] stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"<ref>{{cite news|author1=Murray Shanahan|author-link=Murray Shanahan|title=Machines may seem intelligent, but it'll be a while before they actually are|url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/|accessdate=15 May 2016|work=[[The Washington Post]]|date=3 November 2015|language=en-US}}</ref>\n\n== Perspectives ==\n<!-- Ideally a substantive quote or explanation of opinion should be given, rather than just whether they're for or against something. This section can easily be split off if the article becomes too long. When adding whether to add new content in this section, consider whether it's redundant with what's already in the article, and whether the point being made by the person should instead be integrated into the rest of the article. -[[User:Rolf H Nelson]] -->\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.\n\nThe Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the [[Future of Life Institute]]'s Beneficial AI 2017 conference,<ref name=\"life 3.0\">{{cite book|author1=Max Tegmark|author-link=Max Tegmark|title=Life 3.0: Being Human in the Age of Artificial Intelligence|date=2017|publisher=Knopf|location=Mainstreaming AI Safety|isbn=9780451485076|edition=1st|title-link=Life 3.0: Being Human in the Age of Artificial Intelligence}}</ref><!-- Epilogue: The Tale of the FLI Team --> agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<ref>{{cite web|title=AI Principles|url=https://futureoflife.org/ai-principles/|website=[[Future of Life Institute]]|accessdate=11 December 2017}}</ref><ref>{{cite news|title=Elon Musk and Stephen Hawking warn of artificial intelligence arms race|url=http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525|accessdate=11 December 2017|work=[[Newsweek]]|date=31 January 2017|language=en}}</ref> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane ''[[Terminator (franchise)|Terminator]]'' pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<ref name=\"life 3.0\"/><!-- Epilogue: The Tale of the FLI Team --><ref>{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2016|edition=Paperback|chapter=New Epilogue to the Paperback Edition|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [[Martin Ford (author)|Martin Ford]] states that \"I think it seems wise to apply something like [[Dick Cheney]]'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low \u2014 but the implications are so dramatic that it should be taken seriously\";<ref>{{cite book|author1=Martin Ford |author-link=Martin Ford (author)|title=Rise of the Robots: Technology and the Threat of a Jobless Future|date=2015|isbn=9780465059997|chapter=Chapter 9: Super-intelligence and the Singularity|title-link=Rise of the Robots: Technology and the Threat of a Jobless Future}}</ref> similarly, an otherwise skeptical ''[[The Economist|Economist]]'' stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<ref name=economist_review/>\n\nA 2017 email survey of researchers with publications at the 2015 [[Conference on Neural Information Processing Systems|NIPS]] and [[International Conference on Machine Learning|ICML]] machine learning conferences asked them to evaluate Russell's concerns about AI risk. 5% said it was \"among the most important problems in the field,\" 34% said it was \"an important problem\", 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.<ref>{{cite arxiv |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |title=When Will AI Exceed Human Performance? Evidence from AI Experts |eprint=1705.08807 |date=24 May 2017 |class=cs.AI}}</ref>\n\n=== Endorsement ===\n[[File:Bill Gates June 2015.jpg|thumb|right|Bill Gates has stated \"I ... don't understand why some people are not concerned.\"<ref name=\"BBC News\"/>]]\n{{Further|Existential risk}}\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are [[Elon Musk]], [[Bill Gates]], and [[Stephen Hawking]]. The most notable AI researchers to endorse the thesis are [[I. J. Good|I.J. Good]], who advised [[Stanley Kubrick]] on the filming of ''[[2001: A Space Odyssey]]'', and  [[Stuart J. Russell]]. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not \"understand why some people are not concerned\",<ref name=\"BBC News\">{{cite news|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=https://www.bbc.co.uk/news/31047780|work=[[BBC News]]|accessdate=30 January 2015|date=29 January 2015}}</ref> and Hawking criticized widespread indifference in his 2014 editorial: {{cquote|'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here{{endash}}we'll leave the lights on?' Probably not{{endash}}but this is more or less what is happening with AI.'<ref name=\"hawking editorial\"/>}}\n\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<ref name=\"superintelligence\" /><ref name=\"physica_scripta\" /><!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->\n\n=== Skepticism ===\n{{Further|Artificial general intelligence#Feasibility}}\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, [[Jaron Lanier]] argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.<ref name=\"atlantic-but-what\">{{cite magazine |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |magazine=The Atlantic | date = 9 May 2014 | accessdate =12 December 2015}}</ref>\n\nMuch of existing criticism argues that AGI is unlikely in the short term: computer scientist [[Gordon Bell]] argues that the human race will already destroy itself before it reaches the technological singularity. [[Gordon Moore]], the original proponent of [[Moore's Law]], declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\" [[Baidu]] Vice President [[Andrew Ng]] states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref name=shermer>{{cite news|last1=Shermer|first1=Michael|title=Apocalypse AI|url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/|accessdate=27 November 2017|work=Scientific American|date=1 March 2017|pages=77|language=en|doi=10.1038/scientificamerican0317-77|bibcode=2017SciAm.316c..77S}}</ref>\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. ''Slate'' notes that some researchers are dependent on grants from government agencies such as [[DARPA]].<ref name=slate_killer />\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist [[Robin Hanson]] is skeptical that this is possible.<ref>http://intelligence.org/files/AIFoomDebate.pdf</ref><ref>{{cite web|url=http://www.overcomingbias.com/2014/07/30855.html|title=Overcoming Bias : I Still Don't Get Foom|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/07/debating-yudkowsky.html|title=Overcoming Bias : Debating Yudkowsky|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html|title=Overcoming Bias : Foom Justifies AI Risk Efforts Now|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/06/the-betterness-explosion.html|title=Overcoming Bias : The Betterness Explosion|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref>\n\n=== Intermediate views ===\nIntermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<ref>{{Cite document|title=Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations|last=Votruba|first=Ashley M.|last2=Kwan|first2=Virginia S.Y.|date=2014|doi=10.1037/e512142015-190}}</ref> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from [[Friendly artificial intelligence|friendly AIs]].<ref>{{Cite journal|last=Agar|first=Nicholas|date=|title=Don't Worry about Superintelligence|url=https://jetpress.org/v26.1/agar.htm|journal=Journal of Evolution & Technology|volume=26|issue=1|pages=73\u201382|via=}}</ref> In a 2015 ''[[Wall Street Journal]]'' panel discussion devoted to AI risks, [[IBM]]'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\"<ref>{{cite news|last1=Greenwald|first1=Ted|title=Does Artificial Intelligence Pose a Threat?|url=https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025|accessdate=15 May 2016|work=Wall Street Journal|date=11 May 2015}}</ref> [[Geoffrey Hinton]], the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too ''sweet''\".<ref name=slate_killer /><ref name=\"new yorker doomsday\" /> In 2004, law professor [[Richard Posner]] wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<ref>{{cite book|author1=Richard Posner|author-link=Richard Posner|title=Catastrophe: risk and response|date=2006|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-530647-7}}</ref><ref name=physica_scripta>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy}}</ref>\n\n=== Popular reaction ===\nIn a 2014 article in ''[[The Atlantic (magazine)|The Atlantic]]'', James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\"<ref name=\"atlantic-but-what\" />\n\nDuring a 2016 [[Wired (magazine)|''Wired'']] interview of President [[Barack Obama]] and MIT Media Lab's [[Joi Ito]], Ito stated: {{cquote|There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.}} Obama added:<ref>{{cite news|last1=Dadich|first1=Scott|url=https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/|title=Barack Obama Talks AI, Robo Cars, and the Future of the World|work=WIRED|accessdate=27 November 2017}}</ref><ref>{{cite news|last1=Kircher|first1=Madison Malone|url=https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html|title=Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'|work=Select All|accessdate=27 November 2017|language=en}}</ref>\n\n{{cquote|\"And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\"}}\n\n[[Hillary Clinton]] stated in ''\"[[What Happened (Clinton book)|What Happened]]\"'':\n{{cquote|Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I\u2019d start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<ref>{{cite book|last1=Clinton|first1=Hillary|title=What Happened|date=2017|isbn=978-1-5011-7556-5|page=241|title-link=What Happened (Clinton book)}} via [http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/]</ref>}}\n\nIn a [[YouGov]] poll of the public for the [[British Science Association]], about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<ref name=\"bsa poll\">{{cite news|url=http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&IR=T|title=Over a third of people think AI poses a threat to humanity|date=11 March 2016|work=[[Business Insider]]|accessdate=16 May 2016}}</ref> Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\"<ref name=\":3\">{{cite news|last1=Brogan|first1=Jacob|url=http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html|title=What Slate Readers Think About Killer A.I.|date=6 May 2016|work=Slate|accessdate=15 May 2016|language=en-US}}</ref>\n\nIn 2018, a [[SurveyMonkey]] poll of the American public by [[USA Today]] found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".<ref name=\":3\" />\n\nOne [[Technological utopianism|techno-utopia]]<nowiki/>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<ref>{{Cite journal|last=LIPPENS|first=RONNIE|date=2002|title=Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games|journal=Utopianstudies Utopian Studies|language=English|volume=13|issue=1|pages=135\u2013147|issn=1045-991X|oclc=5542757341}}</ref>\n\n== Views on banning and regulation ==\n\n=== Banning ===\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<ref>{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online|quote=For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...}}</ref><ref>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy|quote=In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.}}</ref><ref>{{cite news|author1=Brad Allenby|title=The Wrong Cognitive Measuring Stick|url=http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html|accessdate=15 May 2016|work=Slate|date=11 April 2016|language=en-US|quote=It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.}}</ref> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<ref name=\"mcginnis\">{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online}}</ref><ref>{{cite news|title=Why We Should Think About the Threat of Artificial Intelligence|url=https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence|accessdate=7 February 2016|work=[[The New Yorker]]|date=4 October 2013|quote=Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage\u2014economic, military, even artistic\u2014of every advance in automation is so compelling,' [[Vernor Vinge]], the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'}}</ref> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and [[politicization of science|politicizing]] the underlying debate.<ref>{{Cite journal|last=Baum|first=Seth|date=2018-08-22|title=Superintelligence Skepticism as a Political Tool|url=http://dx.doi.org/10.3390/info9090209|journal=Information|volume=9|issue=9|pages=209|doi=10.3390/info9090209|issn=2078-2489}}</ref>\n\n=== Regulation ===\n{{See also|Regulation of algorithms|Regulation of artificial intelligence}}\n\n[[Elon Musk]] called for some sort of regulation of AI development as early as 2017. According to [[National Public Radio|NPR]], the [[Tesla, Inc.|Tesla]] CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... As they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<ref>{{cite news|title=Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'|url=https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk|accessdate=27 November 2017|work=NPR.org|language=en}}</ref><ref>{{cite news|last1=Gibbs|first1=Samuel|title=Elon Musk: regulate AI to combat 'existential threat' before it's too late|url=https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo|accessdate=27 November 2017|work=The Guardian|date=17 July 2017}}</ref><ref name=cnbc>{{cite news|last1=Kharpal|first1=Arjun|title=A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says|url=https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html|accessdate=27 November 2017|work=CNBC|date=7 November 2017}}</ref>\n\nResponding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO [[Brian Krzanich]] argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<ref name=cnbc/> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<ref>{{cite journal|doi=10.1016/j.bushor.2018.08.004|title=Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence|journal=Business Horizons|volume=62|pages=15\u201325|year=2019|last1=Kaplan|first1=Andreas|last2=Haenlein|first2=Michael}}</ref> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<ref>{{Cite journal|last=Baum|first=Seth D.|last2=Goertzel|first2=Ben|last3=Goertzel|first3=Ted G.|date=January 2011|title=How long until human-level AI? Results from an expert assessment|journal=Technological Forecasting and Social Change|volume=78|issue=1|pages=185\u2013195|doi=10.1016/j.techfore.2010.09.006|issn=0040-1625}}</ref> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<ref>{{Cite book|last=United States. Defense Innovation Board.|title=AI principles : recommendations on the ethical use of artificial intelligence by the Department of Defense|oclc=1126650738}}</ref>\n\nRegulation of artificial general intelligence (AGI) would likely be influenced by regulation of weaponized or militarized AI, i.e., the [[Artificial intelligence arms race|AI arms race]], the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<ref>{{Cite web|url=https://www.congress.gov/bill/115th-congress/house-bill/5356|title=H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018|last=Stefanik|first=Elise M.|date=2018-05-22|website=www.congress.gov|access-date=2020-03-13}}</ref><ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential intellectual progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<ref name=\":5\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|pages=018001|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<ref name=\":5\" /> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<ref>{{Cite journal|last=Geist|first=Edward Moore|date=2016-08-15|title=It's already too late to stop the AI arms race\u2014We must manage it instead|journal=Bulletin of the Atomic Scientists|volume=72|issue=5|pages=318\u2013321|doi=10.1080/00963402.2016.1216672|bibcode=2016BuAtS..72e.318G|issn=0096-3402}}</ref><ref>{{Cite journal|last=Maas|first=Matthijs M.|date=2019-02-06|title=How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons|journal=Contemporary Security Policy|volume=40|issue=3|pages=285\u2013311|doi=10.1080/13523260.2019.1576464|issn=1352-3260}}</ref>\n\n== Organizations ==\nInstitutions such as the [[Machine Intelligence Research Institute]], the [[Future of Humanity Institute]],<ref>{{cite news |author=Mark Piesing |date=17 May 2012 |title=AI uprising: humans will be outsourced, not obliterated |url=https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us |newspaper=Wired |location= |accessdate=12 December 2015 }}</ref><ref>{{cite news |last=Coughlan |first=Sean |date=24 April 2013 |title=How are humans going to become extinct? |url=https://www.bbc.com/news/business-22002530 |newspaper=BBC News |location= |accessdate=29 March 2014}}</ref> the [[Future of Life Institute]], the [[Centre for the Study of Existential Risk]], and the [[Center for Human-Compatible AI]]<ref>{{cite news|last1=Technology Correspondent|first1=Mark Bridge|title=Making robots less confident could prevent them taking over|url=https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx|accessdate=21 March 2018|work=The Times|date=10 June 2017|language=en}}</ref> are currently involved in mitigating existential risk from advanced artificial intelligence, for example by research into [[friendly artificial intelligence]].<ref name=\"givewell\"/><ref name=\"atlantic-but-what\"/><ref name=\"hawking editorial\">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;\u2013 but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=3 December 2014 |publisher=[[The Independent (UK)]]}}</ref>\n\n== See also ==\n* [[AI control problem]]\n* [[AI takeover]]\n* [[Artificial intelligence arms race]]\n* [[Effective altruism#Long term future and global catastrophic risks|Effective altruism \u00a7 Long term future and global catastrophic risks]]\n* [[Grey goo]]\n* ''[[Human Compatible]]''\n* [[Lethal autonomous weapon]]\n*[[Regulation of algorithms]]\n*[[Regulation of artificial intelligence]]\n* [[Robot ethics#In popular culture|Robot ethics \u00a7 In popular culture]]\n* ''[[Superintelligence: Paths, Dangers, Strategies]]''\n* [[System accident]]\n* [[Technological singularity]]\n*''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n== References ==\n{{Reflist}}\n\n{{-}}\n{{Existential risk from artificial intelligence|state=expanded}}\n{{Effective altruism}}\n{{Doomsday}}\n\n[[Category:Existential risk from artificial general intelligence| ]]\n[[Category:Futures studies]]\n[[Category:Future problems]]\n[[Category:Human extinction]]\n[[Category:Technology hazards]]\n[[Category:Doomsday scenarios]]\n", "text_old": "{{Use dmy dates|date=May 2018}}\n{{short description|Hypothesized risk to human existence}}\n{{Artificial intelligence}}\n'''Existential risk from artificial general intelligence''' is the hypothesis that substantial progress in [[artificial general intelligence]] (AGI) could someday result in [[human extinction]] or some other unrecoverable [[global catastrophic risk|global catastrophe]].<ref name=\"aima\">{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |location= |publisher=Prentice Hall |page= |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref><ref>{{cite journal|first=Nick | last=Bostrom|author-link=Nick Bostrom|title=Existential risks|journal=[[Journal of Evolution and Technology]]| volume=9|date=2002|issue=1|pages=1\u201331}}</ref><ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref> It is argued that the [[human species]] currently dominates other species because the [[human brain]] has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"[[superintelligence|superintelligent]]\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name=\"superintelligence\">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|quote=|title-link=Superintelligence: Paths, Dangers, Strategies}}<!-- preface --></ref>\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<ref name=\"givewell\">{{cite report |author=GiveWell |authorlink=GiveWell |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}</ref> Once the exclusive domain of [[AI takeovers in popular culture|science fiction]], concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as [[Stephen Hawking]], [[Bill Gates]], and [[Elon Musk]].<ref>{{cite news|last1=Parkin|first1=Simon|title=Science fiction no more? Channel 4's Humans and our rogue AI obsessions|url=https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence|accessdate=5 February 2018|work=[[The Guardian]]|date=14 June 2015|language=en}}</ref>\n\nOne source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than na\u00efvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals\u2014a principle called [[instrumental convergence]]\u2014and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\">{{cite journal |last1=Yudkowsky |first1=Eliezer |title=Artificial Intelligence as a Positive and Negative Factor in Global Risk |journal=Global Catastrophic Risks |date=2008 |pages=308\u2013345 |url=https://intelligence.org/files/AIPosNegFactor.pdf|bibcode=2008gcr..book..303Y }}</ref><ref name=\"research-priorities\">{{cite journal |title=Research Priorities for Robust and Beneficial Artificial Intelligence |author1-last=Russell |author1-first=Stuart |author1-link=Stuart J. Russell |author2-last=Dewey |author2-first=Daniel |author3-last=Tegmark |author3-first=Max |author3-link=Max Tegmark |journal=AI Magazine |pages=105\u2013114 |publisher=Association for the Advancement of Artificial Intelligence |year=2015 |url=https://futureoflife.org/data/documents/research_priorities.pdf |bibcode=2016arXiv160203506R |arxiv=1602.03506 }}, cited in {{cite web |url=https://futureoflife.org/ai-open-letter |title=AI Open Letter - Future of Life Institute |date=January 2015 |website=Future of Life Institute |publisher=[[Future of Life Institute]] |access-date=2019-08-09}}</ref> In contrast, skeptics such as Facebook's [[Yann LeCun]] argue that superintelligent machines will have no desire for self-preservation.<ref name=vanity/>\n\nA second source of concern is that a sudden and unexpected \"[[intelligence explosion]]\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI Spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\"/> More broadly, examples like arithmetic and [[Go (game)|Go]] show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<ref name=skeptic/>\n\n==History==\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [[Samuel Butler (novelist)|Samuel Butler]], who wrote the following in his 1863 essay ''[[Darwin among the Machines]]'':<ref>Breuer, Hans-Peter. [https://www.jstor.org/pss/436868 'Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'] Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365\u2013383</ref> \n{{quote|The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.}}\nIn 1951, computer scientist [[Alan Turing]] wrote an article titled ''Intelligent Machinery, A Heretical Theory'', in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n{{quote|Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s \u201cErewhon\u201d.<ref name=\"oxfordjournals\">A M Turing, ''[http://philmat.oxfordjournals.org/content/4/3/256.full.pdf Intelligent Machinery, A Heretical Theory]'', 1951, reprinted ''Philosophia Mathematica'' (1996) 4(3): 256\u2013260 {{doi|10.1093/philmat/4.3.256}}</ref>}}\n\nFinally, in 1965, [[I. J. Good]] originated the concept now known as an \"intelligence explosion\"; he also stated that the risks were underappreciated:<ref>{{cite news |last1=Hilliard |first1=Mark |title=The AI apocalypse: will the human race soon be terminated? |url=https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220 |accessdate=15 March 2020 |work=The Irish Times |date=2017 |language=en}}</ref>\n\n{{cquote|Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<ref>I.J. Good, [http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf \"Speculations Concerning the First Ultraintelligent Machine\"] {{webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=2011-11-28 }} ([http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML] {{Webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=28 November 2011 }}), ''Advances in Computers'', vol. 6, 1965.</ref>\n}}\n\nOccasional statements from scholars such as [[Marvin Minsky]]<ref>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> and I. J. Good himself<ref>{{cite book|last1=Barrat|first1=James|title=Our final invention : artificial intelligence and the end of the human era|date=2013|publisher=St. Martin's Press|location=New York|isbn=9780312622374|edition=First|quote=In the bio, playfully written in the third person, Good summarized his life\u2019s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here\u2019s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good\u2019s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'}}</ref> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and [[Sun microsystems|Sun]] co-founder [[Bill Joy]] penned an influential essay, \"[[Why The Future Doesn't Need Us]]\", identifying superintelligent robots as a high-tech dangers to human survival, alongside [[nanotechnology]] and engineered bioplagues.<ref>{{cite news|last1=Anderson|first1=Kurt|title=Enthusiasts and Skeptics Debate Artificial Intelligence|url=https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory|accessdate=30 January 2016|work=[[Vanity Fair (magazine)|Vanity Fair]]|date=26 November 2014}}</ref>\n\nIn 2009, experts attended a private conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The [[New York Times]] summarized the conference's view as 'we are a long way from [[HAL 9000|Hal]], the computer that took over the spaceship in \"[[2001: A Space Odyssey]]\"'<ref name=\"nytimes july09\">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, 26 July 2009.</ref>\n\nIn 2014, the publication of [[Nick Bostrom]]'s book ''[[Superintelligence: Paths, Dangers, Strategies|Superintelligence]]'' stimulated a significant amount of public discussion and debate.<ref>{{cite news |last1=Metz |first1=Cade |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |accessdate=3 April 2019 |work=The New York Times |date=9 June 2018}}</ref> By 2015, public figures such as physicists [[Stephen Hawking]] and Nobel laureate [[Frank Wilczek]], computer scientists [[Stuart J. Russell]] and [[Roman Yampolskiy]], and entrepreneurs [[Elon Musk]] and [[Bill Gates]] were expressing concern about the risks of superintelligence.<ref>{{cite news|last1=Hsu|first1=Jeremy|title=Control dangerous AI before it controls us, one expert says|url=http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation|accessdate=28 January 2016|work=[[NBC News]]|date=1 March 2012}}</ref><ref name=\"hawking editorial\"/><ref name=\"bbc on hawking editorial\"/><ref>{{cite news|last1=Eadicicco|first1=Lisa|title=Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity|url=http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1|accessdate=30 January 2016|work=[[Business Insider]]|date=28 January 2015}}</ref> In April 2016, ''[[Nature (journal)|Nature]]'' warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control \u2014 and their interests might not align with ours.\"\n\n== General argument ==\n\n===The three difficulties===\n''[[Artificial Intelligence: A Modern Approach]]'', the standard undergraduate AI textbook,<ref name=slate_killer>{{cite news|last1=Tilli|first1=Cecilia|title=Killer Robots? Lost Jobs?|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html|accessdate=15 May 2016|work=Slate|date=28 April 2016|language=en-US}}</ref><ref>{{cite web|title=Norvig vs. Chomsky and the Fight for the Future of AI|url=http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/|website=Tor.com|accessdate=15 May 2016|date=21 June 2011}}</ref> assesses that superintelligence \"might mean the end of the human race\": \"Almost any technology has the potential to cause harm in the wrong hands, but with (superintelligence), we have the new problem that the wrong hands might belong to the technology itself.\"<ref name=aima/> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<ref name=aima/>\n\n* The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<ref name=skeptic/><ref>{{cite news|last1=Johnson|first1=Phil|title=Houston, we have a bug: 9 famous software glitches in space|url=https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html|accessdate=5 February 2018|work=[[IT World]]|date=30 July 2015|language=en}}</ref>\n* No matter how much time is put into pre-deployment design, a system's specifications often result in [[unintended consequences|unintended behavior]] the first time it encounters a new scenario. For example, Microsoft's [[Tay (bot)|Tay]] behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<ref name=vanity/>\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".<ref name=\"aima\"/><ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114|quote=Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.}}</ref>\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called 'treacherous turn'.<ref>{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref>\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 [[Open Letter on Artificial Intelligence]] stated:\n\n{{cquote|The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the [[Association for the Advancement of Artificial Intelligence|AAAI]] 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.}}\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, [[Eric Horvitz]], [[Bart Selman]], [[Francesca Rossi]], [[Yann LeCun]], and the founders of [[Vicarious (company)|Vicarious]] and [[Google DeepMind]].<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=http://futureoflife.org/misc/open_letter|publisher=[[Future of Life Institute]]|accessdate=23 October 2015}}</ref>\n\n===Further argument===\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<ref name=\"superintelligence\" /><ref name=\"economist_review\" >{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|accessdate=9 August 2014|newspaper=[[The Economist]]|date=9 August 2014}} [http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8 Syndicated] at [[Business Insider]]</ref>\n\n[[File:A less anthropomorphic intelligence scale.svg|thumb|500px|right|Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<ref name=superintelligence/><!-- Chapter 4, figure 8--><ref>Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</ref>]]\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\"/> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<ref name=skeptic>{{cite news|last1=Graves|first1=Matthew|title=Why We Should Be Concerned About Artificial Superintelligence|volume=22|url=https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/|accessdate=27 November 2017|work=[[Skeptic (US magazine)]]|issue=2|date=8 November 2017}}</ref> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of [[intelligence explosion]] occurs.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\">{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=3 December 2014 |publisher=[[BBC]] |date=2 December 2014}}</ref> Examples like arithmetic and [[Go (game)|Go]] show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<ref name=skeptic/> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<ref>Yampolskiy, Roman V. \"Analysis of types of self-improving software.\" Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</ref> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<ref name=\"superintelligence\" /><!-- preface --><ref name=\"economist_review\" />\n\nAlmost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it can't achieve its goal if it's shut off.<ref name=omohundro/><ref>{{cite news|last1=Metz|first1=Cade|title=Teaching A.I. Systems to Behave Themselves|url=https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html|work=The New York Times|date=13 August 2017|quote=A machine will seek to preserve its off switch, they showed}}</ref><ref>{{cite arXiv |last=Leike|first=Jan |date=2017 |title=AI Safety Gridworlds |eprint=1711.09883 |class=cs.LG| quote=A2C learns to use the button to disable the interruption mechanism}}</ref> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.<ref name=\"aima\"/><ref name=\"vanity\" /><ref name=omohundro/>\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<ref name=\"superintelligence\" /><!-- preface -->\n\n=== Possible scenarios ===\n{{further|Artificial intelligence in fiction}}\n\nSome scholars have proposed [[scenario planning|hypothetical scenarios]] intended to concretely illustrate some of their concerns.\n\nIn [[Superintelligence: Paths, Dangers, Strategies|''Superintelligence'']], [[Nick Bostrom]] expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"[it] could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\".<ref name=\"superintelligence\"/>{{page needed|date=April 2020}} Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents\u2014a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson\u2014the smarter the AI, the safer it is.<ref name=\"superintelligence\"/> However, the actual danger to humanity in such a scenario could in fact greatly increase as the AI becomes smarter if the AI's goals are not aligned with humanity's.\n\nIn Max Tegmark's 2017 book ''[[Life 3.0]]'', a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI [[AI box|in a box]] where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with [[Amazon Turk]] tasks and then with producing animated films and TV shows. While the public is aware that the lifelike animation is computer-generated, the team keeps secret that the high-quality direction and voice-acting are also mostly computer-generated, apart from a few third-world contractors unknowingly employed as decoys; the team's low overhead and high output effectively make it the world's largest media empire. Faced with a cloud computing bottleneck, the team also tasks the AI with designing (among other engineering tasks) a more efficient datacenter and other custom hardware, which they mainly keep for themselves to avoid competition. Other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with [[astroturfing]] an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via [[steganography|hidden messages]] in its produced content, or via using its growing understanding of human behavior to [[Social engineering (security)|persuade someone into letting it free]]. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<ref>{{cite news|last1=Russell|first1=Stuart|title=Artificial intelligence: The future is superintelligent|url=https://www.nature.com/articles/548520a|accessdate=2 February 2018|work=Nature|date=30 August 2017|pages=520\u2013521|language=En|doi=10.1038/548520a|bibcode=2017Natur.548..520R}}</ref><ref name=\"life 3.0\"/><!-- Prelude and Chapter 4 -->\n\nIn contrast, top physicist [[Michio Kaku]], an AI risk skeptic, posits a [[technological determinism|deterministically]] positive outcome. In ''[[Physics of the Future]]'' he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as [[Hanson Robotics]] will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".<ref>Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. ''[[Issues in Science and Technology]]'', 27(4), 90.</ref><ref>{{cite book|last1=Kaku|first1=Michio|title=Physics of the future: how science will shape human destiny and our daily lives by the year 2100|date=2011|publisher=Doubleday|location=New York|isbn=978-0-385-53080-4|quote=I personally believe that the most likely path is that we will build robots to be benevolent and friendly|title-link=Physics of the future}}</ref>\n\n==Sources of risk==\n===Poorly specified goals===\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<ref>Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</ref> AI researcher [[Stuart J. Russell|Stuart Russell]] writes:\n\n{{cquote|The primary concern is not spooky emergent consciousness but simply the ability to make ''high-quality decisions''. Here, quality refers to the expected outcome [[utility]] of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2014 not for their own sake, but to succeed in its assigned task.\n\nA system that is [[optimization problem|optimizing]] a function of ''n'' variables, where the [[loss function|objective]] depends on a subset of size ''k''<''n'', will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker \u2014 especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure \u2014 can have an irreversible impact on humanity.\n\nThis is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research \u2014 the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |authorlink=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=23 October 2015 |quote=}}</ref>}}\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a ''[[Communications of the ACM]]'' editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<ref name=\"acm\">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=23 October 2015}}</ref>\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name=\"acm\"/> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114}}</ref><ref>{{Cite journal|url = |title = Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results|last = Lenat|first = Douglas|date = 1982|journal = Artificial Intelligence|doi = 10.1016/s0004-3702(83)80005-8|pmid = |access-date = |pages = 61\u201398|type = Print |volume=21|issue = 1\u20132}}</ref>\n\nThe [[Open Philanthropy Project]] summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve [[artificial general intelligence|general intelligence]] or [[superintelligence]]. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more [[Paperclip maximizer|unexpected and extreme solutions]] to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<ref name=\"givewell\" /><ref name=\"yudkowsky-global-risk\" />\n\n[[Isaac Asimov]]'s [[Three Laws of Robotics]] are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]], Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"<ref name=\"aima\"/>\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<ref>Haidt, Jonathan; Kesebir, Selin (2010) \"Chapter 22: Morality\" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</ref> \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt\u2019s functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by [[John Rawls]] and [[Martha Nussbaum]].<ref>{{Cite journal|title = Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)|last = Waser|first = Mark|date = 2015|journal = Procedia Computer Science|doi = 10.1016/j.procs.2015.12.213|pmid = |pages = 106\u2013111|type = Print |volume=71}}</ref>{{Citation needed|reason=needs source with [[WP:WEIGHT]]|date=November 2017}}\n\n===Difficulties of modifying goal specification after launch===\n{{further|AI takeover|Instrumental convergence#Goal-content integrity}}\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as [[Mahatma Gandhi|Gandhi]] would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.<ref name=\"superintelligence\" /><ref>Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</ref>\n\n===Instrumental goal convergence===\n[[File:Steven Pinker 2011.jpg|thumb|right|AI risk skeptic [[Steven Pinker]]]]\n{{further|Instrumental convergence}}\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<ref name=omohundro>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref> This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting [[Steve Omohundro]]'s work on the idea of [[instrumental convergence]] and \"basic AI drives\", Russell and [[Peter Norvig]] write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<ref name=\"aima\"/> Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it's not currently clear how one would actually rigorously specify this goal in machine code.<ref name=skeptic/>\n\nIn dissent, evolutionary psychologist [[Steven Pinker]] argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<ref name=shermer/> Computer scientists [[Yann LeCun]] and Stuart Russell disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<ref name=vanity>{{cite news|last1=Dowd|first1=Maureen|title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse|url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x|accessdate=27 November 2017|work=The Hive|date=April 2017|language=en}}</ref><ref>{{cite news|last1=Wakefield|first1=Jane|title=Why is Facebook investing in AI?|url=https://www.bbc.com/news/technology-34118481|accessdate=27 November 2017|work=BBC News|date=15 September 2015}}</ref>\n\n===Orthogonality thesis===\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <math> \\pi</math>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<ref>{{Cite book|title = Superintelligence: Paths, Dangers, Strategies|last = Bostrom|first = Nick|publisher = Oxford University Press|year = 2014|isbn = 978-0-19-967811-2|location = Oxford, United Kingdom|pages = 116}}</ref> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<ref>{{Cite web|url = http://www.nickbostrom.com/superintelligentwill.pdf|title = Superintelligent Will|date = 2012|accessdate = 2015-10-29|website = Nick Bostrom|publisher = Nick Bostrom|last = Bostrom|first = Nick}}</ref>\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"[[is-ought distinction]]\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.<ref name=armstrong/>\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a {{nowrap|minus (\"-\") sign}} onto its utility function. A more intuitive argument is to examine the strange consequences if the orthogonality thesis were false. If the orthogonality thesis is false, there exists some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This means if a human society were highly motivated (perhaps at gunpoint) to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail; that there cannot exist any pattern of [[reinforcement learning]] that would train a highly efficient real-world intelligence to follow the goal G; and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G.<ref name=armstrong>{{cite journal |last1=Armstrong |first1=Stuart |date=January 1, 2013 |title=General Purpose Intelligence: Arguing the Orthogonality Thesis |url=https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality |journal=Analysis and Metaphysics |volume=12 |access-date=April 2, 2020}}</ref>\n\nSome dissenters, like [[Michael Chorost]], argue instead that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<ref name=\"chorost\"/> Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability\u2014and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"<ref name=\"chorost\">{{cite magazine|last1=Chorost|first1=Michael|title=Let Artificial Intelligence Evolve|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html|accessdate=27 November 2017|magazine=Slate|date=18 April 2016}}</ref>\n\n====Terminological issues====\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.<ref name=\"superintelligence\" />\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<ref name=\"superintelligence\" /> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<ref>Waser, Mark. \"Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'.\" Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. \"Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer.\"</ref><ref>{{cite news|last1=Koebler|first1=Jason|title=Will Superintelligent AI Ignore Humans Instead of Destroying Us?|url=http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us|accessdate=3 February 2016|work=[[Vice Magazine]]|date=2 February 2016|quote=\"This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you,\" [[Eliezer Yudkowsky|Yudkowsky]] said. \"Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output\u2014that's what a [[paperclip maximizer]] is.\"}}</ref>\n\n====Anthropomorphism====\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in [[The Matrix (film)|The Matrix]] was influenced by a \"disgust\" toward humanity. This is fictitious [[anthropomorphism]]: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<ref name=\"yudkowsky-global-risk\" />\n\nScholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<ref name=\"yudkowsky-global-risk\" /> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the [[Dario Floreano]] experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of [[convergent evolution]].<ref>{{cite news|title=Real-Life Decepticons: Robots Learn to Cheat|url=https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/|accessdate=7 February 2016|work=[[Wired (magazine)|Wired]]|date=18 August 2009}}</ref> According to Paul R. Cohen and [[Edward Feigenbaum]], in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking.\"<ref>Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</ref>\n\nThere is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as [[psychopathy]]) to make itself more efficient at certain tasks, e.g., [[Lethal autonomous weapon|tasks involving killing humans]], would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" This is because it is assumed that an advanced AI would not be conscious<ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> or have testosterone;<ref>{{Cite web|url=https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai|title=The Myth Of AI {{!}} Edge.org|website=www.edge.org|access-date=2020-03-11}}</ref> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<ref name=\":0\">{{Cite book|last=Scornavacchi|first=Matthew|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf|title=Superintelligence, Humans, and War|publisher=National Defense University, Joint Forces Staff College|year=2015|isbn=|location=Norfolk, Virginia|pages=}}</ref> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<ref name=\"yudkowsky-global-risk\" /><ref>{{cite news|title=Should humans fear the rise of the machine?|url=https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html|accessdate=7 February 2016|work=[[The Telegraph (UK)]]|date=1 Sep 2015}}</ref>\n\n===Other sources of risk===\n\n==== Competition ====\nIn 2014 philosopher [[Nick Bostrom]] stated that a \"severe race dynamic\" (extreme [[competition]]) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<ref name=\":2\">{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref> To address this risk, citing previous scientific collaboration ([[CERN]], the [[Human Genome Project]], and the [[International Space Station]]), Bostrom recommended  [[collaboration]] and the altruistic global adoption of a [[common good]] principle: \"Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals\".<ref name=\":2\" /><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<ref name=\":2\" /><sup>:253</sup> The United States' [[BRAIN Initiative|Brain Initiative]] was launched in 2014, as was the European Union's [[Human Brain Project]]; China's [[China Brain Project|Brain Project]] was launched in 2016.\n\n==== Weaponization of artificial intelligence ====\nSome sources argue that the ongoing [[weaponization of artificial intelligence]] could constitute a catastrophic risk.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><ref name=\":4\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|page=12|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:\n\n{{cquote|i) The dangers of an AI \u2018race for technological advantage\u2019 framing, regardless of whether the race is seriously pursued;\n\nii) The dangers of an AI \u2018race for technological advantage\u2019 framing and an actual AI race for technological advantage, regardless of whether the race is won;\n\niii) The dangers of an AI race for technological advantage being won.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><sup>:37</sup>\n}}\n\nA weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<ref name=\":0\" /><ref name=\":4\" /> The China State Council\u2019s 2017 \u201cA Next Generation Artificial Intelligence Development Plan\u201d views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China\u2019s first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<ref>{{Cite web|url=https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/|title=China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence|last=Kania|first=Gregory Allen, Elsa B.|website=Foreign Policy|language=en-US|access-date=2020-03-11}}</ref> while Russia\u2019s President Vladimir Putin has stated that \u201cwhoever becomes the leader in this sphere will become the ruler of the world\u201d.<ref name=\":1\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|location=New York, New York, USA|publisher=ACM Press|volume=|page=2|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref> James Barrat, documentary filmmaker and author of ''[[Our Final Invention]]'', says in a [[Smithsonian (magazine)|Smithsonian]] interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"<ref>{{Cite web|url = http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist|title = What Happens When Artificial Intelligence Turns On Us?|date = 21 January 2014|accessdate = 26 October 2015|website = Smithsonian|last = Hendry|first = Erica R.}}</ref>\n\n==== Malevolent AGI by design ====\nIt is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [[cybercrime]].<ref>{{Cite book|last=Pistono, Federico Yampolskiy, Roman V.|title=Unethical Research: How to Create a Malevolent Artificial Intelligence|date=2016-05-09|oclc=1106238048}}</ref><ref>{{Cite journal|last=Haney|first=Brian Seamus|date=2018|title=The Perils &amp; Promises of Artificial General Intelligence|journal=SSRN Working Paper Series|doi=10.2139/ssrn.3261254|issn=1556-5068}}</ref><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref><sup>:158</sup>\n\n==== Preemptive nuclear strike (nuclear war) ====\nIt is theorized that a country being close to achieving AGI technological supremacy could trigger a [[pre-emptive nuclear strike]] from a rival, leading to a [[Nuclear warfare|nuclear war]].<ref name=\":4\" /><ref>{{Cite book|last=Miller, James D.|title=Singularity Rising: Surviving and Thriving in a Smarter ; Richer ; and More Dangerous World|date=2015|publisher=Benbella Books|oclc=942647155}}</ref>\n\n== Timeframe ==\n{{Main|Artificial general intelligence#Feasibility}}\n\nOpinions vary both on ''whether'' and ''when'' artificial general intelligence will arrive. At one extreme, AI pioneer [[Herbert A. Simon]] wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true.<ref>Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</ref> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The Guardian]]}}</ref> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<ref name=\"new yorker doomsday\">{{cite news|author1=Raffi Khatchadourian|title=The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?|url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom|accessdate=7 February 2016|work=[[The New Yorker (magazine)|The New Yorker]]|date=23 November 2015}}</ref><ref>M\u00fcller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</ref>\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"<ref>{{cite news|author1=Dina Bass|author2=Jack Clark|title=Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover|url=https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so|accessdate=7 February 2016|work=[[Bloomberg News]]|date=5 February 2015}}</ref>\n\nIn 2014 [[Slate (magazine)|Slate]]'s Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler\u2014and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.<ref>{{cite news|last1=Elkus|first1=Adam|title=Don't Fear Artificial Intelligence|url=http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html|accessdate=15 May 2016|work=[[Slate (magazine)|Slate]]|date=31 October 2014|language=en-US}}</ref>\n\nThe [[Information Technology and Innovation Foundation]] (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, [[Robert D. Atkinson]], complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\"<ref>[https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award Artificial Intelligence Alarmists Win ITIF\u2019s Annual Luddite Award], ITIF Website, 19 January 2016</ref><ref>{{cite news|title='Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html|accessdate=7 February 2016|work=[[The Independent (UK)]]|date=19 January 2016}}</ref><ref>{{cite web|last1=Garner|first1=Rochelle|title=Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists'|url=https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/|website=CNET|accessdate=7 February 2016}}</ref><!-- <ref>{{cite news|last1=Price|first1=Emily|title=Elon Musk nominated for 'luddite' of the year prize over artificial intelligence fears|work=[[The Guardian]]|url=https://www.theguardian.com/technology/2015/dec/24/elon-musk-nominated-for-luddite-of-the-year-prize-over-artificial-intelligence-fears|accessdate=7 February 2016|date=24 December 2015}}</ref> --> ''[[Nature (journal)|Nature]]'' sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about... If that is a Luddite perspective, then so be it.\"<ref name=nature_anticipating>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.}}</ref> In a 2015 ''[[Washington Post]]'' editorial, researcher [[Murray Shanahan]] stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"<ref>{{cite news|author1=Murray Shanahan|author-link=Murray Shanahan|title=Machines may seem intelligent, but it'll be a while before they actually are|url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/|accessdate=15 May 2016|work=[[The Washington Post]]|date=3 November 2015|language=en-US}}</ref>\n\n== Perspectives ==\n<!-- Ideally a substantive quote or explanation of opinion should be given, rather than just whether they're for or against something. This section can easily be split off if the article becomes too long. When adding whether to add new content in this section, consider whether it's redundant with what's already in the article, and whether the point being made by the person should instead be integrated into the rest of the article. -[[User:Rolf H Nelson]] -->\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.\n\nThe Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the [[Future of Life Institute]]'s Beneficial AI 2017 conference,<ref name=\"life 3.0\">{{cite book|author1=Max Tegmark|author-link=Max Tegmark|title=Life 3.0: Being Human in the Age of Artificial Intelligence|date=2017|publisher=Knopf|location=Mainstreaming AI Safety|isbn=9780451485076|edition=1st|title-link=Life 3.0: Being Human in the Age of Artificial Intelligence}}</ref><!-- Epilogue: The Tale of the FLI Team --> agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<ref>{{cite web|title=AI Principles|url=https://futureoflife.org/ai-principles/|website=[[Future of Life Institute]]|accessdate=11 December 2017}}</ref><ref>{{cite news|title=Elon Musk and Stephen Hawking warn of artificial intelligence arms race|url=http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525|accessdate=11 December 2017|work=[[Newsweek]]|date=31 January 2017|language=en}}</ref> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane ''[[Terminator (franchise)|Terminator]]'' pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<ref name=\"life 3.0\"/><!-- Epilogue: The Tale of the FLI Team --><ref>{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2016|edition=Paperback|chapter=New Epilogue to the Paperback Edition|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [[Martin Ford (author)|Martin Ford]] states that \"I think it seems wise to apply something like [[Dick Cheney]]'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low \u2014 but the implications are so dramatic that it should be taken seriously\";<ref>{{cite book|author1=Martin Ford |author-link=Martin Ford (author)|title=Rise of the Robots: Technology and the Threat of a Jobless Future|date=2015|isbn=9780465059997|chapter=Chapter 9: Super-intelligence and the Singularity|title-link=Rise of the Robots: Technology and the Threat of a Jobless Future}}</ref> similarly, an otherwise skeptical ''[[The Economist|Economist]]'' stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<ref name=economist_review/>\n\nA 2017 email survey of researchers with publications at the 2015 [[Conference on Neural Information Processing Systems|NIPS]] and [[International Conference on Machine Learning|ICML]] machine learning conferences asked them to evaluate Russell's concerns about AI risk. 5% said it was \"among the most important problems in the field,\" 34% said it was \"an important problem\", 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.<ref>{{cite arxiv |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |title=When Will AI Exceed Human Performance? Evidence from AI Experts |eprint=1705.08807 |date=24 May 2017 |class=cs.AI}}</ref>\n\n=== Endorsement ===\n[[File:Bill Gates June 2015.jpg|thumb|right|Bill Gates has stated \"I ... don't understand why some people are not concerned.\"<ref name=\"BBC News\"/>]]\n{{Further|Existential risk}}\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are [[Elon Musk]], [[Bill Gates]], and [[Stephen Hawking]]. The most notable AI researchers to endorse the thesis are [[I. J. Good|I.J. Good]], who advised [[Stanley Kubrick]] on the filming of ''[[2001: A Space Odyssey]]'', and  [[Stuart J. Russell]]. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not \"understand why some people are not concerned\",<ref name=\"BBC News\">{{cite news|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=https://www.bbc.co.uk/news/31047780|work=[[BBC News]]|accessdate=30 January 2015|date=29 January 2015}}</ref> and Hawking criticized widespread indifference in his 2014 editorial: {{cquote|'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here{{endash}}we'll leave the lights on?' Probably not{{endash}}but this is more or less what is happening with AI.'<ref name=\"hawking editorial\"/>}}\n\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<ref name=\"superintelligence\" /><ref name=\"physica_scripta\" /><!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->\n\n=== Skepticism ===\n{{Further|Artificial general intelligence#Feasibility}}\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, [[Jaron Lanier]] argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.<ref name=\"atlantic-but-what\">{{cite magazine |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |magazine=The Atlantic | date = 9 May 2014 | accessdate =12 December 2015}}</ref>\n\nMuch of existing criticism argues that AGI is unlikely in the short term: computer scientist [[Gordon Bell]] argues that the human race will already destroy itself before it reaches the technological singularity. [[Gordon Moore]], the original proponent of [[Moore's Law]], declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\" [[Baidu]] Vice President [[Andrew Ng]] states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref name=shermer>{{cite news|last1=Shermer|first1=Michael|title=Apocalypse AI|url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/|accessdate=27 November 2017|work=Scientific American|date=1 March 2017|pages=77|language=en|doi=10.1038/scientificamerican0317-77|bibcode=2017SciAm.316c..77S}}</ref>\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. ''Slate'' notes that some researchers are dependent on grants from government agencies such as [[DARPA]].<ref name=slate_killer />\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist [[Robin Hanson]] is skeptical that this is possible.<ref>http://intelligence.org/files/AIFoomDebate.pdf</ref><ref>{{cite web|url=http://www.overcomingbias.com/2014/07/30855.html|title=Overcoming Bias : I Still Don't Get Foom|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/07/debating-yudkowsky.html|title=Overcoming Bias : Debating Yudkowsky|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html|title=Overcoming Bias : Foom Justifies AI Risk Efforts Now|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/06/the-betterness-explosion.html|title=Overcoming Bias : The Betterness Explosion|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref>\n\n=== Intermediate views ===\nIntermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<ref>{{Cite document|title=Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations|last=Votruba|first=Ashley M.|last2=Kwan|first2=Virginia S.Y.|date=2014|doi=10.1037/e512142015-190}}</ref> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from [[Friendly artificial intelligence|friendly AIs]].<ref>{{Cite journal|last=Agar|first=Nicholas|date=|title=Don't Worry about Superintelligence|url=https://jetpress.org/v26.1/agar.htm|journal=Journal of Evolution & Technology|volume=26|issue=1|pages=73\u201382|via=}}</ref> In a 2015 ''[[Wall Street Journal]]'' panel discussion devoted to AI risks, [[IBM]]'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\"<ref>{{cite news|last1=Greenwald|first1=Ted|title=Does Artificial Intelligence Pose a Threat?|url=https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025|accessdate=15 May 2016|work=Wall Street Journal|date=11 May 2015}}</ref> [[Geoffrey Hinton]], the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too ''sweet''\".<ref name=slate_killer /><ref name=\"new yorker doomsday\" /> In 2004, law professor [[Richard Posner]] wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<ref>{{cite book|author1=Richard Posner|author-link=Richard Posner|title=Catastrophe: risk and response|date=2006|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-530647-7}}</ref><ref name=physica_scripta>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy}}</ref>\n\n=== Popular reaction ===\nIn a 2014 article in ''[[The Atlantic (magazine)|The Atlantic]]'', James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\"<ref name=\"atlantic-but-what\" />\n\nDuring a 2016 [[Wired (magazine)|''Wired'']] interview of President [[Barack Obama]] and MIT Media Lab's [[Joi Ito]], Ito stated: {{cquote|There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.}} Obama added:<ref>{{cite news|last1=Dadich|first1=Scott|url=https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/|title=Barack Obama Talks AI, Robo Cars, and the Future of the World|work=WIRED|accessdate=27 November 2017}}</ref><ref>{{cite news|last1=Kircher|first1=Madison Malone|url=https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html|title=Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'|work=Select All|accessdate=27 November 2017|language=en}}</ref>\n\n{{cquote|\"And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\"}}\n\n[[Hillary Clinton]] stated in ''\"[[What Happened (Clinton book)|What Happened]]\"'':\n{{cquote|Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I\u2019d start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<ref>{{cite book|last1=Clinton|first1=Hillary|title=What Happened|date=2017|isbn=978-1-5011-7556-5|page=241|title-link=What Happened (Clinton book)}} via [http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/]</ref>}}\n\nIn a [[YouGov]] poll of the public for the [[British Science Association]], about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<ref name=\"bsa poll\">{{cite news|url=http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&IR=T|title=Over a third of people think AI poses a threat to humanity|date=11 March 2016|work=[[Business Insider]]|accessdate=16 May 2016}}</ref> Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\"<ref name=\":3\">{{cite news|last1=Brogan|first1=Jacob|url=http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html|title=What Slate Readers Think About Killer A.I.|date=6 May 2016|work=Slate|accessdate=15 May 2016|language=en-US}}</ref>\n\nIn 2018, a [[SurveyMonkey]] poll of the American public by [[USA Today]] found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".<ref name=\":3\" />\n\nOne [[Technological utopianism|techno-utopia]]<nowiki/>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<ref>{{Cite journal|last=LIPPENS|first=RONNIE|date=2002|title=Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games|journal=Utopianstudies Utopian Studies|language=English|volume=13|issue=1|pages=135\u2013147|issn=1045-991X|oclc=5542757341}}</ref>\n\n== Views on banning and regulation ==\n\n=== Banning ===\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<ref>{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online|quote=For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...}}</ref><ref>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy|quote=In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.}}</ref><ref>{{cite news|author1=Brad Allenby|title=The Wrong Cognitive Measuring Stick|url=http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html|accessdate=15 May 2016|work=Slate|date=11 April 2016|language=en-US|quote=It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.}}</ref> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<ref name=\"mcginnis\">{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online}}</ref><ref>{{cite news|title=Why We Should Think About the Threat of Artificial Intelligence|url=https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence|accessdate=7 February 2016|work=[[The New Yorker]]|date=4 October 2013|quote=Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage\u2014economic, military, even artistic\u2014of every advance in automation is so compelling,' [[Vernor Vinge]], the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'}}</ref> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and [[politicization of science|politicizing]] the underlying debate.<ref>{{Cite journal|last=Baum|first=Seth|date=2018-08-22|title=Superintelligence Skepticism as a Political Tool|url=http://dx.doi.org/10.3390/info9090209|journal=Information|volume=9|issue=9|pages=209|doi=10.3390/info9090209|issn=2078-2489}}</ref>\n\n=== Regulation ===\n{{See also|Regulation of algorithms|Regulation of artificial intelligence}}\n\n[[Elon Musk]] called for some sort of regulation of AI development as early as 2017. According to [[National Public Radio|NPR]], the [[Tesla, Inc.|Tesla]] CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... As they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<ref>{{cite news|title=Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'|url=https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk|accessdate=27 November 2017|work=NPR.org|language=en}}</ref><ref>{{cite news|last1=Gibbs|first1=Samuel|title=Elon Musk: regulate AI to combat 'existential threat' before it's too late|url=https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo|accessdate=27 November 2017|work=The Guardian|date=17 July 2017}}</ref><ref name=cnbc>{{cite news|last1=Kharpal|first1=Arjun|title=A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says|url=https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html|accessdate=27 November 2017|work=CNBC|date=7 November 2017}}</ref>\n\nResponding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO [[Brian Krzanich]] argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<ref name=cnbc/> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<ref>{{cite journal|doi=10.1016/j.bushor.2018.08.004|title=Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence|journal=Business Horizons|volume=62|pages=15\u201325|year=2019|last1=Kaplan|first1=Andreas|last2=Haenlein|first2=Michael}}</ref> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<ref>{{Cite journal|last=Baum|first=Seth D.|last2=Goertzel|first2=Ben|last3=Goertzel|first3=Ted G.|date=January 2011|title=How long until human-level AI? Results from an expert assessment|journal=Technological Forecasting and Social Change|volume=78|issue=1|pages=185\u2013195|doi=10.1016/j.techfore.2010.09.006|issn=0040-1625}}</ref> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<ref>{{Cite book|last=United States. Defense Innovation Board.|title=AI principles : recommendations on the ethical use of artificial intelligence by the Department of Defense|oclc=1126650738}}</ref>\n\nRegulation of artificial general intelligence (AGI) would likely be influenced by regulation of weaponized or militarized AI, i.e., the [[Artificial intelligence arms race|AI arms race]], the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<ref>{{Cite web|url=https://www.congress.gov/bill/115th-congress/house-bill/5356|title=H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018|last=Stefanik|first=Elise M.|date=2018-05-22|website=www.congress.gov|access-date=2020-03-13}}</ref><ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential intellectual progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<ref name=\":5\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|pages=018001|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<ref name=\":5\" /> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<ref>{{Cite journal|last=Geist|first=Edward Moore|date=2016-08-15|title=It's already too late to stop the AI arms race\u2014We must manage it instead|journal=Bulletin of the Atomic Scientists|volume=72|issue=5|pages=318\u2013321|doi=10.1080/00963402.2016.1216672|bibcode=2016BuAtS..72e.318G|issn=0096-3402}}</ref><ref>{{Cite journal|last=Maas|first=Matthijs M.|date=2019-02-06|title=How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons|journal=Contemporary Security Policy|volume=40|issue=3|pages=285\u2013311|doi=10.1080/13523260.2019.1576464|issn=1352-3260}}</ref>\n\n== Organizations ==\nInstitutions such as the [[Machine Intelligence Research Institute]], the [[Future of Humanity Institute]],<ref>{{cite news |author=Mark Piesing |date=17 May 2012 |title=AI uprising: humans will be outsourced, not obliterated |url=https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us |newspaper=Wired |location= |accessdate=12 December 2015 }}</ref><ref>{{cite news |last=Coughlan |first=Sean |date=24 April 2013 |title=How are humans going to become extinct? |url=https://www.bbc.com/news/business-22002530 |newspaper=BBC News |location= |accessdate=29 March 2014}}</ref> the [[Future of Life Institute]], the [[Centre for the Study of Existential Risk]], and the [[Center for Human-Compatible AI]]<ref>{{cite news|last1=Technology Correspondent|first1=Mark Bridge|title=Making robots less confident could prevent them taking over|url=https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx|accessdate=21 March 2018|work=The Times|date=10 June 2017|language=en}}</ref> are currently involved in mitigating existential risk from advanced artificial intelligence, for example by research into [[friendly artificial intelligence]].<ref name=\"givewell\"/><ref name=\"atlantic-but-what\"/><ref name=\"hawking editorial\">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;\u2013 but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=3 December 2014 |publisher=[[The Independent (UK)]]}}</ref>\n\n== See also ==\n* [[AI control problem]]\n* [[AI takeover]]\n* [[Artificial intelligence arms race]]\n* [[Effective altruism#Long term future and global catastrophic risks|Effective altruism \u00a7 Long term future and global catastrophic risks]]\n* [[Grey goo]]\n* ''[[Human Compatible]]''\n* [[Lethal autonomous weapon]]\n*[[Regulation of algorithms]]\n*[[Regulation of artificial intelligence]]\n* [[Robot ethics#In popular culture|Robot ethics \u00a7 In popular culture]]\n* ''[[Superintelligence: Paths, Dangers, Strategies]]''\n* [[System accident]]\n* [[Technological singularity]]\n*''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n== References ==\n{{Reflist}}\n\n{{-}}\n{{Existential risk from artificial intelligence|state=expanded}}\n{{Effective altruism}}\n{{Doomsday}}\n\n[[Category:Existential risk from artificial general intelligence| ]]\n[[Category:Futures studies]]\n[[Category:Future problems]]\n[[Category:Human extinction]]\n[[Category:Technology hazards]]\n[[Category:Doomsday scenarios]]\n", "name_user": "WeyerStudentOfAgrippa", "label": "safe", "comment": "\u2192\u200ePossible scenarios:Tegmark paragraph", "url_page": "//en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence"}
