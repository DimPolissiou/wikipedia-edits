{"title_page": "RAS syndrome", "text_new": "{{short description|Using an acronym followed by one of the words composing that acronym}}\n[[Image:ATM Machine RAS Syndrome sign.jpg|thumb|\"ATM machine\" is a common example of RAS syndrome]]\n'''RAS syndrome''' (where \"RAS\" stands for \"'''redundant acronym syndrome'''\", making the phrase \"RAS syndrome\" [[self-reference|self-referential]]) is the use of one or more of the words that make up an [[acronym]] (or other [[initialism]]) in conjunction with the abbreviated form. This means, in effect, repeating one or more words from the [[acronym]]. Two common examples are \"[[Personal identification number|PIN]] number\" / \"[[Vehicle identification number|VIN]] number\" (the \"N\" in PIN and VIN stands for \"number\") and \"[[Automated teller machine|ATM]] machine\" (the \"M\" in ATM stands for \"machine\"). The term ''RAS syndrome'' was coined in 2001 in a light-hearted column in ''[[New Scientist]]''.<ref>{{Cite news |last=Clothier |first=Gary |date=8 November 2006 |title=Ask Mr. Know-It-All |newspaper=The York Dispatch}}</ref><ref name=Newman>{{Cite news |last=Newman |first=Stanley |date=December 20, 2008 |url=http://www2.canada.com/windsorstar/news/readersatplay/story.html?id=ea936740-3787-49be-813d-937b3a63eb74 |title=Sushi by any other name |newspaper=Windsor Star |page=G4 |url-status=dead |archiveurl=https://web.archive.org/web/20120503121728/http://www2.canada.com/windsorstar/news/readersatplay/story.html?id=ea936740-3787-49be-813d-937b3a63eb74 |archivedate=May 3, 2012 }}</ref><ref>{{cite news |title=Feedback |newspaper=[[New Scientist]]|issue=2285 |date=2001-04-07 |page=108 |url=https://www.newscientist.com/article/mg17022858.000 |format=fee required |accessdate=2006-12-08}}</ref>  \n\nMany style guides advise against usage of these '''redundant acronyms''',<ref name=Garner>[[Bryan A. Garner|Garner, Bryan A]]. (2000) ''The Oxford Dictionary of American Usage and Style.'' Oxford and New York: Oxford University Press.</ref> but they continue to have widespread usage in colloquial speech.\n\n== Examples ==\n<!--\nPlease do not just add new definitions to the list. Consensus is to not add more items. If you want to change one, please discuss it on the talk page first.         New examples will be deleted if added without consensus.\n-->\nOther [[Nonce word|nonce]] coinages continue to arise. Select examples of RAS phrases include:\n\n* [[Human immunodeficiency virus|HIV]] virus (human immunodeficiency virus virus)<ref>{{Cite web|url=https://www.thoughtco.com/ras-syndrome-words-1691894|title=RAS Syndrome: Redundant Acronym Syndrome Syndrome|first1=Richard |last1=Nordquist |website=ThoughtCo}}</ref>\n* [[Liquid crystal display|LCD]] display (liquid crystal display display)<ref>{{Cite web |last=Brians |first=Paul |url=http://public.wsu.edu/~brians/errors/lcd.html |title=LCD display |work=Common Errors in English Usage |accessdate=2012-05-01}}</ref><ref>{{Cite web|url=https://www.npr.org/sections/memmos/2015/01/06/605393666/do-you-suffer-from-ras-syndrome|title=Do You Suffer From RAS Syndrome?|website=NPR.org}}</ref>\n* [[Portable Document Format|PDF]] format (portable document format format)\n* [[Universal Product Code|UPC]] code (universal product code code)<ref>{{Cite web|url=https://www.rd.com/funny-stuff/ras-syndrome/|title=Warning: If You Say \"PIN Number\" You May Have RAS Syndrome.|website=Reader's Digest}}</ref>\n<!---DO NOT ADD ANOTHER EXAMPLE WITHOUT CONSENSUS - IT WILL BE IMMEDIATELY REMOVED--->\n\n== Reasons for use ==\nAlthough there are many instances in editing where removal of redundancy improves clarity,<ref name=\"Bryson2002\">{{Citation |last=Bryson |first=Bill |authorlink=Bill Bryson |title=Bryson's Dictionary of Troublesome Words |year=2002 |isbn=0-7679-1043-5 |postscript=.}}</ref> the pure-logic ideal of ''zero'' redundancy is seldom maintained in human languages. [[Bill Bryson]] says,<ref name=\"Bryson2002\"/> \"Not all repetition is bad. It can be used for effect&nbsp;..., or for clarity, or in deference to [[wikt:idiom|idiom]]. '[[OPEC]] countries', '[[Strategic Arms Limitation Talks|SALT]] talks' and '[[HIV]] virus' are all technically redundant because the second word is already contained in the preceding abbreviation, but only the ultra-finicky would deplore them. Similarly, in 'Wipe that smile off your face' the last two words are tautological\u2014there is no other place a smile could be\u2014but the sentence would not stand without them.\"<ref name=\"Bryson2002\"/>\n\nA limited amount of redundancy can improve the effectiveness of communication, either for the whole readership or at least to offer help to those readers who need it. A phonetic example of that principle is the need for [[spelling alphabet]]s in radiotelephony. Some instances of RAS can be viewed as syntactic examples of the principle. The redundancy may help the listener by providing context and decreasing the \"[[alphabet soup (linguistics)|alphabet soup]] quotient\" (the [[wikt:cryptic|cryptic]] overabundance of abbreviations and acronyms) of the communication.\n\nAcronyms and initialisms from foreign languages are often treated as unanalyzed [[morphemes]] when they are not translated.  For example, in French, \"le protocole IP\" (the Internet protocol protocol) is often used, and in English \"please [[RSVP]]\" (roughly \"please respond please\") is very common.<ref name=Garner/><ref>{{cite web|url=http://www.linguistlist.org/issues/4/4-532.html#1 |title=LINGUIST List 4.532: Last Posting: Acronyms |publisher=Linguistlist.org |date= |accessdate=2009-05-22}}</ref> This occurs for the same [[linguistics|linguistic]] reasons that cause [[List of tautological place names|many toponyms to be tautological]]. The [[tautology (rhetoric)|tautology]] is not parsed by the mind in most instances of real-world use (in many cases because the foreign word's meaning is not known anyway; in others simply because the usage is idiomatic).\n\n== See also ==\n{{Wiktionary|RAS syndrome}}\n* [[Pleonasm#Bilingual tautological expressions|Bilingual tautological expressions]]\n* [[Tautology (rhetoric)]]\n* [[Recursive acronym]]\n* [[List of tautological place names]]\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Ras Syndrome}}\n[[Category:Acronyms]]\n[[Category:Rhetoric]]\n[[Category:Semantics]]\n[[Category:Syntax]]\n[[Category:Linguistic morphology]]\n[[Category:Word play]]\n\n[[de:Akronym#Redundantes Akronym]]\n", "text_old": "{{short description|Using an acronym followed by one of the words composing that acronym}}\n[[Image:ATM Machine RAS Syndrome sign.jpg|thumb|\"ATM machine\" is a common example of RAS syndrome]]\n'''RAS syndrome''' (where \"RAS\" stands for \"'''redundant acronym syndrome'''\", making the phrase \"RAS syndrome\" [[self-reference|self-referential]]) is the use of one or more of the words that make up an [[acronym]] (or other [[initialism]]) in conjunction with the abbreviated form. This means, in effect, repeating one or more words from the [[acronym]]. Two common examples are \"[[Personal identification number|PIN]] number\" / \"[[Vehicle identification number|VIN]] number\" (the \"N\" in PIN and VIN stands for \"number\") and \"[[Automated teller machine|ATM]] machine\" (the \"M\" in ATM stands for \"machine\"). The term ''RAS syndrome'' was coined in 2001 in a light-hearted column in ''[[New Scientist]]''.<ref>{{Cite news |last=Clothier |first=Gary |date=8 November 2006 |title=Ask Mr. Know-It-All |newspaper=The York Dispatch}}</ref><ref name=Newman>{{Cite news |last=Newman |first=Stanley |date=December 20, 2008 |url=http://www2.canada.com/windsorstar/news/readersatplay/story.html?id=ea936740-3787-49be-813d-937b3a63eb74 |title=Sushi by any other name |newspaper=Windsor Star |page=G4 |url-status=dead |archiveurl=https://web.archive.org/web/20120503121728/http://www2.canada.com/windsorstar/news/readersatplay/story.html?id=ea936740-3787-49be-813d-937b3a63eb74 |archivedate=May 3, 2012 }}</ref><ref>{{cite news |title=Feedback |newspaper=[[New Scientist]]|issue=2285 |date=2001-04-07 |page=108 |url=https://www.newscientist.com/article/mg17022858.000 |format=fee required |accessdate=2006-12-08}}</ref>  \n\nMany style guides advise against usage of these '''redundant acronyms''',<ref name=Garner>[[Bryan A. Garner|Garner, Bryan A]]. (2000) ''The Oxford Dictionary of American Usage and Style.'' Oxford and New York: Oxford University Press.</ref> but they continue to have widespread usage in colloquial speech.\n\n== Examples ==\n<!--\nPlease do not just add new definitions to the list. Consensus is to not add more items. If you want to change one, please discuss it on the talk page first.         New examples will be deleted if added without consensus.\n-->\nOther [[Nonce word|nonce]] coinages continue to arise. Select examples of RAS phrases include:\n\n* [[Human immunodeficiency virus|HIV]] virus (human immunodeficiency virus virus)<ref>{{Cite web|url=https://www.thoughtco.com/ras-syndrome-words-1691894|title=RAS Syndrome: Redundant Acronym Syndrome Syndrome|first1=Richard |last1=Nordquist |website=ThoughtCo}}</ref>\n* [[Liquid crystal display|LCD]] display (liquid crystal display display)<ref>{{Cite web |last=Brians |first=Paul |url=http://public.wsu.edu/~brians/errors/lcd.html |title=LCD display |work=Common Errors in English Usage |accessdate=2012-05-01}}</ref><ref>{{Cite web|url=https://www.npr.org/sections/memmos/2015/01/06/605393666/do-you-suffer-from-ras-syndrome|title=Do You Suffer From RAS Syndrome?|website=NPR.org}}</ref>\n* [[Portable Document Format|PDF]] format (portable document format format)\n* [[Universal Product Code|UPC]] code (universal product code code)<ref>{{Cite web|url=https://www.rd.com/funny-stuff/ras-syndrome/|title=Warning: If You Say \"PIN Number\" You May Have RAS Syndrome.|website=Reader's Digest}}</ref>\n<!---DO NOT ADD ANOTHER EXAMPLE WITHOUT CONSENSUS - IT WILL BE IMMEDIATELY REMOVED--->\n\n== Reasons for use ==\nAlthough there are many instances in editing where removal of redundancy improves clarity,<ref name=\"Bryson2002\">{{Citation |last=Bryson |first=Bill |authorlink=Bill Bryson |title=Bryson's Dictionary of Troublesome Words |year=2002 |isbn=0-7679-1043-5 |postscript=.}}</ref> the pure-logic ideal of ''zero'' redundancy is seldom maintained in human languages. [[Bill Bryson]] says,<ref name=\"Bryson2002\"/> \"Not all repetition is bad. It can be used for effect&nbsp;..., or for clarity, or in deference to [[wikt:idiom|idiom]]. '[[OPEC]] countries', '[[Strategic Arms Limitation Talks|SALT]] talks' and '[[HIV]] virus' are all technically redundant because the second word is already contained in the preceding abbreviation, but only the ultra-finicky would deplore them. Similarly, in 'Wipe that smile off your face' the last two words are tautological\u2014there is no other place a smile could be\u2014but the sentence would not stand without them.\"<ref name=\"Bryson2002\"/>\n\nA limited amount of redundancy can improve the effectiveness of communication, either for the whole readership or at least to offer help to those readers who need it. A phonetic example of that principle is the need for [[spelling alphabet]]s in radiotelephony. Some instances of RAS can be viewed as syntactic examples of the principle. The redundancy may help the listener by providing context and decreasing the \"[[alphabet soup (linguistics)|alphabet soup]] quotient\" (the [[wikt:cryptic|cryptic]] overabundance of abbreviations and acronyms) of the communication.\n\nAcronyms and initialisms from foreign languages are often treated as unanalyzed [[morphemes]] when they are not translated.  For example, in French, \"le protocole IP\" (the Internet protocol protocol) is often used, and in English \"please [[RSVP (invitations)|RSVP]]\" (roughly \"please respond please\") is very common.<ref name=Garner/><ref>{{cite web|url=http://www.linguistlist.org/issues/4/4-532.html#1 |title=LINGUIST List 4.532: Last Posting: Acronyms |publisher=Linguistlist.org |date= |accessdate=2009-05-22}}</ref> This occurs for the same [[linguistics|linguistic]] reasons that cause [[List of tautological place names|many toponyms to be tautological]]. The [[tautology (rhetoric)|tautology]] is not parsed by the mind in most instances of real-world use (in many cases because the foreign word's meaning is not known anyway; in others simply because the usage is idiomatic).\n\n== See also ==\n{{Wiktionary|RAS syndrome}}\n* [[Pleonasm#Bilingual tautological expressions|Bilingual tautological expressions]]\n* [[Tautology (rhetoric)]]\n* [[Recursive acronym]]\n* [[List of tautological place names]]\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Ras Syndrome}}\n[[Category:Acronyms]]\n[[Category:Rhetoric]]\n[[Category:Semantics]]\n[[Category:Syntax]]\n[[Category:Linguistic morphology]]\n[[Category:Word play]]\n\n[[de:Akronym#Redundantes Akronym]]\n", "name_user": "Elshad", "label": "safe", "comment": "\u2192\u200eReasons for use:reduntant piping", "url_page": "//en.wikipedia.org/wiki/RAS_syndrome"}
{"title_page": "Marcos Senesi", "text_new": "{{expand language|topic=|langcode=ES|otherarticle=Marcos Senesi|date=October 2016}}\n{{Use dmy dates|date=April 2020}}\n{{Infobox football biography\n| name           = Marcos Senesi\n| image          = \n| caption        =\n| fullname       = Marcos Nicol\u00e1s Senesi Bar\u00f3n\n| birth_date     = {{birth date and age|1997|5|10|df=y}}\n| birth_place    = [[Concordia, Entre R\u00edos]], Argentina\n| height         = 1.84 m\n| currentclub    = [[Feyenoord]]\n| clubnumber     = 4\n| position       = [[Defender (association football)|Defender]]\n| youthyears1    = 2009\u20132016\n| youthclubs1    = [[San Lorenzo de Almagro|San Lorenzo]]\n| years1         = 2016\u20132019 | clubs1 = [[San Lorenzo de Almagro|San Lorenzo]] | caps1 = 51       | goals1 = 1\n| years2         = 2019\u2013     | clubs2 = [[Feyenoord]]                          | caps2 = 16        | goals2 = 1\n| nationalyears1 =           | nationalteam1 =                                 | nationalcaps1 =  | nationalgoals1 = \n| pcupdate       = 16 March 2020\n| ntupdate       =\n}}\n\n'''Marcos Nicol\u00e1s Senesi Bar\u00f3n''' (born 10 May 1997) is an Argentine professional [[Association football|footballer]] who plays as a [[Defender (association football)|defender]] for [[Feyenoord]] of the Dutch [[Eredivisie]].<ref>{{cite web|title=M. SENESI |url=https://int.soccerway.com/players/marcos-senesi/432117/|website=Soccerway|accessdate=13 October 2016}}</ref>\n\n==Club Career==\n\n===San Lorenzo===\nSenesi made his debut for [[San Lorenzo de Almagro|San Lorenzo]] on 25 September 2016 as a starter in the [[Argentine Primera Divisi\u00f3n]] match against [[Club Atl\u00e9tico Patronato]]. On 16 September 2017, he scored his first and only goal for the club in a 1\u20130 win against [[Arsenal de Sarand\u00ed]].\n\n===Feyenoord===\nOn 2 September 2019, Senesi signed a four-year contract with Dutch football club Feyenoord.<ref>{{cite web|title=#SeneSi - Marcos Senesi voor vier seizoenen naar Feyenoord|url=https://www.feyenoord.nl/nieuws/nieuwsoverzicht/marcos-senesi-voor-vier-seizoenen-naar-feyenoord---020919|website=Feyenoord|language=Dutch|date=2 September 2019|accessdate=2 September 2019}}</ref> On 10 November 2019, he scored his first goal for the club, the winning goal in a 3\u20132 win against [[RKC Waalwijk]], heading in a free kick in the 85th minute.<ref>{{cite web|title=Senesi voltooit comeback Feyenoord tegen RKC|url=https://www.feyenoord.nl/nieuws/nieuwsoverzicht/senesi-voltooit-comeback-feyenoord-tegen-rkc---2---101119|website=Feyenoord|language=Dutch|date=10 November 2019|accessdate=10 November 2019}}</ref>\n\n==Career statistics==\n{{updated|match played 10 November 2019}}<ref>{{cite web|title=M. Senesi|url=https://int.soccerway.com/players/marcos-senesi/432117/|website=Soccerway|accessdate=2 September 2019}}</ref>\n\n{| class=\"wikitable\" style=\"text-align: center\"\n|-\n!rowspan=\"2\"|Club\n!rowspan=\"2\"|Season\n!colspan=\"3\"|League\n!colspan=\"2\"|National Cup\n!colspan=\"2\"|International Cup\n!colspan=\"2\"|Total\n|-\n!Division!!Apps!!Goals!!Apps!!Goals!!Apps!!Goals!!Apps!!Goals\n|-\n|rowspan=\"5\"|[[San Lorenzo de Almagro|San Lorenzo]]\n|[[2016\u201317 Argentine Primera Divisi\u00f3n|2016\u201317]]\n|rowspan=\"4\"|[[Argentine Primera Divisi\u00f3n]]\n|11||0||0||0||2||0||13||0\n|-\n|[[2017\u201318 Argentine Primera Divisi\u00f3n|2017\u201318]]\n|13||1||0||0||2||0||15||1\n|-\n|[[2018\u201319 Argentine Primera Divisi\u00f3n|2018\u201319]]\n|23||0||6||0||9||0||38||0\n|-\n|[[2019\u201320 Argentine Primera Divisi\u00f3n|2019\u201320]]\n|4||0||0||0||2||0||6||0\n|-\n!colspan=\"2\"|Total\n!51!!1!!6!!0!!15!!0!!72!!1\n|-\n|[[Feyenoord]]\n|[[2019\u201320 Eredivisie|2019\u201320]]\n|[[Eredivisie]]\n|16||1||4||1||5||0||25||2\n|-\n!colspan=\"3\"|Career total\n!67!!3!!10!!1!!20!!0!!107!!4\n|}\n\n==References==\n{{Reflist}}\n\n{{Feyenoord squad}}\n\n{{DEFAULTSORT:Senesi, Marcos}}\n[[Category:1997 births]]\n[[Category:Living people]]\n[[Category:Association football defenders]]\n[[Category:Argentine footballers]]\n[[Category:Argentine Primera Divisi\u00f3n players]]\n[[Category:Eredivisie players]]\n[[Category:San Lorenzo footballers]]\n[[Category:Feyenoord players]]\n", "text_old": "{{expand language|topic=|langcode=ES|otherarticle=Marcos Senesi|date=October 2016}}\n{{Infobox football biography\n| name           = Marcos Senesi\n| image          = \n| caption        =\n| fullname       = Marcos Nicol\u00e1s Senesi Bar\u00f3n\n| birth_date     = {{birth date and age|1997|5|10|df=y}}\n| birth_place    = [[Concordia, Entre R\u00edos]], Argentina\n| height         = 1.84 m\n| currentclub    = [[Feyenoord]]\n| clubnumber     = 4\n| position       = [[Defender (association football)|Defender]]\n| youthyears1    = 2009\u20132016\n| youthclubs1    = [[San Lorenzo de Almagro|San Lorenzo]]\n| years1         = 2016\u20132019 | clubs1 = [[San Lorenzo de Almagro|San Lorenzo]] | caps1 = 51       | goals1 = 1\n| years2         = 2019\u2013     | clubs2 = [[Feyenoord]]                          | caps2 = 16        | goals2 = 1\n| nationalyears1 =           | nationalteam1 =                                 | nationalcaps1 =  | nationalgoals1 = \n| pcupdate       = 16 March 2020\n| ntupdate       =\n}}\n\n'''Marcos Nicol\u00e1s Senesi Bar\u00f3n''' (born 10 May 1997) is an [[Argentina|Argentine]] [[Association football|football]] [[Defender (association football)|defender]] currently playing for [[Feyenoord]] of the Dutch [[Eredivisie]].<ref>{{cite web|title=M. SENESI |url=https://int.soccerway.com/players/marcos-senesi/432117/|website=Soccerway|accessdate=13 October 2016}}</ref>\n\n==Club Career==\n\n===San Lorenzo===\nSenesi made his debut for [[San Lorenzo de Almagro|San Lorenzo]] on 25 September 2016 as a starter in the [[Argentine Primera Divisi\u00f3n]] match against [[Club Atl\u00e9tico Patronato]]. On 16 September 2017 he scored his first and only goal for the club in a 1\u20130 win against [[Arsenal de Sarand\u00ed]].\n\n===Feyenoord===\nOn 2 September 2019, Senesi signed a 4-year contract with Dutch football club Feyenoord.<ref>{{cite web|title=#SeneSi - Marcos Senesi voor vier seizoenen naar Feyenoord|url=https://www.feyenoord.nl/nieuws/nieuwsoverzicht/marcos-senesi-voor-vier-seizoenen-naar-feyenoord---020919|website=Feyenoord|language=Dutch|date=2 September 2019|accessdate=2 September 2019}}</ref> On 10 November 2019, Senesi scored his first goal for the club, scoring the winning goal in a 3\u20132 win against [[RKC Waalwijk]] after heading in a free kick in the 85th minute.<ref>{{cite web|title=Senesi voltooit comeback Feyenoord tegen RKC|url=https://www.feyenoord.nl/nieuws/nieuwsoverzicht/senesi-voltooit-comeback-feyenoord-tegen-rkc---2---101119|website=Feyenoord|language=Dutch|date=10 November 2019|accessdate=10 November 2019}}</ref>\n\n==Career statistics==\n{{updated|match played 10 November 2019}}<ref>{{cite web|title=M. Senesi|url=https://int.soccerway.com/players/marcos-senesi/432117/|website=Soccerway|accessdate=2 September 2019}}</ref>\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n!rowspan=2|Club\n!rowspan=2|Season\n!colspan=3|League\n!colspan=2|National Cup\n!colspan=2|International Cup\n!colspan=2|Total\n|-\n!Division!!Apps!!Goals!!Apps!!Goals!!Apps!!Goals!!Apps!!Goals\n|-\n|rowspan=5|[[San Lorenzo de Almagro|San Lorenzo]]\n|[[2016\u201317 Argentine Primera Divisi\u00f3n|2016\u201317]]\n|rowspan=4|[[Argentine Primera Divisi\u00f3n]]\n|11||0||0||0||2||0||13||0\n|-\n|[[2017\u201318 Argentine Primera Divisi\u00f3n|2017\u201318]]\n|13||1||0||0||2||0||15||1\n|-\n|[[2018\u201319 Argentine Primera Divisi\u00f3n|2018\u201319]]\n|23||0||6||0||9||0||38||0\n|-\n|[[2019\u201320 Argentine Primera Divisi\u00f3n|2019\u201320]]\n|4||0||0||0||2||0||6||0\n|-\n!colspan=2|Total\n!51!!1!!6!!0!!15!!0!!72!!1\n|-\n|[[Feyenoord]]\n|[[2019\u201320 Eredivisie|2019\u201320]]\n|[[Eredivisie]]\n|16||1||4||1||5||0||25||2\n|-\n!colspan=3|Career total\n!67!!3!!10!!1!!20!!0!!107!!4\n|}\n\n==References==\n{{Reflist}}\n\n{{Feyenoord squad}}\n\n{{DEFAULTSORT:Senesi, Marcos}}\n[[Category:1997 births]]\n[[Category:Living people]]\n[[Category:Argentine footballers]]\n[[Category:San Lorenzo footballers]]\n[[Category:Argentine Primera Divisi\u00f3n players]]\n[[Category:Association football defenders]]\n", "name_user": "Robby.is.on", "label": "safe", "comment": "Minor corrections.", "url_page": "//en.wikipedia.org/wiki/Marcos_Senesi"}
{"title_page": "Extreme learning machine", "text_new": "{{machine learning bar}}\n'''Extreme learning machines''' are [[feedforward neural network]]s for [[statistical classification|classification]], [[regression analysis|regression]], [[Cluster analysis|clustering]], [[sparse approximation]], compression and [[feature learning]] with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are [[random projection]] but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. The name \"extreme learning machine\" (ELM) was given to such models by its main inventor Guang-Bin Huang.\n\nAccording to their creators, these models are able to produce good generalization performance and learn thousands of times faster than networks trained using [[backpropagation]].<ref>{{cite journal |last1=Huang |first1=Guang-Bin |first2=Qin-Yu |last2=Zhu |first3=Chee-Kheong |last3=Siew |title=Extreme learning machine: theory and applications |journal=Neurocomputing |volume=70 |issue=1 |year=2006 |pages=489\u2013501 |doi=10.1016/j.neucom.2005.12.126 |citeseerx=10.1.1.217.3692}}</ref>  In literature, it also shows that  these models can outperform [[support vector machine]]s (SVM) and SVM provides suboptimal solutions in both classification and regression applications.<ref name=\":4\">{{Cite journal|last=Huang|first=Guang-Bin; Hongming Zhou; Xiaojian Ding; and Rui Zhang|date=2012|title=Extreme Learning Machine for Regression and Multiclass Classification|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Unified-Learning.pdf|journal=IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics|volume=42|issue=2|pages=513\u2013529|via=|doi=10.1109/tsmcb.2011.2168604|pmid=21984515|citeseerx=10.1.1.298.1213}}</ref><ref name=\":0\" /><ref>{{Cite journal|last=Huang|first=Guang-Bin|date=2014|title=An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Randomness-Kernel.pdf|journal=Cognitive Computation|volume=6|issue=3|pages=376\u2013390|via=|doi=10.1007/s12559-014-9255-2}}</ref>\n\n== History ==\nFrom 2001-2010, ELM research mainly focused on the unified learning framework for \"generalized\" single-hidden layer feedforward neural networks (SLFNs), including but not limited to sigmoid networks, RBF networks, threshold networks,<ref>{{Cite journal|last=Huang|first=Guang-Bin, Qin-Yu Zhu, K. Z. Mao, Chee-Kheong Siew, P. Saratchandran, and N. Sundararajan|date=2006|title=Can Threshold Networks Be Trained Directly?|url=http://www.ntu.edu.sg/home/egbhuang/pdf/TCASII-ELM-Threshold-Network.pdf|journal=IEEE Transactions on Circuits and Systems-II: Express Briefs|volume=53|issue=3|pages=187\u2013191|via=|doi=10.1109/tcsii.2005.857540}}</ref> trigonometric networks, fuzzy inference systems, Fourier series,<ref name=\":1\">{{Cite journal|last=Huang|first=Guang-Bin, Lei Chen, and Chee-Kheong Siew|date=2006|title=Universal Approximation Using Incremental Constructive Feedforward Networks with Random Hidden Nodes|url=http://www.ntu.edu.sg/home/egbhuang/pdf/I-ELM.pdf|journal=IEEE Transactions on Neural Networks|volume=17|issue=4|pages=879\u2013892|via=|doi=10.1109/tnn.2006.875977|pmid=16856652}}</ref><ref>{{Cite journal|last=Rahimi|first=Ali, and Benjamin Recht|date=2008|title=Weighted Sums of Random Kitchen Sinks: Replacing Minimization with Randomization in Learning|url=https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf|journal=Advances in Neural Information Processing Systems 21|volume=|pages=|via=}}</ref> Laplacian transform, wavelet networks,<ref>{{Cite journal|last=Cao|first=Jiuwen, Zhiping Lin, Guang-Bin Huang|title=Composite Function Wavelet Neural Networks with Extreme Learning Machine|journal=Neurocomputing|volume=73|issue=7\u20139|pages=1405\u20131416|doi=10.1016/j.neucom.2009.12.007|year=2010}}</ref> etc. One significant achievement made in those years is to successfully prove the universal approximation and classification capabilities of ELM in theory.<ref name=\":1\" /><ref name=\":2\">{{Cite journal|last=Huang|first=Guang-Bin, Lei Chen|date=2007|title=Convex Incremental Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/CI-ELM.pdf|journal=Neurocomputing|volume=70|issue=16\u201318|pages=3056\u20133062|via=|doi=10.1016/j.neucom.2007.02.009}}</ref><ref name=\":3\">{{Cite journal|last=Huang|first=Guang-Bin, and Lei Chen|date=2008|title=Enhanced Random Search Based Incremental Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/EI-ELM.pdf|journal=Neurocomputing|volume=71|issue=16\u201318|pages=3460\u20133468|via=|doi=10.1016/j.neucom.2007.10.008|citeseerx=10.1.1.217.3009}}</ref>\n\nFrom 2010 to 2015, ELM research extended to the unified learning framework for kernel learning, SVM and a few typical feature learning methods such as [[Principal component analysis|Principal Component Analysis]] (PCA) and [[Non-negative matrix factorization|Non-negative Matrix Factorization]] (NMF). It is shown that SVM actually provides suboptimal solutions compared to ELM, and ELM can provide the whitebox kernel mapping, which is implemented by ELM random feature mapping, instead of the blackbox kernel used in SVM. PCA and NMF can be considered as special cases where linear hidden nodes are used in ELM.<ref>{{Cite journal|last=He|first=Qing, Xin Jin, Changying Du, Fuzhen Zhuang, Zhongzhi Shi|date=2014|title=Clustering in Extreme Learning Machine Feature Space|url=http://www.intsci.ac.cn/users/jinxin/Mypapers/ELM-Neurocomputing-2013.pdf|journal=Neurocomputing|volume=128|pages=88\u201395|via=|doi=10.1016/j.neucom.2012.12.063}}</ref><ref>{{Cite journal|last=Kasun|first=Liyanaarachchi Lekamalage Chamara, Yan Yang, Guang-Bin Huang, and Zhengyou Zhang|date=2016|title=Dimension Reduction With Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Dimensionality-Reduction.pdf|journal=IEEE Transactions on Image Processing|volume=25|issue=8|pages=3906\u20133918|via=|doi=10.1109/tip.2016.2570569|pmid=27214902|bibcode=2016ITIP...25.3906K}}</ref>\n\nFrom 2015 to 2017, an increased focus has been placed on hierarchical implementations<ref name=\":5\">{{Cite journal|last=Huang|first=Guang-Bin, Zuo Bai, and Liyanaarachchi Lekamalage Chamara Kasun, and Chi Man Vong|date=2015|title=Local Receptive Fields Based Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-LRF.pdf|journal=IEEE Computational Intelligence Magazine|volume=10|issue=2|pages=18\u201329|via=|doi=10.1109/mci.2015.2405316}}</ref><ref name=\":6\">{{Cite journal|last=Tang|first=Jiexiong, Chenwei Deng, and Guang-Bin Huang|date=2016|title=Extreme Learning Machine for Multilayer Perceptron|url=http://www.ntu.edu.sg/home/egbhuang/pdf/Multiple-ELM.pdf|journal=IEEE Transactions on Neural Networks and Learning Systems|volume=27|issue=4|pages=809\u2013821|via=|doi=10.1109/tnnls.2015.2424995|pmid=25966483}}</ref> of ELM. Additionally since 2011, significant biological studies have been made that support certain ELM theories.<ref>{{Cite journal|last=Barak|first=Omri; Rigotti, Mattia; and Fusi, Stefano|date=2013|title=The Sparseness of Mixed Selectivity Neurons Controls the Generalization-Discrimination Trade-off|journal=Journal of Neuroscience|volume=33|issue=9|pages=3844\u20133856|doi=10.1523/jneurosci.2753-12.2013|pmid=23447596|pmc=6119179}}</ref><ref>{{Cite journal|last=Rigotti|first=Mattia; Barak, Omri; Warden, Melissa R.; Wang, Xiao-Jing; Daw, Nathaniel D.; Miller, Earl K.; and Fusi, Stefano|date=2013|title=The Importance of Mixed Selectivity in Complex Cognitive Tasks|url=|journal=Nature|volume=497|issue=7451|pages=585\u2013590|doi=10.1038/nature12160|pmid=23685452|pmc=4412347|bibcode=2013Natur.497..585R}}</ref><ref>{{Cite journal|last=Fusi|first=Stefano, Earl K Miller and Mattia Rigotti|date=2015|title=Why Neurons Mix: High Dimensionality for Higher Cognition|url=http://www.ntu.edu.sg/home/egbhuang/pdf/Why-Neurons-Mix-ELM.pdf|journal=Current Opinion in Neurobiology|volume=37|pages=66\u201374|via=|doi=10.1016/j.conb.2016.01.010|pmid=26851755}}</ref>\n\nFrom 2017 onwards, to overcome low-convergence problem during training [[LU Decomposition]], [[Bartels\u2013Stewart_algorithm#The_Hessenberg\u2013Schur_algorithm|Hessenberg Decomposition]] and [[QR Decomposition]] based approaches with [[Regularization_(mathematics)|regularization]] have begun to attract attention<ref name=\":29\">{{Cite journal|last=Kutlu|first=Yakup Kutlu, Apdullah Yay\u0131k, and Esen Y\u0131ld\u0131r\u0131m, and Serdar Y\u0131ld\u0131r\u0131m|date=2017|title=LU triangularization extreme learning machine in EEG cognitive task classification|url=https://link.springer.com/article/10.1007/s00521-017-3142-1|journal=Neural Computation and Applications|volume=31|issue=4|pages=1117\u20131126|via=|doi=10.1007/s00521-017-3142-1}}</ref><ref name=\":30\">{{Cite journal|last=Yay\u0131k|first=Apdullah Yay\u0131k, Yakup Kutlu, and G\u00f6khan Altan|date=2019|title=Regularized HessELM and Inclined Entropy Measurement forCongestive Heart Failure Prediction|url=https://arxiv.org/pdf/1907.05888.pdf}}</ref><ref name=\":31\">{{Cite journal|last=Altan|first=G\u00f6khan Altan, Yakup Kutlu, Adnan \u00d6zhan Pekmezci and Apdullah Yay\u0131k |date=2018|title=Diagnosis of Chronic Obstructive Pulmonary Disease using Deep Extreme Learning Machines with LU Autoencoder Kernel|url=https://www.researchgate.net/publication/325617941_Diagnosis_of_Chronic_Obstructive_Pulmonary_Disease_using_Deep_Extreme_Learning_Machines_with_LU_Autoencoder_Kernel|journal= International Conference on Advanced Technologies}}</ref>\n\nIn a recent announcement from [[Google Scholar]]: \"[https://scholar.googleblog.com/2017/06/classic-papers-articles-that-have-stood.html Classic Papers: Articles That Have Stood The Test of Time]\", two ELM papers have been listed in the \"[https://scholar.google.com/citations?view_op=list_classic_articles&hl=en&by=2006&vq=eng_artificialintelligence Top 10 in Artificial Intelligence for 2006],\" taking positions 2 and 7.\n\n==Algorithms==\nGiven a single hidden layer of ELM, suppose that the output function of the <math>i</math>-th hidden node is <math>h_i(\\mathbf{x})=G(\\mathbf{a}_i,b_i,\\mathbf{x})</math>, where <math>\\mathbf{a}_i</math> and <math>b_i</math> are the parameters of the <math>i</math>-th hidden node. The output function of the ELM for SLFNs with <math>L</math> hidden nodes is:\n\n<math>f_L({\\bf x})=\\sum_{i=1}^L{\\boldsymbol \\beta}_ih_i({\\bf x})</math>, where <math>{\\boldsymbol \\beta}_i</math> is the output weight of the <math>i</math>-th hidden node.\n\n<math>\\mathbf{h}(\\mathbf{x})=[G(h_i(\\mathbf{x}),...,h_L(\\mathbf{x}))]</math> is the hidden layer output mapping of ELM. Given <math>N</math> training samples, the hidden layer output matrix <math>\\mathbf{H}</math> of ELM is given as: <math>{\\bf H}=\\left[\\begin{matrix}\n{\\bf h}({\\bf x}_1)\\\\\n\\vdots\\\\\n{\\bf h}({\\bf x}_N)\n\\end{matrix}\\right]=\\left[\\begin{matrix}\nG({\\bf a}_1, b_1, {\\bf x}_1) &\\cdots & G({\\bf a}_L, b_L, {\\bf x}_1)\\\\\n\\vdots &\\vdots&\\vdots\\\\\nG({\\bf a}_1, b_1, {\\bf x}_N) &\\cdots & G({\\bf a}_L, b_L, {\\bf x}_N)\n\\end{matrix}\\right]\n</math>\n\nand <math>\\mathbf{T}</math> is the training data target matrix: <math>{\\bf T}=\\left[\\begin{matrix}\n{\\bf t}_1\\\\\n\\vdots\\\\\n{\\bf t}_N\n\\end{matrix}\\right]\n</math>\n\nGenerally speaking, ELM is a kind of regularization neural networks but with non-tuned hidden layer mappings (formed by either random hidden nodes, kernels or other implementations), its objective function is:\n\n<math>\n\\text{Minimize: } \\|{\\boldsymbol \\beta}\\|_p^{\\sigma_1}+C\\|{\\bf H}{\\boldsymbol \\beta}-{\\bf T}\\|_q^{\\sigma_2}\n</math>\n\nwhere <math>\\sigma_1>0, \\sigma_2>0, p,q=0, \\frac{1}{2}, 1, 2, \\cdots, +\\infty</math>.\n\nDifferent combinations of <math>\\sigma_1</math>, <math>\\sigma_2</math>, <math>p</math> and <math>q</math> can be used and result in different learning algorithms for regression, classification, sparse coding, compression, feature learning and clustering.\n\nAs a special case, a simplest ELM training algorithm learns a model of the form (for single hidden layer sigmoid neural networks):\n\n:<math>\\mathbf{\\hat{Y}} = \\mathbf{W}_2 \\sigma(\\mathbf{W}_1 x)</math>\n\nwhere {{math|'''W'''<sub>1</sub>}} is the matrix of input-to-hidden-layer weights, <math>\\sigma</math> is an activation function, and {{math|'''W'''<sub>2</sub>}} is the matrix of hidden-to-output-layer weights. The algorithm proceeds as follows:\n\n# Fill {{math|'''W'''<sub>1</sub>}} with random values (e.g., [[Gaussian noise|Gaussian random noise]]);\n# estimate {{math|'''W'''<sub>2</sub>}} by [[least-squares fit]] to a matrix of response variables {{math|'''Y'''}}, computed using the [[Moore\u2013Penrose pseudoinverse|pseudoinverse]] {{math|\u22c5<sup>+</sup>}}, given a [[design matrix]] {{math|'''X'''}}:\n#:<math>\\mathbf{W}_2 = \\sigma(\\mathbf{W}_1 \\mathbf{X})^+ \\mathbf{Y}</math>\n\n== Architectures ==\nIn most cases, ELM is used as a single hidden layer feedforward network (SLFN) including but not limited to sigmoid networks, RBF networks, threshold networks, fuzzy inference networks, complex neural networks, wavelet networks, Fourier transform, Laplacian transform, etc. Due to its different learning algorithm implementations for regression, classification, sparse coding, compression, feature learning and clustering, multi ELMs have been used to form multi hidden layer networks, [[deep learning]] or hierarchical networks.<ref name=\":5\" /><ref name=\":6\" /><ref name=\":7\" />\n\nA hidden node in ELM is a computational element, which need not be considered as classical neuron. A hidden node in ELM can be classical artificial neurons, basis functions, or a subnetwork formed by some hidden nodes.<ref name=\":2\" />\n\n== Theories ==\nBoth universal approximation and classification capabilities<ref name=\":4\" /><ref name=\":0\" /> have been proved for ELM in literature. Especially, [http://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=egbhuang Guang-Bin Huang] and his team spent almost seven years (2001-2008) on the rigorous proofs of ELM's  universal approximation capability.<ref name=\":1\" /><ref name=\":2\" /><ref name=\":3\" />\n\n=== Universal approximation capability ===\nIn theory, any nonconstant piecewise continuous function can be used as activation function in ELM hidden nodes, such an activation function need not be differential. If tuning the parameters of hidden nodes could make SLFNs approximate any target function <math>f(\\mathbf{x})</math>, then hidden node parameters can be randomly generated according to any continuous distribution probability, and <math>\\lim_{L\\rightarrow \\infty}\\left\\|\\sum_{i=1}^L{\\boldsymbol \\beta}_ih_i({\\bf x})-f({\\bf x})\\right\\|=0</math> holds with probability one with appropriate output weights <math>\\boldsymbol\\beta</math>.\n\n=== Classification capability ===\nGiven any nonconstant piecewise continuous function as the activation function in SLFNs, if tuning the parameters of hidden nodes can make SLFNs approximate any target function <math>f(\\mathbf{x})</math>, then SLFNs with random hidden layer mapping <math>\\mathbf{h}(\\mathbf{x})</math> can separate arbitrary disjoint regions of any shapes.\n\n== Neurons ==\nWide type of nonlinear piecewise continuous functions <math>G(\\mathbf{a}, b, \\mathbf{x})</math> can be used in hidden neurons of ELM, for example:\n\n===Real domain===\nSigmoid function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\frac{1}{1+\\exp(-(\\mathbf{a}\\cdot\\mathbf{x}+b))}</math>\n\nFourier function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\sin(\\mathbf{a}\\cdot\\mathbf{x}+b)</math>\n\nHardlimit function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\n\\begin{cases}\n1, &\\text{if }{\\bf a}\\cdot{\\bf x}-b\\geq 0\\\\\n0, &\\text{otherwise}\n\\end{cases}\n</math>\n\nGaussian function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\exp(-b\\|\\mathbf{x}-\\mathbf{a}\\|^2)</math>\n\nMultiquadrics function: <math>G(\\mathbf{a}, b, \\mathbf{x})=(\\|\\mathbf{x}-\\mathbf{a}\\|^2+b^2)^{1/2}</math>\n\nWavelet: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\|a\\|^{-1/2}\\Psi\\left(\\frac{\\mathbf{x}-\\mathbf{a}}{b}\\right)</math> where <math>\\Psi</math> is a single mother wavelet function.\n\n===Complex domain===\n\nCircular functions:\n\n<math>\\tan(z)= \\frac{e^{iz}-e^{-iz}}{i(e^{iz}+e^{-iz})}</math>\n\n<math>\\sin(z)= \\frac{e^{iz}-e^{-iz}}{2i}</math>\n\nInverse circular functions:\n\n<math>\\arctan(z)= \\int_0^z\\frac{dt}{1+t^2}</math>\n\n<math>\\arccos(z)= \\int_0^z\\frac{dt}{(1-t^2)^{1/2}}</math>\n\nHyperbolic functions:\n\n<math>\\tanh(z)= \\frac{e^z-e^{-z}}{e^z+e^{-z}}</math>\n\n<math>\\sinh(z)= \\frac{e^z-e^{-z}}{2}</math>\n\nInverse hyperbolic functions:\n\n<math>\\text{arctanh}(z)=\\int_0^z\\frac{dt}{1-t^2}</math>\n\n<math>\\text{arcsinh}(z)=\\int_0^z\\frac{dt}{(1+t^2)^{1/2}}</math>\n\n== Reliability ==\n{{see also|Explainable AI}}\nThe [[black-box]] character of neural networks in general and extreme learning machines (ELM) in particular is one of the major concerns that repels engineers from application in unsafe automation tasks. This particular issue was approached by means of several different techniques. One approach is to reduce the dependence on the random input.<ref>{{Cite journal|last=Neumann|first=Klaus; Steil, Jochen J.|date=2011|title=Batch intrinsic plasticity for extreme learning machines|url=https://pub.uni-bielefeld.de/download/2141968/2904481|format=|journal=Proc. Of International Conference on Artificial Neural Networks|volume=|pages=339\u2013346|via=}}</ref><ref>{{Cite journal|last=Neumann|first=Klaus; Steil, Jochen J.|date=2013|title=Optimizing extreme learning machines via ridge regression and batch intrinsic plasticity|url=https://pub.uni-bielefeld.de/download/2465823/2903542|format=|journal=Neurocomputing|volume=102|pages=23\u201330|via=|doi=10.1016/j.neucom.2012.01.041}}</ref> Another approach focuses on the incorporation of continuous constraints into the learning process of ELMs<ref>{{Cite journal|last=Neumann|first=Klaus; Rolf, Matthias; Steil, Jochen J.|date=2013|title=Reliable integration of continuous constraints into extreme learning machines|journal=International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems|volume=21|issue=supp02|pages=35\u201350|doi=10.1142/S021848851340014X|issn=0218-4885|url=https://pub.uni-bielefeld.de/record/2547909}}</ref><ref>{{Cite book|url=https://pub.uni-bielefeld.de/download/2656403/2656405|title=Reliability|last=Neumann|first=Klaus|publisher=University Library Bielefeld|year=2014|isbn=|location=|pages=49\u201374}}</ref> which are derived from prior knowledge about the specific task. This is reasonable, because machine learning solutions have to guarantee a safe operation in many application domains. The mentioned studies revealed that the special form of ELMs, with its functional separation and the linear read-out weights, is particularly well suited for the efficient incorporation of continuous constraints in predefined regions of the input space.\n\n==Controversy==\nThere are two main complaints from academic community concerning this work, the first one is about \"reinventing and ignoring previous ideas\", the second one is about \"improper naming and popularizing\", as shown in some debates in 2008 and 2015.<ref>{{cite web |title=The Official Homepage on Origins of Extreme Learning Machines (ELM) |url=http://elmorigin.wixsite.com/originofelm |accessdate=15 December 2018}}</ref> In particular, it was pointed out in a letter<ref>{{cite journal|last1=Wang|first1=Lipo P.|last2=Wan|first2=Chunru R.|title=Comments on \"The Extreme Learning Machine\"|journal=IEEE Trans. Neural Networks|citeseerx=10.1.1.217.2330}}</ref> to the editor of ''IEEE Transactions on Neural Networks'' that the idea of using a hidden layer connected to the inputs by random untrained weights was already suggested in the original papers on [[RBF network]]s in the late 1980s; Guang-Bin Huang replied by pointing out subtle differences.<ref>{{Cite journal|last=Huang|first=Guang-Bin|date=2008|title=Reply to \"comments on 'the extreme learning machine' \"|url=|journal=IEEE Transactions on Neural Networks|volume=19|issue=8|pages=1495\u20131496|doi=10.1109/tnn.2008.2002275}}</ref> In a 2015 paper,<ref name=\":0\" /> Huang responded to complaints about his invention of the name ELM for already-existing methods, complaining of \"very negative and unhelpful comments on ELM in neither academic nor professional manner due to various reasons and intentions\" and an \"irresponsible anonymous attack which intends to destroy harmony research environment\", arguing that his work \"provides a unifying learning platform\" for various types of neural nets,<ref name=\":0\">{{cite journal |title=What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt's Dream and John von Neumann's Puzzle |journal=Cognitive Computation |volume=7 |issue=3 |pages=263\u2013278 |year=2015 |first=Guang-Bin |last=Huang |url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Rosenblatt-Neumann.pdf |doi=10.1007/s12559-015-9333-0}}</ref> including hierarchical structured ELM.<ref name=\":7\">{{Cite book|last=Zhu|first=W.|last2=Miao|first2=J.|last3=Qing|first3=L.|last4=Huang|first4=G. B.|date=2015-07-01|title=Hierarchical Extreme Learning Machine for unsupervised representation learning|journal=2015 International Joint Conference on Neural Networks (IJCNN)|pages=1\u20138|doi=10.1109/IJCNN.2015.7280669|isbn=978-1-4799-1960-4}}</ref> In 2015, Huang also gave a formal rebuttal to what he considered as \"malign and attack.\"<ref>{{Cite web|url=http://www.ntu.edu.sg/home/egbhuang/pdf/Huang-GB-Statement.pdf|title=WHO behind the malign and attack on ELM, GOAL of the attack and ESSENCE of ELM|last=Guang-Bin|first=Huang|date=2015|website=www.extreme-learning-machines.org}}</ref> Recent research replaces the random weights with constrained random weights.<ref name=\":4\" /><ref>{{Cite book|last=Zhu|first=W.|last2=Miao|first2=J.|last3=Qing|first3=L.|date=2014-07-01|title=Constrained Extreme Learning Machine: A novel highly discriminative random feedforward neural network|journal=2014 International Joint Conference on Neural Networks (IJCNN)|pages=800\u2013807|doi=10.1109/IJCNN.2014.6889761|isbn=978-1-4799-1484-5}}</ref>\n\n== Open sources ==\n* [http://www.ntu.edu.sg/home/egbhuang/reference.html Matlab Library]\n* Python Library<ref>{{Cite journal|last=Akusok|first=Anton; Bjork, Kaj-Mikael; Miche, Yoan; Lendasse, Amaury|date=2015|title=High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications|journal=IEEE Access|volume=3|pages=1011\u20131025|doi=10.1109/access.2015.2450498}}</ref>\n\n==See also==\n* [[Reservoir computing]]\n* [[Random projection]]\n* [[Random matrix]]\n\n==References==\n{{reflist}}\n\n[[Category:Artificial neural networks]]\n", "text_old": "{{machine learning bar}}\n'''Extreme learning machines''' are [[feedforward neural network]]s for [[statistical classification|classification]], [[regression analysis|regression]], [[Cluster analysis|clustering]], [[sparse approximation]], compression and [[feature learning]] with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are [[random projection]] but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. The name \"extreme learning machine\" (ELM) was given to such models by its main inventor Guang-Bin Huang.\n\nAccording to their creators, these models are able to produce good generalization performance and learn thousands of times faster than networks trained using [[backpropagation]].<ref>{{cite journal |last1=Huang |first1=Guang-Bin |first2=Qin-Yu |last2=Zhu |first3=Chee-Kheong |last3=Siew |title=Extreme learning machine: theory and applications |journal=Neurocomputing |volume=70 |issue=1 |year=2006 |pages=489\u2013501 |doi=10.1016/j.neucom.2005.12.126 |citeseerx=10.1.1.217.3692}}</ref>  In literature, it also shows that  these models can outperform [[support vector machine]]s (SVM) and SVM provides suboptimal solutions in both classification and regression applications.<ref name=\":4\">{{Cite journal|last=Huang|first=Guang-Bin; Hongming Zhou; Xiaojian Ding; and Rui Zhang|date=2012|title=Extreme Learning Machine for Regression and Multiclass Classification|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Unified-Learning.pdf|journal=IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics|volume=42|issue=2|pages=513\u2013529|via=|doi=10.1109/tsmcb.2011.2168604|pmid=21984515|citeseerx=10.1.1.298.1213}}</ref><ref name=\":0\" /><ref>{{Cite journal|last=Huang|first=Guang-Bin|date=2014|title=An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Randomness-Kernel.pdf|journal=Cognitive Computation|volume=6|issue=3|pages=376\u2013390|via=|doi=10.1007/s12559-014-9255-2}}</ref>\n\n== History ==\nFrom 2001-2010, ELM research mainly focused on the unified learning framework for \"generalized\" single-hidden layer feedforward neural networks (SLFNs), including but not limited to sigmoid networks, RBF networks, threshold networks,<ref>{{Cite journal|last=Huang|first=Guang-Bin, Qin-Yu Zhu, K. Z. Mao, Chee-Kheong Siew, P. Saratchandran, and N. Sundararajan|date=2006|title=Can Threshold Networks Be Trained Directly?|url=http://www.ntu.edu.sg/home/egbhuang/pdf/TCASII-ELM-Threshold-Network.pdf|journal=IEEE Transactions on Circuits and Systems-II: Express Briefs|volume=53|issue=3|pages=187\u2013191|via=|doi=10.1109/tcsii.2005.857540}}</ref> trigonometric networks, fuzzy inference systems, Fourier series,<ref name=\":1\">{{Cite journal|last=Huang|first=Guang-Bin, Lei Chen, and Chee-Kheong Siew|date=2006|title=Universal Approximation Using Incremental Constructive Feedforward Networks with Random Hidden Nodes|url=http://www.ntu.edu.sg/home/egbhuang/pdf/I-ELM.pdf|journal=IEEE Transactions on Neural Networks|volume=17|issue=4|pages=879\u2013892|via=|doi=10.1109/tnn.2006.875977|pmid=16856652}}</ref><ref>{{Cite journal|last=Rahimi|first=Ali, and Benjamin Recht|date=2008|title=Weighted Sums of Random Kitchen Sinks: Replacing Minimization with Randomization in Learning|url=https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf|journal=Advances in Neural Information Processing Systems 21|volume=|pages=|via=}}</ref> Laplacian transform, wavelet networks,<ref>{{Cite journal|last=Cao|first=Jiuwen, Zhiping Lin, Guang-Bin Huang|title=Composite Function Wavelet Neural Networks with Extreme Learning Machine|journal=Neurocomputing|volume=73|issue=7\u20139|pages=1405\u20131416|doi=10.1016/j.neucom.2009.12.007|year=2010}}</ref> etc. One significant achievement made in those years is to successfully prove the universal approximation and classification capabilities of ELM in theory.<ref name=\":1\" /><ref name=\":2\">{{Cite journal|last=Huang|first=Guang-Bin, Lei Chen|date=2007|title=Convex Incremental Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/CI-ELM.pdf|journal=Neurocomputing|volume=70|issue=16\u201318|pages=3056\u20133062|via=|doi=10.1016/j.neucom.2007.02.009}}</ref><ref name=\":3\">{{Cite journal|last=Huang|first=Guang-Bin, and Lei Chen|date=2008|title=Enhanced Random Search Based Incremental Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/EI-ELM.pdf|journal=Neurocomputing|volume=71|issue=16\u201318|pages=3460\u20133468|via=|doi=10.1016/j.neucom.2007.10.008|citeseerx=10.1.1.217.3009}}</ref>\n\nFrom 2010 to 2015, ELM research extended to the unified learning framework for kernel learning, SVM and a few typical feature learning methods such as [[Principal component analysis|Principal Component Analysis]] (PCA) and [[Non-negative matrix factorization|Non-negative Matrix Factorization]] (NMF). It is shown that SVM actually provides suboptimal solutions compared to ELM, and ELM can provide the whitebox kernel mapping, which is implemented by ELM random feature mapping, instead of the blackbox kernel used in SVM. PCA and NMF can be considered as special cases where linear hidden nodes are used in ELM.<ref>{{Cite journal|last=He|first=Qing, Xin Jin, Changying Du, Fuzhen Zhuang, Zhongzhi Shi|date=2014|title=Clustering in Extreme Learning Machine Feature Space|url=http://www.intsci.ac.cn/users/jinxin/Mypapers/ELM-Neurocomputing-2013.pdf|journal=Neurocomputing|volume=128|pages=88\u201395|via=|doi=10.1016/j.neucom.2012.12.063}}</ref><ref>{{Cite journal|last=Kasun|first=Liyanaarachchi Lekamalage Chamara, Yan Yang, Guang-Bin Huang, and Zhengyou Zhang|date=2016|title=Dimension Reduction With Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Dimensionality-Reduction.pdf|journal=IEEE Transactions on Image Processing|volume=25|issue=8|pages=3906\u20133918|via=|doi=10.1109/tip.2016.2570569|pmid=27214902|bibcode=2016ITIP...25.3906K}}</ref>\n\nFrom 2015 to 2017, an increased focus has been placed on hierarchical implementations<ref name=\":5\">{{Cite journal|last=Huang|first=Guang-Bin, Zuo Bai, and Liyanaarachchi Lekamalage Chamara Kasun, and Chi Man Vong|date=2015|title=Local Receptive Fields Based Extreme Learning Machine|url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-LRF.pdf|journal=IEEE Computational Intelligence Magazine|volume=10|issue=2|pages=18\u201329|via=|doi=10.1109/mci.2015.2405316}}</ref><ref name=\":6\">{{Cite journal|last=Tang|first=Jiexiong, Chenwei Deng, and Guang-Bin Huang|date=2016|title=Extreme Learning Machine for Multilayer Perceptron|url=http://www.ntu.edu.sg/home/egbhuang/pdf/Multiple-ELM.pdf|journal=IEEE Transactions on Neural Networks and Learning Systems|volume=27|issue=4|pages=809\u2013821|via=|doi=10.1109/tnnls.2015.2424995|pmid=25966483}}</ref> of ELM. Additionally since 2011, significant biological studies have been made that support certain ELM theories.<ref>{{Cite journal|last=Barak|first=Omri; Rigotti, Mattia; and Fusi, Stefano|date=2013|title=The Sparseness of Mixed Selectivity Neurons Controls the Generalization-Discrimination Trade-off|journal=Journal of Neuroscience|volume=33|issue=9|pages=3844\u20133856|doi=10.1523/jneurosci.2753-12.2013|pmid=23447596|pmc=6119179}}</ref><ref>{{Cite journal|last=Rigotti|first=Mattia; Barak, Omri; Warden, Melissa R.; Wang, Xiao-Jing; Daw, Nathaniel D.; Miller, Earl K.; and Fusi, Stefano|date=2013|title=The Importance of Mixed Selectivity in Complex Cognitive Tasks|url=|journal=Nature|volume=497|issue=7451|pages=585\u2013590|doi=10.1038/nature12160|pmid=23685452|pmc=4412347|bibcode=2013Natur.497..585R}}</ref><ref>{{Cite journal|last=Fusi|first=Stefano, Earl K Miller and Mattia Rigotti|date=2015|title=Why Neurons Mix: High Dimensionality for Higher Cognition|url=http://www.ntu.edu.sg/home/egbhuang/pdf/Why-Neurons-Mix-ELM.pdf|journal=Current Opinion in Neurobiology|volume=37|pages=66\u201374|via=|doi=10.1016/j.conb.2016.01.010|pmid=26851755}}</ref>\n\nFrom 2017 onwards, to overcome low-convergence problem during training [[LU Decomposition]], [[Bartels\u2013Stewart_algorithm#The_Hessenberg\u2013Schur_algorithm|Hessenberg Decomposition]] and [[QR Decomposition]] based approaches with [[Regularization_(mathematics)|regularization]] have begun to attract attention<ref name=\":29\">{{Cite journal|last=Kutlu|first=Yakup Kutlu, Apdullah Yay\u0131k, and Esen Y\u0131ld\u0131r\u0131m, and Serdar Y\u0131ld\u0131r\u0131m|date=2017|title=LU triangularization extreme learning machine in EEG cognitive task classification|url=https://link.springer.com/article/10.1007/s00521-017-3142-1|journal=Neural Computation and Applications|volume=31|issue=4|pages=1117\u20131126|via=|doi=10.1007/s00521-017-3142-1}}</ref><ref name=\":30\">{{Cite journal|last=Yay\u0131k|first=Apdullah Yay\u0131k, Yakup Kutlu, and G\u00f6khan Altan|date=2019|title=Regularized HessELM and Inclined Entropy Measurement forCongestive Heart Failure Prediction|url=https://arxiv.org/pdf/1907.05888.pdf}</ref><ref name=\":31\">{{Cite journal|last=Altan|first=G\u00f6khan Altan, Yakup Kutlu, Adnan \u00d6zhan Pekmezci and Apdullah Yay\u0131k |date=2018|title=Diagnosis of Chronic Obstructive Pulmonary Disease using Deep Extreme Learning Machines with LU Autoencoder Kernel|url=https://www.researchgate.net/publication/325617941_Diagnosis_of_Chronic_Obstructive_Pulmonary_Disease_using_Deep_Extreme_Learning_Machines_with_LU_Autoencoder_Kernel|journal= International Conference on Advanced Technologies}}</ref>\n\nIn a recent announcement from [[Google Scholar]]: \"[https://scholar.googleblog.com/2017/06/classic-papers-articles-that-have-stood.html Classic Papers: Articles That Have Stood The Test of Time]\", two ELM papers have been listed in the \"[https://scholar.google.com/citations?view_op=list_classic_articles&hl=en&by=2006&vq=eng_artificialintelligence Top 10 in Artificial Intelligence for 2006],\" taking positions 2 and 7.\n\n==Algorithms==\nGiven a single hidden layer of ELM, suppose that the output function of the <math>i</math>-th hidden node is <math>h_i(\\mathbf{x})=G(\\mathbf{a}_i,b_i,\\mathbf{x})</math>, where <math>\\mathbf{a}_i</math> and <math>b_i</math> are the parameters of the <math>i</math>-th hidden node. The output function of the ELM for SLFNs with <math>L</math> hidden nodes is:\n\n<math>f_L({\\bf x})=\\sum_{i=1}^L{\\boldsymbol \\beta}_ih_i({\\bf x})</math>, where <math>{\\boldsymbol \\beta}_i</math> is the output weight of the <math>i</math>-th hidden node.\n\n<math>\\mathbf{h}(\\mathbf{x})=[G(h_i(\\mathbf{x}),...,h_L(\\mathbf{x}))]</math> is the hidden layer output mapping of ELM. Given <math>N</math> training samples, the hidden layer output matrix <math>\\mathbf{H}</math> of ELM is given as: <math>{\\bf H}=\\left[\\begin{matrix}\n{\\bf h}({\\bf x}_1)\\\\\n\\vdots\\\\\n{\\bf h}({\\bf x}_N)\n\\end{matrix}\\right]=\\left[\\begin{matrix}\nG({\\bf a}_1, b_1, {\\bf x}_1) &\\cdots & G({\\bf a}_L, b_L, {\\bf x}_1)\\\\\n\\vdots &\\vdots&\\vdots\\\\\nG({\\bf a}_1, b_1, {\\bf x}_N) &\\cdots & G({\\bf a}_L, b_L, {\\bf x}_N)\n\\end{matrix}\\right]\n</math>\n\nand <math>\\mathbf{T}</math> is the training data target matrix: <math>{\\bf T}=\\left[\\begin{matrix}\n{\\bf t}_1\\\\\n\\vdots\\\\\n{\\bf t}_N\n\\end{matrix}\\right]\n</math>\n\nGenerally speaking, ELM is a kind of regularization neural networks but with non-tuned hidden layer mappings (formed by either random hidden nodes, kernels or other implementations), its objective function is:\n\n<math>\n\\text{Minimize: } \\|{\\boldsymbol \\beta}\\|_p^{\\sigma_1}+C\\|{\\bf H}{\\boldsymbol \\beta}-{\\bf T}\\|_q^{\\sigma_2}\n</math>\n\nwhere <math>\\sigma_1>0, \\sigma_2>0, p,q=0, \\frac{1}{2}, 1, 2, \\cdots, +\\infty</math>.\n\nDifferent combinations of <math>\\sigma_1</math>, <math>\\sigma_2</math>, <math>p</math> and <math>q</math> can be used and result in different learning algorithms for regression, classification, sparse coding, compression, feature learning and clustering.\n\nAs a special case, a simplest ELM training algorithm learns a model of the form (for single hidden layer sigmoid neural networks):\n\n:<math>\\mathbf{\\hat{Y}} = \\mathbf{W}_2 \\sigma(\\mathbf{W}_1 x)</math>\n\nwhere {{math|'''W'''<sub>1</sub>}} is the matrix of input-to-hidden-layer weights, <math>\\sigma</math> is an activation function, and {{math|'''W'''<sub>2</sub>}} is the matrix of hidden-to-output-layer weights. The algorithm proceeds as follows:\n\n# Fill {{math|'''W'''<sub>1</sub>}} with random values (e.g., [[Gaussian noise|Gaussian random noise]]);\n# estimate {{math|'''W'''<sub>2</sub>}} by [[least-squares fit]] to a matrix of response variables {{math|'''Y'''}}, computed using the [[Moore\u2013Penrose pseudoinverse|pseudoinverse]] {{math|\u22c5<sup>+</sup>}}, given a [[design matrix]] {{math|'''X'''}}:\n#:<math>\\mathbf{W}_2 = \\sigma(\\mathbf{W}_1 \\mathbf{X})^+ \\mathbf{Y}</math>\n\n== Architectures ==\nIn most cases, ELM is used as a single hidden layer feedforward network (SLFN) including but not limited to sigmoid networks, RBF networks, threshold networks, fuzzy inference networks, complex neural networks, wavelet networks, Fourier transform, Laplacian transform, etc. Due to its different learning algorithm implementations for regression, classification, sparse coding, compression, feature learning and clustering, multi ELMs have been used to form multi hidden layer networks, [[deep learning]] or hierarchical networks.<ref name=\":5\" /><ref name=\":6\" /><ref name=\":7\" />\n\nA hidden node in ELM is a computational element, which need not be considered as classical neuron. A hidden node in ELM can be classical artificial neurons, basis functions, or a subnetwork formed by some hidden nodes.<ref name=\":2\" />\n\n== Theories ==\nBoth universal approximation and classification capabilities<ref name=\":4\" /><ref name=\":0\" /> have been proved for ELM in literature. Especially, [http://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=egbhuang Guang-Bin Huang] and his team spent almost seven years (2001-2008) on the rigorous proofs of ELM's  universal approximation capability.<ref name=\":1\" /><ref name=\":2\" /><ref name=\":3\" />\n\n=== Universal approximation capability ===\nIn theory, any nonconstant piecewise continuous function can be used as activation function in ELM hidden nodes, such an activation function need not be differential. If tuning the parameters of hidden nodes could make SLFNs approximate any target function <math>f(\\mathbf{x})</math>, then hidden node parameters can be randomly generated according to any continuous distribution probability, and <math>\\lim_{L\\rightarrow \\infty}\\left\\|\\sum_{i=1}^L{\\boldsymbol \\beta}_ih_i({\\bf x})-f({\\bf x})\\right\\|=0</math> holds with probability one with appropriate output weights <math>\\boldsymbol\\beta</math>.\n\n=== Classification capability ===\nGiven any nonconstant piecewise continuous function as the activation function in SLFNs, if tuning the parameters of hidden nodes can make SLFNs approximate any target function <math>f(\\mathbf{x})</math>, then SLFNs with random hidden layer mapping <math>\\mathbf{h}(\\mathbf{x})</math> can separate arbitrary disjoint regions of any shapes.\n\n== Neurons ==\nWide type of nonlinear piecewise continuous functions <math>G(\\mathbf{a}, b, \\mathbf{x})</math> can be used in hidden neurons of ELM, for example:\n\n===Real domain===\nSigmoid function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\frac{1}{1+\\exp(-(\\mathbf{a}\\cdot\\mathbf{x}+b))}</math>\n\nFourier function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\sin(\\mathbf{a}\\cdot\\mathbf{x}+b)</math>\n\nHardlimit function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\n\\begin{cases}\n1, &\\text{if }{\\bf a}\\cdot{\\bf x}-b\\geq 0\\\\\n0, &\\text{otherwise}\n\\end{cases}\n</math>\n\nGaussian function: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\exp(-b\\|\\mathbf{x}-\\mathbf{a}\\|^2)</math>\n\nMultiquadrics function: <math>G(\\mathbf{a}, b, \\mathbf{x})=(\\|\\mathbf{x}-\\mathbf{a}\\|^2+b^2)^{1/2}</math>\n\nWavelet: <math>G(\\mathbf{a}, b, \\mathbf{x})=\\|a\\|^{-1/2}\\Psi\\left(\\frac{\\mathbf{x}-\\mathbf{a}}{b}\\right)</math> where <math>\\Psi</math> is a single mother wavelet function.\n\n===Complex domain===\n\nCircular functions:\n\n<math>\\tan(z)= \\frac{e^{iz}-e^{-iz}}{i(e^{iz}+e^{-iz})}</math>\n\n<math>\\sin(z)= \\frac{e^{iz}-e^{-iz}}{2i}</math>\n\nInverse circular functions:\n\n<math>\\arctan(z)= \\int_0^z\\frac{dt}{1+t^2}</math>\n\n<math>\\arccos(z)= \\int_0^z\\frac{dt}{(1-t^2)^{1/2}}</math>\n\nHyperbolic functions:\n\n<math>\\tanh(z)= \\frac{e^z-e^{-z}}{e^z+e^{-z}}</math>\n\n<math>\\sinh(z)= \\frac{e^z-e^{-z}}{2}</math>\n\nInverse hyperbolic functions:\n\n<math>\\text{arctanh}(z)=\\int_0^z\\frac{dt}{1-t^2}</math>\n\n<math>\\text{arcsinh}(z)=\\int_0^z\\frac{dt}{(1+t^2)^{1/2}}</math>\n\n== Reliability ==\n{{see also|Explainable AI}}\nThe [[black-box]] character of neural networks in general and extreme learning machines (ELM) in particular is one of the major concerns that repels engineers from application in unsafe automation tasks. This particular issue was approached by means of several different techniques. One approach is to reduce the dependence on the random input.<ref>{{Cite journal|last=Neumann|first=Klaus; Steil, Jochen J.|date=2011|title=Batch intrinsic plasticity for extreme learning machines|url=https://pub.uni-bielefeld.de/download/2141968/2904481|format=|journal=Proc. Of International Conference on Artificial Neural Networks|volume=|pages=339\u2013346|via=}}</ref><ref>{{Cite journal|last=Neumann|first=Klaus; Steil, Jochen J.|date=2013|title=Optimizing extreme learning machines via ridge regression and batch intrinsic plasticity|url=https://pub.uni-bielefeld.de/download/2465823/2903542|format=|journal=Neurocomputing|volume=102|pages=23\u201330|via=|doi=10.1016/j.neucom.2012.01.041}}</ref> Another approach focuses on the incorporation of continuous constraints into the learning process of ELMs<ref>{{Cite journal|last=Neumann|first=Klaus; Rolf, Matthias; Steil, Jochen J.|date=2013|title=Reliable integration of continuous constraints into extreme learning machines|journal=International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems|volume=21|issue=supp02|pages=35\u201350|doi=10.1142/S021848851340014X|issn=0218-4885|url=https://pub.uni-bielefeld.de/record/2547909}}</ref><ref>{{Cite book|url=https://pub.uni-bielefeld.de/download/2656403/2656405|title=Reliability|last=Neumann|first=Klaus|publisher=University Library Bielefeld|year=2014|isbn=|location=|pages=49\u201374}}</ref> which are derived from prior knowledge about the specific task. This is reasonable, because machine learning solutions have to guarantee a safe operation in many application domains. The mentioned studies revealed that the special form of ELMs, with its functional separation and the linear read-out weights, is particularly well suited for the efficient incorporation of continuous constraints in predefined regions of the input space.\n\n==Controversy==\nThere are two main complaints from academic community concerning this work, the first one is about \"reinventing and ignoring previous ideas\", the second one is about \"improper naming and popularizing\", as shown in some debates in 2008 and 2015.<ref>{{cite web |title=The Official Homepage on Origins of Extreme Learning Machines (ELM) |url=http://elmorigin.wixsite.com/originofelm |accessdate=15 December 2018}}</ref> In particular, it was pointed out in a letter<ref>{{cite journal|last1=Wang|first1=Lipo P.|last2=Wan|first2=Chunru R.|title=Comments on \"The Extreme Learning Machine\"|journal=IEEE Trans. Neural Networks|citeseerx=10.1.1.217.2330}}</ref> to the editor of ''IEEE Transactions on Neural Networks'' that the idea of using a hidden layer connected to the inputs by random untrained weights was already suggested in the original papers on [[RBF network]]s in the late 1980s; Guang-Bin Huang replied by pointing out subtle differences.<ref>{{Cite journal|last=Huang|first=Guang-Bin|date=2008|title=Reply to \"comments on 'the extreme learning machine' \"|url=|journal=IEEE Transactions on Neural Networks|volume=19|issue=8|pages=1495\u20131496|doi=10.1109/tnn.2008.2002275}}</ref> In a 2015 paper,<ref name=\":0\" /> Huang responded to complaints about his invention of the name ELM for already-existing methods, complaining of \"very negative and unhelpful comments on ELM in neither academic nor professional manner due to various reasons and intentions\" and an \"irresponsible anonymous attack which intends to destroy harmony research environment\", arguing that his work \"provides a unifying learning platform\" for various types of neural nets,<ref name=\":0\">{{cite journal |title=What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt's Dream and John von Neumann's Puzzle |journal=Cognitive Computation |volume=7 |issue=3 |pages=263\u2013278 |year=2015 |first=Guang-Bin |last=Huang |url=http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Rosenblatt-Neumann.pdf |doi=10.1007/s12559-015-9333-0}}</ref> including hierarchical structured ELM.<ref name=\":7\">{{Cite book|last=Zhu|first=W.|last2=Miao|first2=J.|last3=Qing|first3=L.|last4=Huang|first4=G. B.|date=2015-07-01|title=Hierarchical Extreme Learning Machine for unsupervised representation learning|journal=2015 International Joint Conference on Neural Networks (IJCNN)|pages=1\u20138|doi=10.1109/IJCNN.2015.7280669|isbn=978-1-4799-1960-4}}</ref> In 2015, Huang also gave a formal rebuttal to what he considered as \"malign and attack.\"<ref>{{Cite web|url=http://www.ntu.edu.sg/home/egbhuang/pdf/Huang-GB-Statement.pdf|title=WHO behind the malign and attack on ELM, GOAL of the attack and ESSENCE of ELM|last=Guang-Bin|first=Huang|date=2015|website=www.extreme-learning-machines.org}}</ref> Recent research replaces the random weights with constrained random weights.<ref name=\":4\" /><ref>{{Cite book|last=Zhu|first=W.|last2=Miao|first2=J.|last3=Qing|first3=L.|date=2014-07-01|title=Constrained Extreme Learning Machine: A novel highly discriminative random feedforward neural network|journal=2014 International Joint Conference on Neural Networks (IJCNN)|pages=800\u2013807|doi=10.1109/IJCNN.2014.6889761|isbn=978-1-4799-1484-5}}</ref>\n\n== Open sources ==\n* [http://www.ntu.edu.sg/home/egbhuang/reference.html Matlab Library]\n* Python Library<ref>{{Cite journal|last=Akusok|first=Anton; Bjork, Kaj-Mikael; Miche, Yoan; Lendasse, Amaury|date=2015|title=High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications|journal=IEEE Access|volume=3|pages=1011\u20131025|doi=10.1109/access.2015.2450498}}</ref>\n\n==See also==\n* [[Reservoir computing]]\n* [[Random projection]]\n* [[Random matrix]]\n\n==References==\n{{reflist}}\n\n[[Category:Artificial neural networks]]\n", "name_user": "Grimes2", "label": "safe", "comment": "\u2192\u200eHistory:format cite template", "url_page": "//en.wikipedia.org/wiki/Extreme_learning_machine"}
{"title_page": "Uenuku", "text_new": "{{Use dmy dates|date=April 2020}}\n{{Use New Zealand English|date=April 2020}}\n{{about||the M\u0101ori carving|Te Uenuku|the Moeraki wharenui|Uenuku (Moeraki wharenui)}}\n{{Infobox deity\n| type         = Polynesian\n| name         = Uenuku\n| god_of       = [[Atua]] of rainbows\n| other_names  = \n| gender       = Male\n| region       = [[New Zealand]]\n| ethnic_group = [[M\u0101ori people|M\u0101ori]]\n| cult_centre  = \n| symbol       = <!-- or | symbols = -->\n| consort      = Hinep\u016bkohurangi\n| offspring    = Unnamed daughter\n| parents      = \n| siblings     = \n| equivalent1_type = \n| equivalent1 =\n| equivalent2_type = \n| equivalent2 =\n}}\n'''Uenuku''' is the [[atua]] of [[rainbow]]s in [[M\u0101ori mythology]]. He is particularly special to the [[Tainui]] and [[Ng\u0101i T\u016bhoe]] M\u0101ori. Uenuku also simply means 'rainbow'.<ref name=\"Uenuku\">{{cite web| url=https://maoridictionary.co.nz/word/8861| title=Uenuku \u2013 M\u0101ori Dictionary| publisher=[[John Moorfield|John C Moorfield]]| accessdate=13 April 2020}}</ref> In some areas he may be considered a [[war god]], or Uenuku as a war god may be a separate entity all together.<ref name=tearagods>{{cite web|url=https://teara.govt.nz/en/traditional-maori-religion-nga-karakia-a-te-maori/page-1|title=Ng\u0101 atua \u2013 the gods|publisher=Te Ara - the Encyclopedia of New Zealand|accessdate=13 April 2020}}</ref>\n\n'''Haere''' is the personification of the rainbow, supposedly there are three representing brothers: Haere-kohiko, Haere-waewae and Haere-atautu.<ref>{{cite web| url=https://maoridictionary.co.nz/word/40882| title=Haere \u2013 M\u0101ori Dictionary| publisher=John C Moorfield| accessdate=13 April 2020}}</ref>\n\n==Ng\u0101i T\u016bhoe legend==\nThe [[legend]] of Uenuku is similar to many other vanishing [[love]]r tales such as [[Cupid and Psyche]] or [[Beauty and the Beast]].\n\nUenuku, once human, was out hunting very early one morning when, in a clearing, he saw two women. One was named Hinep\u016bkohurangi who seemed to coalesce out of the morning mist, her sister was Hinewai.<ref name=\"NZETC\">{{cite web |title=The Origin of the Rainbow |url=http://nzetc.victoria.ac.nz/tm/scholarly/tei-BesMaor-c3-7.html |website=New Zealand Electronic Text Collection |publisher=Victoria University |accessdate=7 November 2018}}</ref> He persuaded Hinep\u016bkohurangi to stay and talk with him for a moment and to return the next night, and the next, and the next, and before long they fell in love. As a mist maiden her home was in the sky, so she had to leave him at dawn by the calling of Hinewai. At last, she agreed to marry Uenuku on condition that he tell no-one about her.\n[[File:Rainbow in front of the Remarkables with patches of evening sunlight.jpg|left|thumb|250 px|Rainbow in the evening sun at [[The Remarkables]]. Uenuku is the [[atua]] of the rainbow.]]\nThey had a few months of happiness, though she still appeared only at night and left at dawn, and in time Hinep\u016bkohurangi became pregnant, but no one else could see her and therefore he was ridiculed. Uenuku's friends were sceptical of this wife they had never seen<ref name=\"NZETC\"/> - in some tellings, their daughter was already born.<ref name=\"Fables\">{{cite web|url=http://www.fables.org/winter03/mist_maiden.html|title=Uenuku and the Mist Maiden|publisher=Meredith Miller Memorial Internet Project|accessdate=21 September 2013|archiveurl=https://web.archive.org/web/20070403195330/http://www.fables.org/winter03/mist_maiden.html|archivedate=2007-04-03}}</ref> He tried to explain that his wife left him each morning at first light, so his friends suggested that he block up the doors and windows so she could not see the sun. Finally after more torturous ridicule, he was convinced to block the windows and door when she came to him one night so that she couldn\u2019t see the sun in the morning, then he could prove she existed. This he did, but of course, the mist maiden felt tricked when she found he had deceived her, and so she left him.<ref name=\"NZETC\"/>\n \nUenuku wandered the world searching for his beloved wife. At last, nearer to his death, seeing him lonely and bent with age, [[Rangi and Papa|Rangi]] the [[sky father]] took pity on him and changed him into a rainbow so that he could join his family in the sky.<ref name=\"NZETC\"/>\n\n==Artifacts==\n{{Main|Te Uenuku}}\nThe [[Te Awamutu#Facilities and attractions|Te Awamutu Museum]] in [[New Zealand]] has a large [[Rock (geology)|stone]] said to be inhabited by the [[spirit]] of Uenuku.{{citation needed|date=September 2013}} According to local legend, the spirit of Uenuku was brought to New Zealand by the people on the [[Tainui canoe]], in a stone.\n\nWhen they landed, they made the large [[M\u0101ori carving|carving]] known as either ''Uenuku'' or ''[[Te Uenuku]]'' out of [[Podocarpus totara|t\u014dtara]] with a round opening at the top, in which the stone was placed so that the spirit of Uenuku inhabited the carving.<ref name=\"TAMphoto\">{{cite web|url=https://collection.tamuseum.org.nz/objects/3126/uenuku|title=Uenuku|publisher=Te Awamutu Museum|accessdate=13 April 2020}}<!--old url (http://collections.tamuseum.org.nz/search.do?id=671&db=object&page=1&view=detail) is dead--></ref> The carving is unique in form, and bears a noted resemblance to [[Hawaiian carving]] styles.<ref>{{cite web|url=https://teara.govt.nz/en/object/27074/uenuku|title=Uenuku \u2013 Waikato region|publisher=Te Ara - the Encyclopedia of New Zealand|accessdate=13 April 2020}}</ref> Due to his spiritual significance, photographs of the figure of Uenuku are prohibited without the permission of the M\u0101ori sovereign.<ref name=\"TAMexhibition\">{{cite web|url=http://tamuseum.org.nz/exhibition/uenuku-a-tainuawamutu-museum/|title=Uenuku \u2013 A Tainui Taonga|publisher=Te Awamutu Museum|accessdate=13 April 2020}}</ref>\n\n==1974 film==\n[[Geoff Murphy]] (of ''[[Goodbye Pork Pie]]'', ''[[Utu (film)|Utu]]'' fame) directed a freewheeling adaptation for television, his first film. It was the first TV drama to be entirely performed in [[te reo]] (''[[New Zealand Listener|The Listener]]'' magazine softened viewers by providing a translation prior to screening).<ref name=\"Film\">{{cite web|url=http://www.nzonscreen.com/title/uenuku-1974|title=Uenuku - Television|work=[[NZ On Screen]]|accessdate=21 September 2013}}</ref>\n\n==Other characters==\n===Uenuku===\nUenuku shares his name with an important ancestral figure of [[Ng\u0101ti Porou]], [[Ng\u0101i Tahu]], and their associated tribes as [[Paikea]] and [[Ruatapu]]'s father. In these stories, Uenuku is a great chief of [[Hawaiki]].<ref name=\"Uenuku\"/><ref name=\"Tainui\">{{cite web|url=http://jps.auckland.ac.nz/document/Volume_55_1946/Volume_55%2C_No._3/Ngai-Tahu%3A_notes_relating_to%2C_by_Rahera_Tainui%2C_p_221-235/p1|title=Ngai-Tahu, Notes Relating to, By Rahera Tainui, P 221-235| publisher=Journal of the Polynesian Society|accessdate=11 April 2020}}</ref>\n[[File:Rainbows.jpg|thumb|200 px|[[Double rainbow|Two rainbows]]. Kahukura is god of the [[rainbow]], T\u016b\u0101whiorangi is his wife represented by the lower rainbow.]]\n===Kahukura===\nKahukura is the name of another god of rainbows.<ref name=\"Kahukura\">{{cite web| url=https://maoridictionary.co.nz/word/1880| title=Kahukura \u2013 M\u0101ori Dictionary| publisher=John C Moorfield| accessdate=13 April 2020}}</ref> Te Tihi o Kahukura or Castle Rock on the [[Banks Peninsula]] in [[Canterbury, New Zealand|Canterbury]] is named after him. He was the spirit guardian invoked by tribal [[tohunga]] and appealed to for advice and omens in times of war. Each hap\u016b had an image of Kahukura, often a small carved wooden figure, which was kept in a tapu place.<ref>{{cite book |last1=Cowan |first1=James |title=M\u0101ori Folk Tales of the Port Hills, Canterbury, New Zealand |date=1923 |publisher=Cadsonbury Publications |edition= Third}}</ref> This god's wife is T\u016b\u0101whiorangi, who appears as the lower rainbow during a [[double rainbow]].<ref>{{cite web| url=https://maoridictionary.co.nz/word/39795| title=T\u016b\u0101whiorangi \u2013 M\u0101ori Dictionary| publisher=John C Moorfield| accessdate=13 April 2020}}</ref> A literal translation of Kahukura is a red garment and the rainbow is the celestial embodiment of Kahukura in our skies.\n\nKahukura is also the name of another god, a god of war,<ref name=tearagods/> as well as the name of two separate ancestors. One a [[North Island|Northern]] ancestor who learnt the art of making nets from the [[patupaiarehe]], the other returned to [[Hawaiki]] aboard [[Horouta]] to bring the [[Sweet potato#New Zealand|k\u016bmara]] back to New Zealand.<ref name=\"Kahukura\"/>\n\n==References==\n{{Portal|Mythology|New Zealand|Oceania}}\n{{Reflist}}\n\n[[Category:M\u0101ori gods]]\n[[Category:M\u0101ori mythology]]\n[[Category:Sky and weather gods]]\n", "text_old": "{{Use dmy dates|date=April 2020}}\n{{Use New Zealand English|date=April 2020}}\n{{about||the M\u0101ori carving|Te Uenuku|the Moeraki wharenui|Uenuku (Moeraki wharenui)}}\n{{Infobox deity\n| type         = Polynesian\n| name         = Uenuku\n| god_of       = [[Atua]] of rainbows\n| other_names  = \n| gender       = Male\n| region       = [[New Zealand]]\n| ethnic_group = [[M\u0101ori people|M\u0101ori]]\n| cult_centre  = \n| symbol       = <!-- or | symbols = -->\n| consort      = Hinep\u016bkohurangi\n| offspring    = Unnamed daughter\n| parents      = \n| siblings     = \n| equivalent1_type = \n| equivalent1 =\n| equivalent2_type = \n| equivalent2 =\n}}\n'''Uenuku''' is the [[atua]] of [[rainbow]]s in [[M\u0101ori mythology]]. He is particularly special to the [[Tainui]] and [[Ng\u0101i T\u016bhoe]] M\u0101ori. Uenuku also simply means 'rainbow'.<ref name=\"Uenuku\">{{cite web| url=https://maoridictionary.co.nz/word/8861| title=Uenuku \u2013 M\u0101ori Dictionary| publisher=[[John Moorfield|John C Moorfield]]| accessdate=13 April 2020}}</ref> In some areas he may be considered a [[war god]], or Uenuku as a war god may be a separate entity all together.<ref name=tearagods>{{cite web|url=https://teara.govt.nz/en/traditional-maori-religion-nga-karakia-a-te-maori/page-1|title=Ng\u0101 atua \u2013 the gods|publisher=Te Ara - the Encyclopedia of New Zealand|accessdate=13 April 2020}}</ref>\n\n'''Haere''' is the personification of the rainbow, supposedly there are three representing brothers: Haere-kohiko, Haere-waewae and Haere-atautu.<ref>{{cite web| url=https://maoridictionary.co.nz/word/40882| title=Haere \u2013 M\u0101ori Dictionary| publisher=John C Moorfield| accessdate=13 April 2020}}</ref>\n\n==Ng\u0101i T\u016bhoe legend==\nThe [[legend]] of Uenuku is similar to many other vanishing [[love]]r tales such as [[Cupid and Psyche]] or [[Beauty and the Beast]].\n\nUenuku, once human, was out hunting very early one morning when, in a clearing, he saw two women. One was named Hinep\u016bkohurangi who seemed to coalesce out of the morning mist, her sister was Hinewai.<ref name=\"NZETC\">{{cite web |title=The Origin of the Rainbow |url=http://nzetc.victoria.ac.nz/tm/scholarly/tei-BesMaor-c3-7.html |website=New Zealand Electronic Text Collection |publisher=Victoria University |accessdate=7 November 2018}}</ref> He persuaded Hinep\u016bkohurangi to stay and talk with him for a moment and to return the next night, and the next, and the next, and before long they fell in love. As a mist maiden her home was in the sky, so she had to leave him at dawn by the calling of Hinewai. At last, she agreed to marry Uenuku on condition that he tell no-one about her.\n[[File:Rainbow in front of the Remarkables with patches of evening sunlight.jpg|left|thumb|250 px|Rainbow in the evening sun at [[The Remarkables]]. Uenuku is the [[atua]] of the rainbow.]]\nThey had a few months of happiness, though she still appeared only at night and left at dawn, and in time Hinep\u016bkohurangi became pregnant, but no one else could see her and therefore he was ridiculed. Uenuku's friends were sceptical of this wife they had never seen<ref name=\"NZETC\"/> - in some tellings, their daughter was already born.<ref name=\"Fables\">{{cite web|url=http://www.fables.org/winter03/mist_maiden.html|title=Uenuku and the Mist Maiden|publisher=Meredith Miller Memorial Internet Project|accessdate=21 September 2013|archiveurl=https://web.archive.org/web/20070403195330/http://www.fables.org/winter03/mist_maiden.html|archivedate=2007-04-03}}</ref> He tried to explain that his wife left him each morning at first light, so his friends suggested that he block up the doors and windows so she could not see the sun. Finally after more torturous ridicule, he was convinced to block the windows and door when she came to him one night so that she couldn\u2019t see the sun in the morning, then he could prove she existed. This he did, but of course, the mist maiden felt tricked when she found he had deceived her, and so she left him.<ref name=\"NZETC\"/>\n \nUenuku wandered the world searching for his beloved wife. At last, nearer to his death, seeing him lonely and bent with age, [[Rangi and Papa|Rangi]] the [[sky father]] took pity on him and changed him into a rainbow so that he could join his family in the sky.<ref name=\"NZETC\"/>\n\n==Artifacts==\n{{Main|Te Uenuku}}\nThe [[Te Awamutu#Facilities and attractions|Te Awamutu Museum]] in [[New Zealand]] has a large [[Rock (geology)|stone]] said to be inhabited by the [[spirit]] of Uenuku.{{citation needed|date=September 2013}} According to local legend, the spirit of Uenuku was brought to New Zealand by the people on the [[Tainui canoe]], in a stone.\n\nWhen they landed, they made the large [[M\u0101ori carving|carving]] known as either ''Uenuku'' or ''[[Te Uenuku]]'' out of [[Podocarpus totara|t\u014dtara]] with a round opening at the top, in which the stone was placed so that the spirit of Uenuku inhabited the carving.<ref name=\"TAMphoto\">{{cite web|url=https://collection.tamuseum.org.nz/objects/3126/uenuku|title=Uenuku|publisher=Te Awamutu Museum|accessdate=13 April 2020}}<!--old url (http://collections.tamuseum.org.nz/search.do?id=671&db=object&page=1&view=detail) is dead--></ref> The carving is unique in form, and bears a noted resemblance to [[Hawaiian carving]] styles.<ref>{{cite web|url=https://teara.govt.nz/en/object/27074/uenuku|title=Uenuku \u2013 Waikato region|publisher=Te Ara - the Encyclopedia of New Zealand|accessdate=13 April 2020}}</ref> Due to his spiritual significance, photographs of the figure of Uenuku are prohibited without the permission of the M\u0101ori sovereign.<ref name=\"TAMexhibition\">{{cite web|url=http://tamuseum.org.nz/exhibition/uenuku-a-tainuawamutu-museum/|title=Uenuku \u2013 A Tainui Taonga|publisher=Te Awamutu Museum|accessdate=13 April 2020}}</ref>\n\n==1974 film==\n[[Geoff Murphy]] (of ''[[Goodbye Pork Pie]]'', ''[[Utu (film)|Utu]]'' fame) directed a freewheeling adaptation for television, his first film. It was the first TV drama to be entirely performed in [[te reo]] (''[[New Zealand Listener|The Listener]]'' magazine softened viewers by providing a translation prior to screening).<ref name=\"Film\">{{cite web|url=http://www.nzonscreen.com/title/uenuku-1974|title=Uenuku - Television|work=[[NZ On Screen]]|accessdate=21 September 2013}}</ref>\n\n==Other characters==\n===Uenuku===\nUenuku shares his name with an important ancestral figure of [[Ng\u0101ti Porou]], [[Ng\u0101i Tahu]], and their associated tribes as [[Paikea]] and Ruatapu's father. In these stories, Uenuku is a great chief of [[Hawaiki]].<ref name=\"Uenuku\"/><ref name=\"Tainui\">{{cite web|url=http://jps.auckland.ac.nz/document/Volume_55_1946/Volume_55%2C_No._3/Ngai-Tahu%3A_notes_relating_to%2C_by_Rahera_Tainui%2C_p_221-235/p1|title=Ngai-Tahu, Notes Relating to, By Rahera Tainui, P 221-235| publisher=Journal of the Polynesian Society|accessdate=11 April 2020}}</ref>\n[[File:Rainbows.jpg|thumb|200 px|[[Double rainbow|Two rainbows]]. Kahukura is god of the [[rainbow]], T\u016b\u0101whiorangi is his wife represented by the lower rainbow.]]\n===Kahukura===\nKahukura is the name of another god of rainbows.<ref name=\"Kahukura\">{{cite web| url=https://maoridictionary.co.nz/word/1880| title=Kahukura \u2013 M\u0101ori Dictionary| publisher=John C Moorfield| accessdate=13 April 2020}}</ref> Te Tihi o Kahukura or Castle Rock on the [[Banks Peninsula]] in [[Canterbury, New Zealand|Canterbury]] is named after him. He was the spirit guardian invoked by tribal [[tohunga]] and appealed to for advice and omens in times of war. Each hap\u016b had an image of Kahukura, often a small carved wooden figure, which was kept in a tapu place.<ref>{{cite book |last1=Cowan |first1=James |title=M\u0101ori Folk Tales of the Port Hills, Canterbury, New Zealand |date=1923 |publisher=Cadsonbury Publications |edition= Third}}</ref> This god's wife is T\u016b\u0101whiorangi, who appears as the lower rainbow during a [[double rainbow]].<ref>{{cite web| url=https://maoridictionary.co.nz/word/39795| title=T\u016b\u0101whiorangi \u2013 M\u0101ori Dictionary| publisher=John C Moorfield| accessdate=13 April 2020}}</ref> A literal translation of Kahukura is a red garment and the rainbow is the celestial embodiment of Kahukura in our skies.\n\nKahukura is also the name of another god, a god of war,<ref name=tearagods/> as well as the name of two separate ancestors. One a [[North Island|Northern]] ancestor who learnt the art of making nets from the [[patupaiarehe]], the other returned to [[Hawaiki]] aboard [[Horouta]] to bring the [[Sweet potato#New Zealand|k\u016bmara]] back to New Zealand.<ref name=\"Kahukura\"/>\n\n==References==\n{{Portal|Mythology|New Zealand|Oceania}}\n{{Reflist}}\n\n[[Category:M\u0101ori gods]]\n[[Category:M\u0101ori mythology]]\n[[Category:Sky and weather gods]]\n", "name_user": "Lerf Lerfsson", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Uenuku"}
