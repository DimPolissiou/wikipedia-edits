{"title_page": "Balanced audio", "text_new": "'''Balanced audio''' is a method of interconnecting audio equipment using [[balanced line]]s.  This type of connection is very important in [[sound recording]] and production because it allows the use of long cables while reducing susceptibility to external noise caused by electromagnetic interference.\n\nBalanced connections typically use shielded twisted-pair cable and three-conductor connectors. The connectors are usually three-pin [[XLR connector|XLR]] or {{convert|1/4|in|mm|2}} [[Phone connector (audio)|TRS phone]] connectors. When used in this manner, each cable carries one channel, therefore stereo audio (for example) would require two of them.<ref>{{Cite news|url=http://www.aviom.com/blog/balanced-vs-unbalanced/|title=What's the Difference Between Balanced and Unbalanced?|date=2014-03-27|work=Aviom Blog|access-date=2017-09-24|language=en-US}}</ref>\n\n== Applications ==\n\nMany microphones operate at low voltage levels and some with high [[output impedance]] (hi-Z), which makes long microphone cables especially susceptible to [[electromagnetic interference]]. Microphone interconnections are therefore a common application for a balanced interconnection, which cancels out most of this induced noise. If the [[power amplifier]]s of a [[public address system]] are located at any distance from the [[mixing console]], it is also normal to use balanced lines for the signal paths from the mixer to these amplifiers. Many other components, such as graphic equalizers and effects units, have balanced inputs and outputs to allow this. In recording and for short cable runs in general, a compromise is necessary between the noise reduction given by balanced lines and the cost introduced by the extra [[Electronic circuit|circuitry]] they require.\n\n== Interference reduction ==\n\nBalanced audio connections use a number of techniques to reduce noise. \n\nA typical balanced cable contains two identical wires, which are twisted together and then wrapped with a third conductor (foil or braid) that acts as a [[Shielded cable|shield]]. The two wires form a circuit carrying the [[audio signal]]{{snd}}one wire is in phase with respect to the source signal; the other wire is reversed in polarity. The in-phase wire is called ''non-inverting'', ''positive'', or ''hot'', while the out-of-phase wire is called ''inverting'', ''phase-inverted'', ''anti-phase'', ''negative'' or ''cold''. The hot and cold connections are often shown as In+ and In\u2212 (\"in plus\" and \"in minus\") on [[circuit diagram]]s.<ref>{{cite book |url=https://books.google.com/books?id=PvKPEFu2PVkC&pg=PA344 |page=344 |last=Sel |first=Douglas |title=Small Signal Audio Design |publisher=Taylor & Francis US |year=2010 |isbn=978-0240521770}}</ref>\n\nThe term ''balanced'' comes from the method of connecting each wire to identical [[electrical impedance|impedances]] at [[source impedance|source]] and [[load impedance|load]]. This means that much of the electromagnetic interference<!-- predominantly capacitive? --> will induce an equal noise voltage in each wire. Since the amplifier at the receiving end measures the [[subtraction|difference]] in voltage between the two signal lines, noise that is identical on both wires is rejected. The noise received in the second, inverted line is applied against the first, upright signal, and cancels it out when the two signals are subtracted. \n\nThis differential signal recombination can be implemented with a [[differential amplifier]]. A [[balun]] may also be used instead of an active differential amplifier device.\n\nThe wires are also twisted together, to reduce interference from [[electromagnetic induction]]. A [[twisted pair]] makes the loop area between the conductors as small as possible, and ensures that a magnetic field that passes equally through adjacent loops will induce equal levels of noise on both lines, which is canceled out by the differential amplifier. If the noise source is extremely close to the cable, then it is possible it will be induced on one of the lines more than the other, and it will not be canceled as well, but canceling will still occur to the extent of the amount of noise that is equal on both lines.\n\nThe separate shield of a balanced audio connection also yields a [[noise rejection]] advantage over an unbalanced two-conductor arrangement (such as used in typical [[hi-fi|home stereos]]) where the shield must also act as the signal return wire. Therefore, any noise currents induced into a balanced audio shield will not be directly modulated onto the signal, whereas in a two-conductor system they will be. This also prevents [[ground loop (electricity)|ground loop]] problems, by separating the shield/chassis from signal ground.\n\n== Differential signaling ==\n\nSignals are often transmitted over balanced connections using the [[differential signaling|differential mode]], meaning the wires carry signals of opposite [[Electrical polarity|polarity]] to each other (for instance, in an [[XLR connector]], pin 2 carries the signal with normal polarity, and pin 3 carries an inverted version of the same signal). Despite popular belief, this arrangement is not necessary for noise rejection.  As long as the impedances are balanced, noise will couple equally into the two wires (and be rejected by a differential amplifier), regardless of the signal that is present on them.<ref name=\"Blyth\">{{cite web\n|url=http://www.soundcraft.com/support/white_papers.aspx#\n|title=Audio Balancing Issues\n|accessdate=2010-12-30\n|author=Graham Blyth\n|authorlink=Graham Blyth\n|work=White Papers\n|publisher=Soundcraft\n| archiveurl= https://web.archive.org/web/20101204065754/http://www.soundcraft.com/support/white_papers.aspx| archivedate= 4 December 2010 | url-status= live}}</ref><ref>{{cite book\n | title = Sound system equipment\n | edition = Third\n | year = 2000\n | publisher = [[International Electrotechnical Commission]]\n | location = Geneva\n | id = IEC 602689-3:2001\n | pages = 111\n | chapter = Part 3: Amplifiers\n | quote = Only the common-mode impedance balance of the driver, line, and receiver play a role in noise or interference rejection. This noise or interference rejection property is independent of the presence of a desired differential signal.\n}}</ref>  A simple method of driving a balanced line is to inject the signal into the \"hot\" wire through a known [[source impedance]], and connect the \"cold\" wire to the signal's local ground reference through an identical impedance.  Due to common misconceptions about differential signalling, this is often referred to as a <em>quasi-balanced</em> or <em>impedance-balanced</em> output, though it is, in fact, fully balanced and will reject common-mode interference. \n\nHowever, there are some minor benefits to driving the line with a fully differential output:\n* The electromagnetic field around a differential line is ideally zero, which reduces [[crosstalk]] ''into'' adjacent cables, useful for telephone pairs.\n* Though the signal level would not be changed due to [[nominal level]] standardization, the maximum output from the differential drivers is twice as much, giving 6&nbsp;dB extra [[headroom (audio signal processing)|headroom]].<ref name=\"Blyth\"/>\n*Increasing cable capacitance over long cable runs decreases the signal level at which high frequencies are attenuated. If each wire carries half the signal voltage swing as in fully differential outputs then longer cable runs can be used without the loss of high frequencies.\n* Noise that is correlated between the two amps (from imperfect [[power supply rejection]], for instance), would be cancelled out.\n* At higher frequencies, the output impedance of the output amplifier can change, resulting in a small imbalance.  When driven in differential mode by two identical amplifiers, this impedance change will be the same for both lines, and thus cancelled out.<ref name=\"Blyth\"/>\n* Differential drivers are also more forgiving of incorrectly wired adapters or equipment that unbalances the signal by shorting pin 2.<ref name=\"Blyth\"/>\n\n==Internally balanced audio design==\n\nMost audio products (recording, public address, etc.) provide differential balanced inputs and outputs, typically via [[XLR connector|XLR]] or [[Phone connector (audio)|TRS phone connector]]s. However, in most cases, a differential balanced input signal is internally converted to a single-ended signal via [[transformer]] or electronic [[amplifier]]. After internal processing, the single-ended signal is converted back to a differential balanced signal and fed to an output.\n\nA small number of audio products have been designed with an entirely differential balanced signal path from input to output; the audio signal never unbalances. This design is achieved by providing identical (mirrored) internal signal paths for both the \"non-inverting\" and \"inverting\" audio signals. In critical applications, a 100% differential [[balanced circuit]] design can offer better [[signal integrity]] by avoiding the extra amplifier stages or [[transformer]]s required for front-end unbalancing and back-end rebalancing. Fully balanced internal circuitry has been promoted as yielding 3&nbsp;dB better dynamic range, though at increased cost over single-ended designs.\n\n==Connectors==\nThree-pin [[XLR connector]]s and quarter-inch (\u00bc\" or 6.35 mm) [[Phone connector (audio)|TRS phone connectors]] are commonly used for balanced audio signals. Many jacks are now designed to take either XLR or TRS phone plugs. Equipment intended for long-term installation sometimes uses [[Screw terminal|terminal strips]] or [[Euroblock]] connectors.\n\n<gallery>\nImage:trsconnectors.jpg|2.5, 3.5 and 6.35 mm TRS phone plugs\nImage:Xlr-connectors.jpg|3-pin XLR connectors, female on left and male on right\nImage:XLR-phone jack combo connector.jpg|3-pin XLR + 6.35 mm TRS phone hybrid jack.\n</gallery>\n\nWith XLR connectors, pins 1, 2, and 3 are usually used for the shield ([[Ground (electricity)|earthed]] or [[chassis]]), the non-inverting signal, and the inverting signal, respectively. (The phrase \"ground, live, return\", corresponding to \"X, L, R\", is often offered as a memory aid, although the inverting signal is not simply a \"return.\")  On TRS phone plugs, the tip is non-inverting, the ring is inverting, and the sleeve is ground. \n\nIf a [[stereophonic]] or other [[binaural recording|binaural]] signal is plugged into such a jack, one channel (usually the right) will be subtracted from the other (usually the left), leaving an unlistenable  L \u2212 R (left minus right) signal instead of normal [[Monaural|monophonic]] L + R.  Reversing the [[Electrical polarity|polarity]] at any other point in a balanced audio system will also result in this effect at some point when it is later mixed-down with its other channel.\n\n[[Telephone line]]s also carry balanced audio, though this is generally now limited to the [[local loop]].  It is called this because the two wires form a balanced loop through which both sides of the [[telephone call]] travel. Note that the telephone line is balanced for AC (audio) signals but is actually unbalanced at DC, as one wire is fed from the exchange power bus, typically -50 volts, and the other grounded, both via equal value inductors which have about 400 ohms DC resistance, to avoid short-circuiting the wanted AC signal while transmitting DC power to the telephone and allowing simple on/off hook detection. \n\n[[Digital audio]] connections in professional environments are also frequently balanced, normally following the [[AES3]] (AES/EBU) standard. This uses XLR connectors and twisted-pair cable with 110-ohm impedance. By contrast, the coaxial [[S/PDIF]] interface commonly seen on consumer equipment is an unbalanced signal.\n\n==Converters==\nUnbalanced signals can be converted to balanced signals by the use of a [[balun]], often through a [[DI unit]] (also called a \"DI box\" or \"direct box\"). \n\nIf balanced audio must be fed into an unbalanced connection, the electronic design used for the balanced output stage must be known. In most cases the negative output can be tied to ground, but in certain cases the negative output should be left disconnected.<ref>{{Cite web|url=http://www.rane.com/note151.html|title=Grounding and Shielding Audio Devices|last=Macatee|first=Steve|website=www.rane.com|access-date=2019-04-04}}</ref>\n\n== See also ==\n*[[Differential pair (disambiguation)|Differential pair]]\n*[[AES3]]\n\n== References ==\n<references/>\n\n== External links ==\n* [https://web.archive.org/web/20130314084434/http://www.ukslc.org/articles/sound/balanced_lines_2005011620.html UK Sound And Lighting Community \u2014 Article On Balanced Lines]\n* [http://207.67.68.51/forums/storage/4/612848/bal_to_unbal.pdf Ray Rayburn: \"Balanced-to-Unbalanced; How to Do it Right\"]\n* [http://www.aviom.com/blog/balanced-vs-unbalanced/ Aviom Blog - The Difference Between Balanced and Unbalanced Audio]\n\n{{DEFAULTSORT:Balanced Audio}}\n[[Category:Audiovisual connectors]]\n[[Category:Microphones]]\n[[Category:Audio engineering]]\n[[Category:Sound recording]]\n[[Category:Sound reinforcement system]]\n\n[[de:Symmetrische Signal\u00fcbertragung]]\n[[hu:Szimmetrikus audio vonal]]\n", "text_old": "'''Balanced audio''' is a method of interconnecting audio equipment using [[balanced line]]s.  This type of connection is very important in [[sound recording]] and production because it allows the use of long cables while reducing susceptibility to external noise caused by electromagnetic interference.\n\nBalanced connections typically use shielded twisted-pair cable and three-conductor connectors. The connectors are usually three-pin [[XLR connector|XLR]] or {{convert|1/4|in|mm|2}} [[Phone connector (audio)|TRS phone]] connectors. When used in this manner, each cable carries one channel, therefore stereo audio (for example) would require two of them.<ref>{{Cite news|url=http://www.aviom.com/blog/balanced-vs-unbalanced/|title=What's the Difference Between Balanced and Unbalanced?|date=2014-03-27|work=Aviom Blog|access-date=2017-09-24|language=en-US}}</ref>\n\n== Applications ==\n\nMany microphones operate at low voltage levels and some with high [[output impedance]] (hi-Z), which makes long microphone cables especially susceptible to [[electromagnetic interference]]. Microphone interconnections are therefore a common application for a balanced interconnection, which cancels out most of this induced noise. If the [[power amplifier]]s of a [[public address system]] are located at any distance from the [[mixing console]], it is also normal to use balanced lines for the signal paths from the mixer to these amplifiers. Many other components, such as graphic equalizers and effects units, have balanced inputs and outputs to allow this. In recording and for short cable runs in general, a compromise is necessary between the noise reduction given by balanced lines and the cost introduced by the extra [[Electronic circuit|circuitry]] they require.\n\n== Interference reduction ==\n\nBalanced audio connections use a number of techniques to reduce noise. \n\nA typical balanced cable contains two identical wires, which are twisted together and then wrapped with a third conductor (foil or braid) that acts as a [[Shielded cable|shield]]. The two wires form a circuit carrying the [[audio signal]]{{snd}}one wire is in phase with respect to the source signal; the other wire is reversed in polarity. The in-phase wire is called ''non-inverting'', ''positive'', or ''hot'', while the out-of-phase wire is called ''inverting'', ''phase-inverted'', ''anti-phase'', ''negative'' or ''cold''. The hot and cold connections are often shown as In+ and In\u2212 (\"in plus\" and \"in minus\") on [[circuit diagram]]s.<ref>{{cite book |url=https://books.google.com/books?id=PvKPEFu2PVkC&pg=PA344 |page=344 |last=Sel |first=Douglas |title=Small Signal Audio Design |publisher=Taylor & Francis US |year=2010 |isbn=978-0240521770}}</ref>\n\nThe term ''balanced'' comes from the method of connecting each wire to identical [[electrical impedance|impedances]] at [[source impedance|source]] and [[load impedance|load]]. This means that much of the electromagnetic interference<!-- predominantly capacitive? --> will induce an equal noise voltage in each wire. Since the amplifier at the receiving end measures the [[subtraction|difference]] in voltage between the two signal lines, noise that is identical on both wires is rejected. The noise received in the second, inverted line is applied against the first, upright signal, and cancels it out when the two signals are subtracted. \n\nThis differential signal recombination can be implemented with a [[differential amplifier]]. A [[balun]] may also be used instead of an active differential amplifier device.\n\nThe wires are also twisted together, to reduce interference from [[electromagnetic induction]]. A [[twisted pair]] makes the loop area between the conductors as small as possible, and ensures that a magnetic field that passes equally through adjacent loops will induce equal levels of noise on both lines, which is canceled out by the differential amplifier. If the noise source is extremely close to the cable, then it is possible it will be induced on one of the lines more than the other, and it won't be canceled as well, but canceling will still occur to the extent of the amount of noise that is equal on both lines.\n\nThe separate shield of a balanced audio connection also yields a [[noise rejection]] advantage over an unbalanced two-conductor arrangement (such as used in typical [[hi-fi|home stereos]]) where the shield must also act as the signal return wire. Therefore, any noise currents induced into a balanced audio shield will not be directly modulated onto the signal, whereas in a two-conductor system they will be. This also prevents [[ground loop (electricity)|ground loop]] problems, by separating the shield/chassis from signal ground.\n\n== Differential signaling ==\n\nSignals are often transmitted over balanced connections using the [[differential signaling|differential mode]], meaning the wires carry signals of opposite [[Electrical polarity|polarity]] to each other (for instance, in an [[XLR connector]], pin 2 carries the signal with normal polarity, and pin 3 carries an inverted version of the same signal). Despite popular belief, this arrangement is not necessary for noise rejection.  As long as the impedances are balanced, noise will couple equally into the two wires (and be rejected by a differential amplifier), regardless of the signal that is present on them.<ref name=\"Blyth\">{{cite web\n|url=http://www.soundcraft.com/support/white_papers.aspx#\n|title=Audio Balancing Issues\n|accessdate=2010-12-30\n|author=Graham Blyth\n|authorlink=Graham Blyth\n|work=White Papers\n|publisher=Soundcraft\n| archiveurl= https://web.archive.org/web/20101204065754/http://www.soundcraft.com/support/white_papers.aspx| archivedate= 4 December 2010 | url-status= live}}</ref><ref>{{cite book\n | title = Sound system equipment\n | edition = Third\n | year = 2000\n | publisher = [[International Electrotechnical Commission]]\n | location = Geneva\n | id = IEC 602689-3:2001\n | pages = 111\n | chapter = Part 3: Amplifiers\n | quote = Only the common-mode impedance balance of the driver, line, and receiver play a role in noise or interference rejection. This noise or interference rejection property is independent of the presence of a desired differential signal.\n}}</ref>  A simple method of driving a balanced line is to inject the signal into the \"hot\" wire through a known [[source impedance]], and connect the \"cold\" wire to the signal's local ground reference through an identical impedance.  Due to common misconceptions about differential signalling, this is often referred to as a <em>quasi-balanced</em> or <em>impedance-balanced</em> output, though it is, in fact, fully balanced and will reject common-mode interference. \n\nHowever, there are some minor benefits to driving the line with a fully differential output:\n* The electromagnetic field around a differential line is ideally zero, which reduces [[crosstalk]] ''into'' adjacent cables, useful for telephone pairs.\n* Though the signal level would not be changed due to [[nominal level]] standardization, the maximum output from the differential drivers is twice as much, giving 6&nbsp;dB extra [[headroom (audio signal processing)|headroom]].<ref name=\"Blyth\"/>\n*Increasing cable capacitance over long cable runs decreases the signal level at which high frequencies are attenuated. If each wire carries half the signal voltage swing as in fully differential outputs then longer cable runs can be used without the loss of high frequencies.\n* Noise that is correlated between the two amps (from imperfect [[power supply rejection]], for instance), would be cancelled out.\n* At higher frequencies, the output impedance of the output amplifier can change, resulting in a small imbalance.  When driven in differential mode by two identical amplifiers, this impedance change will be the same for both lines, and thus cancelled out.<ref name=\"Blyth\"/>\n* Differential drivers are also more forgiving of incorrectly wired adapters or equipment that unbalances the signal by shorting pin 2.<ref name=\"Blyth\"/>\n\n==Internally balanced audio design==\n\nMost audio products (recording, public address, etc.) provide differential balanced inputs and outputs, typically via [[XLR connector|XLR]] or [[Phone connector (audio)|TRS phone connector]]s. However, in most cases, a differential balanced input signal is internally converted to a single-ended signal via [[transformer]] or electronic [[amplifier]]. After internal processing, the single-ended signal is converted back to a differential balanced signal and fed to an output.\n\nA small number of audio products have been designed with an entirely differential balanced signal path from input to output; the audio signal never unbalances. This design is achieved by providing identical (mirrored) internal signal paths for both the \"non-inverting\" and \"inverting\" audio signals. In critical applications, a 100% differential [[balanced circuit]] design can offer better [[signal integrity]] by avoiding the extra amplifier stages or [[transformer]]s required for front-end unbalancing and back-end rebalancing. Fully balanced internal circuitry has been promoted as yielding 3&nbsp;dB better dynamic range, though at increased cost over single-ended designs.\n\n==Connectors==\nThree-pin [[XLR connector]]s and quarter-inch (\u00bc\" or 6.35 mm) [[Phone connector (audio)|TRS phone connectors]] are commonly used for balanced audio signals. Many jacks are now designed to take either XLR or TRS phone plugs. Equipment intended for long-term installation sometimes uses [[Screw terminal|terminal strips]] or [[Euroblock]] connectors.\n\n<gallery>\nImage:trsconnectors.jpg|2.5, 3.5 and 6.35 mm TRS phone plugs\nImage:Xlr-connectors.jpg|3-pin XLR connectors, female on left and male on right\nImage:XLR-phone jack combo connector.jpg|3-pin XLR + 6.35 mm TRS phone hybrid jack.\n</gallery>\n\nWith XLR connectors, pins 1, 2, and 3 are usually used for the shield ([[Ground (electricity)|earthed]] or [[chassis]]), the non-inverting signal, and the inverting signal, respectively. (The phrase \"ground, live, return\", corresponding to \"X, L, R\", is often offered as a memory aid, although the inverting signal is not simply a \"return.\")  On TRS phone plugs, the tip is non-inverting, the ring is inverting, and the sleeve is ground. \n\nIf a [[stereophonic]] or other [[binaural recording|binaural]] signal is plugged into such a jack, one channel (usually the right) will be subtracted from the other (usually the left), leaving an unlistenable  L \u2212 R (left minus right) signal instead of normal [[Monaural|monophonic]] L + R.  Reversing the [[Electrical polarity|polarity]] at any other point in a balanced audio system will also result in this effect at some point when it is later mixed-down with its other channel.\n\n[[Telephone line]]s also carry balanced audio, though this is generally now limited to the [[local loop]].  It is called this because the two wires form a balanced loop through which both sides of the [[telephone call]] travel. Note that the telephone line is balanced for AC (audio) signals but is actually unbalanced at DC, as one wire is fed from the exchange power bus, typically -50 volts, and the other grounded, both via equal value inductors which have about 400 ohms DC resistance, to avoid short-circuiting the wanted AC signal while transmitting DC power to the telephone and allowing simple on/off hook detection. \n\n[[Digital audio]] connections in professional environments are also frequently balanced, normally following the [[AES3]] (AES/EBU) standard. This uses XLR connectors and twisted-pair cable with 110-ohm impedance. By contrast, the coaxial [[S/PDIF]] interface commonly seen on consumer equipment is an unbalanced signal.\n\n==Converters==\nUnbalanced signals can be converted to balanced signals by the use of a [[balun]], often through a [[DI unit]] (also called a \"DI box\" or \"direct box\"). \n\nIf balanced audio must be fed into an unbalanced connection, the electronic design used for the balanced output stage must be known. In most cases the negative output can be tied to ground, but in certain cases the negative output should be left disconnected.<ref>{{Cite web|url=http://www.rane.com/note151.html|title=Grounding and Shielding Audio Devices|last=Macatee|first=Steve|website=www.rane.com|access-date=2019-04-04}}</ref>\n\n== See also ==\n*[[Differential pair (disambiguation)|Differential pair]]\n*[[AES3]]\n\n== References ==\n<references/>\n\n== External links ==\n* [https://web.archive.org/web/20130314084434/http://www.ukslc.org/articles/sound/balanced_lines_2005011620.html UK Sound And Lighting Community \u2014 Article On Balanced Lines]\n* [http://207.67.68.51/forums/storage/4/612848/bal_to_unbal.pdf Ray Rayburn: \"Balanced-to-Unbalanced; How to Do it Right\"]\n* [http://www.aviom.com/blog/balanced-vs-unbalanced/ Aviom Blog - The Difference Between Balanced and Unbalanced Audio]\n\n{{DEFAULTSORT:Balanced Audio}}\n[[Category:Audiovisual connectors]]\n[[Category:Microphones]]\n[[Category:Audio engineering]]\n[[Category:Sound recording]]\n[[Category:Sound reinforcement system]]\n\n[[de:Symmetrische Signal\u00fcbertragung]]\n[[hu:Szimmetrikus audio vonal]]\n", "name_user": "Bubba73", "label": "safe", "comment": "\u2192\u200eInterference reduction", "url_page": "//en.wikipedia.org/wiki/Balanced_audio"}
{"title_page": "A Year of the Quiet Sun", "text_new": "{{more citations needed|date=May 2019}}\n{{short description|1984 Polish film by Krzysztof Zanussi}}\n{{other uses of|Year of the Quiet Sun}}\n\n{{Infobox film\n  | name = A Year of the Quiet Sun \n  | image = A Year of the Quiet Sun poster.jpg\n  | caption =\n  | director = [[Krzysztof Zanussi]]\t\n  | producer = [[Film Polski]]\n  | writer = [[Krzysztof Zanussi]]\n  | starring =[[Maja Komorowska]]<br>[[Scott Wilson (actor)|Scott Wilson]]\n  | music = [[Wojciech Kilar]]\n  | cinematography = [[S\u0142awomir Idziak]]\n  | editing =[[Marek Denys]]\n  | distributor = [[Film Polski]]<br> [[SPI International Poland (TV KinoPolska \"Masterpieces of Polish Cinema\")]]\n  | released = {{Film date|1984|09||[[Venice Film Festival|VFF]]|1985|02|25|df=y}}\n  | country = Poland\n  | runtime = 110 minutes\n  | language = Polish<br>English\n  | budget = \n  }}\n'''''A Year of the Quiet Sun''''' ({{lang-pl|'''Rok spokojnego s\u0142o\u0144ca'''}}) is a 1984 Polish film written and directed by [[Krzysztof Zanussi]] and starring [[Maja Komorowska]] and [[Scott Wilson (actor)|Scott Wilson]]. It tells the story of a romance between a Polish woman and an [[United States|American]] soldier in Poland, shortly after [[WWII]].\n\nThe film<ref>{{Cite web|url=https://www.rogerebert.com/reviews/great-movie-a-year-of-the-quiet-sun-1984|title=A Year of the Quiet Sun movie review (1984) {{!}} Roger Ebert|last=Ebert|first=Roger|website=www.rogerebert.com|language=en|access-date=2020-04-13}}</ref> was nominated for 1986's [[Golden Globe Award]] for [[Golden Globe Award for Best Foreign Language Film|Best Foreign Language Film]]. At the [[Venice Film Festival]], the film was awarded the [[Golden Lion]] and Pasinetti Awards.\n\n==Cast==\n* [[Maja Komorowska]] as Emilia\n* [[Scott Wilson (actor)|Scott Wilson]] as Norman\n* [[Hanna Skar\u017canka]] as Emilia's Mother\n* [[Ewa Da\u0142kowska]] as Stella\n* [[Vadim Glowna]] as Herman\n* [[Danny Webb (actor)|Daniel Webb]] as David\n* [[Zbigniew Zapasiewicz]] as Szary\n* [[Zofia Rysi\u00f3wna]] as Interpreter\n* [[Janusz Gajos]] as Moonlighter\n* [[Jerzy Stuhr]] as Adzio\n* [[Gustaw Lutkiewicz]] as Bakery Owner\n* [[Marek Kondrat]] as Malutki\n* [[Jerzy Nowak]] as English Doctor\n\n== See also ==\n*[[Cinema of Poland]]\n*[[List of Polish language films]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title| id=0088009 | title=A Year of the Quiet Sun}}\n\n{{The Golden Lion 1970\u20131989}}\n{{Krzysztof Zanussi}}\n\n{{DEFAULTSORT:Year of the Quiet Sun, A}}\n[[Category:1984 films]]\n[[Category:1980s romantic drama films]]\n[[Category:Polish romance films]]\n[[Category:Polish war drama films]]\n[[Category:Polish films]]\n[[Category:West German films]]\n[[Category:Polish-language films]]\n[[Category:English-language films]]\n[[Category:Romantic period films]]\n[[Category:Films set in 1946]]\n[[Category:War romance films]]\n[[Category:World War II films]]\n[[Category:Golden Lion winners]]\n[[Category:Films directed by Krzysztof Zanussi]]\n[[Category:German war drama films]]\n\n\n{{war-drama-film-stub}}\n{{Poland-film-stub}}\n", "text_old": "{{more citations needed|date=May 2019}}\n{{short description|1984 Polish film by Krzysztof Zanussi}}\n{{other uses of|Year of the Quiet Sun}}\n\n{{Infobox film\n  | name = A Year of the Quiet Sun \n  | image = A Year of the Quiet Sun poster.jpg\n  | caption =\n  | director = [[Krzysztof Zanussi]]\t\n  | producer = [[Film Polski]]\n  | writer = [[Krzysztof Zanussi]]\n  | starring =[[Maja Komorowska]]<br>[[Scott Wilson (actor)|Scott Wilson]]\n  | music = [[Wojciech Kilar]]\n  | cinematography = [[S\u0142awomir Idziak]]\n  | editing =[[Marek Denys]]\n  | distributor = [[Film Polski]]<br> [[SPI International Poland (TV KinoPolska \"Masterpieces of Polish Cinema\")]]\n  | released = {{Film date|1984|09||[[Venice Film Festival|VFF]]|1985|02|25|df=y}}\n  | country = Poland\n  | runtime = 110 minutes\n  | language = Polish<br>English\n  | budget = \n  }}\n'''''A Year of the Quiet Sun''''' ({{lang-pl|'''Rok spokojnego s\u0142o\u0144ca'''}}) is a 1984 Polish film written and directed by [[Krzysztof Zanussi]] and starring [[Maja Komorowska]] and [[Scott Wilson (actor)|Scott Wilson]]. It tells the story of a romance between a Polish woman and an [[United States|American]] soldier in Poland, shortly after [[WWII]].\n\nThe film<ref>{{Cite web|url=https://www.rogerebert.com/reviews/great-movie-a-year-of-the-quiet-sun-1984|title=A Year of the Quiet Sun movie review (1984) {{!}} Roger Ebert|last=Ebert|first=Roger|website=www.rogerebert.com|language=en|access-date=2020-04-13}}</ref> was nominated for 1986's [[Golden Globe Award]] for [[Golden Globe Award for Best Foreign Language Film|Best Foreign Language Film]]. At the [[Venice Film Festival]], the film was awarded the [[Golden Lion]] and Pasinetti Awards.\n\n==Cast==\n* [[Maja Komorowska]] as Emilia\n* [[Scott Wilson (actor)|Scott Wilson]] as Norman\n* [[Hanna Skar\u017canka]] as Emilia's Mother\n* [[Ewa Da\u0142kowska]] as Stella\n* [[Vadim Glowna]] as Herman\n* [[Danny Webb (actor)|Daniel Webb]] as David\n* [[Zbigniew Zapasiewicz]] as Szary\n* [[Zofia Rysi\u00f3wna]] as Interpreter\n* [[Janusz Gajos]] as Moonlighter\n* [[Jerzy Stuhr]] as Adzio\n* [[Gustaw Lutkiewicz]] as Bakery Owner\n* [[Marek Kondrat]] as Malutki\n* [[Jerzy Nowak]] as English Doctor\n\n== See also ==\n*[[Cinema of Poland]]\n*[[List of Polish language films]]\n\n==External links==\n*{{IMDb title| id=0088009 | title=A Year of the Quiet Sun}}\n\n{{The Golden Lion 1970\u20131989}}\n{{Krzysztof Zanussi}}\n\n{{DEFAULTSORT:Year of the Quiet Sun, A}}\n[[Category:1984 films]]\n[[Category:1980s romantic drama films]]\n[[Category:Polish romance films]]\n[[Category:Polish war drama films]]\n[[Category:Polish films]]\n[[Category:West German films]]\n[[Category:Polish-language films]]\n[[Category:English-language films]]\n[[Category:Romantic period films]]\n[[Category:Films set in 1946]]\n[[Category:War romance films]]\n[[Category:World War II films]]\n[[Category:Golden Lion winners]]\n[[Category:Films directed by Krzysztof Zanussi]]\n[[Category:German war drama films]]\n\n\n{{war-drama-film-stub}}\n{{Poland-film-stub}}\n", "name_user": "G\u00fcnniX", "label": "safe", "comment": "Reflist", "url_page": "//en.wikipedia.org/wiki/A_Year_of_the_Quiet_Sun"}
{"title_page": "Methods of computing square roots", "text_new": "{{short description|Algorithms for calculating square roots}}\n{{multiple issues|\n{{Original research|date=January 2012}}\n{{technical|date=September 2012}}\n{{more citations needed|date=July 2017}}\n{{very long|date=June 2019|rps=61}}\n}}\n\n'''Methods of computing square roots''' are [[numerical analysis]] [[algorithm]]s for finding the principal, or non-negative, [[square root]] (usually denoted {{sqrt|''S''}}, {{radic|''S''|2}}, or S<sup>1/2</sup>) of a real number. Arithmetically, it means given S, a procedure for finding a number which when multiplied by itself, yields S; algebraically, it means a procedure for finding the non-negative root of the equation x<sup>2</sup> - S = 0; geometrically, it means given the area of a square, a procedure for constructing a side of the square.\n\nEvery real number has two square roots.<ref group=Note>In addition to the principal square root, there is a negative square root equal in magnitude but opposite in sign to the principal square root, except for zero, which has double square roots of zero.</ref> The principal square root of most numbers is an irrational number or repeating decimal, so that any computed result is a finite approximation to the square root.  For some numbers, the square root is finitely representable in the base (i.e. decimal or binary); for these numbers, including integers known as [[perfect squares]], the square root is exact. However, even if the square root is exact, the procedure used to compute it may return an approximation. \n\nThe most common analytical methods are iterative and consist of two steps: finding a suitable starting value, followed by iterative refinement until some termination criteria is met.  The starting value can be any number, but fewer iterations will be required the closer it is to the final result.  The most familiar such method, most suited for programmatic calculation, is Newton's method, which is based on  a property of the derivative in the calculus. A few methods like paper-and-pencil synthetic division and series expansion, do not require a starting value. In some applications, an [[integer square root]] is required, which is the square root rounded or truncated to the nearest integer (a modified procedure may be employed in this case).\n\nThe method employed depends on what the result is to be used for (i.e. how accurate it has to be), how much effort one is willing to put into the procedure, and what tools are at hand.  The methods may be roughly classified as those suitable for mental calculation, those usually requiring at least paper and pencil, and those which are implemented as programs to be executed on a digital electronic computer or other computing device. Algorithms may take into account convergence (how many iterations are required to achieve a specified precision), computational complexity of individual operations (i.e. division) or iterations, and error propagation (the accuracy of the final result).\n\nProcedures for finding square roots (particularly the square root of 2) have been known since at least the period of ancient Babylon in the 17th century BCE.  Heron's method from first century Egypt was the first ascertainable algorithm for computing square root. Modern analytic methods began to be developed after introduction of the Arabic numeral system to western Europe in the early Renaissance.  Today, nearly all computing devices have a fast and accurate square root function, either as a programming language construct, a compiler intrinsic or library function, or as a hardware operator, based on one of the described procedures.\n\n==Initial estimate==\nMany iterative square root algorithms require an initial [[seed value]]. The seed must be a non-zero positive number; it should be between 1 and <math>S</math>, the number whose square root is desired, because the square root must be in that range. If the seed is far away from the root, the algorithm will require more iterations. If one initializes with ''x''<sub>0</sub> = 1 (or ''S''), then approximately <math> \\tfrac12 \\vert \\log_2 S \\vert </math> iterations will be wasted just getting the order of magnitude of the root. It is therefore useful to have a rough estimate, which may have limited accuracy but is easy to calculate. In general, the better the initial estimate, the faster the convergence.  For Newton's method (also called Babylonian or Heron's method), a seed somewhat larger than the root will converge slightly faster than a seed somewhat smaller than the root.\n\nIn general, an estimate is pursuant to an arbitrary interval known to contain the root (such as [x<sub>0</sub>, 1/x<sub>0</sub>]). The estimate is a specific value of a functional approximation to f(x) = {{sqrt|x}} over the interval. Obtaining a better estimate involves either obtaining tighter bounds on the interval, or finding a better functional approximation to f(x). The latter usually means using a higher order polynomial in the approximation, though not all approximations are polynomial. Common methods of estimating include scalar, linear, hyperbolic and logarithmic. A decimal base is usually used for mental or paper-and-pencil estimating. A binary base is more suitable for computer estimates.  In estimating, the exponent and mantissa are usually treated separately, as the number would be expressed in scientific notation.\n\n===Decimal estimates===\nTypically the number <math>S</math> is expressed in [[scientific notation]] as <math>a\\times10^{2n}</math> where <math> 1\\leq a<100</math> and ''n'' is an integer, and the range of possible square roots is <math>\\sqrt a\\times10^n</math> where <math>1\\leq \\sqrt a<10</math>. \n====Scalar estimates====\nScalar methods divide the range into intervals, and the estimate in each interval is represented by a single scalar number.  If the range is considered as a single interval, the arithmetic mean (5) or geometric mean (<math>\\sqrt{10}\\approx3.16</math>) times <math>10^n</math> are plausible estimates.  The absolute and relative error for these will differ.  In general, a single scalar will be very inaccurate.  Better estimates divide the range into two or more intervals, but scalar estimates have inherently low accuracy.\n\nFor two intervals, divided geometrically, the square root <math>\\sqrt{S} = \\sqrt{a}\\times10^n</math> can be estimated as<ref group=\"Note\">The factors two and six are used because they approximate the [[geometric mean]]s of the lowest and highest possible values with the given number of digits: <math>\\sqrt{\\sqrt{1} \\cdot \\sqrt{10}} = \\sqrt[4]{10} \\approx 1.78 \\,</math> and <math>\\sqrt{\\sqrt{10} \\cdot \\sqrt{100}} = \\sqrt[4]{1000} \\approx 5.62 \\,</math>.</ref>\n\n:<math> \\sqrt{S} \\approx \\begin{cases}\n2 \\cdot 10^n & \\text{if } a < 10, \\\\\n6 \\cdot 10^n & \\text{if } a \\geq 10.\n\\end{cases}</math>\n\nThis estimate has maximum absolute error of <math>4\\cdot10^n</math> at a = 100, and maximum relative error of 100% at a = 1.\n \nFor example, for <math> S = 125348</math> factored as <math>12.5348 \\times 10^4</math>, the estimate is <math> \\sqrt{S} \\approx 6 \\cdot 10^2 = 600</math>. <math>\\sqrt{125348} = 354.0</math>, an absolute error of 246 and relative error of almost 70%.\n====Linear estimates====\nA better estimate, and the standard method used, is a linear approximation to the function <math>y = x^2</math> over a small arc.  If, as above, powers of the base are factored out of the number <math>S</math> and the interval reduced to [1,100], a secant line spanning the arc, or a tangent line somewhere along the arc may be used as the approximation, but a least-squares regression line intersecting the arc will be more accurate.\n\nA least-squares regression line minimizes the average difference between the estimate and the value of the function.  Its equation is <math>y=8.7x-10</math>.  Reordering, <math>x=0.115y+1.15</math>.  Rounding the coefficients for ease of computation,\n:<math>\\sqrt{S}\\approx (a/10+1.2)\\cdot 10^n</math>\n\nThat is the best estimate ''on average'' that can be achieved with a single piece linear approximation of the function  y=x<sup>2</sup> in the interval [1,100].  It has a maximum absolute error of 1.2 at a=100, and maximum relative error of 30% at S=1 and 10.<ref group=Note>The unrounded estimate has maximum absolute error of 2.65 at 100 and maximum relative error of 26.5% at y=1, 10 and 100</ref> \n\nTo divide by 10, subtract one from the exponent of <math>a</math>, or figuratively move the decimal point one digit to the left. For this formulation, any additive constant 1 plus a small increment will make a satisfactory estimate so remembering the exact number isn't a burden.  The approximation (rounded or not) using a single line spanning the range [1,100] is less than one significant digit of precision; the relative error is greater than 1/2<sup>2</sup>, so less than 2 bits of information are provided. The accuracy is severely limited because the range is two orders of magnitude, quite large for this kind of estimation. \n\nA much better estimate can be obtained by a piece-wise linear approximation: multiple line segments, each approximating some subarc of the original.  The more line segments used, the better the approximation.  The most common way is to use tangent lines; the critical choices are how to divide the arc and where to place the tangent points. An efficacious way to divide the arc from y=1 to y=100 is geometrically: for two intervals, the bounds of the intervals are the square root of the bounds of the original interval, 1*100, i.e. [1,{{radic|100|2}}] and [{{radic|100|2}},100].  For three intervals, the bounds are the cube roots of 100: [1,{{radic|100|3}}], [{{radic|100|3}},({{radic|100|3}})<sup>2</sup>], and [({{radic|100|3}})<sup>2</sup>,100], etc.  For two intervals, {{radic|100|2}} = 10, a very convenient number.  Tangent lines are easy to derive, and are located at x = {{sqrt|1*{{sqrt|10}}}} and x = {{sqrt|10*{{sqrt|10}}}}.  Their equations are:  y = 3.56x - 3.16  and y = 11.2x - 31.6.  Inverting, the square roots are: x = 0.28y + 0.89 and x = .089y + 2.8. Thus for S = a * 10<sup>2n</sup>:\n\n:<math> \\sqrt{S} \\approx \\begin{cases}\n(0.28a + 0.89) \\cdot 10^n & \\text{if } a < 10, \\\\\n(.089a + 2.8) \\cdot 10^n & \\text{if } a \\geq 10.\n\\end{cases}</math>\n\nThe maximum absolute errors occur at the high points of the intervals, at a=10 and 100, and are 0.54 and 1.7 respectively. The maximum relative errors are at the endpoints of the intervals, at a=1, 10 and 100, and are 17% in both cases.  17% or 0.17 is larger than 1/10, so the method yields less than a decimal digit of accuracy.\n\n====Hyperbolic estimates====\nIn some cases, hyperbolic estimates may be efficacious, because a hyperbola is also a convex curve and may lie along an arc of Y = x<sup>2</sup> better than a line. Hyperbolic estimates are more computationally complex, because they necessarily require a floating division.  A near-optimal hyperbolic approximation to x<sup>2</sup> on the interval [1,100] is y=190/(10-x)-20.  Transposing, the square root is x = -190/(y+20)+10.  Thus for <math>S=a\\cdot10^{2n}</math>:\n\n:<math>\\sqrt{S}\\approx \\left(\\frac{-190}{a+20}+10\\right)\\cdot 10^n</math> \n\nThe floating division need be accurate to only one decimal digit, because the estimate overall is only that accurate, and can be done mentally.  A hyperbolic estimate is better on average than scalar or linear estimates. It has maximum absolute error of 1.58 at 100 and maximum relative error of 16.0% at 10.  For the worst case at a=10, the estimate is 3.67. If one starts with 10 and applies Newton-Raphson iterations straight away, two iterations will be required, yielding 3.66, before the accuracy of the hyperbolic estimate is exceeded.  For a more typical case like 75, the hyperbolic estimate is 8.00, and 5 Newton-Raphson iterations starting at 75 would be required to obtain a more accurate result.\n\n\n====Arithmetic estimates====\nA method analogous to piece-wise linear approximation but using only arithmetic instead of algebraic equations, uses the multiplication tables in reverse: the square root of a number between 1 and 100 is between 1 and 10, so if we know 25 is a perfect square (5x5), and 36 is a perfect square (6x6), then the square root of a number greater than or equal to 25 but less than 36, begins with a 5. Similarly for numbers between other squares. This method will yield a correct first digit, but it is not accurate to one digit: the first digit of the square root of 35 for example, is 5, but the square root of 35 is almost 6. \n\nA better way is to the divide the range into intervals half way between the squares.  So any number between 25 and half way to 36, which is 30.5, estimate 5; any number greater than 30.5 up to 36, estimate 6.<ref group=Note>If the number is exactly half way between two squares, like 30.5, guess the higher number which is 6 in this case</ref>  The procedure only requires a little arithmetic to find a boundary number in the middle of two products from the multiplication table. Here is a reference table of those boundaries:\n\n{| class=\"wikitable\"\n|-\n! <math>a</math>\n| nearest square\n! <math>k = \\sqrt{a}</math> est.\n|-\n| 1 - 2.5\n| style=\"text-align: center;\" |1 (1<sup>2</sup>)\n| 1\n|-\n| 2.5 - 6.5\n| style=\"text-align: center;\" |4 (2<sup>2</sup>)\n| 2\n|-\n| 6.5 - 12.5\n| style=\"text-align: center;\" |9 (3<sup>2</sup>)\n| 3\n|-\n| 12.5 - 20.5\n| style=\"text-align: center;\" |16 (4<sup>2</sup>)\n| 4\n|-\n| 20.5 - 30.5\n| style=\"text-align: center;\" |25 (5<sup>2</sup>)\n| 5\n|-\n| 30.5 - 42.5\n| style=\"text-align: center;\" |36 (6<sup>2</sup>)\n| 6\n|-\n| 42.5 - 56.5\n| style=\"text-align: center;\" |49 (7<sup>2</sup>)\n| 7\n|-\n| 56.5 - 72.5\n| style=\"text-align: center;\" | 64 (8<sup>2</sup>)\n| 8\n|-\n| 72.5 - 90.5\n| style=\"text-align: center;\" | 81 (9<sup>2</sup>)\n| 9\n|-\n| 90.5 - 100\n| style=\"text-align: center;\" | 100 (10<sup>2</sup>)\n| 10\n|}\n\nThe final operation is to multiply the estimate <math>k</math> by the power of ten divided by 2, so for <math>S = a\\cdot 10^{2n}</math>,\n:<math>\\sqrt{S}\\approx k\\cdot 10^n</math>\n\nThe method implicitly yields one significant digit of accuracy, since it rounds to the best first digit.\n\nThe method can be extended 3 significant digits in most cases, by interpolating between the nearest squares bounding the operand.  If <math>k^2 \\le a < (k+1)^2</math>, then <math>\\sqrt{a}</math> is approximately k plus a fraction, the difference between a and k<sup>2</sup> divided by the difference between the two squares:\n\n<math>\\sqrt{a}\\approx k + R</math> where <math>R = \\frac{(a-k^2)}{(k+1)^2-k^2}</math>\n\nThe final operation, as above, is to multiply the result by the power of ten divided by 2;\n:<math>\\sqrt{S} = \\sqrt{a}\\cdot 10^n \\approx (k + R)\\cdot 10^n</math>\n\n<math>k</math> is a decimal digit and <math>R</math> is a fraction that must be converted to decimal. It usually has only a single digit in the numerator, and one or two digits in the denominator, so the conversion to decimal can be done mentally.\n\nExample: find the square root of 75.  75 = 75 * 10<sup>2{{dot}}0</sup>, so <math>a</math> is 75 and <math>n</math> is 0. From the multiplication tables, the square root of the mantissa must be 8 point ''something'' because 8x8 is 64, but 9x9 is 81, too big, so <math>k</math> is 8; ''something'' is the decimal representation of <math>R</math>.  The fraction R is 75-k<sup>2</sup> = 11, the numerator, and 81-k<sup>2</sup> = 17, the denominator.  11/17 is a little less than 12/18, which is 2/3s or .67, so guess .66 (it's ok to guess here, the error is very small).  So the estimate is 8+.66=8.66. {{sqrt|75}} to three significant digits is 8.66, so the estmate is good to 3 significant digits.  Not all such estimates using this method will be so accurate, but they will be close.\n\n===Binary estimates===\nWhen working in the [[binary numeral system]] (as computers do internally), by expressing <math>S</math> as <math>a\\times2^{2n}</math> where <math> 0.1_2\\leq a<10_2</math>, the square root <math>\\sqrt{S} = \\sqrt{a}\\times2^n</math> can be estimated as \n:<math> \\sqrt{S} \\approx (0.485 + 0.485 \\cdot a) \\cdot 2^n</math>\n\nwhich is the least-squares regression line to 3 significant digit coefficients. {{sqrt|a}} has maximum absolute error of 0.0408 at a=2, and maximum relative error of 3.0% at a=1.  A computationally convenient rounded estimate (because the coefficients are powers of 2) is:\n:<math> \\sqrt{S} \\approx (0.5 + 0.5 \\cdot a) \\cdot 2^n</math><ref group=Note>This is incidentally the equation of the tangent line to y=x<sup>2</sup> at y=1.</ref>\nwhich has maximum absolute error of 0.086 at 2 and maximum relative error of 6.1% at <math>a</math>=0.5 and <math>a</math>=2.0.\n\nFor <math> S = 125348 = 1\\;1110\\;1001\\;1010\\;0100_2 = 1.1110\\;1001\\;1010\\;0100_2\\times2^{16}\\, </math> the binary approximation gives <math> \\sqrt{S} \\approx (0.5 + 0.5 \\cdot a) \\cdot 2^8 = 1.0111\\;0100\\;1101\\;0010_2 \\cdot 1\\;0000\\;0000_2 = 1.456 \\cdot 256 = 372.8</math>. <math>\\sqrt{125348}=354.0</math>, so the estimate has an absolute error of 19 and relative error of 5.3%.  The relative error is a little less than 1/2<sup>4</sup>, so the estimate is good to 4+ bits.\n\nAn estimate for <math>a</math> good to 8 bits can be obtained by table lookup on the high 8 bits of <math>a</math>, remembering that the high bit is implicit in most floating point representations, and the bottom bit of the 8 should be rounded. The table is 256 bytes of precomputed 8-bit square root values.  For example, for the index 11101101<sub>2</sub> representing 1.8515625<sub>10</sub>, the entry is 10101110<sub>2</sub> representing 1.359375<sub>10</sub>, the square root of 1.8515625<sub>10</sub> to 8 bit precision (2+ decimal digits).\n\n==Babylonian method==<!-- this section is linked from [[Babylonian method]] -->\n{{redirect|Heron's method|the formula used to find the area of a triangle|Heron's formula}}\n[[Image:Babylonian method.svg|300px|right|thumb|\nGraph charting the use of the Babylonian method for approximating a square root of 100 (\u00b110) using starting values\n<span style=\"color:#008\">''x''<sub>0</sub>&nbsp;=&nbsp;50</span>,\n<span style=\"color:#F00\">''x''<sub>0</sub>&nbsp;=&nbsp;1</span>,\nand <span style=\"color:#080\">''x''<sub>0</sub>&nbsp;=&nbsp;&minus;5</span>. Note that a positive starting value yields the positive root, and a negative starting value the negative root.]]\n\nPerhaps the first [[algorithm]] used for approximating <math>\\sqrt{S}</math> is known as the '''Babylonian method''', despite there being no direct evidence, beyond informed conjecture, that the eponymous [[Babylonian mathematics|Babylonian mathematicians]] employed exactly this method.<ref name=\"Fowler and Robson\">{{cite journal |last1=Fowler |first1=David |last2=Robson |first2=Eleanor |title=Square Root Approximations in Old Babylonian Mathematics: YBC 7289 in Context |journal=Historia Mathematica |volume=25 |date=1998 |issue=4 |page=376 |doi=10.1006/hmat.1998.2209 }}</ref> The method is also known as '''Heron's method''', after the first-century Greek mathematician [[Hero of Alexandria]] who gave the first explicit description of the method in his [[AD 60]] work ''[[Hero of Alexandria#Bibliography|Metrica]]''.<ref>{{cite book\n  | last = Heath\n  | first = Thomas\n  | authorlink = Thomas Little Heath\n  | title = A History of Greek Mathematics, Vol. 2\n  | publisher = Clarendon Press\n  | year = 1921\n  | location = Oxford\n  | pages = [https://archive.org/details/ahistorygreekma00heatgoog/page/n339 323]\u2013324\n  | url = https://archive.org/details/ahistorygreekma00heatgoog\n  | doi =\n  | id =\n  | isbn = }}</ref> The basic idea is that if {{mvar|x}} is an overestimate to the square root of a non-negative real number {{mvar|S}} then {{math|{{sfrac|''S''|''x''}}}} will be an underestimate, or vice versa, and so the average of these two numbers may reasonably be expected to provide a better approximation (though the formal proof of that assertion depends on the [[inequality of arithmetic and geometric means]] that shows this average is always an overestimate of the square root, as noted in the article on [[Square root#Geometric construction of the square root|square roots]], thus assuring convergence). This is equivalent to using [[Newton's method]] to solve <math> x^2 - S = 0 </math>.\n\nMore precisely, if {{mvar|x}} is our initial guess of <math>\\sqrt{S}</math> and {{mvar|e}} is the error in our estimate such that {{math|1=''S'' = (''x''+ ''e'')<sup>2</sup>}}, then we can expand the binomial and solve for \n:<math>e=\\frac{S-x^2}{2x+e} \\approx \\frac{S-x^2}{2x}, </math> since <math> e \\ll x </math>.\nTherefore, we can compensate for the error and update our old estimate as \n:<math>x + e \\approx x + \\frac{S-x^2}{2x} = \\frac{S+x^2}{2x} = \\frac{\\frac{S}{x}+x}{2} \\equiv x_\\text{revised}</math> \nSince the computed error was not exact, this becomes our next best guess. The process of updating is iterated until desired accuracy is obtained. This is a [[quadratic convergence|quadratically convergent]] algorithm, which means that the number of correct digits of the approximation roughly doubles with each iteration. It proceeds as follows:\n#Begin with an arbitrary positive starting value {{math|''x''<sub>0</sub>}} (the closer to the actual square root of {{mvar|S}}, the better).\n#Let {{math|''x''<sub>''n'' + 1</sub>}} be the average of {{math|''x''<sub>''n''</sub>}} and {{math|{{sfrac|''S''|''x''<sub>''n''</sub>}}}} (using the [[arithmetic mean]] to approximate the [[geometric mean]]).\n#Repeat step 2 until the desired accuracy is achieved.\nIt can also be represented as:\n:<math>x_0 \\approx \\sqrt{S},</math>\n:<math>x_{n+1} = \\frac{1}{2} \\left(x_n + \\frac{S}{x_n}\\right),</math>\n:<math>\\sqrt S = \\lim_{n \\to \\infty} x_n.</math>\n\nThis algorithm works equally well in the [[p-adic number|{{mvar|p}}-adic number]]s, but cannot be used to identify real square roots with {{mvar|p}}-adic square roots; one can, for example, construct a sequence of rational numbers by this method that converges to +3 in the reals, but to &minus;3 in the 2-adics.\n===Example===\nTo calculate {{math|{{sqrt|''S''}}}}, where {{mvar|S}} = 125348, to six significant figures, use the rough estimation method above to get\n\n:<math>\\begin{align}\n\\begin{array}{rlll}\nx_0 & = 6 \\cdot 10^2 && = 600.000 \\\\[0.3em]\nx_1 & = \\frac{1}{2} \\left(x_0 + \\frac{S}{x_0}\\right) & = \\frac{1}{2} \\left(600.000 + \\frac{125348}{600.000}\\right) & = 404.457 \\\\[0.3em]\nx_2 & = \\frac{1}{2} \\left(x_1 + \\frac{S}{x_1}\\right) & = \\frac{1}{2} \\left(404.457 + \\frac{125348}{404.457}\\right) & = 357.187 \\\\[0.3em]\nx_3 & = \\frac{1}{2} \\left(x_2 + \\frac{S}{x_2}\\right) & = \\frac{1}{2} \\left(357.187 + \\frac{125348}{357.187}\\right) & = 354.059 \\\\[0.3em]\nx_4 & = \\frac{1}{2} \\left(x_3 + \\frac{S}{x_3}\\right) & = \\frac{1}{2} \\left(354.059 + \\frac{125348}{354.059}\\right) & = 354.045 \\\\[0.3em]\nx_5 & = \\frac{1}{2} \\left(x_4 + \\frac{S}{x_4}\\right) & = \\frac{1}{2} \\left(354.045 + \\frac{125348}{354.045}\\right) & = 354.045\n\\end{array}\n\\end{align}</math>\n\nTherefore, {{math|{{sqrt|125348}} \u2248 354.045}}.\n\n===Convergence===\nSuppose that ''x''<sub>0</sub> > 0 and ''S'' > 0. Then for any natural number ''n'', ''x''<sub>''n''</sub> > 0. Let the [[relative error]] in ''x''<sub>''n''</sub> be defined by\n\n:<math>\\varepsilon_n = \\frac {x_n}{\\sqrt{S}} - 1 > -1</math>\n\nand thus\n:<math>x_n = \\sqrt {S} \\cdot (1 + \\varepsilon_n) .</math>\n\nThen it can be shown that\n:<math>\\varepsilon_{n+1} = \\frac {\\varepsilon_n^2}{2 (1 + \\varepsilon_n)} \\geq 0 .</math>\n\nAnd thus that\n:<math>\\varepsilon_{n+2} \\leq \\min \\left\\{\\frac {\\varepsilon_{n+1}^2}{2}, \\frac {\\varepsilon_{n+1}}{2} \\right\\}</math>\n\nand consequently that convergence is assured, and [[quadratic convergence|quadratic]].\n\n====Worst case for convergence====\nIf using the rough estimate above with the Babylonian method, then the least accurate cases in ascending order are as follows:\n\n:<math>\n\\begin{align}\nS & = 1;   & x_0 & = 2; & x_1 & = 1.250;  & \\varepsilon_1 & = 0.250. \\\\\nS & = 10;  & x_0 & = 2; & x_1 & = 3.500;  & \\varepsilon_1 & < 0.107. \\\\\nS & = 10;  & x_0 & = 6; & x_1 & = 3.833;  & \\varepsilon_1 & < 0.213. \\\\\nS & = 100; & x_0 & = 6; & x_1 & = 11.333; & \\varepsilon_1 & < 0.134.\n\\end{align}\n</math>\n\nThus in any case,\n:<math> \\varepsilon_1 \\leq 2^{-2}. \\, </math>\n:<math> \\varepsilon_2 < 2^{-5} < 10^{-1}. \\, </math>\n:<math> \\varepsilon_3 < 2^{-11} < 10^{-3}. \\, </math>\n:<math> \\varepsilon_4 < 2^{-23} < 10^{-6}. \\, </math>\n:<math> \\varepsilon_5 < 2^{-47} < 10^{-14}. \\, </math>\n:<math> \\varepsilon_6 < 2^{-95} < 10^{-28}. \\, </math>\n:<math> \\varepsilon_7 < 2^{-191} < 10^{-57}. \\, </math>\n:<math> \\varepsilon_8 < 2^{-383} < 10^{-115}. \\, </math>\n\nRounding errors will slow the convergence. It is recommended to keep at least one extra digit beyond the desired accuracy of the {{mvar|x<sub>n</sub>}} being calculated to minimize round off error.\n\n==Bakhshali method==\nThis method for finding an approximation to a square root was described in an ancient Indian mathematical manuscript called the [[Bakhshali manuscript]]. It is equivalent to two iterations of the Babylonian method beginning with ''x''<sub>0</sub>. Thus, the algorithm is quartically convergent, which means that the number of correct digits of the approximation roughly quadruples with each iteration.<ref>{{cite news|title=Ancient Indian Square Roots: An Exercise in Forensic Paleo-Mathematics| last1=Bailey| first1=David| last2=Borwein|authorlink2=Jonathan_Borwein| first2=Jonathan| journal=American Mathematical Monthly|volume = 119| issue=8| pages=646\u2013657| date=2012| url=http://crd-legacy.lbl.gov/~dhbailey/dhbpapers/india-sqrt.pdf| access-date=2017-09-14}}</ref> The original presentation, using modern notation, is as follows: To calculate <math>\\sqrt{S}</math>, let ''x''<sub>0</sub><sup>2</sup> be the initial approximation to ''S''. Then, successively iterate as:\n:<math>a_n = \\frac{S - x_n^2}{2 x_n},</math>\n\n:<math>b_n = x_n + a_n,</math>\n\n:<math>x_{n+1} = b_n - \\frac{a_n^2}{2 b_n}.</math>\n\nWritten explicitly, it becomes \n:<math> x_{n+1} = (x_n + a_n) - \\frac{a_n^2}{2 (x_n + a_n)}. </math>\n\nLet ''x''<sub>0</sub> = ''N'' be an integer which is the nearest perfect square to ''S''. Also, let the difference ''d'' = ''S'' - ''N''<sup>2</sup>, then the first iteration can be written as:\n:<math>\\sqrt{S} \\approx N + \\frac{d}{2N} - \\frac{d^2}{8N^3 + 4Nd} = \\frac{8N^4 + 8N^2 d + d^2}{8N^3 + 4Nd} = \\frac{N^4 + 6N^2S + S^2}{4N^3 + 4NS} = \\frac{N^2(N^2 + 6S) + S^2}{4N(N^2 + S)}.</math>\n\nThis gives a rational approximation to the square root.\n\n===Example===\nUsing the same example as given in Babylonian method, let <math>S = 125348.</math> Then, the first iterations gives\n\n:<math> x_0 = 600 </math>\n\n:<math> a_1 = \\frac{125348 - 600^2}{2 \\times 600} = -195.543 </math>\n\n:<math> b_1 = 600 + (-195.543) = 404.456 </math>\n\n:<math> x_1 = 404.456 - \\frac{(-195.543)^2}{2 \\times 404.456} = 357.186</math>\n\nLikewise the second iteration gives\n\n:<math> a_2 = \\frac{125348 - 357.186^2}{2 \\times 357.186} = -3.126 </math>\n\n:<math> b_2 = 357.186 + (-3.126) = 354.06 </math>\n\n:<math> x_2 = 354.06 - \\frac{(-3.1269)^2}{2 \\times 354.06} = 354.046 </math>\n\n==Digit-by-digit calculation==\nThis is a method to find each digit of the square root in a sequence. It is slower than the Babylonian method, but it has several advantages:\n* It can be easier for manual calculations.\n* Every digit of the root found is known to be correct, i.e., it does not have to be changed later.\n* If the square root has an expansion that terminates, the algorithm terminates after the last digit is found. Thus, it can be used to check whether a given integer is a [[square number]].\n* The algorithm works for any [[numeral system|base]], and naturally, the way it proceeds depends on the base chosen.\n\n[[Napier's bones]] include an aid for the execution of this algorithm. The [[shifting nth root algorithm|shifting ''n''th root algorithm]] is a generalization of this method.\n\n===Basic principle===\nFirst, consider the case of finding the square root of a number ''Z'', that is the square of a two-digit number ''XY'', where ''X'' is the tens digit and ''Y'' is the units digit. Specifically:\n\nZ = (10X + Y)<sup>2</sup> = 100X<sup>2</sup> + 20XY + Y<sup>2</sup>\n\nNow using the Digit-by-Digit algorithm, we first determine the value of ''X''. ''X'' is the largest digit such that X<sup>2</sup> is less or equal to ''Z'' from which we removed the two rightmost digits.\n\nIn the next iteration, we pair the digits, multiply ''X'' by 2, and place it in the tenth's place while we try to figure out what the value of ''Y'' is.\n\nSince this is a simple case where the answer is a perfect square root ''XY'', the algorithm stops here.\n\nThe same idea can be extended to any arbitrary square root computation next. Suppose we are able to find the square root of ''N'' by expressing it as a sum of ''n'' positive numbers such that\n:<math>N = (a_1+a_2+a_3+\\dotsb+a_n)^2.</math>\n\nBy repeatedly applying the basic identity \n:<math>(x+y)^2 = x^2 +2xy + y^2,</math> \nthe right-hand-side term can be expanded as\n:<math>\n\\begin{align}\n& (a_1+a_2+a_3+ \\dotsb +a_n)^2 \\\\\n=& \\, a_1^2 + 2a_1a_2 + a_2^2 + 2(a_1+a_2) a_3 + a_3^2 + \\dotsb + a_{n-1}^2 + 2  \\left(\\sum_{i=1}^{n-1} a_i\\right) a_n + a_n^2 \\\\\n=& \\, a_1^2 + [2a_1 + a_2] a_2 + [2(a_1+a_2) + a_3] a_3 + \\dotsb + \\left[2 \\left(\\sum_{i=1}^{n-1} a_i\\right) + a_n\\right] a_n.\n\\end{align}\n</math>\n\nThis expression allows us to find the square root by sequentially guessing the values of <math>a_i</math>s. Suppose that the numbers <math>a_1, \\ldots, a_{m-1}</math> have already been guessed, then the m-th term of the right-hand-side of above summation is given by <math>Y_{m} = [2 P_{m-1} + a_{m}]a_{m},</math> where <math>P_{m-1} = \\sum_{i=1}^{m-1} a_i</math> is the approximate square root found so far. Now each new guess <math>a_m</math> should satisfy the recursion \n:<math>X_{m} = X_{m-1} - Y_{m},</math> \nsuch that <math>X_m \\geq 0</math> for all <math>1\\leq m\\leq n,</math> with initialization <math>X_0 = N.</math> When <math>X_n = 0,</math> the exact square root has been found; if not, then the sum of <math>a_i</math>s gives a suitable approximation of the square root, with <math>X_n</math> being the approximation error.\n\nFor example, in the decimal number system we have \n:<math>N = (a_1 \\cdot 10^{n-1} + a_2 \\cdot 10^{n-2} + \\cdots + a_{n-1} \\cdot 10 + a_n)^2,</math> \nwhere <math>10^{n-i}</math> are place holders and the coefficients <math>a_i \\in \\{0,1,2,\\ldots,9\\}</math>. At any m-th stage of the square root calculation, the approximate root found so far, <math>P_{m-1}</math> and the summation term <math>Y_m</math> are given by \n:<math>P_{m-1} = \\sum_{i=1}^{m-1} a_i \\cdot 10^{n-i} = 10^{n-m+1} \\sum_{i=1}^{m-1} a_i \\cdot 10^{m-i-1},</math>\n:<math>Y_m = [2P_{m-1} + a_m \\cdot 10^{n-m}] a_m \\cdot 10^{n-m} = \\left[20 \\sum_{i=1}^{m-1} a_i \\cdot 10^{m-i-1} + a_m \\right] a_m \\cdot 10^{2(n-m)}.</math>\n\nHere since the place value of <math>Y_m</math> is an even power of 10, we only need to work with the pair of most significant digits of the remaining term <math>X_{m-1}</math> at any m-th stage. The section below codifies this procedure.\n\nIt is obvious that a similar method can be used to compute the square root in number systems other than the decimal number system. For instance, finding the digit-by-digit square root in the binary number system is quite efficient since the value of <math>a_i</math> is searched from a smaller set of binary digits {0,1}. This makes the computation faster since at each stage the value of <math>Y_m</math> is either <math>Y_m = 0</math> for <math>a_m = 0</math> or <math>Y_m = 2 P_{m-1} + 1</math> for <math>a_m = 1</math>. The fact that we have only two possible options for <math>a_m</math> also makes the process of deciding the value of <math>a_m</math> at m-th stage of calculation easier. This is because we only need to check if <math>Y_m \\leq X_{m-1}</math> for <math>a_m = 1.</math> If this condition is satisfied, then we take <math>a_m = 1</math>; if not then <math>a_m = 0.</math> Also, the fact that multiplication by 2 is done by left bit-shifts helps in the computation.\n\n===Decimal (base 10)===\nWrite the original number in decimal form. The numbers are written similar to the [[long division]] algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into pairs, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each pair of digits of the square.\n\nBeginning with the left-most pair of digits, do the following procedure for each pair:\n\n# Starting on the left, bring down the most significant (leftmost) pair of digits not yet used (if all the digits have been used, write \"00\") and write them to the right of the remainder from the previous step (on the first step, there will be no remainder). In other words, multiply the remainder by 100 and add the two digits. This will be the '''current value ''c'''''.\n# Find ''p'', ''y'' and ''x'', as follows:\n#* Let '''''p''''' be the '''part of the root found so far''', ignoring any decimal point. (For the first step, ''p'' = 0.)\n#* Determine the greatest digit '''''x''''' such that <math>x(20p + x) \\le c</math>. We will use a new variable '''''y''''' = ''x''(20''p'' + ''x'').\n#** Note: 20''p'' + ''x'' is simply twice ''p'', with the digit ''x'' appended to the right.\n#** Note: ''x'' can be found by guessing what ''c''/(20\u00b7''p'') is and doing a trial calculation of ''y'', then adjusting ''x'' upward or downward as necessary.\n#* Place the digit <math>x</math> as the next digit of the root, i.e., above the two digits of the square you just brought down. Thus the next ''p'' will be the old ''p'' times 10 plus ''x''.\n# Subtract ''y'' from ''c'' to form a new remainder.\n# If the remainder is zero and there are no more digits to bring down, then the algorithm has terminated. Otherwise go back to step 1 for another iteration.\n\n====Examples====\n'''Find the square root of 152.2756.'''\n\n         <u>  1  2. 3  4 </u>\n        /\n      \\/  01 52.27 56\n \n          01                   1*1 <= 1 < 2*2                 x = 1\n         <u> 01 </u>                    y = x*x = 1*1 = 1\n          00 52                22*2 <= 52 < 23*3              x = 2\n         <u> 00 44 </u>                 y = (20+x)*x = 22*2 = 44\n             08 27             243*3 <= 827 < 244*4           x = 3\n            <u> 07 29 </u>              y = (240+x)*x = 243*3 = 729\n                98 56          2464*4 <= 9856 < 2465*5        x = 4\n               <u> 98 56 </u>           y = (2460+x)*x = 2464*4 = 9856\n                00 00          Algorithm terminates: Answer is 12.34\n\n===Binary numeral system (base 2)===\nInherent to digit-by-digit algorithms is a search and test step:  find a digit, <math>\\, e</math>, when added to the right of a current solution <math>\\, r</math>, such that <math>\\,(r+e)\\cdot(r+e) \\le x</math>, where <math>\\, x</math> is the value for which a root is desired. Expanding: <math>\\,r\\cdot r + 2re + e\\cdot e \\le x</math>.  The current value of <math>\\,r\\cdot r</math>\u2014or, usually, the remainder\u2014can be incrementally updated efficiently when working in binary, as the value of <math>\\, e</math> will have a single bit set (a power of 2), and the operations needed to compute <math>\\,2\\cdot r\\cdot e</math> and <math>\\,e\\cdot e</math> can be replaced with faster [[Bitwise operation#Bit shifts|bit shift]] operations.\n\n====Example====\nHere we obtain the square root of 81, which when converted into binary gives 1010001. The numbers in the left column gives the option between that number or zero to be used for subtraction at that stage of computation. The final answer is 1001, which in decimal is 9.\n\n              1 0 0 1\n             ---------\n            \u221a 1010001\n \n       1      1\n              1\n             ---------\n       101     01  \n                0\n              --------\n       1001     100\n                  0\n              --------\n       10001    10001\n                10001\n               -------\n                    0\n\nThis gives rise to simple computer implementations:<ref>[https://web.archive.org/web/20120306040058/http://medialab.freaknet.org/martin/src/sqrt/sqrt.c  Fast integer square root by Mr. Woo's abacus algorithm (archived)]</ref>\n\n<syntaxhighlight lang=\"c\">\nshort isqrt(short num) {\n    short res = 0;\n    short bit = 1 << 14; // The second-to-top bit is set: 1 << 30 for 32 bits\n \n    // \"bit\" starts at the highest power of four <= the argument.\n    while (bit > num)\n        bit >>= 2;\n        \n    while (bit != 0) {\n        if (num >= res + bit) {\n            num -= res + bit;\n            res = (res >> 1) + bit;\n        }\n        else\n            res >>= 1;\n        bit >>= 2;\n    }\n    return res;\n}\n</syntaxhighlight>\n\nUsing the notation above, the variable \"bit\" corresponds to <math>e_m^2</math> which is <math>(2^m)^2=4^m</math>, the variable \"res\" is equal to <math>2re_m</math>, and the variable \"num\" is equal to the current <math>X_m</math> which is the difference of the number we want the square root of and the square of our current approximation with all bits set up to <math>2^{m+1}</math>. Thus in the first loop, we want to find the highest power of 4 in \"bit\" to find the highest power of 2 in <math>e</math>. In the second loop, if num is greater than res + bit, then <math>X_m</math> is greater than <math>2re_m+e_m^2</math> and we can subtract it. The next line, we want to add <math>e_m</math> to <math>r</math> which means we want to add <math>2e^2_m</math> to <math>2re_m</math> so we want <code>res = res + bit<<1</code>. Then update <math>e_m</math> to <math>e_{m-1}</math> inside res which involves dividing by 2 or another shift to the right. Combining these 2 into one line leads to <code>res = res>>1 + bit</code>. If <math>X_m</math> isn't greater than <math>2re_m+e_m^2</math> then we just update <math>e_m</math> to <math>e_{m-1}</math> inside res and divide it by 2. Then we update <math>e_m</math> to <math>e_{m-1}</math> in bit by dividing it by 4. The final iteration of the 2nd loop has bit equal to 1 and will cause update of <math>e</math> to run one extra time removing the factor of 2 from res making it our integer approximation of the root.\n\nFaster algorithms, in binary and decimal or any other base, can be realized by using lookup tables\u2014in effect trading [[space\u2013time tradeoff|more storage space for reduced run time]].<ref>[http://atoms.alife.co.uk/sqrt/SquareRoot.java Integer Square Root function]</ref>\n\n==Exponential identity==\n[[calculator|Pocket calculator]]s typically implement good routines to compute the [[exponential function]] and the [[natural logarithm]], and then compute the square root of ''S'' using the identity found using the properties of logarithms (<math>\\ln x^n = n \\ln x</math>) and exponentials (<math>e^{\\ln x} = x</math>):\n\n:<math>\\sqrt{S} = e^{\\frac{1}{2}\\ln S}.</math>\nThe denominator in the fraction corresponds to the ''n''th root. In the case above the denominator is 2, hence the equation specifies that the square root is to be found. The same identity is used when computing square roots with [[logarithm table]]s or [[slide rule]]s.\n\n==A two-variable iterative method==\nThis method is applicable for finding the square root of <math>0 < S < 3 \\,\\!</math> and converges best for <math>S \\approx 1</math>.\nThis, however, is no real limitation for a computer based calculation, as in base 2 floating point and fixed point representations, it is trivial to multiply <math>S \\,\\!</math> by an integer power of 4, and therefore <math>\\sqrt{S}</math> by the corresponding power of 2, by changing the exponent or by shifting, respectively. Therefore, <math>S \\,\\!</math> can be moved to the range <math>\\frac{1}{2} \\le S <2</math>. Moreover, the following method does not employ general divisions, but only additions, subtractions, multiplications, and divisions by powers of two, which are again trivial to implement. A disadvantage of the method is that numerical errors accumulate, in contrast to single variable iterative methods such as the Babylonian one.\n\nThe initialization step of this method is\n:<math>a_0 = S \\,\\!</math>\n:<math>c_0 = S-1 \\,\\!</math>\nwhile the iterative steps read\n:<math>a_{n+1} = a_n - a_n c_n / 2 \\,\\!</math>\n:<math>c_{n+1} = c_n^2 (c_n - 3) / 4 \\,\\!</math>\nThen, <math>a_n \\rightarrow \\sqrt{S}</math> (while <math>c_n \\rightarrow 0</math>).\n\nNote that the convergence of <math>c_n \\,\\!</math>, and therefore also of <math>a_n \\,\\!</math>, is quadratic.\n\nThe proof of the method is rather easy. First, rewrite the iterative definition of <math>c_n \\,\\!</math> as\n:<math>1 + c_{n+1} = (1 + c_n) (1 - c_n/2)^2  \\,\\!</math>.\nThen it is straightforward to prove by induction that\n:<math>S (1 + c_n) = a_n^2</math>\nand therefore the convergence of <math>a_n \\,\\!</math> to the desired result <math>\\sqrt{S}</math> is ensured by the convergence of <math>c_n \\,\\!</math> to 0, which in turn follows from <math>-1 < c_0 < 2 \\,\\!</math>.\n\nThis method was developed around 1950 by [[Maurice Wilkes|M. V. Wilkes]], [[David Wheeler (computer scientist)|D. J. Wheeler]] and [[Stanley Gill|S. Gill]]<ref>M. V. Wilkes, D. J. Wheeler and S. Gill, \"The Preparation of Programs for an Electronic Digital Computer\", Addison-Wesley, 1951.</ref> for use on [[Electronic Delay Storage Automatic Calculator|EDSAC]], one of the first electronic computers.<ref>M. Campbell-Kelly, \"Origin of Computing\", Scientific American, September 2009.</ref> The method was later generalized, allowing the computation of non-square roots.<ref>J. C. Gower, \"A Note on an Iterative Method for Root Extraction\", The Computer Journal 1(3):142&ndash;143, 1958.</ref>\n\n==Iterative methods for reciprocal square roots==\nThe following are iterative methods for finding the reciprocal square root of ''S'' which is <math>1/\\sqrt{S}</math>. Once it has been found, find <math>\\sqrt{S}</math> by simple multiplication: <math>\\sqrt{S} = S \\cdot (1/\\sqrt{S})</math>. These iterations involve only multiplication, and not division. They are therefore faster than the [[Methods of computing square roots#Babylonian method|Babylonian method]]. However, they are not stable. If the initial value is not close to the reciprocal square root, the iterations will diverge away from it rather than converge to it. It can therefore be advantageous to perform an iteration of the Babylonian method on a rough estimate before starting to apply these methods.\n\n*Applying [[Newton's method]] to the equation <math>(1/x^2) - S = 0</math> produces a method that converges quadratically using three multiplications per step:\n:<math>x_{n+1} = \\frac{x_n}{2} \\cdot (3 - S \\cdot x_n^2).</math>\n\n*Another iteration is obtained by [[Halley's method]], which is the [[Householder's method]] of order two. This [[rate of convergence|converges cubically]], but involves four multiplications per iteration:{{cn|date=August 2019}}\n:<math>y_n = S \\cdot x_n^2</math>, and\n:<math>x_{n+1} = \\frac{x_n}{8} \\cdot (15 - y_n \\cdot (10 - 3 \\cdot y_n))</math>.\n\n===Goldschmidt\u2019s algorithm===\nSome computers use Goldschmidt's algorithm to simultaneously calculate\n<math>\\sqrt{S}</math> and\n<math>1/\\sqrt{S}</math>.\nGoldschmidt's algorithm finds <math>\\sqrt{S}</math> faster than Newton-Raphson iteration on a computer with a [[fused multiply\u2013add]] instruction and either a pipelined floating point unit or two independent floating-point units.<ref name=\"goldschmidt_algo\">\n{{cite conference |citeseerx=10.1.1.85.9648 |title=Software Division and Square Root Using Goldschmidt's Algorithms |first=Peter |last=Markstein |date=November 2004 |conference=6th Conference on Real Numbers and Computers |location=[[Dagstuhl]], Germany |url=http://www.informatik.uni-trier.de/Reports/TR-08-2004/rnc6_12_markstein.pdf |conference-url=http://cca-net.de/rnc6/}}\n</ref>\n\nThe first way of writing Goldschmidt's algorithm begins\n: <math>b_0 = S</math>\n: <math>Y_0 \\approx 1/\\sqrt{S}</math> (typically using a table lookup)\n: <math>y_0 = Y_0</math>\n: <math>x_0 = S y_0</math>\nand iterates\n:<math>b_{n+1} = b_n Y_n^2</math>\n:<math>Y_{n+1} = (3 - b_{n+1})/2</math>\n:<math>x_{n+1} = x_n Y_{n+1}</math>\n:<math>y_{n+1} = y_n Y_{n+1}</math>\nuntil <math>b_i</math> is sufficiently close to 1, or a fixed number of iterations.  The iterations converge to\n:<math>\\lim_{n \\to \\infty} x_n = \\sqrt S</math>, and\n:<math>\\lim_{n \\to \\infty} y_n = 1/\\sqrt S</math>.\nNote that it is possible to omit either <math>x_n</math> and <math>y_n</math> from the computation, and if both are desired then <math>x_n = S y_n</math> may be used at the end rather than computing it through in each iteration.\n\nA second form, using [[fused multiply-add]] operations, begins\n: <math>y_0 \\approx 1/\\sqrt{S}</math> (typically using a table lookup)\n: <math>x_0 = S y_0</math>\n: <math>h_0 = y_0/2</math>\nand iterates\n:<math>r_n = 0.5 - x_n h_n</math>\n:<math>x_{n+1} = x_n + x_n r_n</math>\n:<math>h_{n+1} = h_n + h_n r_n</math>\nuntil <math>r_i</math> is sufficiently close to 0, or a fixed number of iterations.  This converges to\n:<math>\\lim_{n \\to \\infty} x_n = \\sqrt S</math>, and\n:<math>\\lim_{n \\to \\infty} 2h_n = 1/\\sqrt S</math>.\n\n==Taylor series==\nIf ''N'' is an approximation to <math>\\sqrt{S}</math>, a better approximation can be found by using the [[Taylor series]] of the [[square root]] function:\n\n:<math>\\sqrt{N^2+d} = N\\sum_{n=0}^\\infty \\frac{(-1)^{n}(2n)!}{(1-2n)n!^2 4^n}\\frac{d^n}{N^{2n}} = N\\left(1 + \\frac{d}{2N^2} - \\frac{d^2}{8N^4} + \\frac{d^3}{16N^6} - \\frac{5d^4}{128N^8} + \\cdots\\right)</math>\n\nAs an iterative method, the [[order of convergence]] is equal to the number of terms used. With two terms, it is identical to the [[Methods of computing square roots#Babylonian method|Babylonian method]]. With three terms, each iteration takes almost as many operations as the [[Methods of computing square roots#Bakhshali method |Bakhshali approximation]], but converges more slowly.{{Citation needed|date=September 2017}} Therefore, this is not a particularly efficient way of calculation. To maximize the rate of convergence, choose ''N'' so that <math> \\frac{|d|}{N^2} \\,</math> is as small as possible.\n\n==Continued fraction expansion==\n{{see also|solving quadratic equations with continued fractions}}\n\n[[Quadratic irrational]]s (numbers of the form <math>\\frac{a+\\sqrt{b}}{c}</math>, where ''a'', ''b'' and ''c'' are integers), and in particular, square roots of integers, have [[periodic continued fraction]]s. Sometimes what is desired is finding not the numerical value of a square root, but rather its [[continued fraction]] expansion, and hence its rational approximation. Let ''S'' be the positive number for which we are required to find the square root. Then assuming ''a'' to be a number that serves as an initial guess and ''r'' to be the remainder term, we can write <math>S = a^2 + r. </math> Since we have <math>S - a^2 = (\\sqrt{S} + a)(\\sqrt{S} - a) = r</math>, we can express the square root of ''S'' as\n:<math> \\sqrt{S} = a + \\frac{r}{a + \\sqrt{S}}. </math>\n\nBy applying this expression for <math>\\sqrt{S}</math> to the denominator term of the fraction, we have\n:<math> \\sqrt{S} = a + \\frac{r}{a + (a + \\frac{r}{a + \\sqrt{S}})} = a + \\frac{r}{2a + \\frac{r}{a + \\sqrt{S}}}. </math>\n<div style=\"float:right; padding:1em; margin:0 0 0 1em; width:450px; border:1px solid; background:#faf3ee;\">\n<center>'''<big>Compact notation</big>'''</center>\nThe numerator/denominator expansion for continued fractions (see left) is cumbersome to write as well as to embed in  text formatting systems.  Therefore, special notation has been developed to compactly represent the integer and repeating parts of continued fractions.  One such convention is use of a lexical  \"dog leg\" to represent the vinculum between numerator and denominator, which allows the fraction to be expanded horizontally instead of vertically:\n:<math> \\sqrt{S} = a + \\frac{r |}{| 2a} +\\frac{r |}{| 2a} + \\frac{r |}{| 2a} + \\cdots </math>\nHere, each vinculum is represented by three line segments, two vertical and one horizontal, separating <math>r</math> from <math>2a</math>.\n\nAn even more compact notation which omits lexical devices takes a special form:\n:<math>[a;2a,2a,2a,...]</math>\n\nFor repeating continued fractions (which all square roots do), the repetend is represented only once, with an overline to signify a non-terminating repetition of the overlined part:\n:<math>[a;\\overline{2a}]</math>\n\nFor {{sqrt|2}}, the value of <math>a</math> is 1, so its representation is:\n:<math>[1;\\overline{2}]</math>\n</div>\n\nProceeding this way, we get a [[generalized continued fraction]] for the square root as\n<math> \\sqrt{S} = a + \\cfrac{r}{2a + \\cfrac{r}{2a + \\cfrac{r}{2a + \\ddots}}}</math> \n\nThe first step to evaluating such a fraction to obtain a root is to do numerical substitutions for the root of the number desired, and number of denominators selected. For example, in canonical form, <math>r</math> is 1 and for {{sqrt|2}}, <math>a</math> is 1, so the numerical continued fraction for 3 denominators is: \n:<math> \\sqrt{2} \\approx 1 + \\cfrac{1}{2 + \\cfrac{1}{2 + \\cfrac{1}{2}}}</math>\n\nStep 2 is to reduce the continued fraction from the bottom up, one denominator at a time, to yield a rational fraction whose numerator and denominator are integers. The reduction proceeds thus (taking the first three denominators):\n:<math> 1 + \\cfrac{1}{2 + \\cfrac{1}{2 + \\cfrac{1}{2}}} = 1 + \\cfrac{1}{2 + \\cfrac{1}{\\frac{5}{2}}}</math>\n::<math> = 1 + \\cfrac{1}{2 + \\cfrac{2}{5}} = 1 + \\cfrac{1}{\\frac{12}{5}}</math>\n::<math> = 1 + \\cfrac{5}{12} = \\frac{17}{12}</math>\n\nFinally (step 3), divide the numerator by the denominator of the rational fraction to obtain the approximate value of the root:\n:<math>17 \\div 12 = 1.42</math> rounded to three digits of precision.\n\nThe actual value of {{sqrt|2}} is 1.41 to three significant digits. The relative error is 0.17%, so the rational fraction is good to almost three digits of precision.  Taking more denominators gives successively better approximations: four denominators yields the fraction <math>\\frac{41}{29} = 1.4137</math>, good to almost 4 digits of precision, etc.\n\nUsually, the continued fraction for a given square root is looked up rather than expanded in place because it's tedious to expand it. Continued fractions are available for at least square roots of small integers and common constants.  For an arbitrary decimal number, precomputed sources are likely to be useless.  The following is a table of small rational fractions called ''convergents'' reduced from canonical continued fractions for the square roots of a few common constants:\n{| class=\"wikitable\"\n|-\n! {{sqrt|S}}\n! cont. fraction\n! ~decimal\n! serial convergents\n|-\n| {{sqrt|2}}\n| style=\"text-align: center;\" | <math>[1;\\overline {2}]</math>\n| 1.41421\n| <math>\\frac{3}{2}, \\frac{7}{5}, \\frac{17}{12}, \\frac{41}{29}, \\frac{99}{70}, \\frac{140}{99}</math>\n|-\n| {{sqrt|3}}\n| style=\"text-align: center;\" | <math>[1;\\overline {1,2}]</math>\n| 1.73205\n| <math>\\frac{2}{1}, \\frac{5}{3}, \\frac{7}{4}, \\frac{19}{11}, \\frac{26}{15}, \\frac{71}{41}, \\frac{97}{56}</math>\n|-\n| {{sqrt|5}}\n| style=\"text-align: center;\" | <math>[2;\\overline {4}]</math>\n| 2.23607\n| <math>\\frac{9}{4}, \\frac{38}{17}, \\frac{161}{72}</math>\n|-\n| {{sqrt|6}}\n| style=\"text-align: center;\" | <math>[2;\\overline {2,4}]</math>\n| 2.44949\n| <math>\\frac{5}{2}, \\frac{22}{9}, \\frac{49}{20}, \\frac{218}{89}</math>\n|-\n| {{sqrt|10}}\n| style=\"text-align: center;\" | <math>[3;\\overline {6}]</math>\n| 3.16228\n| <math>\\frac{19}{6}, \\frac{117}{37}</math>\n|-\n| <math>\\sqrt{\\pi}</math>\n| style=\"text-align: center;\" | <math>[1;1,3,2,1,1,6...]</math>\n| 1.77245\n| <math>\\frac{2}{1}, \\frac{7}{4},\\frac{16}{9},\\frac{23}{13},\\frac{39}{22}</math>\n|-\n| <math>\\sqrt{e}</math>\n| style=\"text-align: center;\" | <math>[1;1,1,1,5,1,1...]</math>\n| 1.64872\n| <math>\\frac{2}{1}, \\frac{3}{2},\\frac{8}{5},\\frac{28}{17},\\frac{33}{20},\\frac{61}{37}</math>\n|-\n| <math>\\sqrt{\\phi}</math>\n| style=\"text-align: center;\" | <math>[1;3,1,2,11,3,7...]</math>\n| 1.27202\n| <math>\\frac{4}{3}, \\frac{5}{4},\\frac{14}{11}</math>\n|}\n{{small|Note: all convergents up to and including denominator 99 listed.}}\n\nIn general, the larger the denominator of a rational fraction, the better the approximation.  It can also be shown that truncating a continued fraction yields a rational fraction that is the best approximation to the root of any fraction with denominator less than or equal to the denominator of that fraction - e.g., no fraction with a denominator less than or equal to 99 is as good an approximation to {{sqrt|2}} as 140/99.\n\n== Lucas sequence method ==\nthe [[Lucas sequence]] of the first kind ''U<sub>n</sub>''(''P'',''Q'') is defined by the [[Recurrence relation|recurrence relations]]:<blockquote><math>    U_n(P, Q)= \\begin{cases} 0 & \\text{if }n = 0 \\\\ 1 & \\text{if }n = 1 \\\\ P \\cdot U_{n -1}(P, Q) -Q \\cdot U_{n -2}(P, Q) & \\text{Otherwise} \\end{cases}\n</math></blockquote>and the characteristic equation of it is:<blockquote><math>    x^2 -P \\cdot x +Q  = 0\n</math></blockquote>it has the [[discriminant]] <math>    D = P^2 -4Q\n</math> and the roots:<blockquote><math>    \\begin{matrix} x_1 = \\frac{P +\\sqrt{D}}{2}, & x_2 = \\frac{P -\\sqrt{D}}{2}\\end{matrix}\n</math></blockquote>all that yield the following positive value:<blockquote><math>    \\lim_{n \\to \\infty} {\\frac{U_{n +1}}{U_n}} = x_1\n</math></blockquote>so when we want <math>    \\sqrt{a}\n</math>, we can choose <math>    P = 2\n</math> and <math>    Q = 1 -a\n</math>, and then calculate <math>    x_1 = 1 +\\sqrt{a}\n</math> using  <math>    U_{n +1}\n</math> and <math>    U_n\n</math>for large value of <math>    n\n</math>.\n\nThe most effective way to calculate <math>    U_{n +1}\n</math> and <math>    U_n\n</math>is:<blockquote><math>    \\begin{bmatrix} U_n \\\\ U_{n +1} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -Q & P \\end{bmatrix} \\cdot \\begin{bmatrix} U_{n -1} \\\\ U_n \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -Q & P \\end{bmatrix}^n \\cdot \\begin{bmatrix} U_0 \\\\ U_1 \\end{bmatrix}\n</math></blockquote>'''Summary:'''<blockquote><math>    \\begin{bmatrix} 0 & 1 \\\\ a -1 & 2 \\end{bmatrix}^n \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} U_n \\\\ U_{n +1} \\end{bmatrix}\n</math></blockquote>then when <math>    n \\to \\infty\n</math>:<blockquote><math>    \\sqrt{a} = \\frac{U_{n +1}}{U_n} -1\n</math></blockquote>\n\n==Approximations that depend on the floating point representation==\n<!--\n\n    This section needs a diagram or something similar,\n    to make it more accessible to non-techies.\n\n-->\nA number is represented in a [[floating point]] format as <math>m\\times b^p</math> which is also called [[scientific notation]]. Its square root is <math>\\sqrt{m}\\times b^{p/2}</math> and similar formulae would apply for cube roots and logarithms. On the face of it, this is no improvement in simplicity, but suppose that only an approximation is required: then just <math>b^{p/2}</math> is good to an order of magnitude. Next, recognise that some powers, ''p'', will be odd, thus for 3141.59 = 3.14159 &times; 10<sup>3</sup> rather than deal with fractional powers of the base, multiply the mantissa by the base and subtract one from the power to make it even. The adjusted representation will become the equivalent of 31.4159 &times; 10<sup>2</sup> so that the square root will be {{radic|31.4159}}  &times; 10.\n\nIf the integer part of the adjusted mantissa is taken, there can only be the values 1 to 99, and that could be used as an index into a table of 99 pre-computed square roots to complete the estimate. A computer using base sixteen would require a larger table, but one using base two would require only three entries: the possible bits of the integer part of the adjusted mantissa are 01 (the power being even so there was no shift, remembering that a [[Normalized number|normalised]] floating point number always has a non-zero high-order digit) or if the power was odd, 10 or 11, these being the first ''two'' bits of the original mantissa. Thus, 6.25 = 110.01 in binary, normalised to 1.1001 &times; 2<sup>2</sup> an even power so the paired bits of the mantissa are 01, while .625 = 0.101 in binary normalises to 1.01 &times;  2<sup>\u22121</sup> an odd power so the adjustment is to 10.1 &times; 2<sup>\u22122</sup> and the paired bits are 10. Notice that the low order bit of the power is echoed in the high order bit of the pairwise mantissa. An even power has its low-order bit zero and the adjusted mantissa will start with 0, whereas for an odd power that bit is one and the adjusted mantissa will start with 1. Thus, when the power is halved, it is as if its low order bit is shifted out to become the first bit of the pairwise mantissa.\n\nA table with only three entries could be enlarged by incorporating additional bits of the mantissa. However, with computers, rather than calculate an interpolation into a table, it is often better to find some simpler calculation giving equivalent results. Everything now depends on the exact details of the format of the representation, plus what operations are available to access and manipulate the parts of the number. For example, [[Fortran]] offers an <code>EXPONENT(x)</code> function to obtain the power. Effort expended in devising a good initial approximation is to be recouped by thereby avoiding the additional iterations of the refinement process that would have been needed for a poor approximation. Since these are few (one iteration requires a divide, an add, and a halving) the constraint is severe.\n\nMany computers follow the [[IEEE floating-point standard|IEEE]] (or sufficiently similar) representation, and a very rapid approximation to the square root can be obtained for starting Newton's method. The technique that follows is based on the fact that the floating point format (in base two) approximates the base-2 logarithm. That is <math>\\log_2(m\\times 2^p) = p + \\log_2(m)</math>\n\nSo for a 32-bit single precision floating point number in IEEE format (where notably, the power has a [[Exponent bias|bias]] of 127 added for the represented form) you can get the approximate logarithm by interpreting its binary representation as a 32-bit integer, scaling it by <math>2^{-23}</math>, and removing a bias of 127, i.e.\n:<math>x_\\text{int} \\cdot 2^{-23} - 127 \\approx \\log_2(x).</math>\n\nFor example, 1.0 is represented by a [[hexadecimal]] number 0x3F800000, which would represent <math>1065353216 = 127 \\cdot 2^{23}</math> if taken as an integer. Using the formula above you get <math>1065353216 \\cdot 2^{-23} - 127 = 0</math>, as expected from <math>\\log_2(1.0)</math>. In a similar fashion you get 0.5 from 1.5 (0x3FC00000).\n\n[[Image:Log2approx.png]]\n\nTo get the square root, divide the logarithm by 2 and convert the value back. The following program demonstrates the idea. Note that the exponent's lowest bit is intentionally allowed to propagate into the mantissa.  One way to justify the steps in this program is to assume <math>b</math> is the exponent bias and <math>n</math> is the number of explicitly stored bits in the mantissa and then show that\n:<math>(((x_\\text{int} / 2^n - b) / 2) + b) \\cdot 2^n = (x_\\text{int} - 2^n) / 2 + ((b + 1) / 2) \\cdot 2^n.</math>\n<br>\n<syntaxhighlight lang=\"c\">\n\n/* Assumes that float is in the IEEE 754 single precision floating point format\n * and that int is 32 bits. */\nfloat sqrt_approx(float z) {\n    int val_int = *(int*)&z; /* Same bits, but as an int */\n    /*\n     * To justify the following code, prove that\n     *\n     * ((((val_int / 2^m) - b) / 2) + b) * 2^m = ((val_int - 2^m) / 2) + ((b + 1) / 2) * 2^m)\n     *\n     * where\n     *\n     * b = exponent bias\n     * m = number of mantissa bits\n     *\n     * .\n     */\n\n    val_int -= 1 << 23; /* Subtract 2^m. */\n    val_int >>= 1; /* Divide by 2. */\n    val_int += 1 << 29; /* Add ((b + 1) / 2) * 2^m. */\n\n    return *(float*)&val_int; /* Interpret again as float */\n}\n</syntaxhighlight>\n\nThe three mathematical operations forming the core of the above function can be expressed in a single line.  An additional adjustment can be added to reduce the maximum relative error.  So, the three operations, not including the cast, can be rewritten as\n<syntaxhighlight lang=\"c\">\nval_int = (1 << 29) + (val_int >> 1) - (1 << 22) + a;\n</syntaxhighlight>\n\nwhere ''a'' is a bias for adjusting the approximation errors. For example, with ''a'' = 0 the results are accurate for even powers of 2 (e.g., 1.0), but for other numbers the results will be slightly too big (e.g.,1.5 for 2.0 instead of 1.414... with 6% error). With ''a'' = -0x4B0D2, the maximum relative error is minimized to \u00b13.5%.\n\nIf the approximation is to be used for an initial guess for [[Newton's method]] to the equation <math>(1/x^2) - S = 0</math>, then the reciprocal form shown in the following section is preferred.\n\n===Reciprocal of the square root===\n{{Main|Fast inverse square root}}\nA variant of the above routine is included below, which can be used to compute the [[Multiplicative inverse|reciprocal]] of the square root, i.e., <math>x^{-{1\\over2}}</math> instead, was written by Greg Walsh. The integer-shift approximation produced a relative error of less than 4%, and the error dropped further to 0.15% with one iteration of [[Newton's method]] on the following line.<ref>[http://www.lomont.org/Math/Papers/2003/InvSqrt.pdf Fast Inverse Square Root] by Chris Lomont</ref> In computer graphics it is a very efficient way to normalize a vector.\n\n<syntaxhighlight lang=\"c\">\nfloat invSqrt(float x) {\n    float xhalf = 0.5f*x;\n    union {\n        float x;\n        int i;\n    } u;\n    u.x = x;\n    u.i = 0x5f375a86 - (u.i >> 1);\n    /* The next line can be repeated any number of times to increase accuracy */\n    u.x = u.x * (1.5f - xhalf * u.x * u.x);\n    return u.x;\n}\n</syntaxhighlight>\n\nSome VLSI hardware implements inverse square root using a second degree polynomial estimation followed by a [[Division algorithm#Goldschmidt division|Goldschmidt iteration]].<ref>\n[http://portal.acm.org/citation.cfm?id=627261 \"High-Speed Double-Precision Computation of Reciprocal, Division, Square Root and Inverse Square Root\"]\nby Jos\u00e9-Alejandro Pi\u00f1eiro and Javier D\u00edaz Bruguera 2002 (abstract)\n</ref>\n\n==Negative or complex square==\nIf ''S''&nbsp;<&nbsp;0, then its principal square root is\n:<math>\\sqrt {S} = \\sqrt {\\vert S \\vert} \\, \\, i \\,.</math>\n\nIf ''S''&nbsp;=&nbsp;''a''+''bi'' where ''a'' and ''b'' are real and ''b''&nbsp;\u2260&nbsp;0, then its principal square root is\n:<math>\\sqrt {S} = \\sqrt{\\frac{\\vert S \\vert + a}{2}} \\, + \\, \\sgn (b) \\sqrt{\\frac{\\vert S \\vert - a}{2}} \\, \\, i \\,.</math>\n\nThis can be verified by squaring the root.<ref>{{cite book\n|title=Handbook of mathematical functions with formulas, graphs, and mathematical tables\n|edition=\n|first1=Miltonn\n|last1=Abramowitz\n|first2=Irene A.\n|last2=Stegun\n|publisher=Courier Dover Publications\n|year=1964\n|isbn=978-0-486-61272-0\n|page=17\n|url=https://books.google.com/books?id=MtU8uP7XMvoC}}, [http://www.math.sfu.ca/~cbm/aands/page_17.htm Section 3.7.26, p. 17]\n</ref><ref>{{cite book\n|title=Classical algebra: its nature, origins, and uses\n|first1=Roger \n|last1=Cooke\n|publisher=John Wiley and Sons\n|year=2008\n|isbn=978-0-470-25952-8\n|page=59\n|url=https://books.google.com/books?id=lUcTsYopfhkC}}, [https://books.google.com/books?id=lUcTsYopfhkC&pg=PA59 Extract: page 59]\n</ref> Here\n:<math>\\vert S \\vert = \\sqrt{a^2 + b^2}</math>\n\nis the [[absolute value|modulus]] of ''S''. The principal square root of a [[complex number]] is defined to be the root with the non-negative real part.\n\n== See also ==\n* [[Alpha max plus beta min algorithm]]\n* [[nth root algorithm|''n''th root algorithm]]\n* [[Square root of 2]]\n\n== Notes ==\n{{Reflist|group=Note}}\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* {{MathWorld|title=Square root algorithms|urlname=SquareRootAlgorithms}}\n* [http://www.afjarvis.staff.shef.ac.uk/maths/jarvisspec02.pdf Square roots by subtraction]\n* [http://www.andrijar.com/algorithms/algorithms.htm#qusr Integer Square Root Algorithm by Andrija Radovi\u0107]\n* [http://www.hparchive.com/Journals/HPJ-1977-05.pdf Personal Calculator Algorithms I : Square Roots (William E. Egbert), Hewlett-Packard Journal (may 1977) : page 22]\n* [http://www.calculatorsquareroot.com Calculator to learn the square root]\n\n{{DEFAULTSORT:Methods Of Computing Square Roots}}\n[[Category:Root-finding algorithms]]\n[[Category:Computer arithmetic algorithms]]\n", "text_old": "{{short description|Algorithms for calculating square roots}}\n{{multiple issues|\n{{Original research|date=January 2012}}\n{{technical|date=September 2012}}\n{{more citations needed|date=July 2017}}\n{{very long|date=June 2019|rps=61}}\n}}\n\n'''Methods of computing square roots''' are [[numerical analysis]] [[algorithm]]s for finding the principal, or non-negative, [[square root]] (usually denoted {{sqrt|''S''}}, {{radic|''S''|2}}, or S<sup>1/2</sup>) of a real number. Arithmetically, it means given S, a procedure for finding a number which when multiplied by itself, yields S; algebraically, it means a procedure for finding the non-negative root of the equation x<sup>2</sup> - S = 0; geometrically, it means given the area of a square, a procedure for constructing a side of the square.\n\nEvery real number has two square roots.<ref group=Note>In addition to the principal square root, there is a negative square root equal in magnitude but opposite in sign to the principal square root, except for zero, which has double square roots of zero.</ref> The principal square root of most numbers is an irrational number or repeating decimal, so that any computed result is a finite approximation to the square root.  For some numbers, the square root is finitely representable in the base (i.e. decimal or binary); for these numbers, including integers known as [[perfect squares]], the square root is exact. However, even if the square root is exact, the procedure used to compute it may return an approximation. \n\nThe most common analytical methods are iterative and consist of two steps: finding a suitable starting value, followed by iterative refinement until some termination criteria is met.  The starting value can be any number, but fewer iterations will be required the closer it is to the final result.  The most familiar such method, most suited for programmatic calculation, is Newton's method, which is based on  a property of the derivative in the calculus. A few methods like paper-and-pencil synthetic division and series expansion, do not require a starting value. In some applications, an [[integer square root]] is required, which is the square root rounded or truncated to the nearest integer (a modified procedure may be employed in this case).\n\nThe method employed depends on what the result is to be used for (i.e. how accurate it has to be), how much effort one is willing to put into the procedure, and what tools are at hand.  The methods may be roughly classified as those suitable for mental calculation, those usually requiring at least paper and pencil, and those which are implemented as programs to be executed on a digital electronic computer or other computing device. Algorithms may take into account convergence (how many iterations are required to achieve a specified precision), computational complexity of individual operations (i.e. division) or iterations, and error propagation (the accuracy of the final result).\n\nProcedures for finding square roots (particularly the square root of 2) have been known since at least the period of ancient Babylon in the 17th century BCE.  Heron's method from first century Egypt was the first ascertainable algorithm for computing square root. Modern analytic methods began to be developed after introduction of the Arabic numeral system to western Europe in the early Renaissance.  Today, nearly all computing devices have a fast and accurate square root function, either as a programming language construct, a compiler intrinsic or library function, or as a hardware operator, based on one of the described procedures.\n\n==Initial estimate==\nMany iterative square root algorithms require an initial [[seed value]]. The seed must be a non-zero positive number; it should be between 1 and <math>S</math>, the number whose square root is desired, because the square root must be in that range. If the seed is far away from the root, the algorithm will require more iterations. If one initializes with ''x''<sub>0</sub> = 1 (or ''S''), then approximately <math> \\tfrac12 \\vert \\log_2 S \\vert </math> iterations will be wasted just getting the order of magnitude of the root. It is therefore useful to have a rough estimate, which may have limited accuracy but is easy to calculate. In general, the better the initial estimate, the faster the convergence.  For Newton's method (also called Babylonian or Heron's method), a seed somewhat larger than the root will converge slightly faster than a seed somewhat smaller than the root.\n\nIn general, an estimate is pursuant to an arbitrary interval known to contain the root (such as [x<sub>0</sub>, 1/x<sub>0</sub>]). The estimate is a specific value of a functional approximation to f(x) = {{sqrt|x}} over the interval. Obtaining a better estimate involves either obtaining tighter bounds on the interval, or finding a better functional approximation to f(x). The latter usually means using a higher order polynomial in the approximation, though not all approximations are polynomial. Common methods of estimating include scalar, linear, hyperbolic and logarithmic. A decimal base is usually used for mental or paper-and-pencil estimating. A binary base is more suitable for computer estimates.  In estimating, the exponent and mantissa are usually treated separately, as the number would be expressed in scientific notation.\n\n===Decimal estimates===\nTypically the number <math>S</math> is expressed in [[scientific notation]] as <math>a\\times10^{2n}</math> where <math> 1\\leq a<100</math> and ''n'' is an integer, and the range of possible square roots is <math>\\sqrt a\\times10^n</math> where <math>1\\leq \\sqrt a<10</math>. \n====Scalar estimates====\nScalar methods divide the range into intervals, and the estimate in each interval is represented by a single scalar number.  If the range is considered as a single interval, the arithmetic mean (5) or geometric mean (<math>\\sqrt{10}\\approx3.16</math>) times <math>10^n</math> are plausible estimates.  The absolute and relative error for these will differ.  In general, a single scalar will be very inaccurate.  Better estimates divide the range into two or more intervals, but scalar estimates have inherently low accuracy.\n\nFor two intervals, divided geometrically, the square root <math>\\sqrt{S} = \\sqrt{a}\\times10^n</math> can be estimated as<ref group=\"Note\">The factors two and six are used because they approximate the [[geometric mean]]s of the lowest and highest possible values with the given number of digits: <math>\\sqrt{\\sqrt{1} \\cdot \\sqrt{10}} = \\sqrt[4]{10} \\approx 1.78 \\,</math> and <math>\\sqrt{\\sqrt{10} \\cdot \\sqrt{100}} = \\sqrt[4]{1000} \\approx 5.62 \\,</math>.</ref>\n\n:<math> \\sqrt{S} \\approx \\begin{cases}\n2 \\cdot 10^n & \\text{if } a < 10, \\\\\n6 \\cdot 10^n & \\text{if } a \\geq 10.\n\\end{cases}</math>\n\nThis estimate has maximum absolute error of <math>4\\cdot10^n</math> at a = 100, and maximum relative error of 100% at a = 1.\n \nFor example, for <math> S = 125348</math> factored as <math>12.5348 \\times 10^4</math>, the estimate is <math> \\sqrt{S} \\approx 6 \\cdot 10^2 = 600</math>. <math>\\sqrt{125348} = 354.0</math>, an absolute error of 246 and relative error of almost 70%.\n====Linear estimates====\nA better estimate, and the standard method used, is a linear approximation to the function <math>y = x^2</math> over a small arc.  If, as above, powers of the base are factored out of the number <math>S</math> and the interval reduced to [1,100], a secant line spanning the arc, or a tangent line somewhere along the arc may be used as the approximation, but a least-squares regression line intersecting the arc will be more accurate.\n\nA least-squares regression line minimizes the average difference between the estimate and the value of the function.  Its equation is <math>y=8.7x-10</math>.  Reordering, <math>x=0.115y+1.15</math>.  Rounding the coefficients for ease of computation,\n:<math>\\sqrt{S}\\approx (a/10+1.2)\\cdot 10^n</math>\n\nThat is the best estimate ''on average'' that can be achieved with a single piece linear approximation of the function  y=x<sup>2</sup> in the interval [1,100].  It has a maximum absolute error of 1.2 at a=100, and maximum relative error of 30% at S=1 and 10.<ref group=Note>The unrounded estimate has maximum absolute error of 2.65 at 100 and maximum relative error of 26.5% at y=1, 10 and 100</ref> \n\nTo divide by 10, subtract one from the exponent of <math>a</math>, or figuratively move the decimal point one digit to the left. For this formulation, any additive constant 1 plus a small increment will make a satisfactory estimate so remembering the exact number isn't a burden.  The approximation (rounded or not) using a single line spanning the range [1,100] is less than one significant digit of precision; the relative error is greater than 1/2<sup>2</sup>, so less than 2 bits of information are provided. The accuracy is severely limited because the range is two orders of magnitude, quite large for this kind of estimation. \n\nA much better estimate can be obtained by a piece-wise linear approximation: multiple line segments, each approximating some subarc of the original.  The more line segments used, the better the approximation.  The most common way is to use tangent lines; the critical choices are how to divide the arc and where to place the tangent points. An efficacious way to divide the arc from y=1 to y=100 is geometrically: for two intervals, the bounds of the intervals are the square root of the bounds of the original interval, 1*100, i.e. [1,{{radic|100|2}}] and [{{radic|100|2}},100].  For three intervals, the bounds are the cube roots of 100: [1,{{radic|100|3}}], [{{radic|100|3}},({{radic|100|3}})<sup>2</sup>], and [({{radic|100|3}})<sup>2</sup>,100], etc.  For two intervals, {{radic|100|2}} = 10, a very convenient number.  Tangent lines are easy to derive, and are located at x = {{sqrt|1*{{sqrt|10}}}} and x = {{sqrt|10*{{sqrt|10}}}}.  Their equations are:  y = 3.56x - 3.16  and y = 11.2x - 31.6.  Inverting, the square roots are: x = 0.28y + 0.89 and x = .089y + 2.8. Thus for S = a * 10<sup>2n</sup>:\n\n:<math> \\sqrt{S} \\approx \\begin{cases}\n(0.28a + 0.89) \\cdot 10^n & \\text{if } a < 10, \\\\\n(.089a + 2.8) \\cdot 10^n & \\text{if } a \\geq 10.\n\\end{cases}</math>\n\nThe maximum absolute errors occur at the high points of the intervals, at a=10 and 100, and are 0.54 and 1.7 respectively. The maximum relative errors are at the endpoints of the intervals, at a=1, 10 and 100, and are 17% in both cases.  17% or 0.17 is larger than 1/10, so the method yields less than a decimal digit of accuracy.\n\n====Hyperbolic estimates====\nIn some cases, hyperbolic estimates may be efficacious, because a hyperbola is also a convex curve and may lie along an arc of Y = x<sup>2</sup> better than a line. Hyperbolic estimates are more computationally complex, because they necessarily require a floating division.  A near-optimal hyperbolic approximation to x<sup>2</sup> on the interval [1,100] is y=190/(10-x)-20.  Transposing, the square root is x = -190/(y+20)+10.  Thus for <math>S=a\\cdot10^{2n}</math>:\n\n:<math>\\sqrt{S}\\approx (\\frac{-190}{a+20}+10)\\cdot 10^n</math> \n\nThe floating division need be accurate to only one decimal digit, because the estimate overall is only that accurate, and can be done mentally.  A hyperbolic estimate is better on average than scalar or linear estimates. It has maximum absolute error of 1.58 at 100 and maximum relative error of 16.0% at 10.  For the worst case at a=10, the estimate is 3.67. If one starts with 10 and applies Newton-Raphson iterations straight away, two iterations will be required, yielding 3.66, before the accuracy of the hyperbolic estimate is exceeded.  For a more typical case like 75, the hyperbolic estimate is 8.00, and 5 Newton-Raphson iterations starting at 75 would be required to obtain a more accurate result.\n\n====Arithmetic estimates====\nA method analogous to piece-wise linear approximation but using only arithmetic instead of algebraic equations, uses the multiplication tables in reverse: the square root of a number between 1 and 100 is between 1 and 10, so if we know 25 is a perfect square (5x5), and 36 is a perfect square (6x6), then the square root of a number greater than or equal to 25 but less than 36, begins with a 5. Similarly for numbers between other squares. This method will yield a correct first digit, but it is not accurate to one digit: the first digit of the square root of 35 for example, is 5, but the square root of 35 is almost 6. \n\nA better way is to the divide the range into intervals half way between the squares.  So any number between 25 and half way to 36, which is 30.5, estimate 5; any number greater than 30.5 up to 36, estimate 6.<ref group=Note>If the number is exactly half way between two squares, like 30.5, guess the higher number which is 6 in this case</ref>  The procedure only requires a little arithmetic to find a boundary number in the middle of two products from the multiplication table. Here is a reference table of those boundaries:\n\n{| class=\"wikitable\"\n|-\n! <math>a</math>\n| nearest square\n! <math>k = \\sqrt{a}</math> est.\n|-\n| 1 - 2.5\n| style=\"text-align: center;\" |1 (1<sup>2</sup>)\n| 1\n|-\n| 2.5 - 6.5\n| style=\"text-align: center;\" |4 (2<sup>2</sup>)\n| 2\n|-\n| 6.5 - 12.5\n| style=\"text-align: center;\" |9 (3<sup>2</sup>)\n| 3\n|-\n| 12.5 - 20.5\n| style=\"text-align: center;\" |16 (4<sup>2</sup>)\n| 4\n|-\n| 20.5 - 30.5\n| style=\"text-align: center;\" |25 (5<sup>2</sup>)\n| 5\n|-\n| 30.5 - 42.5\n| style=\"text-align: center;\" |36 (6<sup>2</sup>)\n| 6\n|-\n| 42.5 - 56.5\n| style=\"text-align: center;\" |49 (7<sup>2</sup>)\n| 7\n|-\n| 56.5 - 72.5\n| style=\"text-align: center;\" | 64 (8<sup>2</sup>)\n| 8\n|-\n| 72.5 - 90.5\n| style=\"text-align: center;\" | 81 (9<sup>2</sup>)\n| 9\n|-\n| 90.5 - 100\n| style=\"text-align: center;\" | 100 (10<sup>2</sup>)\n| 10\n|}\n\nThe final operation is to multiply the estimate <math>k</math> by the power of ten divided by 2, so for <math>S = a\\cdot 10^{2n}</math>,\n:<math>\\sqrt{S}\\approx k\\cdot 10^n</math>\n\nThe method implicitly yields one significant digit of accuracy, since it rounds to the best first digit.\n\nThe method can be extended 3 significant digits in most cases, by interpolating between the nearest squares bounding the operand.  If <math>k^2 \\le a < (k+1)^2</math>, then <math>\\sqrt{a}</math> is approximately k plus a fraction, the difference between a and k<sup>2</sup> divided by the difference between the two squares:\n\n<math>\\sqrt{a}\\approx k + R</math> where <math>R = \\frac{(a-k^2)}{(k+1)^2-k^2}</math>\n\nThe final operation, as above, is to multiply the result by the power of ten divided by 2;\n:<math>\\sqrt{S} = \\sqrt{a}\\cdot 10^n \\approx (k + R)\\cdot 10^n</math>\n\n<math>k</math> is a decimal digit and <math>R</math> is a fraction that must be converted to decimal. It usually has only a single digit in the numerator, and one or two digits in the denominator, so the conversion to decimal can be done mentally.\n\nExample: find the square root of 75.  75 = 75 * 10<sup>2{{dot}}0</sup>, so <math>a</math> is 75 and <math>n</math> is 0. From the multiplication tables, the square root of the mantissa must be 8 point ''something'' because 8x8 is 64, but 9x9 is 81, too big, so <math>k</math> is 8; ''something'' is the decimal representation of <math>R</math>.  The fraction R is 75-k<sup>2</sup> = 11, the numerator, and 81-k<sup>2</sup> = 17, the denominator.  11/17 is a little less than 12/18, which is 2/3s or .67, so guess .66 (it's ok to guess here, the error is very small).  So the estimate is 8+.66=8.66. {{sqrt|75}} to three significant digits is 8.66, so the estmate is good to 3 significant digits.  Not all such estimates using this method will be so accurate, but they will be close.\n\n===Binary estimates===\nWhen working in the [[binary numeral system]] (as computers do internally), by expressing <math>S</math> as <math>a\\times2^{2n}</math> where <math> 0.1_2\\leq a<10_2</math>, the square root <math>\\sqrt{S} = \\sqrt{a}\\times2^n</math> can be estimated as \n:<math> \\sqrt{S} \\approx (0.485 + 0.485 \\cdot a) \\cdot 2^n</math>\n\nwhich is the least-squares regression line to 3 significant digit coefficients. {{sqrt|a}} has maximum absolute error of 0.0408 at a=2, and maximum relative error of 3.0% at a=1.  A computationally convenient rounded estimate (because the coefficients are powers of 2) is:\n:<math> \\sqrt{S} \\approx (0.5 + 0.5 \\cdot a) \\cdot 2^n</math><ref group=Note>This is incidentally the equation of the tangent line to y=x<sup>2</sup> at y=1.</ref>\nwhich has maximum absolute error of 0.086 at 2 and maximum relative error of 6.1% at <math>a</math>=0.5 and <math>a</math>=2.0.\n\nFor <math> S = 125348 = 1\\;1110\\;1001\\;1010\\;0100_2 = 1.1110\\;1001\\;1010\\;0100_2\\times2^{16}\\, </math> the binary approximation gives <math> \\sqrt{S} \\approx (0.5 + 0.5 \\cdot a) \\cdot 2^8 = 1.0111\\;0100\\;1101\\;0010_2 \\cdot 1\\;0000\\;0000_2 = 1.456 \\cdot 256 = 372.8</math>. <math>\\sqrt{125348}=354.0</math>, so the estimate has an absolute error of 19 and relative error of 5.3%.  The relative error is a little less than 1/2<sup>4</sup>, so the estimate is good to 4+ bits.\n\nAn estimate for <math>a</math> good to 8 bits can be obtained by table lookup on the high 8 bits of <math>a</math>, remembering that the high bit is implicit in most floating point representations, and the bottom bit of the 8 should be rounded. The table is 256 bytes of precomputed 8-bit square root values.  For example, for the index 11101101<sub>2</sub> representing 1.8515625<sub>10</sub>, the entry is 10101110<sub>2</sub> representing 1.359375<sub>10</sub>, the square root of 1.8515625<sub>10</sub> to 8 bit precision (2+ decimal digits).\n\n==Babylonian method==<!-- this section is linked from [[Babylonian method]] -->\n{{redirect|Heron's method|the formula used to find the area of a triangle|Heron's formula}}\n[[Image:Babylonian method.svg|300px|right|thumb|\nGraph charting the use of the Babylonian method for approximating a square root of 100 (\u00b110) using starting values\n<span style=\"color:#008\">''x''<sub>0</sub>&nbsp;=&nbsp;50</span>,\n<span style=\"color:#F00\">''x''<sub>0</sub>&nbsp;=&nbsp;1</span>,\nand <span style=\"color:#080\">''x''<sub>0</sub>&nbsp;=&nbsp;&minus;5</span>. Note that a positive starting value yields the positive root, and a negative starting value the negative root.]]\n\nPerhaps the first [[algorithm]] used for approximating <math>\\sqrt{S}</math> is known as the '''Babylonian method''', despite there being no direct evidence, beyond informed conjecture, that the eponymous [[Babylonian mathematics|Babylonian mathematicians]] employed exactly this method.<ref name=\"Fowler and Robson\">{{cite journal |last1=Fowler |first1=David |last2=Robson |first2=Eleanor |title=Square Root Approximations in Old Babylonian Mathematics: YBC 7289 in Context |journal=Historia Mathematica |volume=25 |date=1998 |issue=4 |page=376 |doi=10.1006/hmat.1998.2209 }}</ref> The method is also known as '''Heron's method''', after the first-century Greek mathematician [[Hero of Alexandria]] who gave the first explicit description of the method in his [[AD 60]] work ''[[Hero of Alexandria#Bibliography|Metrica]]''.<ref>{{cite book\n  | last = Heath\n  | first = Thomas\n  | authorlink = Thomas Little Heath\n  | title = A History of Greek Mathematics, Vol. 2\n  | publisher = Clarendon Press\n  | year = 1921\n  | location = Oxford\n  | pages = [https://archive.org/details/ahistorygreekma00heatgoog/page/n339 323]\u2013324\n  | url = https://archive.org/details/ahistorygreekma00heatgoog\n  | doi =\n  | id =\n  | isbn = }}</ref> The basic idea is that if {{mvar|x}} is an overestimate to the square root of a non-negative real number {{mvar|S}} then {{math|{{sfrac|''S''|''x''}}}} will be an underestimate, or vice versa, and so the average of these two numbers may reasonably be expected to provide a better approximation (though the formal proof of that assertion depends on the [[inequality of arithmetic and geometric means]] that shows this average is always an overestimate of the square root, as noted in the article on [[Square root#Geometric construction of the square root|square roots]], thus assuring convergence). This is equivalent to using [[Newton's method]] to solve <math> x^2 - S = 0 </math>.\n\nMore precisely, if {{mvar|x}} is our initial guess of <math>\\sqrt{S}</math> and {{mvar|e}} is the error in our estimate such that {{math|1=''S'' = (''x''+ ''e'')<sup>2</sup>}}, then we can expand the binomial and solve for \n:<math>e=\\frac{S-x^2}{2x+e} \\approx \\frac{S-x^2}{2x}, </math> since <math> e \\ll x </math>.\nTherefore, we can compensate for the error and update our old estimate as \n:<math>x + e \\approx x + \\frac{S-x^2}{2x} = \\frac{S+x^2}{2x} = \\frac{\\frac{S}{x}+x}{2} \\equiv x_\\text{revised}</math> \nSince the computed error was not exact, this becomes our next best guess. The process of updating is iterated until desired accuracy is obtained. This is a [[quadratic convergence|quadratically convergent]] algorithm, which means that the number of correct digits of the approximation roughly doubles with each iteration. It proceeds as follows:\n#Begin with an arbitrary positive starting value {{math|''x''<sub>0</sub>}} (the closer to the actual square root of {{mvar|S}}, the better).\n#Let {{math|''x''<sub>''n'' + 1</sub>}} be the average of {{math|''x''<sub>''n''</sub>}} and {{math|{{sfrac|''S''|''x''<sub>''n''</sub>}}}} (using the [[arithmetic mean]] to approximate the [[geometric mean]]).\n#Repeat step 2 until the desired accuracy is achieved.\nIt can also be represented as:\n:<math>x_0 \\approx \\sqrt{S},</math>\n:<math>x_{n+1} = \\frac{1}{2} \\left(x_n + \\frac{S}{x_n}\\right),</math>\n:<math>\\sqrt S = \\lim_{n \\to \\infty} x_n.</math>\n\nThis algorithm works equally well in the [[p-adic number|{{mvar|p}}-adic number]]s, but cannot be used to identify real square roots with {{mvar|p}}-adic square roots; one can, for example, construct a sequence of rational numbers by this method that converges to +3 in the reals, but to &minus;3 in the 2-adics.\n===Example===\nTo calculate {{math|{{sqrt|''S''}}}}, where {{mvar|S}} = 125348, to six significant figures, use the rough estimation method above to get\n\n:<math>\\begin{align}\n\\begin{array}{rlll}\nx_0 & = 6 \\cdot 10^2 && = 600.000 \\\\[0.3em]\nx_1 & = \\frac{1}{2} \\left(x_0 + \\frac{S}{x_0}\\right) & = \\frac{1}{2} \\left(600.000 + \\frac{125348}{600.000}\\right) & = 404.457 \\\\[0.3em]\nx_2 & = \\frac{1}{2} \\left(x_1 + \\frac{S}{x_1}\\right) & = \\frac{1}{2} \\left(404.457 + \\frac{125348}{404.457}\\right) & = 357.187 \\\\[0.3em]\nx_3 & = \\frac{1}{2} \\left(x_2 + \\frac{S}{x_2}\\right) & = \\frac{1}{2} \\left(357.187 + \\frac{125348}{357.187}\\right) & = 354.059 \\\\[0.3em]\nx_4 & = \\frac{1}{2} \\left(x_3 + \\frac{S}{x_3}\\right) & = \\frac{1}{2} \\left(354.059 + \\frac{125348}{354.059}\\right) & = 354.045 \\\\[0.3em]\nx_5 & = \\frac{1}{2} \\left(x_4 + \\frac{S}{x_4}\\right) & = \\frac{1}{2} \\left(354.045 + \\frac{125348}{354.045}\\right) & = 354.045\n\\end{array}\n\\end{align}</math>\n\nTherefore, {{math|{{sqrt|125348}} \u2248 354.045}}.\n\n===Convergence===\nSuppose that ''x''<sub>0</sub> > 0 and ''S'' > 0. Then for any natural number ''n'', ''x''<sub>''n''</sub> > 0. Let the [[relative error]] in ''x''<sub>''n''</sub> be defined by\n\n:<math>\\varepsilon_n = \\frac {x_n}{\\sqrt{S}} - 1 > -1</math>\n\nand thus\n:<math>x_n = \\sqrt {S} \\cdot (1 + \\varepsilon_n) .</math>\n\nThen it can be shown that\n:<math>\\varepsilon_{n+1} = \\frac {\\varepsilon_n^2}{2 (1 + \\varepsilon_n)} \\geq 0 .</math>\n\nAnd thus that\n:<math>\\varepsilon_{n+2} \\leq \\min \\left\\{\\frac {\\varepsilon_{n+1}^2}{2}, \\frac {\\varepsilon_{n+1}}{2} \\right\\}</math>\n\nand consequently that convergence is assured, and [[quadratic convergence|quadratic]].\n\n====Worst case for convergence====\nIf using the rough estimate above with the Babylonian method, then the least accurate cases in ascending order are as follows:\n\n:<math>\n\\begin{align}\nS & = 1;   & x_0 & = 2; & x_1 & = 1.250;  & \\varepsilon_1 & = 0.250. \\\\\nS & = 10;  & x_0 & = 2; & x_1 & = 3.500;  & \\varepsilon_1 & < 0.107. \\\\\nS & = 10;  & x_0 & = 6; & x_1 & = 3.833;  & \\varepsilon_1 & < 0.213. \\\\\nS & = 100; & x_0 & = 6; & x_1 & = 11.333; & \\varepsilon_1 & < 0.134.\n\\end{align}\n</math>\n\nThus in any case,\n:<math> \\varepsilon_1 \\leq 2^{-2}. \\, </math>\n:<math> \\varepsilon_2 < 2^{-5} < 10^{-1}. \\, </math>\n:<math> \\varepsilon_3 < 2^{-11} < 10^{-3}. \\, </math>\n:<math> \\varepsilon_4 < 2^{-23} < 10^{-6}. \\, </math>\n:<math> \\varepsilon_5 < 2^{-47} < 10^{-14}. \\, </math>\n:<math> \\varepsilon_6 < 2^{-95} < 10^{-28}. \\, </math>\n:<math> \\varepsilon_7 < 2^{-191} < 10^{-57}. \\, </math>\n:<math> \\varepsilon_8 < 2^{-383} < 10^{-115}. \\, </math>\n\nRounding errors will slow the convergence. It is recommended to keep at least one extra digit beyond the desired accuracy of the {{mvar|x<sub>n</sub>}} being calculated to minimize round off error.\n\n==Bakhshali method==\nThis method for finding an approximation to a square root was described in an ancient Indian mathematical manuscript called the [[Bakhshali manuscript]]. It is equivalent to two iterations of the Babylonian method beginning with ''x''<sub>0</sub>. Thus, the algorithm is quartically convergent, which means that the number of correct digits of the approximation roughly quadruples with each iteration.<ref>{{cite news|title=Ancient Indian Square Roots: An Exercise in Forensic Paleo-Mathematics| last1=Bailey| first1=David| last2=Borwein|authorlink2=Jonathan_Borwein| first2=Jonathan| journal=American Mathematical Monthly|volume = 119| issue=8| pages=646\u2013657| date=2012| url=http://crd-legacy.lbl.gov/~dhbailey/dhbpapers/india-sqrt.pdf| access-date=2017-09-14}}</ref> The original presentation, using modern notation, is as follows: To calculate <math>\\sqrt{S}</math>, let ''x''<sub>0</sub><sup>2</sup> be the initial approximation to ''S''. Then, successively iterate as:\n:<math>a_n = \\frac{S - x_n^2}{2 x_n},</math>\n\n:<math>b_n = x_n + a_n,</math>\n\n:<math>x_{n+1} = b_n - \\frac{a_n^2}{2 b_n}.</math>\n\nWritten explicitly, it becomes \n:<math> x_{n+1} = (x_n + a_n) - \\frac{a_n^2}{2 (x_n + a_n)}. </math>\n\nLet ''x''<sub>0</sub> = ''N'' be an integer which is the nearest perfect square to ''S''. Also, let the difference ''d'' = ''S'' - ''N''<sup>2</sup>, then the first iteration can be written as:\n:<math>\\sqrt{S} \\approx N + \\frac{d}{2N} - \\frac{d^2}{8N^3 + 4Nd} = \\frac{8N^4 + 8N^2 d + d^2}{8N^3 + 4Nd} = \\frac{N^4 + 6N^2S + S^2}{4N^3 + 4NS} = \\frac{N^2(N^2 + 6S) + S^2}{4N(N^2 + S)}.</math>\n\nThis gives a rational approximation to the square root.\n\n===Example===\nUsing the same example as given in Babylonian method, let <math>S = 125348.</math> Then, the first iterations gives\n\n:<math> x_0 = 600 </math>\n\n:<math> a_1 = \\frac{125348 - 600^2}{2 \\times 600} = -195.543 </math>\n\n:<math> b_1 = 600 + (-195.543) = 404.456 </math>\n\n:<math> x_1 = 404.456 - \\frac{(-195.543)^2}{2 \\times 404.456} = 357.186</math>\n\nLikewise the second iteration gives\n\n:<math> a_2 = \\frac{125348 - 357.186^2}{2 \\times 357.186} = -3.126 </math>\n\n:<math> b_2 = 357.186 + (-3.126) = 354.06 </math>\n\n:<math> x_2 = 354.06 - \\frac{(-3.1269)^2}{2 \\times 354.06} = 354.046 </math>\n\n==Digit-by-digit calculation==\nThis is a method to find each digit of the square root in a sequence. It is slower than the Babylonian method, but it has several advantages:\n* It can be easier for manual calculations.\n* Every digit of the root found is known to be correct, i.e., it does not have to be changed later.\n* If the square root has an expansion that terminates, the algorithm terminates after the last digit is found. Thus, it can be used to check whether a given integer is a [[square number]].\n* The algorithm works for any [[numeral system|base]], and naturally, the way it proceeds depends on the base chosen.\n\n[[Napier's bones]] include an aid for the execution of this algorithm. The [[shifting nth root algorithm|shifting ''n''th root algorithm]] is a generalization of this method.\n\n===Basic principle===\nFirst, consider the case of finding the square root of a number ''Z'', that is the square of a two-digit number ''XY'', where ''X'' is the tens digit and ''Y'' is the units digit. Specifically:\n\nZ = (10X + Y)<sup>2</sup> = 100X<sup>2</sup> + 20XY + Y<sup>2</sup>\n\nNow using the Digit-by-Digit algorithm, we first determine the value of ''X''. ''X'' is the largest digit such that X<sup>2</sup> is less or equal to ''Z'' from which we removed the two rightmost digits.\n\nIn the next iteration, we pair the digits, multiply ''X'' by 2, and place it in the tenth's place while we try to figure out what the value of ''Y'' is.\n\nSince this is a simple case where the answer is a perfect square root ''XY'', the algorithm stops here.\n\nThe same idea can be extended to any arbitrary square root computation next. Suppose we are able to find the square root of ''N'' by expressing it as a sum of ''n'' positive numbers such that\n:<math>N = (a_1+a_2+a_3+\\dotsb+a_n)^2.</math>\n\nBy repeatedly applying the basic identity \n:<math>(x+y)^2 = x^2 +2xy + y^2,</math> \nthe right-hand-side term can be expanded as\n:<math>\n\\begin{align}\n& (a_1+a_2+a_3+ \\dotsb +a_n)^2 \\\\\n=& \\, a_1^2 + 2a_1a_2 + a_2^2 + 2(a_1+a_2) a_3 + a_3^2 + \\dotsb + a_{n-1}^2 + 2  \\left(\\sum_{i=1}^{n-1} a_i\\right) a_n + a_n^2 \\\\\n=& \\, a_1^2 + [2a_1 + a_2] a_2 + [2(a_1+a_2) + a_3] a_3 + \\dotsb + \\left[2 \\left(\\sum_{i=1}^{n-1} a_i\\right) + a_n\\right] a_n.\n\\end{align}\n</math>\n\nThis expression allows us to find the square root by sequentially guessing the values of <math>a_i</math>s. Suppose that the numbers <math>a_1, \\ldots, a_{m-1}</math> have already been guessed, then the m-th term of the right-hand-side of above summation is given by <math>Y_{m} = [2 P_{m-1} + a_{m}]a_{m},</math> where <math>P_{m-1} = \\sum_{i=1}^{m-1} a_i</math> is the approximate square root found so far. Now each new guess <math>a_m</math> should satisfy the recursion \n:<math>X_{m} = X_{m-1} - Y_{m},</math> \nsuch that <math>X_m \\geq 0</math> for all <math>1\\leq m\\leq n,</math> with initialization <math>X_0 = N.</math> When <math>X_n = 0,</math> the exact square root has been found; if not, then the sum of <math>a_i</math>s gives a suitable approximation of the square root, with <math>X_n</math> being the approximation error.\n\nFor example, in the decimal number system we have \n:<math>N = (a_1 \\cdot 10^{n-1} + a_2 \\cdot 10^{n-2} + \\cdots + a_{n-1} \\cdot 10 + a_n)^2,</math> \nwhere <math>10^{n-i}</math> are place holders and the coefficients <math>a_i \\in \\{0,1,2,\\ldots,9\\}</math>. At any m-th stage of the square root calculation, the approximate root found so far, <math>P_{m-1}</math> and the summation term <math>Y_m</math> are given by \n:<math>P_{m-1} = \\sum_{i=1}^{m-1} a_i \\cdot 10^{n-i} = 10^{n-m+1} \\sum_{i=1}^{m-1} a_i \\cdot 10^{m-i-1},</math>\n:<math>Y_m = [2P_{m-1} + a_m \\cdot 10^{n-m}] a_m \\cdot 10^{n-m} = [20 \\sum_{i=1}^{m-1} a_i \\cdot 10^{m-i-1} + a_m ] a_m \\cdot 10^{2(n-m)}.</math>\n\nHere since the place value of <math>Y_m</math> is an even power of 10, we only need to work with the pair of most significant digits of the remaining term <math>X_{m-1}</math> at any m-th stage. The section below codifies this procedure.\n\nIt is obvious that a similar method can be used to compute the square root in number systems other than the decimal number system. For instance, finding the digit-by-digit square root in the binary number system is quite efficient since the value of <math>a_i</math> is searched from a smaller set of binary digits {0,1}. This makes the computation faster since at each stage the value of <math>Y_m</math> is either <math>Y_m = 0</math> for <math>a_m = 0</math> or <math>Y_m = 2 P_{m-1} + 1</math> for <math>a_m = 1</math>. The fact that we have only two possible options for <math>a_m</math> also makes the process of deciding the value of <math>a_m</math> at m-th stage of calculation easier. This is because we only need to check if <math>Y_m \\leq X_{m-1}</math> for <math>a_m = 1.</math> If this condition is satisfied, then we take <math>a_m = 1</math>; if not then <math>a_m = 0.</math> Also, the fact that multiplication by 2 is done by left bit-shifts helps in the computation.\n\n===Decimal (base 10)===\nWrite the original number in decimal form. The numbers are written similar to the [[long division]] algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into pairs, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each pair of digits of the square.\n\nBeginning with the left-most pair of digits, do the following procedure for each pair:\n\n# Starting on the left, bring down the most significant (leftmost) pair of digits not yet used (if all the digits have been used, write \"00\") and write them to the right of the remainder from the previous step (on the first step, there will be no remainder). In other words, multiply the remainder by 100 and add the two digits. This will be the '''current value ''c'''''.\n# Find ''p'', ''y'' and ''x'', as follows:\n#* Let '''''p''''' be the '''part of the root found so far''', ignoring any decimal point. (For the first step, ''p'' = 0.)\n#* Determine the greatest digit '''''x''''' such that <math>x(20p + x) \\le c</math>. We will use a new variable '''''y''''' = ''x''(20''p'' + ''x'').\n#** Note: 20''p'' + ''x'' is simply twice ''p'', with the digit ''x'' appended to the right.\n#** Note: ''x'' can be found by guessing what ''c''/(20\u00b7''p'') is and doing a trial calculation of ''y'', then adjusting ''x'' upward or downward as necessary.\n#* Place the digit <math>x</math> as the next digit of the root, i.e., above the two digits of the square you just brought down. Thus the next ''p'' will be the old ''p'' times 10 plus ''x''.\n# Subtract ''y'' from ''c'' to form a new remainder.\n# If the remainder is zero and there are no more digits to bring down, then the algorithm has terminated. Otherwise go back to step 1 for another iteration.\n\n====Examples====\n'''Find the square root of 152.2756.'''\n\n         <u>  1  2. 3  4 </u>\n        /\n      \\/  01 52.27 56\n \n          01                   1*1 <= 1 < 2*2                 x = 1\n         <u> 01 </u>                    y = x*x = 1*1 = 1\n          00 52                22*2 <= 52 < 23*3              x = 2\n         <u> 00 44 </u>                 y = (20+x)*x = 22*2 = 44\n             08 27             243*3 <= 827 < 244*4           x = 3\n            <u> 07 29 </u>              y = (240+x)*x = 243*3 = 729\n                98 56          2464*4 <= 9856 < 2465*5        x = 4\n               <u> 98 56 </u>           y = (2460+x)*x = 2464*4 = 9856\n                00 00          Algorithm terminates: Answer is 12.34\n\n===Binary numeral system (base 2)===\nInherent to digit-by-digit algorithms is a search and test step:  find a digit, <math>\\, e</math>, when added to the right of a current solution <math>\\, r</math>, such that <math>\\,(r+e)\\cdot(r+e) \\le x</math>, where <math>\\, x</math> is the value for which a root is desired. Expanding: <math>\\,r\\cdot r + 2re + e\\cdot e \\le x</math>.  The current value of <math>\\,r\\cdot r</math>\u2014or, usually, the remainder\u2014can be incrementally updated efficiently when working in binary, as the value of <math>\\, e</math> will have a single bit set (a power of 2), and the operations needed to compute <math>\\,2\\cdot r\\cdot e</math> and <math>\\,e\\cdot e</math> can be replaced with faster [[Bitwise operation#Bit shifts|bit shift]] operations.\n\n====Example====\nHere we obtain the square root of 81, which when converted into binary gives 1010001. The numbers in the left column gives the option between that number or zero to be used for subtraction at that stage of computation. The final answer is 1001, which in decimal is 9.\n\n              1 0 0 1\n             ---------\n            \u221a 1010001\n \n       1      1\n              1\n             ---------\n       101     01  \n                0\n              --------\n       1001     100\n                  0\n              --------\n       10001    10001\n                10001\n               -------\n                    0\n\nThis gives rise to simple computer implementations:<ref>[https://web.archive.org/web/20120306040058/http://medialab.freaknet.org/martin/src/sqrt/sqrt.c  Fast integer square root by Mr. Woo's abacus algorithm (archived)]</ref>\n\n<syntaxhighlight lang=\"c\">\nshort isqrt(short num) {\n    short res = 0;\n    short bit = 1 << 14; // The second-to-top bit is set: 1 << 30 for 32 bits\n \n    // \"bit\" starts at the highest power of four <= the argument.\n    while (bit > num)\n        bit >>= 2;\n        \n    while (bit != 0) {\n        if (num >= res + bit) {\n            num -= res + bit;\n            res = (res >> 1) + bit;\n        }\n        else\n            res >>= 1;\n        bit >>= 2;\n    }\n    return res;\n}\n</syntaxhighlight>\n\nUsing the notation above, the variable \"bit\" corresponds to <math>e_m^2</math> which is <math>(2^m)^2=4^m</math>, the variable \"res\" is equal to <math>2re_m</math>, and the variable \"num\" is equal to the current <math>X_m</math> which is the difference of the number we want the square root of and the square of our current approximation with all bits set up to <math>2^{m+1}</math>. Thus in the first loop, we want to find the highest power of 4 in \"bit\" to find the highest power of 2 in <math>e</math>. In the second loop, if num is greater than res + bit, then <math>X_m</math> is greater than <math>2re_m+e_m^2</math> and we can subtract it. The next line, we want to add <math>e_m</math> to <math>r</math> which means we want to add <math>2e^2_m</math> to <math>2re_m</math> so we want <code>res = res + bit<<1</code>. Then update <math>e_m</math> to <math>e_{m-1}</math> inside res which involves dividing by 2 or another shift to the right. Combining these 2 into one line leads to <code>res = res>>1 + bit</code>. If <math>X_m</math> isn't greater than <math>2re_m+e_m^2</math> then we just update <math>e_m</math> to <math>e_{m-1}</math> inside res and divide it by 2. Then we update <math>e_m</math> to <math>e_{m-1}</math> in bit by dividing it by 4. The final iteration of the 2nd loop has bit equal to 1 and will cause update of <math>e</math> to run one extra time removing the factor of 2 from res making it our integer approximation of the root.\n\nFaster algorithms, in binary and decimal or any other base, can be realized by using lookup tables\u2014in effect trading [[space\u2013time tradeoff|more storage space for reduced run time]].<ref>[http://atoms.alife.co.uk/sqrt/SquareRoot.java Integer Square Root function]</ref>\n\n==Exponential identity==\n[[calculator|Pocket calculator]]s typically implement good routines to compute the [[exponential function]] and the [[natural logarithm]], and then compute the square root of ''S'' using the identity found using the properties of logarithms (<math>\\ln x^n = n \\ln x</math>) and exponentials (<math>e^{\\ln x} = x</math>):\n\n:<math>\\sqrt{S} = e^{\\frac{1}{2}\\ln S}.</math>\nThe denominator in the fraction corresponds to the ''n''th root. In the case above the denominator is 2, hence the equation specifies that the square root is to be found. The same identity is used when computing square roots with [[logarithm table]]s or [[slide rule]]s.\n\n==A two-variable iterative method==\nThis method is applicable for finding the square root of <math>0 < S < 3 \\,\\!</math> and converges best for <math>S \\approx 1</math>.\nThis, however, is no real limitation for a computer based calculation, as in base 2 floating point and fixed point representations, it is trivial to multiply <math>S \\,\\!</math> by an integer power of 4, and therefore <math>\\sqrt{S}</math> by the corresponding power of 2, by changing the exponent or by shifting, respectively. Therefore, <math>S \\,\\!</math> can be moved to the range <math>\\frac{1}{2} \\le S <2</math>. Moreover, the following method does not employ general divisions, but only additions, subtractions, multiplications, and divisions by powers of two, which are again trivial to implement. A disadvantage of the method is that numerical errors accumulate, in contrast to single variable iterative methods such as the Babylonian one.\n\nThe initialization step of this method is\n:<math>a_0 = S \\,\\!</math>\n:<math>c_0 = S-1 \\,\\!</math>\nwhile the iterative steps read\n:<math>a_{n+1} = a_n - a_n c_n / 2 \\,\\!</math>\n:<math>c_{n+1} = c_n^2 (c_n - 3) / 4 \\,\\!</math>\nThen, <math>a_n \\rightarrow \\sqrt{S}</math> (while <math>c_n \\rightarrow 0</math>).\n\nNote that the convergence of <math>c_n \\,\\!</math>, and therefore also of <math>a_n \\,\\!</math>, is quadratic.\n\nThe proof of the method is rather easy. First, rewrite the iterative definition of <math>c_n \\,\\!</math> as\n:<math>1 + c_{n+1} = (1 + c_n) (1 - c_n/2)^2  \\,\\!</math>.\nThen it is straightforward to prove by induction that\n:<math>S (1 + c_n) = a_n^2</math>\nand therefore the convergence of <math>a_n \\,\\!</math> to the desired result <math>\\sqrt{S}</math> is ensured by the convergence of <math>c_n \\,\\!</math> to 0, which in turn follows from <math>-1 < c_0 < 2 \\,\\!</math>.\n\nThis method was developed around 1950 by [[Maurice Wilkes|M. V. Wilkes]], [[David Wheeler (computer scientist)|D. J. Wheeler]] and [[Stanley Gill|S. Gill]]<ref>M. V. Wilkes, D. J. Wheeler and S. Gill, \"The Preparation of Programs for an Electronic Digital Computer\", Addison-Wesley, 1951.</ref> for use on [[Electronic Delay Storage Automatic Calculator|EDSAC]], one of the first electronic computers.<ref>M. Campbell-Kelly, \"Origin of Computing\", Scientific American, September 2009.</ref> The method was later generalized, allowing the computation of non-square roots.<ref>J. C. Gower, \"A Note on an Iterative Method for Root Extraction\", The Computer Journal 1(3):142&ndash;143, 1958.</ref>\n\n==Iterative methods for reciprocal square roots==\nThe following are iterative methods for finding the reciprocal square root of ''S'' which is <math>1/\\sqrt{S}</math>. Once it has been found, find <math>\\sqrt{S}</math> by simple multiplication: <math>\\sqrt{S} = S \\cdot (1/\\sqrt{S})</math>. These iterations involve only multiplication, and not division. They are therefore faster than the [[Methods of computing square roots#Babylonian method|Babylonian method]]. However, they are not stable. If the initial value is not close to the reciprocal square root, the iterations will diverge away from it rather than converge to it. It can therefore be advantageous to perform an iteration of the Babylonian method on a rough estimate before starting to apply these methods.\n\n*Applying [[Newton's method]] to the equation <math>(1/x^2) - S = 0</math> produces a method that converges quadratically using three multiplications per step:\n:<math>x_{n+1} = \\frac{x_n}{2} \\cdot (3 - S \\cdot x_n^2).</math>\n\n*Another iteration is obtained by [[Halley's method]], which is the [[Householder's method]] of order two. This [[rate of convergence|converges cubically]], but involves four multiplications per iteration:{{cn|date=August 2019}}\n:<math>y_n = S \\cdot x_n^2</math>, and\n:<math>x_{n+1} = \\frac{x_n}{8} \\cdot (15 - y_n \\cdot (10 - 3 \\cdot y_n))</math>.\n\n===Goldschmidt\u2019s algorithm===\nSome computers use Goldschmidt's algorithm to simultaneously calculate\n<math>\\sqrt{S}</math> and\n<math>1/\\sqrt{S}</math>.\nGoldschmidt's algorithm finds <math>\\sqrt{S}</math> faster than Newton-Raphson iteration on a computer with a [[fused multiply\u2013add]] instruction and either a pipelined floating point unit or two independent floating-point units.<ref name=\"goldschmidt_algo\">\n{{cite conference |citeseerx=10.1.1.85.9648 |title=Software Division and Square Root Using Goldschmidt's Algorithms |first=Peter |last=Markstein |date=November 2004 |conference=6th Conference on Real Numbers and Computers |location=[[Dagstuhl]], Germany |url=http://www.informatik.uni-trier.de/Reports/TR-08-2004/rnc6_12_markstein.pdf |conference-url=http://cca-net.de/rnc6/}}\n</ref>\n\nThe first way of writing Goldschmidt's algorithm begins\n: <math>b_0 = S</math>\n: <math>Y_0 \\approx 1/\\sqrt{S}</math> (typically using a table lookup)\n: <math>y_0 = Y_0</math>\n: <math>x_0 = S y_0</math>\nand iterates\n:<math>b_{n+1} = b_n Y_n^2</math>\n:<math>Y_{n+1} = (3 - b_{n+1})/2</math>\n:<math>x_{n+1} = x_n Y_{n+1}</math>\n:<math>y_{n+1} = y_n Y_{n+1}</math>\nuntil <math>b_i</math> is sufficiently close to 1, or a fixed number of iterations.  The iterations converge to\n:<math>\\lim_{n \\to \\infty} x_n = \\sqrt S</math>, and\n:<math>\\lim_{n \\to \\infty} y_n = 1/\\sqrt S</math>.\nNote that it is possible to omit either <math>x_n</math> and <math>y_n</math> from the computation, and if both are desired then <math>x_n = S y_n</math> may be used at the end rather than computing it through in each iteration.\n\nA second form, using [[fused multiply-add]] operations, begins\n: <math>y_0 \\approx 1/\\sqrt{S}</math> (typically using a table lookup)\n: <math>x_0 = S y_0</math>\n: <math>h_0 = y_0/2</math>\nand iterates\n:<math>r_n = 0.5 - x_n h_n</math>\n:<math>x_{n+1} = x_n + x_n r_n</math>\n:<math>h_{n+1} = h_n + h_n r_n</math>\nuntil <math>r_i</math> is sufficiently close to 0, or a fixed number of iterations.  This converges to\n:<math>\\lim_{n \\to \\infty} x_n = \\sqrt S</math>, and\n:<math>\\lim_{n \\to \\infty} 2h_n = 1/\\sqrt S</math>.\n\n==Taylor series==\nIf ''N'' is an approximation to <math>\\sqrt{S}</math>, a better approximation can be found by using the [[Taylor series]] of the [[square root]] function:\n\n:<math>\\sqrt{N^2+d} = N\\sum_{n=0}^\\infty \\frac{(-1)^{n}(2n)!}{(1-2n)n!^2 4^n}\\frac{d^n}{N^{2n}} = N(1 + \\frac{d}{2N^2} - \\frac{d^2}{8N^4} + \\frac{d^3}{16N^6} - \\frac{5d^4}{128N^8} + \\cdots)</math>\n\nAs an iterative method, the [[order of convergence]] is equal to the number of terms used. With two terms, it is identical to the [[Methods of computing square roots#Babylonian method|Babylonian method]]. With three terms, each iteration takes almost as many operations as the [[Methods of computing square roots#Bakhshali method |Bakhshali approximation]], but converges more slowly.{{Citation needed|date=September 2017}} Therefore, this is not a particularly efficient way of calculation. To maximize the rate of convergence, choose ''N'' so that <math> \\frac{|d|}{N^2} \\,</math> is as small as possible.\n\n==Continued fraction expansion==\n{{see also|solving quadratic equations with continued fractions}}\n\n[[Quadratic irrational]]s (numbers of the form <math>\\frac{a+\\sqrt{b}}{c}</math>, where ''a'', ''b'' and ''c'' are integers), and in particular, square roots of integers, have [[periodic continued fraction]]s. Sometimes what is desired is finding not the numerical value of a square root, but rather its [[continued fraction]] expansion, and hence its rational approximation. Let ''S'' be the positive number for which we are required to find the square root. Then assuming ''a'' to be a number that serves as an initial guess and ''r'' to be the remainder term, we can write <math>S = a^2 + r. </math> Since we have <math>S - a^2 = (\\sqrt{S} + a)(\\sqrt{S} - a) = r</math>, we can express the square root of ''S'' as\n:<math> \\sqrt{S} = a + \\frac{r}{a + \\sqrt{S}}. </math>\n\nBy applying this expression for <math>\\sqrt{S}</math> to the denominator term of the fraction, we have\n:<math> \\sqrt{S} = a + \\frac{r}{a + (a + \\frac{r}{a + \\sqrt{S}})} = a + \\frac{r}{2a + \\frac{r}{a + \\sqrt{S}}}. </math>\n<div style=\"float:right; padding:1em; margin:0 0 0 1em; width:450px; border:1px solid; background:#faf3ee;\">\n<center>'''<big>Compact notation</big>'''</center>\nThe numerator/denominator expansion for continued fractions (see left) is cumbersome to write as well as to embed in  text formatting systems.  Therefore, special notation has been developed to compactly represent the integer and repeating parts of continued fractions.  One such convention is use of a lexical  \"dog leg\" to represent the vinculum between numerator and denominator, which allows the fraction to be expanded horizontally instead of vertically:\n:<math> \\sqrt{S} = a + \\frac{r |}{| 2a} +\\frac{r |}{| 2a} + \\frac{r |}{| 2a} + \\cdots </math>\nHere, each vinculum is represented by three line segments, two vertical and one horizontal, separating <math>r</math> from <math>2a</math>.\n\nAn even more compact notation which omits lexical devices takes a special form:\n:<math>[a;2a,2a,2a,...]</math>\n\nFor repeating continued fractions (which all square roots do), the repetend is represented only once, with an overline to signify a non-terminating repetition of the overlined part:\n:<math>[a;\\overline{2a}]</math>\n\nFor {{sqrt|2}}, the value of <math>a</math> is 1, so its representation is:\n:<math>[1;\\overline{2}]</math>\n</div>\n\nProceeding this way, we get a [[generalized continued fraction]] for the square root as\n<math> \\sqrt{S} = a + \\cfrac{r}{2a + \\cfrac{r}{2a + \\cfrac{r}{2a + \\ddots}}}</math> \n\nThe first step to evaluating such a fraction to obtain a root is to do numerical substitutions for the root of the number desired, and number of denominators selected. For example, in canonical form, <math>r</math> is 1 and for {{sqrt|2}}, <math>a</math> is 1, so the numerical continued fraction for 3 denominators is: \n:<math> \\sqrt{2} \\approx 1 + \\cfrac{1}{2 + \\cfrac{1}{2 + \\cfrac{1}{2}}}</math>\n\nStep 2 is to reduce the continued fraction from the bottom up, one denominator at a time, to yield a rational fraction whose numerator and denominator are integers. The reduction proceeds thus (taking the first three denominators):\n:<math> 1 + \\cfrac{1}{2 + \\cfrac{1}{2 + \\cfrac{1}{2}}} = 1 + \\cfrac{1}{2 + \\cfrac{1}{\\frac{5}{2}}}</math>\n::<math> = 1 + \\cfrac{1}{2 + \\cfrac{2}{5}} = 1 + \\cfrac{1}{\\frac{12}{5}}</math>\n::<math> = 1 + \\cfrac{5}{12} = \\frac{17}{12}</math>\n\nFinally (step 3), divide the numerator by the denominator of the rational fraction to obtain the approximate value of the root:\n:<math>17 \\div 12 = 1.42</math> rounded to three digits of precision.\n\nThe actual value of {{sqrt|2}} is 1.41 to three significant digits. The relative error is 0.17%, so the rational fraction is good to almost three digits of precision.  Taking more denominators gives successively better approximations: four denominators yields the fraction <math>\\frac{41}{29} = 1.4137</math>, good to almost 4 digits of precision, etc.\n\nUsually, the continued fraction for a given square root is looked up rather than expanded in place because it's tedious to expand it. Continued fractions are available for at least square roots of small integers and common constants.  For an arbitrary decimal number, precomputed sources are likely to be useless.  The following is a table of small rational fractions called ''convergents'' reduced from canonical continued fractions for the square roots of a few common constants:\n{| class=\"wikitable\"\n|-\n! {{sqrt|S}}\n! cont. fraction\n! ~decimal\n! serial convergents\n|-\n| {{sqrt|2}}\n| style=\"text-align: center;\" | <math>[1;\\overline {2}]</math>\n| 1.41421\n| <math>\\frac{3}{2}, \\frac{7}{5}, \\frac{17}{12}, \\frac{41}{29}, \\frac{99}{70}, \\frac{140}{99}</math>\n|-\n| {{sqrt|3}}\n| style=\"text-align: center;\" | <math>[1;\\overline {1,2}]</math>\n| 1.73205\n| <math>\\frac{2}{1}, \\frac{5}{3}, \\frac{7}{4}, \\frac{19}{11}, \\frac{26}{15}, \\frac{71}{41}, \\frac{97}{56}</math>\n|-\n| {{sqrt|5}}\n| style=\"text-align: center;\" | <math>[2;\\overline {4}]</math>\n| 2.23607\n| <math>\\frac{9}{4}, \\frac{38}{17}, \\frac{161}{72}</math>\n|-\n| {{sqrt|6}}\n| style=\"text-align: center;\" | <math>[2;\\overline {2,4}]</math>\n| 2.44949\n| <math>\\frac{5}{2}, \\frac{22}{9}, \\frac{49}{20}, \\frac{218}{89}</math>\n|-\n| {{sqrt|10}}\n| style=\"text-align: center;\" | <math>[3;\\overline {6}]</math>\n| 3.16228\n| <math>\\frac{19}{6}, \\frac{117}{37}</math>\n|-\n| <math>\\sqrt{\\pi}</math>\n| style=\"text-align: center;\" | <math>[1;1,3,2,1,1,6...]</math>\n| 1.77245\n| <math>\\frac{2}{1}, \\frac{7}{4},\\frac{16}{9},\\frac{23}{13},\\frac{39}{22}</math>\n|-\n| <math>\\sqrt{e}</math>\n| style=\"text-align: center;\" | <math>[1;1,1,1,5,1,1...]</math>\n| 1.64872\n| <math>\\frac{2}{1}, \\frac{3}{2},\\frac{8}{5},\\frac{28}{17},\\frac{33}{20},\\frac{61}{37}</math>\n|-\n| <math>\\sqrt{\\phi}</math>\n| style=\"text-align: center;\" | <math>[1;3,1,2,11,3,7...]</math>\n| 1.27202\n| <math>\\frac{4}{3}, \\frac{5}{4},\\frac{14}{11}</math>\n|}\n{{small|Note: all convergents up to and including denominator 99 listed.}}\n\nIn general, the larger the denominator of a rational fraction, the better the approximation.  It can also be shown that truncating a continued fraction yields a rational fraction that is the best approximation to the root of any fraction with denominator less than or equal to the denominator of that fraction - e.g., no fraction with a denominator less than or equal to 99 is as good an approximation to {{sqrt|2}} as 140/99.\n\n== Lucas sequence method ==\nthe [[Lucas sequence]] of the first kind ''U<sub>n</sub>''(''P'',''Q'') is defined by the [[Recurrence relation|recurrence relations]]:<blockquote><math>    U_n(P, Q)= \\begin{cases} 0 & \\text{if }n = 0 \\\\ 1 & \\text{if }n = 1 \\\\ P \\cdot U_{n -1}(P, Q) -Q \\cdot U_{n -2}(P, Q) & \\text{Otherwise} \\end{cases}\n</math></blockquote>and the characteristic equation of it is:<blockquote><math>    x^2 -P \\cdot x +Q  = 0\n</math></blockquote>it has the [[discriminant]] <math>    D = P^2 -4Q\n</math> and the roots:<blockquote><math>    \\begin{matrix} x_1 = \\frac{P +\\sqrt{D}}{2}, & x_2 = \\frac{P -\\sqrt{D}}{2}\\end{matrix}\n</math></blockquote>all that yield the following positive value:<blockquote><math>    \\lim_{n \\to \\infty} {\\frac{U_{n +1}}{U_n}} = x_1\n</math></blockquote>so when we want <math>    \\sqrt{a}\n</math>, we can choose <math>    P = 2\n</math> and <math>    Q = 1 -a\n</math>, and then calculate <math>    x_1 = 1 +\\sqrt{a}\n</math> using  <math>    U_{n +1}\n</math> and <math>    U_n\n</math>for large value of <math>    n\n</math>.\n\nThe most effective way to calculate <math>    U_{n +1}\n</math> and <math>    U_n\n</math>is:<blockquote><math>    \\begin{bmatrix} U_n \\\\ U_{n +1} \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -Q & P \\end{bmatrix} \\cdot \\begin{bmatrix} U_{n -1} \\\\ U_n \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ -Q & P \\end{bmatrix}^n \\cdot \\begin{bmatrix} U_0 \\\\ U_1 \\end{bmatrix}\n</math></blockquote>'''Summary:'''<blockquote><math>    \\begin{bmatrix} 0 & 1 \\\\ a -1 & 2 \\end{bmatrix}^n \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} U_n \\\\ U_{n +1} \\end{bmatrix}\n</math></blockquote>then when <math>    n \\to \\infty\n</math>:<blockquote><math>    \\sqrt{a} = \\frac{U_{n +1}}{U_n} -1\n</math></blockquote>\n\n==Approximations that depend on the floating point representation==\n<!--\n\n    This section needs a diagram or something similar,\n    to make it more accessible to non-techies.\n\n-->\nA number is represented in a [[floating point]] format as <math>m\\times b^p</math> which is also called [[scientific notation]]. Its square root is <math>\\sqrt{m}\\times b^{p/2}</math> and similar formulae would apply for cube roots and logarithms. On the face of it, this is no improvement in simplicity, but suppose that only an approximation is required: then just <math>b^{p/2}</math> is good to an order of magnitude. Next, recognise that some powers, ''p'', will be odd, thus for 3141.59 = 3.14159 &times; 10<sup>3</sup> rather than deal with fractional powers of the base, multiply the mantissa by the base and subtract one from the power to make it even. The adjusted representation will become the equivalent of 31.4159 &times; 10<sup>2</sup> so that the square root will be {{radic|31.4159}}  &times; 10.\n\nIf the integer part of the adjusted mantissa is taken, there can only be the values 1 to 99, and that could be used as an index into a table of 99 pre-computed square roots to complete the estimate. A computer using base sixteen would require a larger table, but one using base two would require only three entries: the possible bits of the integer part of the adjusted mantissa are 01 (the power being even so there was no shift, remembering that a [[Normalized number|normalised]] floating point number always has a non-zero high-order digit) or if the power was odd, 10 or 11, these being the first ''two'' bits of the original mantissa. Thus, 6.25 = 110.01 in binary, normalised to 1.1001 &times; 2<sup>2</sup> an even power so the paired bits of the mantissa are 01, while .625 = 0.101 in binary normalises to 1.01 &times;  2<sup>\u22121</sup> an odd power so the adjustment is to 10.1 &times; 2<sup>\u22122</sup> and the paired bits are 10. Notice that the low order bit of the power is echoed in the high order bit of the pairwise mantissa. An even power has its low-order bit zero and the adjusted mantissa will start with 0, whereas for an odd power that bit is one and the adjusted mantissa will start with 1. Thus, when the power is halved, it is as if its low order bit is shifted out to become the first bit of the pairwise mantissa.\n\nA table with only three entries could be enlarged by incorporating additional bits of the mantissa. However, with computers, rather than calculate an interpolation into a table, it is often better to find some simpler calculation giving equivalent results. Everything now depends on the exact details of the format of the representation, plus what operations are available to access and manipulate the parts of the number. For example, [[Fortran]] offers an <code>EXPONENT(x)</code> function to obtain the power. Effort expended in devising a good initial approximation is to be recouped by thereby avoiding the additional iterations of the refinement process that would have been needed for a poor approximation. Since these are few (one iteration requires a divide, an add, and a halving) the constraint is severe.\n\nMany computers follow the [[IEEE floating-point standard|IEEE]] (or sufficiently similar) representation, and a very rapid approximation to the square root can be obtained for starting Newton's method. The technique that follows is based on the fact that the floating point format (in base two) approximates the base-2 logarithm. That is <math>\\log_2(m\\times 2^p) = p + \\log_2(m)</math>\n\nSo for a 32-bit single precision floating point number in IEEE format (where notably, the power has a [[Exponent bias|bias]] of 127 added for the represented form) you can get the approximate logarithm by interpreting its binary representation as a 32-bit integer, scaling it by <math>2^{-23}</math>, and removing a bias of 127, i.e.\n:<math>x_\\text{int} \\cdot 2^{-23} - 127 \\approx \\log_2(x).</math>\n\nFor example, 1.0 is represented by a [[hexadecimal]] number 0x3F800000, which would represent <math>1065353216 = 127 \\cdot 2^{23}</math> if taken as an integer. Using the formula above you get <math>1065353216 \\cdot 2^{-23} - 127 = 0</math>, as expected from <math>\\log_2(1.0)</math>. In a similar fashion you get 0.5 from 1.5 (0x3FC00000).\n\n[[Image:Log2approx.png]]\n\nTo get the square root, divide the logarithm by 2 and convert the value back. The following program demonstrates the idea. Note that the exponent's lowest bit is intentionally allowed to propagate into the mantissa.  One way to justify the steps in this program is to assume <math>b</math> is the exponent bias and <math>n</math> is the number of explicitly stored bits in the mantissa and then show that\n:<math>(((x_\\text{int} / 2^n - b) / 2) + b) \\cdot 2^n = (x_\\text{int} - 2^n) / 2 + ((b + 1) / 2) \\cdot 2^n.</math>\n<br>\n<syntaxhighlight lang=\"c\">\n\n/* Assumes that float is in the IEEE 754 single precision floating point format\n * and that int is 32 bits. */\nfloat sqrt_approx(float z) {\n    int val_int = *(int*)&z; /* Same bits, but as an int */\n    /*\n     * To justify the following code, prove that\n     *\n     * ((((val_int / 2^m) - b) / 2) + b) * 2^m = ((val_int - 2^m) / 2) + ((b + 1) / 2) * 2^m)\n     *\n     * where\n     *\n     * b = exponent bias\n     * m = number of mantissa bits\n     *\n     * .\n     */\n\n    val_int -= 1 << 23; /* Subtract 2^m. */\n    val_int >>= 1; /* Divide by 2. */\n    val_int += 1 << 29; /* Add ((b + 1) / 2) * 2^m. */\n\n    return *(float*)&val_int; /* Interpret again as float */\n}\n</syntaxhighlight>\n\nThe three mathematical operations forming the core of the above function can be expressed in a single line.  An additional adjustment can be added to reduce the maximum relative error.  So, the three operations, not including the cast, can be rewritten as\n<syntaxhighlight lang=\"c\">\nval_int = (1 << 29) + (val_int >> 1) - (1 << 22) + a;\n</syntaxhighlight>\n\nwhere ''a'' is a bias for adjusting the approximation errors. For example, with ''a'' = 0 the results are accurate for even powers of 2 (e.g., 1.0), but for other numbers the results will be slightly too big (e.g.,1.5 for 2.0 instead of 1.414... with 6% error). With ''a'' = -0x4B0D2, the maximum relative error is minimized to \u00b13.5%.\n\nIf the approximation is to be used for an initial guess for [[Newton's method]] to the equation <math>(1/x^2) - S = 0</math>, then the reciprocal form shown in the following section is preferred.\n\n===Reciprocal of the square root===\n{{Main|Fast inverse square root}}\nA variant of the above routine is included below, which can be used to compute the [[Multiplicative inverse|reciprocal]] of the square root, i.e., <math>x^{-{1\\over2}}</math> instead, was written by Greg Walsh. The integer-shift approximation produced a relative error of less than 4%, and the error dropped further to 0.15% with one iteration of [[Newton's method]] on the following line.<ref>[http://www.lomont.org/Math/Papers/2003/InvSqrt.pdf Fast Inverse Square Root] by Chris Lomont</ref> In computer graphics it is a very efficient way to normalize a vector.\n\n<syntaxhighlight lang=\"c\">\nfloat invSqrt(float x) {\n    float xhalf = 0.5f*x;\n    union {\n        float x;\n        int i;\n    } u;\n    u.x = x;\n    u.i = 0x5f375a86 - (u.i >> 1);\n    /* The next line can be repeated any number of times to increase accuracy */\n    u.x = u.x * (1.5f - xhalf * u.x * u.x);\n    return u.x;\n}\n</syntaxhighlight>\n\nSome VLSI hardware implements inverse square root using a second degree polynomial estimation followed by a [[Division algorithm#Goldschmidt division|Goldschmidt iteration]].<ref>\n[http://portal.acm.org/citation.cfm?id=627261 \"High-Speed Double-Precision Computation of Reciprocal, Division, Square Root and Inverse Square Root\"]\nby Jos\u00e9-Alejandro Pi\u00f1eiro and Javier D\u00edaz Bruguera 2002 (abstract)\n</ref>\n\n==Negative or complex square==\nIf ''S''&nbsp;<&nbsp;0, then its principal square root is\n:<math>\\sqrt {S} = \\sqrt {\\vert S \\vert} \\, \\, i \\,.</math>\n\nIf ''S''&nbsp;=&nbsp;''a''+''bi'' where ''a'' and ''b'' are real and ''b''&nbsp;\u2260&nbsp;0, then its principal square root is\n:<math>\\sqrt {S} = \\sqrt{\\frac{\\vert S \\vert + a}{2}} \\, + \\, \\sgn (b) \\sqrt{\\frac{\\vert S \\vert - a}{2}} \\, \\, i \\,.</math>\n\nThis can be verified by squaring the root.<ref>{{cite book\n|title=Handbook of mathematical functions with formulas, graphs, and mathematical tables\n|edition=\n|first1=Miltonn\n|last1=Abramowitz\n|first2=Irene A.\n|last2=Stegun\n|publisher=Courier Dover Publications\n|year=1964\n|isbn=978-0-486-61272-0\n|page=17\n|url=https://books.google.com/books?id=MtU8uP7XMvoC}}, [http://www.math.sfu.ca/~cbm/aands/page_17.htm Section 3.7.26, p. 17]\n</ref><ref>{{cite book\n|title=Classical algebra: its nature, origins, and uses\n|first1=Roger \n|last1=Cooke\n|publisher=John Wiley and Sons\n|year=2008\n|isbn=978-0-470-25952-8\n|page=59\n|url=https://books.google.com/books?id=lUcTsYopfhkC}}, [https://books.google.com/books?id=lUcTsYopfhkC&pg=PA59 Extract: page 59]\n</ref> Here\n:<math>\\vert S \\vert = \\sqrt{a^2 + b^2}</math>\n\nis the [[absolute value|modulus]] of ''S''. The principal square root of a [[complex number]] is defined to be the root with the non-negative real part.\n\n== See also ==\n* [[Alpha max plus beta min algorithm]]\n* [[nth root algorithm|''n''th root algorithm]]\n* [[Square root of 2]]\n\n== Notes ==\n{{Reflist|group=Note}}\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* {{MathWorld|title=Square root algorithms|urlname=SquareRootAlgorithms}}\n* [http://www.afjarvis.staff.shef.ac.uk/maths/jarvisspec02.pdf Square roots by subtraction]\n* [http://www.andrijar.com/algorithms/algorithms.htm#qusr Integer Square Root Algorithm by Andrija Radovi\u0107]\n* [http://www.hparchive.com/Journals/HPJ-1977-05.pdf Personal Calculator Algorithms I : Square Roots (William E. Egbert), Hewlett-Packard Journal (may 1977) : page 22]\n* [http://www.calculatorsquareroot.com Calculator to learn the square root]\n\n{{DEFAULTSORT:Methods Of Computing Square Roots}}\n[[Category:Root-finding algorithms]]\n[[Category:Computer arithmetic algorithms]]\n", "name_user": "Muf99", "label": "safe", "comment": "fixed bracket size", "url_page": "//en.wikipedia.org/wiki/Methods_of_computing_square_roots"}
