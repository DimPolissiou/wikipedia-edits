{"title_page": "Christ the King Cathedral (Reykjav\u00edk)", "text_new": "{{Infobox church\n| name                   = Cathedral of Christ the King\n| fullname               = The Cathedral Basilica of Christ the King\n| other name             = \n| native_name            = Landakotskirkja\n| native_name_lang       = Icelandic\n| image                  = Landakotskirkja - panoramio (3).jpg\n| imagesize              = 250px\n| imagelink              = \n| imagealt               = \n| landscape              = \n| caption                = The Cathedral of Christ the King in 2012\n| pushpin map            = \n| pushpin label position = \n| pushpin map alt        = \n| pushpin mapsize        = \n| relief                 = \n| map caption            = \n| coordinates            = {{coord|64.1475|-21.9485|format=dms|type:landmark_region:IS\n|display=inline,title}}\n| osgraw                 = <!--  TEXT -->\n| osgridref              = <!-- {{gbmappingsmall| TEXT}} -->\n| location               = [[Reykjav\u00edk]]\n| country                = [[Iceland]]\n| denomination           = [[Roman Catholic]]\n| previous denomination  = \n| churchmanship          = \n| membership             = \n| attendance             = \n| website                = \n| former name            = \n| bull date              = \n| founded date           = \n| founder                = \n| dedication             = \n| dedicated date         = \n| consecrated date       = 23 July 1929\n| cult                   = \n| relics                 = \n| events                 = \n| past bishop            = \n| people                 = \n| status                 = Active\n| functional status      = Cathedral\n| heritage designation   = \n| designated date        = \n| architect              = [[Gu\u00f0j\u00f3n Sam\u00faelsson]]\n| architectural type     = \n| style                  = [[Neo-Gothic]]\n| groundbreaking         = \n| completed date         = \n| construction cost      = \n| closed date            = \n| demolished date        = \n| capacity               = \n| length                 = <!-- {{convert| }} -->\n| width                  = <!-- {{convert| }} -->\n| width nave             = <!-- {{convert| }} -->\n| height                 = <!-- {{convert| }} -->\n| diameter               = <!-- {{convert| }} -->\n| other dimensions       = \n| floor count            = \n| floor area             = <!-- {{convert| }} -->\n| dome quantity          = \n| dome height outer      = <!-- {{convert| }} -->\n| dome height inner      = <!-- {{convert| }} -->\n| dome dia outer         = <!-- {{convert| }} -->\n| dome dia inner         = <!-- {{convert| }} -->\n| spire quantity         = \n| spire height           = <!-- {{convert| }} -->\n| materials              = \n| bells                  = \n| bells hung             = \n| bell weight            = <!-- {{CwtQtrLb to kg| }} -->\n| parish                 = \n| deanery                = \n| archdeaconry           = \n| episcopalarea          = \n| archdiocese            = \n| metropolis             = \n| diocese                = [[Reykjav\u00edk]]\n| province               = \n| presbytery             = \n| synod                  = \n| circuit                = \n| district               = \n| division               = \n| subdivision            = \n| archbishop             = \n| bishop                 = [[David Tencer]]\n| dean                   = \n| provost                = \n| provost-rector         = \n| viceprovost            = \n| subdean                = \n| precentor              = \n| chancellor             = \n| canonchancellor        = \n| canon                  = \n| canonmissioner         = \n| canonpastor            = \n| canontreasurer         = \n| succentor              = \n| archdeacon             = \n| prebendary             = \n| rector                 = \n| vicar                  = \n| curate                 = \n| priestincharge         = \n| priest                 = \n| asstpriest             = \n| minister               = \n| assistant              = \n| honpriest              = \n| deacon                 = \n| deaconness             = \n| seniorpastor           = \n| pastor                 = \n| abbot                  = \n| chaplain               = \n| reader                 = \n| student intern         = \n| organistdom            = \n| director               = \n| organist               = \n| organscholar           = \n| chapterclerk           = \n| laychapter             = \n| warden                 = \n| verger                 = \n| businessmgr            = \n| liturgycoord           = \n| reledu                 = \n| rcia                   = \n| youthmin               = \n| flowerguild            = \n| musicgroup             = \n| parishadmin            = \n| serversguild           = \n| logo                   = \n| logosize               = \n| logolink               = \n| logoalt                = \n}}\n\n'''Landakotskirkja''' (Landakot's Church), formally '''Basilika Krists konungs''' (The Basilica of Christ the King), is the cathedral of the [[Roman Catholicism in Iceland|Catholic Church in Iceland]]. Often referred to as Kristskirkja (Christ's Church), Landakotskirkja is in the western part of [[Reykjav\u00edk]], Iceland's capital city.\n\n==Architecture==\nLandakotskirkja has a distinctively flat top instead of a standard spire. Its architect was [[Gu\u00f0j\u00f3n Sam\u00faelsson]], who also designed [[Hallgr\u00edmskirkja]], a Reykjavik landmark, and [[Akureyrarkirkja]] in Akureyri, North Iceland.\n\n==History==\nThe first [[Catholic priest]]s to arrive in Iceland after the [[Protestant Reformation|Reformation]] were the Frenchmen [[Bernard Bernard]] and [[Jean-Baptiste Baudoin]]. They bought the [[Landakot]] farmstead in Reykjav\u00edk and settled there in the early 19th century.  They built a small chapel in 1864. A few years later, a small wooden church was erected by [[T\u00fangata]], close to Landakot. After the [[First World War]], Icelandic Catholics saw the need to build a bigger church for the growing number of Catholics. They decided to build a [[Neo-Gothic]] church and entrusted the task to the architect [[Gu\u00f0j\u00f3n Sam\u00faelsson]].  After years of construction, Landakotskirkja was finally sanctified on 23 July 1929. It was the largest church in Iceland at the time.  Today, Landakotskirkja is a distinct landmark in western Reykjav\u00edk. The only Catholic school in Iceland was located nearby on the same land.\n\nA big part of the furniture comes from the renowned Atelier J.W. Ramakers & Sons sculptors from Geleen, Holland. Ramakers delivered in 1928 the timpan, both the side altars, the St. Joseph altar in 1905 and the Maria altar in 1928, in 1929 the pulpit. Atelier Ramakers made a design of the main altar, however it was never delivered.\n\n==See also==\n*[[Diocese of Reykjav\u00edk]]\n*[[Bishop of Reykjav\u00edk (Catholic)]]\n*[[Roman Catholicism in Iceland]]\n* [[List of cathedrals in Iceland]]\n*[[Christianity in Iceland]]\n\n==External links==\n{{Commons cat|Landakotskirkja}}\n\n*[https://web.archive.org/web/20160303165548/http://kirkjukort.net/kirkjur/landakotskirkja_0308.html Landakotskirkja on the Icelandic Church Map]\n*[http://www.catholica.is/lkin.html Images of the Cathedral of Christ the King]\n*[http://www.catholica.is/ Roman Catholic Diocese of Reykjav\u00edk] {{in lang|en|is}}\n\n[[Category:Gothic Revival architecture in Iceland]]\n[[Category:Gothic Revival church buildings]]\n[[Category:Churches in Reykjav\u00edk]]\n[[Category:Cathedrals in Iceland]]\n[[Category:Basilica churches in Europe]]\n[[Category:Roman Catholic cathedrals in Iceland]]\n[[Category:Roman Catholic churches completed in 1929]]\n[[Category:Tourist attractions in Reykjav\u00edk]]\n", "text_old": "{{Infobox church\n| name                   = Cathedral of Christ the King\n| fullname               = The Cathedral Basilica of Christ the King\n| other name             = \n| native_name            = Landakotskirkja\n| native_name_lang       = Icelandic\n| image                  = Landakotskirkja - panoramio (3).jpg\n| imagesize              = 250px\n| imagelink              = \n| imagealt               = \n| landscape              = \n| caption                = The Cathedral of Christ the King in 2012\n| pushpin map            = \n| pushpin label position = \n| pushpin map alt        = \n| pushpin mapsize        = \n| relief                 = \n| map caption            = \n| coordinates            = {{coord|64.1475|-21.9485|format=dms|type:landmark_region:IS\n|display=inline,title}}\n| osgraw                 = <!--  TEXT -->\n| osgridref              = <!-- {{gbmappingsmall| TEXT}} -->\n| location               = [[Reykjav\u00edk]]\n| country                = [[Iceland]]\n| denomination           = [[Roman Catholic]]\n| previous denomination  = \n| churchmanship          = \n| membership             = \n| attendance             = \n| website                = \n| former name            = \n| bull date              = \n| founded date           = \n| founder                = \n| dedication             = \n| dedicated date         = \n| consecrated date       = 23 July 1929\n| cult                   = \n| relics                 = \n| events                 = \n| past bishop            = \n| people                 = \n| status                 = Active\n| functional status      = Cathedral\n| heritage designation   = \n| designated date        = \n| architect              = [[Gu\u00f0j\u00f3n Sam\u00faelsson]]\n| architectural type     = \n| style                  = [[Neo-Gothic]]\n| groundbreaking         = \n| completed date         = \n| construction cost      = \n| closed date            = \n| demolished date        = \n| capacity               = \n| length                 = <!-- {{convert| }} -->\n| width                  = <!-- {{convert| }} -->\n| width nave             = <!-- {{convert| }} -->\n| height                 = <!-- {{convert| }} -->\n| diameter               = <!-- {{convert| }} -->\n| other dimensions       = \n| floor count            = \n| floor area             = <!-- {{convert| }} -->\n| dome quantity          = \n| dome height outer      = <!-- {{convert| }} -->\n| dome height inner      = <!-- {{convert| }} -->\n| dome dia outer         = <!-- {{convert| }} -->\n| dome dia inner         = <!-- {{convert| }} -->\n| spire quantity         = \n| spire height           = <!-- {{convert| }} -->\n| materials              = \n| bells                  = \n| bells hung             = \n| bell weight            = <!-- {{CwtQtrLb to kg| }} -->\n| parish                 = \n| deanery                = \n| archdeaconry           = \n| episcopalarea          = \n| archdiocese            = \n| metropolis             = \n| diocese                = [[Reykjav\u00edk]]\n| province               = \n| presbytery             = \n| synod                  = \n| circuit                = \n| district               = \n| division               = \n| subdivision            = \n| archbishop             = \n| bishop                 = [[David Tencer]]\n| dean                   = \n| provost                = \n| provost-rector         = \n| viceprovost            = \n| subdean                = \n| precentor              = \n| chancellor             = \n| canonchancellor        = \n| canon                  = \n| canonmissioner         = \n| canonpastor            = \n| canontreasurer         = \n| succentor              = \n| archdeacon             = \n| prebendary             = \n| rector                 = \n| vicar                  = \n| curate                 = \n| priestincharge         = \n| priest                 = \n| asstpriest             = \n| minister               = \n| assistant              = \n| honpriest              = \n| deacon                 = \n| deaconness             = \n| seniorpastor           = \n| pastor                 = \n| abbot                  = \n| chaplain               = \n| reader                 = \n| student intern         = \n| organistdom            = \n| director               = \n| organist               = \n| organscholar           = \n| chapterclerk           = \n| laychapter             = \n| warden                 = \n| verger                 = \n| businessmgr            = \n| liturgycoord           = \n| reledu                 = \n| rcia                   = \n| youthmin               = \n| flowerguild            = \n| musicgroup             = \n| parishadmin            = \n| serversguild           = \n| logo                   = \n| logosize               = \n| logolink               = \n| logoalt                = \n}}\n\n'''Landakotskirkja''' (Landakot's Church), formally '''Basilika Krists konungs''' (The Basilica of Christ the King), is the cathedral of the [[Roman Catholicism in Iceland|Catholic Church in Iceland]]. Often referred to as Kristskirkja (Christ's Church), Landakotskirkja is in the western part of [[Reykjav\u00edk]], Iceland's capital city.\n\n==Architecture==\nLandakotskirkja has a distinctively flat top instead of a standard spire. Its architect was [[Gu\u00f0j\u00f3n Sam\u00faelsson]], who also designed [[Hallgr\u00edmskirkja]], a Reykjavik landmark, and [[Akureyrarkirkja]] in Akureyri, North Iceland.\n\n==History==\nThe first [[Catholic priest]]s to arrive in Iceland after the [[Protestant Reformation|Reformation]] were the Frenchmen [[Bernard Bernard]] and [[Jean-Baptiste Baudoin]]. They bought the [[Landakot]] farmstead in Reykjav\u00edk and settled there in the early 19th century.  They built a small chapel in 1864. A few years later, a small wooden church was erected by [[T\u00fangata]], close to Landakot. After the [[First World War]], Icelandic Catholics saw the need to build a bigger church for the growing number of Catholics. They decided to build a [[Neo-Gothic]] church and entrusted the task to the architect [[Gu\u00f0j\u00f3n Sam\u00faelsson]].  After years of construction, Landakotskirkja was finally sanctified on 23 July 1929. It was the largest church in Iceland at the time.  Today, Landakotskirkja is a distinct landmark in western Reykjav\u00edk. The only Catholic school in Iceland was located nearby on the same land.\n\nA big part of the furniture comes from the renowned Atelier J.W. Ramakers & Sons sculptors from Geleen, Holland. Ramakers delivered in 1928 the timpan, both the side altars, the St. Joseph altar in 1905 and the Maria altar in 1928, in 1929 the pulpit. Atelier Ramakers made a design of the main altar, however it was never delivered.\n\n==See also==\n*[[Diocese of Reykjav\u00edk]]\n*[[Bishop of Reykjav\u00edk (Catholic)]]\n*[[Roman Catholicism in Iceland]]\n* [[List of cathedrals in Iceland]]\n*[[Christianity in Iceland]]\n\n==External links==\n{{Commons cat|Landakotskirkja}}\n\n*[https://web.archive.org/web/20160303165548/http://kirkjukort.net/kirkjur/landakotskirkja_0308.html Landakotskirkja on the Icelandic Church Map]\n*[http://www.catholica.is/lkin.html Images of the Cathedral of Christ the King]\n*[http://www.catholica.is/ Roman Catholic Diocese of Reykjav\u00edk] {{in lang|en|is}}\n\n[[Category:Gothic Revival architecture in Iceland]]\n[[Category:Gothic Revival church buildings]]\n[[Category:Churches in Reykjav\u00edk]]\n[[Category:Cathedrals in Iceland]]\n[[Category:Basilica churches in Europe]]\n[[Category:Roman Catholic cathedrals in Iceland]]\n[[Category:Roman Catholic cathedrals in Europe]]\n[[Category:Roman Catholic churches completed in 1929]]\n[[Category:Tourist attractions in Reykjav\u00edk]]\n", "name_user": "Wikiacc", "label": "safe", "comment": "removedCategory:Roman Catholic cathedrals in EuropeusingHotCat: diffused parent category", "url_page": "//en.wikipedia.org/wiki/Christ_the_King_Cathedral_(Reykjav%C3%ADk)"}
{"title_page": "Chuck Biscuits", "text_new": "{{Infobox musical artist  <!-- See Wikipedia:WikiProject Musicians -->\n| name                  = Chuck Biscuits\n| image                 =\n| caption               = \n| image_size            = <!-- Only for images narrower than 220 pixels -->\n| background            = non_vocal_instrumentalist\n| birth_name            = Charles Montgomery\n| alias                 = \n| birth_date            = {{birth date and age|1965|4|17}}\n| birth_place           \n| death_date            = \n| origin                =  \n| instrument            = [[Drum kit|Drums]]\n| genre                 = [[Punk rock]], [[Heavy metal music|heavy metal]]\n| occupation            = Musician\n| years_active          = 1977&ndash;1999\n| label                 = \n| associated_acts       = [[Danzig (band)|Danzig]]<br>[[Social Distortion]]<br>[[D.O.A. (band)|D.O.A.]]<br>[[Circle Jerks]]<br>[[Samhain (band)|Samhain]]<br>[[Black Flag (band)|Black Flag]]<br>[[The Subhumans (Canadian band)|The Subhumans]]<br>[[The Weirdos]]<br>[[Fear (band)|Fear]]<br>[[The Four Horsemen (band)|The Four Horsemen]]<br>[[The Nig-Heist]]<br>Floorlords<br>Brown Sound<br>Victorian Pork\n| website               = \n}}\n\n'''Chuck Biscuits''' (born '''Charles Montgomery''' on April 17, 1965) is a [[Canadians|Canadian]] drummer best known for his work in rock acts such as [[Danzig (band)|Danzig]], [[Black Flag (band)|Black Flag]], [[D.O.A. (band)|D.O.A.]], and [[Circle Jerks]]. Most recently, he was a member of the punk rock band [[Social Distortion]] in the late 1990s. Biscuits has named his main influences as [[John Bonham]], [[Rat Scabies]] of the Damned, [[Topper Headon]] of the Clash, [[Keith Moon]] and [[Stewart Copeland]].<ref>{{cite web|url=http://www.misfitscentral.com/display.php?t=darticle&u=dforce&f=dforce00.txt|title=Influences/Interests: Chuck Biscuits|year=1990|publisher=DANZIG Force promo package|accessdate=2012-01-05}}</ref>\n\n==Biography==\nBiscuits is the brother of Ken \"Dimwit\" Montgomery, a fixture on the [[Vancouver]] music scene\u2014at different times, both brothers played drums for [[D.O.A. (band)|D.O.A.]]. Biscuits joined [[Black Flag (band)|Black Flag]] in 1982 and toured with them for 5 months. His only studio recordings for the band were the [[1982 Demos|1982 demos]] for the ''[[My War]]'' album, which have been widely bootlegged.<ref name=\"Misfits Central - Black Flag\">{{cite web|url=http://www.misfitscentral.com/bands/black-flag.php|title=Black Flag|publisher=Misfits Central|accessdate=2009-09-26}}</ref>\n\nAfter Black Flag, he played with the short-lived Floorlords. When that band broke up, Biscuits planned to quit the music business and took courses in art and electrical engineering. He briefly filled in on drums for a few shows for the [[Red Hot Chili Peppers]] during their ''[[Freaky Styley tour]]'' in 1986. A year later, producer [[Rick Rubin]] invited Biscuits to become the drummer for Danzig.<ref name=\"Flipside\">{{cite web|url=http://misfitscentral.com/darticle/flip84.html|title=Biscuits|date=May\u2013June 1993|publisher=Flipside|accessdate=2009-09-26}}</ref> Biscuits had been vocalist/songwriter [[Glenn Danzig|Glenn Danzig's]] first choice as drummer for his band.<ref name=\"Metal Mania\">{{cite web|url=http://misfitscentral.com/display.php?t=darticle&f=mania.88|title=Danzig with Danzig|last=Natanael|first=Christine|year=1988|publisher=Metal Mania|accessdate=2009-09-26}}</ref>\n\nBiscuits joined Danzig in 1987 and appeared on the band's first four albums and one EP. In 1990 he recorded drums for one track on Glenn Danzig's final album with the band [[Samhain (band)|Samhain]]. In 1994 he became the first member of the original Danzig line-up to leave the band, citing a contract dispute as the reason for his departure.<ref name=\"Entertainment Weekly\">{{cite magazine|url=http://www.ew.com/ew/article/0,,304064,00.html|title=Sympathy for the Devil|date=October 14, 1994|magazine=Entertainment Weekly|accessdate=2009-09-26}}</ref>\n\nBiscuits participated in a special concert held on December 28, 1994 to honour the lifetime achievements of his brother Ken, who had died earlier in the year of a drug overdose.\n\nOn October 27, 2009, a blogger named James Greene, Jr. who claimed to have been in recent contact with Biscuits posted a report on his blog announcing that the drummer had died on October 24 of [[Head and neck cancer|throat cancer]]. This report quickly circulated to multiple media sources,<ref>[https://www.cbc.ca/news/canada/british-columbia/story/2009/10/29/chuck-biscuits.html Original CBC report of death]{{dead link|date=October 2011}}</ref><ref name=\"inlog.org\">[http://inlog.org/2009/10/29/r-i-p-chuck-biscuits-1965-2009/ R.I.P. Chuck Biscuits (1965 \u2013 2009)]; www.inlog.org.</ref> but was soon questioned by Biscuits' friends and family as a hoax. That evening, Biscuit's sister-in-law e-mailed Greene to confirm that the musician was still alive.<ref>{{cite web |url=http://jgtwo.wordpress.com/2009/10/27/chuck-biscuits-1965-2009/ |title=Chuck Biscuits: 1965-2009|work=[[JG2Land]]|date=October 27, 2009|accessdate=August 14, 2011}}</ref> Biscuits himself never released a statement concerning the death hoax, and Greene would later argue that Biscuits was entirely responsible for the false report.<ref>{{cite web|url=http://jgtwo.wordpress.com/2009/11/11/the-chuck-biscuits-death-hoaxer-chuck-biscuits |title=The Chuck Biscuits Death Hoaxer: Chuck Biscuits? |publisher=Jgtwo.wordpress.com |date=2009-11-11 |accessdate=2011-10-03}}</ref><ref name=\"JG2Land\">[https://archive.is/20130119183702/http://www.crawdaddyarchive.com/index.php/2010/03/11/chuck-biscuits-could-not-care-less-you-thought-he-was-dead/ Chuck Biscuits \"Could Not Care Less\" You Thought He Was Dead]; www.crawdaddyarchive.com.</ref>\n\n==Equipment==\nBiscuits used [[Pro-Mark]]<ref name=\"Musician\">{{cite web|url=http://www.misfitscentral.com/display.php?t=darticle&f=musician.894|title=Danzig Knows the Power of the Dark Side|last=Young|first=Jon|date=August 1994|publisher=[[Musician (magazine)|Musician]]|accessdate=2011-02-09}}</ref> DC-10<ref>Von, Eerie. ''Misery Obscura: The Photography Of Eerie Von (1981-2009)''. Milwaukie, OR: Dark Horse Books, 2009: p. 140</ref> marching sticks to drum. His drumkit at the beginning of Danzig was a black [[Premier Percussion|Premier]] Resonator, though he switched to a chrome covered 1970s era [[Ludwig-Musser|Ludwig]]<ref name=\"Musician\"/> Classic maple kit for ''Danzig II'', and continued with that kit to record and tour for ''Danzig III'' and ''Danzig IV''. Biscuits was usually seen using [[Avedis Zildjian Company|Zildjian]] cymbals. His regular ride cymbal sound during his work with Danzig was a Zildjian 22\" Earth Ride.<ref name=\"Musician\"/> Biscuits also favored [[Paiste]] Rude cymbals. He used medium and rock ride cymbals as crashes. Biscuits' mainstay snare with Danzig was a [[Sonor]] steel model, though he also used a Ludwig piccolo snare.<ref name=\"Musician\"/>\n\nWith D.O.A. and Black Flag, Biscuits used an older blonde maple Ludwig kit. For Social Distortion, Biscuits used a Boom Theory kit, including a Bridgedeck snare built by Al Adinolfi.\n\n==Discography==\n\n===D.O.A.  (1978\u20131982)===\n\n* ''Disco Sucks''\n* ''The Prisoner''\n* ''Triumph of the Ignoroids''\n* ''World War III''\n* ''Vancouver Complication''\n* ''[[Something Better Change]]''\n* ''[[Hardcore '81]]''\n* ''[[Let Them Eat Jellybeans]]''\n* ''Positively D.O.A. (No God, No Country, No Lies)''\n* ''Rat Music For Rat People Vol. 1''\n* ''Bloodied But Unbowed''\n* ''[[War on 45]]'' <small>(partial)</small>\n* ''1978'' (2019 2 LP single cd compilation featuring rare Biscuits performances)\n\n===Black Flag (1982)===\n\n* 1982 - ''[[1982 Demos|The Complete 1982 Demos Plus More]]'' (Bootleg album)\n* 2010 - ''[[Live at the On Broadway 1982]]'' (Live Album)\n\n===Circle Jerks (1983-1984)===\n* ''[[Repo Man (film)|Repo Man]]'' soundtrack\n* ''The Best of Flipside Video''\n\n===Floorlords===\n\n* ''Black Ice Ride 2-Nite''\n\n===Glenn Danzig And The Power And Fury Orchestra===\n\n* 1987 - ''[[Less Than Zero (film)|Less Than Zero]]'' soundtrack\n\n===Danzig (1987-1994)===\n\n* 1988 - ''[[Danzig (album)|Danzig]]''\n* 1990 - ''[[Danzig II: Lucifuge]]''\n* 1992 - ''[[Danzig III: How the Gods Kill]]''\n* 1993 - ''[[Thrall: Demonsweatlive]]''\n* 1994 - ''[[Danzig 4]]''\n* 2001 - ''[[Live on the Black Hand Side]]''\n* 2007 - ''[[The Lost Tracks of Danzig]]''\n\n===Run-D.M.C.===\n\n* 1988 - ''[[Tougher Than Leather]]'' (drum tracks only)\n\n===Samhain===\n\n* 1990 - ''[[Final Descent (album)|Final Descent]]''\n\n===Social Distortion (1996-1999)===\n\n* 1998 - ''[[Live at the Roxy (Social Distortion album)|Live at the Roxy]]''\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.misfitscentral.com/bios/chuck-biscuits.php Biography from Misfits Central]\n\n{{Black Flag}}\n{{Circle Jerks}}\n{{Danzig}}\n{{Fear (band)}}\n{{Social Distortion}}\n{{Samhain}}\n{{D.O.A.}}\n{{Red Hot Chili Peppers}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Biscuits, Chuck}}\n[[Category:1965 births]]\n[[Category:Living people]]\n[[Category:American punk rock drummers]]\n[[Category:American male drummers]]\n[[Category:Black Flag (band) members]]\n[[Category:Place of birth missing (living people)]]\n[[Category:Hardcore punk musicians]]\n[[Category:Social Distortion members]]\n[[Category:Fear (band) members]]\n[[Category:Circle Jerks members]]\n[[Category:Danzig (band) members]]\n[[Category:Horror punk musicians]]\n[[Category:Samhain (band) members]]\n[[Category:20th-century American drummers]]\n[[Category:D.O.A. (band) members]]\n", "text_old": "{{Infobox musical artist  <!-- See Wikipedia:WikiProject Musicians -->\n| name                  = Chuck Biscuits\n| image                 =\n| caption               = \n| image_size            = <!-- Only for images narrower than 220 pixels -->\n| background            = non_vocal_instrumentalist\n| birth_name            = Charles Montgomery\n| alias                 = \n| birth_date            = {{birth date and age|1965|4|17}}\n| birth_place           \n| death_date            = \n| origin                =  \n| instrument            = [[Drum kit|Drums]]\n| genre                 = [[Punk rock]], [[Heavy metal music|heavy metal]]\n| occupation            = Musician\n| years_active          = 1977&ndash;1999\n| label                 = \n| associated_acts       = [[Danzig (band)|Danzig]]<br>[[Social Distortion]]<br>[[D.O.A. (band)|D.O.A.]]<br>[[Circle Jerks]]<br>[[Samhain (band)|Samhain]]<br>[[Black Flag (band)|Black Flag]]<br>[[The Subhumans (Canadian band)|The Subhumans]]<br>[[The Weirdos]]<br>[[Fear (band)|Fear]]<br>[[The Four Horsemen (band)|The Four Horsemen]]<br>[[The Nig-Heist]]<br>Floorlords<br>Brown Sound<br>Victorian Pork\n| website               = \n}}\n\n'''Chuck Biscuits''' (born '''Charles Montgomery''' on April 17, 1965) is a [[Canadians|Canadian]] drummer best known for his work in rock acts such as [[Danzig (band)|Danzig]], [[Black Flag (band)|Black Flag]], [[D.O.A. (band)|D.O.A.]], and [[Circle Jerks]]. Most recently, he was a member of the punk rock band [[Social Distortion]] in the late 1990s. Biscuits has named his main influences as [[John Bonham]], [[Rat Scabies]] of the Damned, [[Topper Headon]] of the Clash, [[Keith Moon]] and [[Stewart Copeland]].<ref>{{cite web|url=http://www.misfitscentral.com/display.php?t=darticle&u=dforce&f=dforce00.txt|title=Influences/Interests: Chuck Biscuits|year=1990|publisher=DANZIG Force promo package|accessdate=2012-01-05}}</ref>\n\n==Biography==\nBiscuits is the brother of Ken \"Dimwit\" Montgomery, a fixture on the [[Vancouver]] music scene\u2014at different times, both brothers played drums for [[D.O.A. (band)|D.O.A.]]. Biscuits joined [[Black Flag (band)|Black Flag]] in 1982 and toured with them for 5 months. His only studio recordings for the band were the [[1982 Demos|1982 demos]] for the ''[[My War]]'' album, which have been widely bootlegged.<ref name=\"Misfits Central - Black Flag\">{{cite web|url=http://www.misfitscentral.com/bands/black-flag.php|title=Black Flag|publisher=Misfits Central|accessdate=2009-09-26}}</ref>\n\nAfter Black Flag, he played with the short-lived Floorlords. When that band broke up, Biscuits planned to quit the music business and took courses in art and electrical engineering. He briefly filled in on drums for a few shows for the [[Red Hot Chili Peppers]] during their ''[[Freaky Styley tour]]'' in 1986. A year later, producer [[Rick Rubin]] invited Biscuits to become the drummer for Danzig.<ref name=\"Flipside\">{{cite web|url=http://misfitscentral.com/darticle/flip84.html|title=Biscuits|date=May\u2013June 1993|publisher=Flipside|accessdate=2009-09-26}}</ref> Biscuits had been vocalist/songwriter [[Glenn Danzig|Glenn Danzig's]] first choice as drummer for his band.<ref name=\"Metal Mania\">{{cite web|url=http://misfitscentral.com/display.php?t=darticle&f=mania.88|title=Danzig with Danzig|last=Natanael|first=Christine|year=1988|publisher=Metal Mania|accessdate=2009-09-26}}</ref>\n\nBiscuits joined Danzig in 1987 and appeared on the band's first four albums and one EP. In 1990 he recorded drums for one track on Glenn Danzig's final album with the band [[Samhain (band)|Samhain]]. In 1994 he became the first member of the original Danzig line-up to leave the band, citing a contract dispute as the reason for his departure.<ref name=\"Entertainment Weekly\">{{cite magazine|url=http://www.ew.com/ew/article/0,,304064,00.html|title=Sympathy for the Devil|date=October 14, 1994|magazine=Entertainment Weekly|accessdate=2009-09-26}}</ref>\n\nBiscuits participated in a special concert held on December 28, 1994 to honour the lifetime achievements of his brother Ken, who had died earlier in the year of a drug overdose.\n\nOn October 27, 2009, a blogger named James Greene, Jr. who claimed to have been in recent contact with Biscuits posted a report on his blog announcing that the drummer had died on October 24 of [[Head and neck cancer|throat cancer]]. This report quickly circulated to multiple media sources,<ref>[https://www.cbc.ca/news/canada/british-columbia/story/2009/10/29/chuck-biscuits.html Original CBC report of death]{{dead link|date=October 2011}}</ref><ref name=\"inlog.org\">[http://inlog.org/2009/10/29/r-i-p-chuck-biscuits-1965-2009/ R.I.P. Chuck Biscuits (1965 \u2013 2009)]; www.inlog.org.</ref> but was soon questioned by Biscuits' friends and family as a hoax. That evening, Biscuit's sister-in-law e-mailed Greene to confirm that the musician was still alive.<ref>{{cite web |url=http://jgtwo.wordpress.com/2009/10/27/chuck-biscuits-1965-2009/ |title=Chuck Biscuits: 1965-2009|work=[[JG2Land]]|date=October 27, 2009|accessdate=August 14, 2011}}</ref> Biscuits himself never released a statement concerning the death hoax, and Greene would later argue that Biscuits was entirely responsible for the false report.<ref>{{cite web|url=http://jgtwo.wordpress.com/2009/11/11/the-chuck-biscuits-death-hoaxer-chuck-biscuits |title=The Chuck Biscuits Death Hoaxer: Chuck Biscuits? |publisher=Jgtwo.wordpress.com |date=2009-11-11 |accessdate=2011-10-03}}</ref><ref name=\"JG2Land\">[https://archive.is/20130119183702/http://www.crawdaddyarchive.com/index.php/2010/03/11/chuck-biscuits-could-not-care-less-you-thought-he-was-dead/ Chuck Biscuits \"Could Not Care Less\" You Thought He Was Dead]; www.crawdaddyarchive.com.</ref>\n\n==Equipment==\nBiscuits used [[Pro-Mark]]<ref name=\"Musician\">{{cite web|url=http://www.misfitscentral.com/display.php?t=darticle&f=musician.894|title=Danzig Knows the Power of the Dark Side|last=Young|first=Jon|date=August 1994|publisher=[[Musician (magazine)|Musician]]|accessdate=2011-02-09}}</ref> DC-10<ref>Von, Eerie. ''Misery Obscura: The Photography Of Eerie Von (1981-2009)''. Milwaukie, OR: Dark Horse Books, 2009: p. 140</ref> marching sticks to drum. His drumkit at the beginning of Danzig was a black [[Premier Percussion|Premier]] Resonator, though he switched to a chrome covered 1970s era [[Ludwig-Musser|Ludwig]]<ref name=\"Musician\"/> Classic maple kit for ''Danzig II'', and continued with that kit to record and tour for ''Danzig III'' and ''Danzig IV''. Biscuits was usually seen using [[Avedis Zildjian Company|Zildjian]] cymbals. His regular ride cymbal sound during his work with Danzig was a Zildjian 22\" Earth Ride.<ref name=\"Musician\"/> Biscuits also favored [[Paiste]] Rude cymbals. He used medium and rock ride cymbals as crashes. Biscuits' mainstay snare with Danzig was a [[Sonor]] steel model, though he also used a Ludwig piccolo snare.<ref name=\"Musician\"/>\n\nWith D.O.A. and Black Flag, Biscuits used an older blonde maple Ludwig kit. For Social Distortion, Biscuits used a Boom Theory kit, including a Bridgedeck snare built by Al Adinolfi.\n\n==Discography==\n\n===D.O.A.  (1978\u20131982)===\n\n* ''Disco Sucks''\n* ''The Prisoner''\n* ''Triumph of the Ignoroids''\n* ''World War III''\n* ''Vancouver Complication''\n* ''[[Something Better Change]]''\n* ''[[Hardcore '81]]''\n* ''[[Let Them Eat Jellybeans]]''\n* ''Positively D.O.A. (No God, No Country, No Lies)''\n* ''Rat Music For Rat People Vol. 1''\n* ''Bloodied But Unbowed''\n* ''[[War on 45]]'' <small>(partial)</small>\n* ''1978'' (2019 2 LP single cd compilation featuring rare Biscuits performances)\n\n===Black Flag (1982)===\n\n* 1982 - ''[[1982 Demos|The Complete 1982 Demos Plus More]]'' (Bootleg album)\n* 2010 - ''[[Live at the On Broadway 1982]]'' (Live Album)\n\n===Circle Jerks (1983-1984)===\n* ''[[Repo Man (film)|Repo Man]]'' soundtrack\n* ''The Best of Flipside Video''\n\n===Floorlords===\n\n* ''Black Ice Ride 2-Nite''\n\n===Glenn Danzig And The Power And Fury Orchestra===\n\n* 1987 - ''[[Less Than Zero (film)|Less Than Zero]]'' soundtrack\n\n===Danzig (1987-1994)===\n\n* 1988 - ''[[Danzig (album)|Danzig]]''\n* 1990 - ''[[Danzig II: Lucifuge]]''\n* 1992 - ''[[Danzig III: How the Gods Kill]]''\n* 1993 - ''[[Thrall: Demonsweatlive]]''\n* 1994 - ''[[Danzig 4]]''\n* 2001 - ''[[Live on the Black Hand Side]]''\n* 2007 - ''[[The Lost Tracks of Danzig]]''\n\n===Run-D.M.C.===\n\n* 1988 - ''[[Tougher Than Leather]]'' (drum tracks only)\n\n===Samhain===\n\n* 1990 - ''[[Final Descent]]''\n\n===Social Distortion (1996-1999)===\n\n* 1998 - ''[[Live at the Roxy (Social Distortion album)|Live at the Roxy]]''\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.misfitscentral.com/bios/chuck-biscuits.php Biography from Misfits Central]\n\n{{Black Flag}}\n{{Circle Jerks}}\n{{Danzig}}\n{{Fear (band)}}\n{{Social Distortion}}\n{{Samhain}}\n{{D.O.A.}}\n{{Red Hot Chili Peppers}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Biscuits, Chuck}}\n[[Category:1965 births]]\n[[Category:Living people]]\n[[Category:American punk rock drummers]]\n[[Category:American male drummers]]\n[[Category:Black Flag (band) members]]\n[[Category:Place of birth missing (living people)]]\n[[Category:Hardcore punk musicians]]\n[[Category:Social Distortion members]]\n[[Category:Fear (band) members]]\n[[Category:Circle Jerks members]]\n[[Category:Danzig (band) members]]\n[[Category:Horror punk musicians]]\n[[Category:Samhain (band) members]]\n[[Category:20th-century American drummers]]\n[[Category:D.O.A. (band) members]]\n", "name_user": "Tassedethe", "label": "safe", "comment": "v2.02 - Repaired 1 link to disambiguation page -(You can help)-Final Descent", "url_page": "//en.wikipedia.org/wiki/Chuck_Biscuits"}
{"title_page": "Ventilator", "text_new": "{{mergeto|Mechanical ventilation|discuss=Talk:Ventilator#Merger proposal|date=April 2020}}\n{{Short description|Device that provides mechanical ventilation to the lungs}}\n{{about|one type of machine used to assist breathing|the broader article, on both positive- and negative-pressure devices|Mechanical ventilation<!-- note: \"medical respirator\" redirects here mainly due to historical confusion-->|respiratory PPE worn on the face|Respirator|ventilation subjects|Ventilation (disambiguation)}}\n{{Infobox medical equipment\n| name           = Medical ventilator\n| acronym        =\n| synonym        =\n| image          = VIP Bird2.jpg\n| caption        = The Bird VIP Infant ventilator\n| alt            =\n| image_size     =\n| specialty      = Pulmonology\n| intervention   =\n| MedlinePlus    =\n| eMedicine      =\n| inventor       =\n| invention date =\n| manufacturer   =\n| related        =\n}}\nA '''ventilator''' is a machine that provides [[mechanical ventilation]] by moving breathable air into and out of the [[lungs]], to deliver breaths to a patient who is physically unable to breathe, or breathing insufficiently. Modern ventilators are [[computer]]ized [[microprocessor control| microprocessor-control]]led machines, but patients can also be ventilated with a simple, hand-operated [[bag valve mask]].  Ventilators are chiefly used in [[intensive-care medicine]], [[home care]], and [[emergency medicine]] (as standalone units) and in [[anesthesiology]] (as a component of an [[anesthesia machine]]).\n\nVentilators are sometimes called \"respirators\", a term commonly used for them in the 1950s (particularly the [[Forrest Bird#Mechanical ventilators| \"Bird respirator\"]]). However, contemporary hospital and medical terminology uses the word \"[[respirator]]\" to refer to a protective face-mask.<ref>\n{{cite journal\n |author= Center for Devices and Radiological Health |date= 2019-02-08 |title= Personal Protective Equipment for Infection Control - Masks and N95 Respirators\n |journal= FDA \n |url= https://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/GeneralHospitalDevicesandSupplies/PersonalProtectiveEquipment/ucm055977.htm\n |access-date= 2017-03-08\n}}\n</ref>\n\n==Function==\nIn its simplest form, a modern [[positive pressure ventilation|positive pressure ventilator]] consists of a compressible [[air]] reservoir or turbine, air and [[oxygen]] supplies, a set of valves and tubes, and a disposable or reusable \"patient circuit\". The air reservoir is pneumatically compressed several times a minute to deliver room-air, or in most cases, an air/oxygen mixture to the patient. If a turbine is used, the turbine pushes air through the ventilator, with a flow valve adjusting pressure to meet patient-specific parameters. When over pressure is released, the patient will exhale passively due to the [[lung]]s' elasticity, the exhaled air being released usually through a [[one-way valve]] within the patient circuit called the patient manifold.\n\nVentilators may also be equipped with monitoring and alarm systems for patient-related parameters (e.g. pressure, volume, and flow) and ventilator function (e.g. air leakage, power failure, mechanical failure), backup batteries, oxygen tanks, and remote control. The pneumatic system is nowadays often replaced by a computer-controlled [[turbopump]].\n\nModern ventilators are electronically controlled by a small [[embedded system]] to allow exact adaptation of pressure and flow characteristics to an individual patient's needs. Fine-tuned ventilator settings also serve to make ventilation more tolerable and comfortable for the patient. In Canada and the United States, [[respiratory therapists]] are responsible for tuning these settings, while biomedical technologists are responsible for the maintenance.  In the United Kingdom and Europe the management of the patient's interaction with the ventilator is done by [[Critical care nursing | critical care]] nurses.\n\nThe patient circuit usually consists of a set of three durable, yet lightweight plastic tubes, separated by function (e.g. inhaled air, patient pressure, exhaled air). Determined by the type of ventilation needed, the patient-end of the circuit may be either noninvasive or invasive.\n\nNoninvasive methods, such as [[Continuous_positive_airway_pressure|continuous positive airway pressure (CPAP)]] and [[non-invasive ventilation]], which are adequate for patients who require a ventilator only while sleeping and resting, mainly employ a nasal mask. Invasive methods require [[intubation]], which for long-term ventilator dependence will normally be a [[tracheotomy]] cannula, as this is much more comfortable and practical for long-term care than is larynx or nasal intubation.\n\n===Life-critical system===\n{{unreferenced section|date=April 2020}}\nBecause failure may result in death, mechanical ventilation systems are classified as [[life-critical system]]s, and precautions must be taken to ensure that they are highly reliable, including their [[power-supply|power supply]]. Mechanical ventilators are therefore carefully designed so that no [[single point of failure]] can endanger the patient. They may have manual backup mechanisms to enable hand-driven respiration in the absence of power (such as the mechanical ventilator integrated into an [[anaesthetic machine]]). They may also have safety valves, which open to atmosphere in the absence of power to act as an anti-suffocation valve for spontaneous breathing of the patient. Some systems are also equipped with compressed-gas tanks, air compressors or backup batteries to provide ventilation in case of power failure or defective gas supplies, and methods to operate or call for help if their mechanisms or software fail.<ref>{{cite news |last1=Johnson |first1=Carolyn Y. |last2=reporterEmailEmailBioBioFollowFollow |first2=closeCarolyn Y. JohnsonScience reporterEmailEmailBioBioFollowFollowAriana Eunjung Cha closeAriana Eunjung ChaNational |title=The dark side of ventilators: Those hooked up for long periods face difficult recoveries |url=https://www.washingtonpost.com/health/2020/04/03/coronavirus-survivors-recovery/?utm_campaign=wp_first_reads&utm_medium=email&utm_source=newsletter&wpisrc=nl_rainbow&wpmm=1 |accessdate=8 April 2020 |work=Washington Post |language=en}}</ref>\n\n==Major manufacturers==\n{| class=\"wikitable sortable\"\n|+ICU ventilators<ref>{{Cite news|last=Giorgio V.\u2005M\u00fcller|first=|url=https://www.nzz.ch/wirtschaft/weltweit-hat-es-zu-wenig-beatmungsgeraete-ld.1549108|title=Hersteller von Beatmungsger\u00e4ten produzieren massiv mehr, aber k\u00f6nnen die Nachfrage trotzdem nicht decken|date=2020-03-30|work=Neue Z\u00fcrcher Zeitung|access-date=2020-03-30|url-status=live|language=de}} With reference to: [[IPG Research]].</ref>\n!Manufacturer\n!Country\n!Market share (2019)\n|-\n|[[Getinge Group|Getinge]] \n|Sweden\n|22%\n|-\n|[[Hamilton Medical]] \n|USA, Switzerland\n|22%\n|-\n|[[Dr\u00e4gerwerk|Dr\u00e4ger]]\n|Germany\n|16 %\n|-\n|[[Mindray]] \n|China\n|10%\n|-\n|[[Medtronic]] \n|Ireland, USA\n|5%\n|-\n|[[L\u00f6wenstein Medical UK|L\u00f6wenstein Medical]] \n|Germany\n|3%\n|-\n|[[Vyaire Medical]]\n|USA\n|3%\n|-\n|[[GE Healthcare]]\n|USA\n|3%\n|-\n|[[Philips Respironics]]\n|Netherlands\n|3%\n|-\n|Others\n|\n|15%\n|}\n\n{| class=\"wikitable sortable\"\n|+Mobile ventilators<ref>{{Cite news|last=M\u00fcller, Giorgio V.|url=https://www.nzz.ch/wirtschaft/weltweit-hat-es-zu-wenig-beatmungsgeraete-ld.1549108|title=Hersteller von Beatmungsger\u00e4ten produzieren massiv mehr, aber k\u00f6nnen die Nachfrage trotzdem nicht decken|date=2020-03-30|work=Neue Z\u00fcrcher Zeitung|access-date=2020-03-30|url-status=live|language=de}} With reference to: [[IPG Research]].</ref>\n!Manufacturer\n!Country\n!Market share (rank order in 2019)\n|-\n|[[Dr\u00e4gerwerk|Dr\u00e4ger]] \n|Germany\n|24th\n|-\n|[[Weinmann Medical]] \n|Germany\n|21st\n|-\n|[[Hamilton Medical]] \n|USA, Switzerland\n|18th\n|-\n|[[Vyaire Medical]]\n|USA\n|5th\n|-\n|Customs \n|Japan\n|4th\n|-\n|O Two \n|Canada\n|4th\n|-\n|Smiths Medical\n|USA\n|4th\n|-\n|[[Medtronic]] \n|Ireland, USA\n|4th\n|-\n|Air Liquide Healthcare\n|France\n|3rd\n|-\n|(and 13 other vendors)\n|\n|\n|}\n\n== History ==\n{{refimprove section|date=April 2020}}\nThe history of mechanical ventilation begins with various versions of what was eventually called the [[Negative pressure ventilator|iron lung]], a form of noninvasive negative-pressure ventilator widely used during the [[Poliomyelitis|polio]] epidemics of the twentieth century after the introduction of the \"Drinker respirator\" in 1928, improvements introduced by [[John Haven Emerson]] in 1931,<ref name=\"pmid18189086\">{{cite journal |last=Geddes |first=LA |year=2007 |title=The history of artificial respiration |journal=IEEE Engineering in Medicine and Biology Magazine |pmid=18189086 |doi=10.1109/EMB.2007.907081 |volume=26 |issue=6 |pages=38\u201341}}</ref> and the [[Both respirator]] in 1937. Other forms of noninvasive ventilators, also used widely for polio patients, include [[Biphasic Cuirass Ventilation]], the rocking bed, and rather primitive positive pressure machines.<ref name=\"pmid18189086\" />\n\nIn 1949, John Haven Emerson developed a mechanical assister for anaesthesia with the cooperation of the anaesthesia department at [[Harvard University]]. Mechanical ventilators began to be used increasingly in anaesthesia and intensive care during the 1950s. Their development was stimulated both by the need to treat polio patients and the increasing use of [[muscle relaxant]]s during anaesthesia. Relaxant drugs paralyse the patient and improve operating conditions for the surgeon but also paralyse the respiratory muscles.\n[[File:East-Radcliffe Respirator Wellcome L0001305.jpg|alt=A machine with hoses and gauges on a wheeled cart|thumb|An East-Radcliffe respirator model from the mid-twentieth century]]\nIn the United Kingdom, the East Radcliffe and Beaver models were early examples. The former used a [[Sturmey-Archer]] bicycle [[hub gear]] to provide a range of speeds, and the latter an automotive [[windscreen wiper]] motor to drive the bellows used to inflate the lungs.<ref name=\"pmid13320798\">{{cite journal |vauthors=Russell WR, Schuster E, Smith AC, Spalding JM |date=April 1956 |title=Radcliffe respiration pumps |journal=[[The Lancet]] |pmid=13320798 |doi=10.1016/s0140-6736(56)90597-9 |volume=270 |issue=6922 |pages=539\u201341}}</ref> Electric motors were, however, a problem in the operating theatres of that time, as their use caused an explosion hazard in the presence of flammable anaesthetics such as [[diethyl ether|ether]] and [[cyclopropane]]. In 1952, Roger Manley of the [[Westminster Hospital]], London, developed a ventilator which was entirely gas-driven and became the most popular model used in Europe. It was an elegant design, and became a great favourite with European anaesthetists for four decades, prior to the introduction of models controlled by electronics. It was independent of electrical power and caused no explosion hazard. The original Mark I unit was developed to become the Manley Mark II in collaboration with the Blease company, which manufactured many thousands of these units. Its principle of operation was very simple, an incoming gas flow was used to lift a weighted bellows unit, which fell intermittently under gravity, forcing breathing gases into the patient's lungs. The inflation pressure could be varied by sliding the movable weight on top of the bellows. The volume of gas delivered was adjustable using a curved slider, which restricted bellows excursion. Residual pressure after the completion of expiration was also configurable, using a small weighted arm visible to the lower right of the front panel. This was a robust unit and its availability encouraged the introduction of positive pressure ventilation techniques into mainstream European anesthetic practice.\n\nThe 1955 release of [[Forrest Bird]]'s \"Bird Universal Medical Respirator\" in the United States changed the way mechanical ventilation was performed, with the small green box becoming a familiar piece of medical equipment.<ref name=AboutBird>{{cite web |last=Bellis |first=Mary |title=Forrest Bird invented a fluid control device, respirator & pediatric ventilator |publisher=About.com |url=http://inventors.about.com/od/bstartinventors/a/Forrest_Bird.htm |access-date=2009-06-04}}</ref> The unit was sold as the Bird Mark 7 Respirator and informally called the \"Bird\". It was a [[Pneumatics|pneumatic]] device and therefore required no [[electrical power]] source to operate.\n\nIn 1965, the Army Emergency Respirator was developed in collaboration with the Harry Diamond Laboratories (now part of the [[United States Army Research Laboratory|U.S. Army Research Laboratory]]) and [[Walter Reed Army Institute of Research]]. Its design incorporated the principle of fluid amplification in order to govern pneumatic functions. Fluid amplification allowed the respirator to be manufactured entirely without moving parts, yet capable of complex resuscitative functions.<ref>{{cite book |date=1965 |title=Army R, D & A. |publisher=Development and Engineering Directorate, HQ, U.S. Army Materiel Development and Readiness Command |url=https://books.google.com/?id=sTrO77DCgkwC&pg=RA6-PA33&lpg=RA6-PA33&dq=Army+Emergency+Respirator,+HDL#v=onepage&q=Army%20Emergency%20Respirator,%20HDL&f=false}}</ref> Elimination of moving parts increased performance reliability and minimized maintenance.<ref name=\":0\">{{cite journal |last1=Mon |first1=George |last2=Woodward |first2=Kenneth E. |last3=Straub |first3=Henrik |last4=Joyce |first4=James |last5=Meyer |first5=James |date=1966 |title=Fluid Amplifier-Controlled Medical Devices |journal=SAE Transactions |issn=0096-736X |jstor=44554326 |volume=74 |pages=217\u2013222}}</ref> The mask is composed of a [[poly(methyl methacrylate)]] (commercially known as [[Poly(methyl methacrylate)|Lucite]]) block, about the size of a pack of cards, with machined channels and a cemented or screwed-in cover plate.<ref name=\":1\">{{cite web |title=Army Research and Development Monthly Magazine |volume=6 |number=9 |url=https://asc.army.mil/docs/pubs/alt/archives/1965/Sep_1965.PDF}}</ref> The reduction of moving parts cut manufacturing costs and increased durability.<ref name=\":0\" />\n\nThe bistable fluid amplifier design allowed the respirator to function as both a respiratory assistor and controller. It could functionally transition between assistor and controller automatically, based on the patient's needs.<ref name=\":1\" /><ref name=\":0\" /> The dynamic pressure and turbulent jet flow of gas from inhalation to exhalation allowed the respirator to synchronize with the breathing of the patient.<ref>{{cite web |date=October 1965 |title=Fluid Amplification Symposium |volume=III |url=https://apps.dtic.mil/dtic/tr/fulltext/u2/623457.pdf}}</ref>\n\nIntensive care environments around the world revolutionized in 1971 by the introduction of the first SERVO 900 ventilator (Elema-Sch\u00f6nander). It was a small, silent and effective electronic ventilator, with the famous SERVO feedback system controlling what had been set and regulating delivery. For the first time, the machine could deliver the set volume in volume control ventilation.\n\nVentilators used under increased pressure (hyperbaric) require special precautions, and few ventilators can operate under these conditions.<ref name=SPUMS1998>{{cite journal |last=Skinner |first=M |year=1998 |title=Ventilator function under hyperbaric conditions |journal=South Pacific Underwater Medicine Society Journal |volume=28 |issue=2 |url=http://archive.rubicon-foundation.org/5927 |access-date=2009-06-04}}</ref> In 1979, Sechrist Industries introduced their Model 500A ventilator, which was specifically designed for use with [[hyperbaric chamber]]s.<ref name=JHM1988>{{cite journal |vauthors=Weaver LK, Greenway L, Elliot CG |year=1988 |title=Performance of the Seachrist 500A Hyperbaric Ventilator in a Monoplace Hyperbaric Chamber |journal=Journal of Hyperbaric Medicine |volume=3 |issue=4 |pages=215\u2013225 |url=http://archive.rubicon-foundation.org/4377 |access-date=2009-06-04}}</ref>\n\n===Microprocessor ventilators===\n[[Microprocessor control]] led to the third generation of [[intensive care unit]] (ICU) ventilators, starting with the [[Dr\u00e4gerwerk|Dr\u00e4ger]] EV-A<ref>{{cite web |title=Dr\u00e4ger - die Geschichte des Unternehmens |language= |publisher=Dr\u00e4ger |website=Dr\u00e4ger |url=https://www.draeger.com/Corporate/Content/draeger_die_geschichte_des_unternehmens.pdf |access-date=March 22, 2020}}</ref> in 1982 in Germany which allowed monitoring the patient's [[Breathing#Respiratory_disorders|breathing curve]] on an [[LCD monitor]]. One year later followed [[Puritan Bennett]] 7200 and Bear 1000, SERVO 300 and Hamilton Veolar over the next decade. [[Microprocessors]] enable customized gas delivery and monitoring, and mechanisms for gas delivery that are much more responsive to patient needs than previous generations of mechanical ventilators.<ref>{{cite journal |last=Kacmarek |first=Robert M. |date=August 2011 |title=The Mechanical Ventilator: Past, Present, and Future |journal=[[Respiratory Care (journal)|Respiratory Care]] |issn=0020-1324 |doi=10.4187/respcare.01420 |volume=56 |issue=8 |pages=1170\u20131180|doi-access=free }}</ref>\n<!--\nIn 1991, the SERVO 300 ventilator series was introduced, enabling treatment of all patient categories from adult to neonate with one ventilator. The SERVO 300 series had a unique gas delivery system with rapid flow-triggering response.\n\nIn 1999, the new LTV (Laptop Ventilator) Series was significantly smaller than other ventilators of the time, weighing approximately 6.4 kg (14 lb) and about the size of a laptop computer. This design kept the same functionality of the in-hospital ventilators while allowing patient mobility.\n\nA modular concept was introduced with SERVO-i in 2001, with one ventilator model throughout the ICU department, instead of a fleet of various models and brands for different user needs. With the modular ventilator, ICU departments could choose the modes and options, software and hardware needed for a particular patient category.\n\nIn the twenty-first century small portable ventilators like the SAVe II have been manufactured for forward combat use.<ref name=Automedx>{{cite web |title=SAVe II The Smallest and Easiest to Use Pre-hospital Ventilator |publisher=Automedx |website=Automedx |url=http://automedx.com/save-ii/ |access-date=March 31, 2019}}</ref>-->\n\n== Open-source ventilators ==\n{{Main articles|Open-source ventilator}}\nAn [[Open source|open-source]] ventilator is a disaster-situation ventilator made using a freely-licensed design, and ideally, freely-available components and parts. Designs, components, and parts may be anywhere from completely reverse-engineered to completely new creations, components may be [[Jury rigging|adaptations]] of various inexpensive existing products, and special hard-to-find and/or expensive parts may be 3D printed instead of sourced.<ref>{{cite web |last=Bender |first=Maddie |date=2020-03-17 |title=People Are Trying to Make DIY Ventilators to Meet Coronavirus Demand |website=Vice |url=https://www.vice.com/en_uk/article/5dm4mb/people-are-trying-to-make-diy-ventilators-to-meet-coronavirus-demand |access-date=2020-03-21}}</ref><ref>{{cite web |last=Toussaint |first=Kristin |date=2020-03-16 |title=These Good Samaritans with a 3D printer are saving lives by making new respirator valves for free |website=Fast Company |url=https://www.fastcompany.com/90477940/these-good-samaritans-with-a-3d-printer-are-saving-lives-by-making-new-respirator-valves-for-free |access-date=2020-03-17}}</ref> \n\nDuring the 2019-2020 [[COVID-19 pandemic]], various kinds of ventilators have been considered. Deaths caused by [[Coronavirus disease 2019|COVID-19]] have occurred when the most severely infected experience [[acute respiratory distress syndrome]], a widespread inflammation in the lungs that impairs the lungs' ability to absorb oxygen and expel carbon dioxide. These patients require a capable ventilator to continue breathing.\n\nAmong ventilators that might be brought into the fight, there have been many concerns. These include current availability,<ref>{{Cite web|url=https://www.npr.org/sections/health-shots/2020/03/14/815675678/as-the-pandemic-spreads-will-there-be-enough-ventilators|title=As The Pandemic Spreads, Will There Be Enough Ventilators?|last=NEIGHMOND|first=PATTI|date=March 14, 2020|website=NPR|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref><ref>{{Cite web|url=https://www.nsmedicaldevices.com/analysis/coronavirus-ventilators-global-demand/|title=880,000 more ventilators needed to cope with coronavirus outbreak, says analyst|last=Parker|first=Thomas|date=March 25, 2020|website=NS Medical Devices|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> the challenge of making more and lower cost ventilators,<ref>{{Cite web|url=https://www.youtube.com/watch?v=7vLPefHYWpY|title=A Guide To Designing Low-Cost Ventilators for COVID-19|last=|first=|date=April 4, 2020|website=YouTube|publisher=Real Engineering|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> effectiveness,<ref>{{Cite web|url=https://www.physiciansweekly.com/mortality-rate-of-covid-19-patients-on-ventilators/|title=Mortality rate of COVID-19 patients on ventilators|last=|first=|date=March 30, 2020|website=Physician's Weekly|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> [[functional design]], safety,<ref>{{Cite web|url=https://www.aarc.org/wp-content/uploads/2017/03/Issue-Paper-Safe-Initiation-and-Management-of-Mechanical-Ventilation.pdf|title=SAFE INITIATION AND MANAGEMENT OF MECHANICAL VENTILATION|last=|first=|date=2016|website=American Association for Respiratory Care|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref><ref>{{Cite web|url=https://www.ecri.org/components/HDJournal/Pages/Mechanical-Ventilation-of-SARS-Patients-2003-SARS-Outbreak.aspx|title=Mechanical Ventilation of SARS Patients: Lessons from the 2003 SARS Outbreak|last=|first=|date=February 18, 2020|website=ECRI|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> portability,<ref>{{Cite web|url=https://techcrunch.com/2020/03/30/medtronic-is-sharing-its-portable-ventilator-design-specifications-and-code-for-free-to-all/|title=Medtronic is sharing its portable ventilator design specifications and code for free to all|last=Etherington|first=Darrell|date=March 30, 2020|website=TechCrunch|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> suitability for infants,<ref>{{Cite web|url=https://www.bemesonline.com/bird-v-i-p-standard-infant-and-pediatric-ventilator/|title=Bird V.I.P Standard Infant and Pediatric Ventilator|last=|first=|date=|website=BemesOnline|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> assignment to treat other illnesses,<ref>{{Cite web|url=https://www.healthline.com/health/ventilator#uses|title=When a ventilator is used|last=Iftikhar|first=Noreen|date=September 23, 2019|website=Healthline|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> and operator training.<ref>{{Cite web|url=https://www.ncbi.nlm.nih.gov/pubmed/30252300|title=Ventilator Safety|last=Williams|first=LM|date=January 30, 2020|website=National Center for Biotechnology Information|publisher=StatPearls Publishing|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> Deploying the best possible mix of ventilators can save the most lives.\n\nA major worldwide design effort began during the [[2019\u201320 coronavirus pandemic|2019-2020 coronavirus pandemic]] after a [[Hackaday]] project was started,<ref>{{cite web |last=Coetzee |first=Gerrit |date=2020-03-12 |title=Ultimate Medical Hackathon: How Fast Can We Design And Deploy An Open Source Ventilator? |website=Hackaday |url=https://hackaday.com/2020/03/12/ultimate-medical-hackathon-how-fast-can-we-design-and-deploy-an-open-source-ventilator/ |access-date=2020-03-17}}</ref>{{Primary source inline|date=March 2020}} in order to respond to [[2019\u201320 coronavirus pandemic related shortages|expected ventilator shortages]] causing higher mortality rate among severe patients.\n\nOn March 20, 2020, the [[Health Service Executive|Irish Health Service]]<ref>{{Cite web|url=https://www.forbes.com/sites/alexandrasternlicht/2020/03/18/theres-a-shortage-of-ventilators-for-coronavirus-patients-so-this-international-group-invented-an-open-source-alternative-thats-being-tested-next-week/|title=There\u2019s A Shortage Of Ventilators For Coronavirus Patients, So This International Group Invented An Open Source Alternative That\u2019s Being Tested Next Week|last=Sternlicht|first=Alexandra|website=Forbes|language=en|access-date=2020-03-21}}</ref> began reviewing designs.<ref>{{Cite web|url=https://thehill.com/policy/technology/488637-irish-health-officials-to-review-3d-printed-ventilator|title=Irish health officials to review 3D-printed ventilator|last=Rodrigo|first=Chris Mills|date=2020-03-20|website=TheHill|language=en|access-date=2020-03-21}}</ref> A prototype is being designed and tested in [[Colombia]].<ref>{{Cite web|url=https://colombiareports.com/colombia-close-to-having-worlds-first-open-source-and-low-cost-ventilator-to-beat-covid-19/|title=Colombia close to having world's first open source and low-cost ventilator to 'beat Covid-19'|last=colombiareports|date=2020-03-21|website=Colombia News {{!}} Colombia Reports|language=en-US|access-date=2020-03-21}}</ref>.\n\nThe Polish company Urbicum reports successful testing<ref>{{Cite web|url=https://ventilaid.org|title=VentilAid -open-source ventilator, that can be made anywhere locally|last=urbicum|date=2020-03-23|website=VentilAid|language=en-US|access-date=2020-03-23}}</ref> of a 3D-printed open-source prototype device called VentilAid. The makers describe it as a last resort device when professional equipment is missing. The design is publicly available.<ref>{{Cite web|url=https://gitlab.com/Urbicum/ventilaid|title=GitHub - VentilAid / VentilAid|last=urbicum|date=2020-03-23|website=VentilAid|language=en-US|access-date=2020-03-23}}</ref> The first Ventilaid prototype requires compressed air to run.\n\nOn March 21, 2020 the [[New England Complex Systems Institute]] (NECSI) began maintaining a strategic list of open source designs being worked on.<ref>{{Cite web|url=https://medium.com/@brucefenton/ventilator-project-update-march-21th-2020-bd2ef9d587e0|title=Ventilator Project Update: March 21th, 2020|last=Fenton|first=Bruce|date=March 21, 2020|website=Medium|url-status=live|archive-url=|archive-date=|access-date=March 27, 2020}}</ref><ref>{{Cite web|url=https://github.com/PubInv/covid19-vent-list|title=A list projects to make emergency ventilators in response to COVID-19, focusing on free-libre open source|last=|first=|date=|website=GitHub|url-status=live|archive-url=|archive-date=|access-date=March 27, 2020}}</ref> The NECSI project considers manufacturing capability, medical safety and need for treating patients in various conditions, speed dealing with legal and political issues, logistics and supply.<ref name=\":2\">{{Cite web|url=https://medium.com/@brucefenton/we-need-ventilators-we-need-you-to-help-build-them-30805e5ee2ea|title=We need Ventilators - We Need You to Help Get Them Built|last=Fenton|first=Bruce|date=March 14, 2020|website=Medium|url-status=live|archive-url=|archive-date=|access-date=March 27, 2020}}</ref> NECSI is staffed with scientists from Harvard and MIT and others who have an understanding of pandemics, medicine, systems, risk, and data collection.<ref name=\":2\" />\n\n==2020 COVID-19 pandemic==\nThe [[COVID-19 pandemic]] has led to [[2019\u201320 coronavirus pandemic related shortages|shortages of essential goods and services - from hand sanitizers to masks to beds to ventilators]]. Several countries have already experienced a shortage of ventilators.<ref>{{Cite web|url=https://healthmanagement.org/c/icu/news/allocating-ventilators-in-a-pandemic|title=Allocating Ventilators in a Pandemic|date=2020-03-24|website=healthmanagement.org|language=en-US|access-date=2020-03-25}}</ref>\n\nIn 2006 (under President [[George W. Bush]]), the [[Biomedical Advanced Research and Development Authority]] ([[BARDA]]) of the [[United States]] realized that the country was likely to have an epidemic of respiratory disease and would need more ventilators, so it awarded a $6 million contract to [[Newport Medical Instruments]], a small company in California, to make 40,000 ventilators for under $3,000 apiece. In 2011, Newport sent three prototypes to the [[Centers for Disease Control]]. In 2012, [[Covidien]], a $12 billion/year medical device manufacturer, which manufactured more expensive competing ventilators, bought Newport for $100 million. Covidien delayed and in 2014 cancelled the contract.\n\nBARDA started over again with a new company, [[Philips]], and in July 2019, the [[FDA]] approved the Philips ventilator, and the government ordered 10,000 ventilators for delivery in mid-2020.<ref name=\"Aura\">{{cite news\n| author = Nicholas Kulish, Sarah Kliff and Jessica Silver-Greenberg\n| title = The U.S. Tried to Build a New Fleet of Ventilators. The Mission Failed. As the coronavirus spreads, the collapse of the project helps explain America\u2019s acute shortage.\n| quote = \n| newspaper = New York Times\n| date = March 29, 2020\n| pages = \n| url =  https://www.nytimes.com/2020/03/29/business/coronavirus-us-ventilator-shortage.html\n}}</ref> \n\nFifty-four governments, including many in Europe and Asia, imposed restrictions on medical supply exports in response to the coronavirus pandemic.<ref>{{Cite web|url=https://www.politico.com/newsletters/morning-trade/2020/03/24/export-restrictions-threaten-ventilator-availability-786327|title=Export restrictions threaten ventilator availability|date=2020-03-24|website=politico.com|language=en-US|access-date=2020-03-25}}</ref>\n\n==See also==\n{{Commons category|Ventilators}}\n{{wiktionary}}\n* [[Artificial ventilation]]\n* [[Intensive care unit]]\n* [[Joseph Stoddart]]\n* [[Mechanical ventilation]]\n* [[Open-source hardware]]\n* [[Respirator]]\n* [[Respiratory therapy]]\n* [[Robert Martensen]]\n* [[Two-balloon experiment]]\n\n==References==\n{{Reflist}}\n{{Mechanical ventilation}}\n{{Authority control}}\n\n[[Category:Respiratory therapy]]\n[[Category:Medical pumps]]\n", "text_old": "{{mergeto|Mechanical ventilation|discuss=Talk:Ventilator#Merger proposal|date=April 2020}}\n{{Short description|Device that provides mechanical ventilation to the lungs}}\n{{about|one type of machine used to assist breathing|the broader article, on both positive- and negative-pressure devices|Mechanical ventilation<!-- note: \"medical respirator\" redirects here mainly due to historical confusion-->|respiratory PPE worn on the face|Respirator|ventilation subjects|Ventilation (disambiguation)}}\n{{Infobox medical equipment\n| name           = Medical ventilator\n| acronym        =\n| synonym        =\n| image          = VIP Bird2.jpg\n| caption        = The Bird VIP Infant ventilator\n| alt            =\n| image_size     =\n| specialty      = Pulmonology\n| intervention   =\n| MedlinePlus    =\n| eMedicine      =\n| inventor       =\n| invention date =\n| manufacturer   =\n| related        =\n}}\nA '''ventilator''' is a machine that provides [[mechanical ventilation]] by moving breathable air into and out of the [[lungs]], to deliver breaths to a patient who is physically unable to breathe, or breathing insufficiently. Modern ventilators are [[computer]]ized [[microprocessor control| microprocessor-control]]led machines, but patients can also be ventilated with a simple, hand-operated [[bag valve mask]].  Ventilators are chiefly used in [[intensive-care medicine]], [[home care]], and [[emergency medicine]] (as standalone units) and in [[anesthesiology]] (as a component of an [[anesthesia machine]]).\n\nVentilators are sometimes called \"respirators\", a term commonly used for them in the 1950s (particularly the [[Forrest Bird#Mechanical ventilators| \"Bird respirator\"]]). However, contemporary hospital and medical terminology uses the word \"[[respirator]]\" to refer to a protective face-mask.<ref>\n{{cite journal\n |author= Center for Devices and Radiological Health |date= 2019-02-08 |title= Personal Protective Equipment for Infection Control - Masks and N95 Respirators\n |journal= FDA \n |url= https://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/GeneralHospitalDevicesandSupplies/PersonalProtectiveEquipment/ucm055977.htm\n |access-date= 2017-03-08\n}}\n</ref>\n\n==Function==\nIn its simplest form, a modern [[positive pressure ventilation|positive pressure ventilator]] consists of a compressible [[air]] reservoir or turbine, air and [[oxygen]] supplies, a set of valves and tubes, and a disposable or reusable \"patient circuit\". The air reservoir is pneumatically compressed several times a minute to deliver room-air, or in most cases, an air/oxygen mixture to the patient. If a turbine is used, the turbine pushes air through the ventilator, with a flow valve adjusting pressure to meet patient-specific parameters. When over pressure is released, the patient will exhale passively due to the [[lung]]s' elasticity, the exhaled air being released usually through a [[one-way valve]] within the patient circuit called the patient manifold.\n\nVentilators may also be equipped with monitoring and alarm systems for patient-related parameters (e.g. pressure, volume, and flow) and ventilator function (e.g. air leakage, power failure, mechanical failure), backup batteries, oxygen tanks, and remote control. The pneumatic system is nowadays often replaced by a computer-controlled [[turbopump]].\n\nModern ventilators are electronically controlled by a small [[embedded system]] to allow exact adaptation of pressure and flow characteristics to an individual patient's needs. Fine-tuned ventilator settings also serve to make ventilation more tolerable and comfortable for the patient. In Canada and the United States, [[respiratory therapists]] are responsible for tuning these settings, while biomedical technologists are responsible for the maintenance.  In the United Kingdom and Europe the management of the patient's interaction with the ventilator is done by [[Critical care nursing | critical care]] nurses.\n\nThe patient circuit usually consists of a set of three durable, yet lightweight plastic tubes, separated by function (e.g. inhaled air, patient pressure, exhaled air). Determined by the type of ventilation needed, the patient-end of the circuit may be either noninvasive or invasive.\n\nNoninvasive methods, such as [[Continuous_positive_airway_pressure|continuous positive airway pressure (CPAP)]] and [[non-invasive ventilation]], which are adequate for patients who require a ventilator only while sleeping and resting, mainly employ a nasal mask. Invasive methods require [[intubation]], which for long-term ventilator dependence will normally be a [[tracheotomy]] cannula, as this is much more comfortable and practical for long-term care than is larynx or nasal intubation.\n\n===Life-critical system===\n{{unreferenced section|date=April 2020}}\nBecause failure may result in death, mechanical ventilation systems are classified as [[life-critical system]]s, and precautions must be taken to ensure that they are highly reliable, including their [[power-supply|power supply]]. Mechanical ventilators are therefore carefully designed so that no [[single point of failure]] can endanger the patient. They may have manual backup mechanisms to enable hand-driven respiration in the absence of power (such as the mechanical ventilator integrated into an [[anaesthetic machine]]). They may also have safety valves, which open to atmosphere in the absence of power to act as an anti-suffocation valve for spontaneous breathing of the patient. Some systems are also equipped with compressed-gas tanks, air compressors or backup batteries to provide ventilation in case of power failure or defective gas supplies, and methods to operate or call for help if their mechanisms or software fail.<ref>{{cite news |last1=Johnson |first1=Carolyn Y. |last2=reporterEmailEmailBioBioFollowFollow |first2=closeCarolyn Y. JohnsonScience reporterEmailEmailBioBioFollowFollowAriana Eunjung Cha closeAriana Eunjung ChaNational |title=The dark side of ventilators: Those hooked up for long periods face difficult recoveries |url=https://www.washingtonpost.com/health/2020/04/03/coronavirus-survivors-recovery/?utm_campaign=wp_first_reads&utm_medium=email&utm_source=newsletter&wpisrc=nl_rainbow&wpmm=1 |accessdate=8 April 2020 |work=Washington Post |language=en}}</ref>\n\n==Major manufacturers==\n{| class=\"wikitable sortable\"\n|+ICU ventilators<ref>{{Cite news|last=Giorgio V.\u2005M\u00fcller|first=|url=https://www.nzz.ch/wirtschaft/weltweit-hat-es-zu-wenig-beatmungsgeraete-ld.1549108|title=Hersteller von Beatmungsger\u00e4ten produzieren massiv mehr, aber k\u00f6nnen die Nachfrage trotzdem nicht decken|date=2020-03-30|work=Neue Z\u00fcrcher Zeitung|access-date=2020-03-30|url-status=live|language=de}} With reference to: [[IPG Research]].</ref>\n!Manufacturer\n!Country\n!Market share (2019)\n|-\n|[[Getinge Group|Getinge]] \n|Sweden\n|22%\n|-\n|[[Hamilton Medical]] \n|USA, Switzerland\n|22%\n|-\n|[[Dr\u00e4gerwerk|Dr\u00e4ger]]\n|Germany\n|16 %\n|-\n|[[Mindray]] \n|China\n|10%\n|-\n|[[Medtronic]] \n|Ireland, USA\n|5%\n|-\n|[[L\u00f6wenstein Medical UK|L\u00f6wenstein Medical]] \n|Germany\n|3%\n|-\n|[[Vyaire Medical]]\n|USA\n|3%\n|-\n|[[GE Healthcare]]\n|USA\n|3%\n|-\n|[[Philips Respironics]]\n|Netherlands\n|3%\n|-\n|Others\n|\n|15%\n|}\n\n{| class=\"wikitable sortable\"\n|+Mobile ventilators<ref>{{Cite news|last=M\u00fcller, Giorgio V.|url=https://www.nzz.ch/wirtschaft/weltweit-hat-es-zu-wenig-beatmungsgeraete-ld.1549108|title=Hersteller von Beatmungsger\u00e4ten produzieren massiv mehr, aber k\u00f6nnen die Nachfrage trotzdem nicht decken|date=2020-03-30|work=Neue Z\u00fcrcher Zeitung|access-date=2020-03-30|url-status=live|language=de}} With reference to: [[IPG Research]].</ref>\n!Manufacturer\n!Country\n!Market share (rank order in 2019)\n|-\n|[[Dr\u00e4gerwerk|Dr\u00e4ger]] \n|Germany\n|24th\n|-\n|[[Weinmann Medical]] \n|Germany\n|21st\n|-\n|[[Hamilton Medical]] \n|USA, Switzerland\n|18th\n|-\n|[[Vyaire Medical]]\n|USA\n|5th\n|-\n|Customs \n|Japan\n|4th\n|-\n|O Two \n|Canada\n|4th\n|-\n|Smiths Medical\n|USA\n|4th\n|-\n|[[Medtronic]] \n|Ireland, USA\n|4th\n|-\n|Air Liquide Healthcare\n|France\n|3rd\n|-\n|(and 13 other vendors)\n|\n|\n|}\n\n== History ==\n{{refimprove section|date=April 2020}}\nThe history of mechanical ventilation begins with various versions of what was eventually called the [[Negative pressure ventilator|iron lung]], a form of noninvasive negative-pressure ventilator widely used during the [[Poliomyelitis|polio]] epidemics of the twentieth century after the introduction of the \"Drinker respirator\" in 1928, improvements introduced by [[John Haven Emerson]] in 1931,<ref name=\"pmid18189086\">{{cite journal |last=Geddes |first=LA |year=2007 |title=The history of artificial respiration |journal=IEEE Engineering in Medicine and Biology Magazine |pmid=18189086 |doi=10.1109/EMB.2007.907081 |volume=26 |issue=6 |pages=38\u201341}}</ref> and the [[Both respirator]] in 1937. Other forms of noninvasive ventilators, also used widely for polio patients, include [[Biphasic Cuirass Ventilation]], the rocking bed, and rather primitive positive pressure machines.<ref name=\"pmid18189086\" />\n\nIn 1949, John Haven Emerson developed a mechanical assister for anaesthesia with the cooperation of the anaesthesia department at [[Harvard University]]. Mechanical ventilators began to be used increasingly in anaesthesia and intensive care during the 1950s. Their development was stimulated both by the need to treat polio patients and the increasing use of [[muscle relaxant]]s during anaesthesia. Relaxant drugs paralyse the patient and improve operating conditions for the surgeon but also paralyse the respiratory muscles.\n[[File:East-Radcliffe Respirator Wellcome L0001305.jpg|alt=A machine with hoses and gauges on a wheeled cart|thumb|An East-Radcliffe respirator model from the mid-twentieth century]]\nIn the United Kingdom, the East Radcliffe and Beaver models were early examples. The former used a [[Sturmey-Archer]] bicycle [[hub gear]] to provide a range of speeds, and the latter an automotive [[windscreen wiper]] motor to drive the bellows used to inflate the lungs.<ref name=\"pmid13320798\">{{cite journal |vauthors=Russell WR, Schuster E, Smith AC, Spalding JM |date=April 1956 |title=Radcliffe respiration pumps |journal=[[The Lancet]] |pmid=13320798 |doi=10.1016/s0140-6736(56)90597-9 |volume=270 |issue=6922 |pages=539\u201341}}</ref> Electric motors were, however, a problem in the operating theatres of that time, as their use caused an explosion hazard in the presence of flammable anaesthetics such as [[diethyl ether|ether]] and [[cyclopropane]]. In 1952, Roger Manley of the [[Westminster Hospital]], London, developed a ventilator which was entirely gas-driven and became the most popular model used in Europe. It was an elegant design, and became a great favourite with European anaesthetists for four decades, prior to the introduction of models controlled by electronics. It was independent of electrical power and caused no explosion hazard. The original Mark I unit was developed to become the Manley Mark II in collaboration with the Blease company, which manufactured many thousands of these units. Its principle of operation was very simple, an incoming gas flow was used to lift a weighted bellows unit, which fell intermittently under gravity, forcing breathing gases into the patient's lungs. The inflation pressure could be varied by sliding the movable weight on top of the bellows. The volume of gas delivered was adjustable using a curved slider, which restricted bellows excursion. Residual pressure after the completion of expiration was also configurable, using a small weighted arm visible to the lower right of the front panel. This was a robust unit and its availability encouraged the introduction of positive pressure ventilation techniques into mainstream European anesthetic practice.\n\nThe 1955 release of [[Forrest Bird]]'s \"Bird Universal Medical Respirator\" in the United States changed the way mechanical ventilation was performed, with the small green box becoming a familiar piece of medical equipment.<ref name=AboutBird>{{cite web |last=Bellis |first=Mary |title=Forrest Bird invented a fluid control device, respirator & pediatric ventilator |publisher=About.com |url=http://inventors.about.com/od/bstartinventors/a/Forrest_Bird.htm |access-date=2009-06-04}}</ref> The unit was sold as the Bird Mark 7 Respirator and informally called the \"Bird\". It was a [[Pneumatics|pneumatic]] device and therefore required no [[electrical power]] source to operate.\n\nIn 1965, the Army Emergency Respirator was developed in collaboration with the Harry Diamond Laboratories (now part of the [[United States Army Research Laboratory|U.S. Army Research Laboratory]]) and [[Walter Reed Army Institute of Research]]. Its design incorporated the principle of fluid amplification in order to govern pneumatic functions. Fluid amplification allowed the respirator to be manufactured entirely without moving parts, yet capable of complex resuscitative functions.<ref>{{cite book |date=1965 |title=Army R, D & A. |publisher=Development and Engineering Directorate, HQ, U.S. Army Materiel Development and Readiness Command |url=https://books.google.com/?id=sTrO77DCgkwC&pg=RA6-PA33&lpg=RA6-PA33&dq=Army+Emergency+Respirator,+HDL#v=onepage&q=Army%20Emergency%20Respirator,%20HDL&f=false}}</ref> Elimination of moving parts increased performance reliability and minimized maintenance.<ref name=\":0\">{{cite journal |last1=Mon |first1=George |last2=Woodward |first2=Kenneth E. |last3=Straub |first3=Henrik |last4=Joyce |first4=James |last5=Meyer |first5=James |date=1966 |title=Fluid Amplifier-Controlled Medical Devices |journal=SAE Transactions |issn=0096-736X |jstor=44554326 |volume=74 |pages=217\u2013222}}</ref> The mask is composed of a [[poly(methyl methacrylate)]] (commercially known as [[Poly(methyl methacrylate)|Lucite]]) block, about the size of a pack of cards, with machined channels and a cemented or screwed-in cover plate.<ref name=\":1\">{{cite web |title=Army Research and Development Monthly Magazine |volume=6 |number=9 |url=https://asc.army.mil/docs/pubs/alt/archives/1965/Sep_1965.PDF}}</ref> The reduction of moving parts cut manufacturing costs and increased durability.<ref name=\":0\" />\n\nThe bistable fluid amplifier design allowed the respirator to function as both a respiratory assistor and controller. It could functionally transition between assistor and controller automatically, based on the patient's needs.<ref name=\":1\" /><ref name=\":0\" /> The dynamic pressure and turbulent jet flow of gas from inhalation to exhalation allowed the respirator to synchronize with the breathing of the patient.<ref>{{cite web |date=October 1965 |title=Fluid Amplification Symposium |volume=III |url=https://apps.dtic.mil/dtic/tr/fulltext/u2/623457.pdf}}</ref>\n\nIntensive care environments around the world revolutionized in 1971 by the introduction of the first SERVO 900 ventilator (Elema-Sch\u00f6nander). It was a small, silent and effective electronic ventilator, with the famous SERVO feedback system controlling what had been set and regulating delivery. For the first time, the machine could deliver the set volume in volume control ventilation.\n\nVentilators used under increased pressure (hyperbaric) require special precautions, and few ventilators can operate under these conditions.<ref name=SPUMS1998>{{cite journal |last=Skinner |first=M |year=1998 |title=Ventilator function under hyperbaric conditions |journal=South Pacific Underwater Medicine Society Journal |volume=28 |issue=2 |url=http://archive.rubicon-foundation.org/5927 |access-date=2009-06-04}}</ref> In 1979, Sechrist Industries introduced their Model 500A ventilator, which was specifically designed for use with [[hyperbaric chamber]]s.<ref name=JHM1988>{{cite journal |vauthors=Weaver LK, Greenway L, Elliot CG |year=1988 |title=Performance of the Seachrist 500A Hyperbaric Ventilator in a Monoplace Hyperbaric Chamber |journal=Journal of Hyperbaric Medicine |volume=3 |issue=4 |pages=215\u2013225 |url=http://archive.rubicon-foundation.org/4377 |access-date=2009-06-04}}</ref>\n\n===Microprocessor ventilators===\n[[Microprocessor control]] led to the third generation of [[intensive care unit]] (ICU) ventilators, starting with the [[Dr\u00e4gerwerk|Dr\u00e4ger]] EV-A<ref>{{cite web |title=Dr\u00e4ger - die Geschichte des Unternehmens |language= |publisher=Dr\u00e4ger |website=Dr\u00e4ger |url=https://www.draeger.com/Corporate/Content/draeger_die_geschichte_des_unternehmens.pdf |access-date=March 22, 2020}}</ref> in 1982 in Germany which allowed monitoring the patient's [[Breathing#Respiratory_disorders|breathing curve]] on an [[LCD monitor]]. One year later followed [[Puritan Bennett]] 7200 and Bear 1000, SERVO 300 and Hamilton Veolar over the next decade. [[Microprocessors]] enable customized gas delivery and monitoring, and mechanisms for gas delivery that are much more responsive to patient needs than previous generations of mechanical ventilators.<ref>{{cite journal |last=Kacmarek |first=Robert M. |date=August 2011 |title=The Mechanical Ventilator: Past, Present, and Future |journal=[[Respiratory Care (journal)|Respiratory Care]] |issn=0020-1324 |doi=10.4187/respcare.01420 |volume=56 |issue=8 |pages=1170\u20131180|doi-access=free }}</ref>\n<!--\nIn 1991, the SERVO 300 ventilator series was introduced, enabling treatment of all patient categories from adult to neonate with one ventilator. The SERVO 300 series had a unique gas delivery system with rapid flow-triggering response.\n\nIn 1999, the new LTV (Laptop Ventilator) Series was significantly smaller than other ventilators of the time, weighing approximately 6.4 kg (14 lb) and about the size of a laptop computer. This design kept the same functionality of the in-hospital ventilators while allowing patient mobility.\n\nA modular concept was introduced with SERVO-i in 2001, with one ventilator model throughout the ICU department, instead of a fleet of various models and brands for different user needs. With the modular ventilator, ICU departments could choose the modes and options, software and hardware needed for a particular patient category.\n\nIn the twenty-first century small portable ventilators like the SAVe II have been manufactured for forward combat use.<ref name=Automedx>{{cite web |title=SAVe II The Smallest and Easiest to Use Pre-hospital Ventilator |publisher=Automedx |website=Automedx |url=http://automedx.com/save-ii/ |access-date=March 31, 2019}}</ref>-->\n\n== Open-source ventilators ==\n{{Main articles|Open-source ventilator}}\nAn [[Open source|open-source]] ventilator is a disaster-situation ventilator made using a freely-licensed design, and ideally, freely-available components and parts. Designs, components, and parts may be anywhere from completely reverse-engineered to completely new creations, components may be [[Jury rigging|adaptations]] of various inexpensive existing products, and special hard-to-find and/or expensive parts may be 3D printed instead of sourced.<ref>{{cite web |last=Bender |first=Maddie |date=2020-03-17 |title=People Are Trying to Make DIY Ventilators to Meet Coronavirus Demand |website=Vice |url=https://www.vice.com/en_uk/article/5dm4mb/people-are-trying-to-make-diy-ventilators-to-meet-coronavirus-demand |access-date=2020-03-21}}</ref><ref>{{cite web |last=Toussaint |first=Kristin |date=2020-03-16 |title=These Good Samaritans with a 3D printer are saving lives by making new respirator valves for free |website=Fast Company |url=https://www.fastcompany.com/90477940/these-good-samaritans-with-a-3d-printer-are-saving-lives-by-making-new-respirator-valves-for-free |access-date=2020-03-17}}</ref> \n\nDuring the 2019-2020 [[COVID-19 pandemic]], various kinds of ventilators have been considered. Deaths caused by [[Coronavirus disease 2019|COVID-19]] have occurred when the most severely infected experience [[acute respiratory distress syndrome]], a widespread inflammation in the lungs that impairs the lungs' ability to absorb oxygen and expel carbon dioxide. These patients require a capable ventilator to continue breathing.\n\nAmong ventilators that might be brought into the fight, there have been many concerns. These include current availability,<ref>{{Cite web|url=https://www.npr.org/sections/health-shots/2020/03/14/815675678/as-the-pandemic-spreads-will-there-be-enough-ventilators|title=As The Pandemic Spreads, Will There Be Enough Ventilators?|last=NEIGHMOND|first=PATTI|date=March 14, 2020|website=NPR|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref><ref>{{Cite web|url=https://www.nsmedicaldevices.com/analysis/coronavirus-ventilators-global-demand/|title=880,000 more ventilators needed to cope with coronavirus outbreak, says analyst|last=Parker|first=Thomas|date=March 25, 2020|website=NS Medical Devices|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> the challenge of making more and lower cost ventilators,<ref>{{Cite web|url=https://www.youtube.com/watch?v=7vLPefHYWpY|title=A Guide To Designing Low-Cost Ventilators for COVID-19|last=|first=|date=April 4, 2020|website=YouTube|publisher=Real Engineering|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> effectiveness,<ref>{{Cite web|url=https://www.physiciansweekly.com/mortality-rate-of-covid-19-patients-on-ventilators/|title=Mortality rate of COVID-19 patients on ventilators|last=|first=|date=March 30, 2020|website=Physician's Weekly|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> [[functional design]], safety,<ref>{{Cite web|url=https://www.aarc.org/wp-content/uploads/2017/03/Issue-Paper-Safe-Initiation-and-Management-of-Mechanical-Ventilation.pdf|title=SAFE INITIATION AND MANAGEMENT OF MECHANICAL VENTILATION|last=|first=|date=2016|website=American Association for Respiratory Care|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref><ref>{{Cite web|url=https://www.ecri.org/components/HDJournal/Pages/Mechanical-Ventilation-of-SARS-Patients-2003-SARS-Outbreak.aspx|title=Mechanical Ventilation of SARS Patients: Lessons from the 2003 SARS Outbreak|last=|first=|date=February 18, 2020|website=ECRI|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> portability,<ref>{{Cite web|url=https://techcrunch.com/2020/03/30/medtronic-is-sharing-its-portable-ventilator-design-specifications-and-code-for-free-to-all/|title=Medtronic is sharing its portable ventilator design specifications and code for free to all|last=Etherington|first=Darrell|date=March 30, 2020|website=TechCrunch|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> suitability for infants,<ref>{{Cite web|url=https://www.bemesonline.com/bird-v-i-p-standard-infant-and-pediatric-ventilator/|title=Bird V.I.P Standard Infant and Pediatric Ventilator|last=|first=|date=|website=BemesOnline|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> assignment to treat other illnesses,<ref>{{Cite web|url=https://www.healthline.com/health/ventilator#uses|title=When a ventilator is used|last=Iftikhar|first=Noreen|date=September 23, 2019|website=Healthline|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> and operator training.<ref>{{Cite web|url=https://www.ncbi.nlm.nih.gov/pubmed/30252300|title=Ventilator Safety|last=Williams|first=LM|date=January 30, 2020|website=National Center for Biotechnology Information|publisher=StatPearls Publishing|url-status=live|archive-url=|archive-date=|access-date=April 6, 2020}}</ref> Deploying the best possible mix of ventilators can save the most lives.\n\nA major worldwide design effort began during the [[2019\u201320 coronavirus pandemic|2019-2020 coronavirus pandemic]] after a [[Hackaday]] project was started,<ref>{{cite web |last=Coetzee |first=Gerrit |date=2020-03-12 |title=Ultimate Medical Hackathon: How Fast Can We Design And Deploy An Open Source Ventilator? |website=Hackaday |url=https://hackaday.com/2020/03/12/ultimate-medical-hackathon-how-fast-can-we-design-and-deploy-an-open-source-ventilator/ |access-date=2020-03-17}}</ref>{{Primary source inline|date=March 2020}} in order to respond to [[2019\u201320 coronavirus pandemic related shortages|expected ventilator shortages]] causing higher mortality rate among severe patients.\n\nOn March 20, 2020, the [[Health Service Executive|Irish Health Service]]<ref>{{Cite web|url=https://www.forbes.com/sites/alexandrasternlicht/2020/03/18/theres-a-shortage-of-ventilators-for-coronavirus-patients-so-this-international-group-invented-an-open-source-alternative-thats-being-tested-next-week/|title=There\u2019s A Shortage Of Ventilators For Coronavirus Patients, So This International Group Invented An Open Source Alternative That\u2019s Being Tested Next Week|last=Sternlicht|first=Alexandra|website=Forbes|language=en|access-date=2020-03-21}}</ref> began reviewing designs.<ref>{{Cite web|url=https://thehill.com/policy/technology/488637-irish-health-officials-to-review-3d-printed-ventilator|title=Irish health officials to review 3D-printed ventilator|last=Rodrigo|first=Chris Mills|date=2020-03-20|website=TheHill|language=en|access-date=2020-03-21}}</ref> A prototype is being designed and tested in [[Colombia]].<ref>{{Cite web|url=https://colombiareports.com/colombia-close-to-having-worlds-first-open-source-and-low-cost-ventilator-to-beat-covid-19/|title=Colombia close to having world's first open source and low-cost ventilator to 'beat Covid-19'|last=colombiareports|date=2020-03-21|website=Colombia News {{!}} Colombia Reports|language=en-US|access-date=2020-03-21}}</ref>.\n\nThe Polish company Urbicum reports successful testing<ref>{{Cite web|url=https://ventilaid.org|title=VentilAid -open-source ventilator, that can be made anywhere locally|last=urbicum|date=2020-03-23|website=VentilAid|language=en-US|access-date=2020-03-23}}</ref> of a 3D-printed open-source prototype device called VentilAid. The makers describe it as a last resort device when professional equipment is missing. The design is publicly available.<ref>{{Cite web|url=https://gitlab.com/Urbicum/ventilaid|title=GitHub - VentilAid / VentilAid|last=urbicum|date=2020-03-23|website=VentilAid|language=en-US|access-date=2020-03-23}}</ref> The first Ventilaid prototype requires compressed air to run.\n\nOn March 21, 2020 the [[New England Complex Systems Institute]] (NECSI) began maintaining a strategic list of open source designs being worked on.<ref>{{Cite web|url=https://medium.com/@brucefenton/ventilator-project-update-march-21th-2020-bd2ef9d587e0|title=Ventilator Project Update: March 21th, 2020|last=Fenton|first=Bruce|date=March 21, 2020|website=Medium|url-status=live|archive-url=|archive-date=|access-date=March 27, 2020}}</ref><ref>{{Cite web|url=https://github.com/PubInv/covid19-vent-list|title=A list projects to make emergency ventilators in response to COVID-19, focusing on free-libre open source|last=|first=|date=|website=GitHub|url-status=live|archive-url=|archive-date=|access-date=March 27, 2020}}</ref> The NECSI project considers manufacturing capability, medical safety and need for treating patients in various conditions, speed dealing with legal and political issues, logistics and supply.<ref name=\":2\">{{Cite web|url=https://medium.com/@brucefenton/we-need-ventilators-we-need-you-to-help-build-them-30805e5ee2ea|title=We need Ventilators - We Need You to Help Get Them Built|last=Fenton|first=Bruce|date=March 14, 2020|website=Medium|url-status=live|archive-url=|archive-date=|access-date=March 27, 2020}}</ref> NECSI is staffed with scientists from Harvard and MIT and others who have an understanding of pandemics, medicine, systems, risk, and data collection.<ref name=\":2\" />\n\n==2020 COVID-19 pandemic==\nThe [[COVID-19 pandemic]] has led to [[2019\u201320 coronavirus pandemic related shortages|shortages of essential goods and services - from hand sanitizers to masks to beds to ventilators]]. Several countries have already experienced a shortage of ventilators.<ref>{{Cite web|url=https://healthmanagement.org/c/icu/news/allocating-ventilators-in-a-pandemic|title=Allocating Ventilators in a Pandemic|date=2020-03-24|website=healthmanagement.org|language=en-US|access-date=2020-03-25}}</ref>\n\nIn 2006 (under President [[George W. Bush]]), the [[Biomedical Advanced Research and Development Authority]] ([[BARDA]]) of the [[United States]] realized that the country was likely to have an epidemic of respiratory disease and would need more ventilators, so it awarded a $6 million contract to [[Newport Medical Instruments]], a small company in California, to make 40,000 ventilators for under $3,000 apiece. In 2011, Newport sent three prototypes to the [[Centers for Disease Control]]. In 2012, [[Covidien]], a $12 billion/year medical device manufacturer, which manufactured more expensive competing ventilators, bought Newport for $100 million. Covidien delayed and in 2014 cancelled the contract.\n\nBARDA started over again with a new company, [[Philips]], and in July 2019, the [[FDA]] approved the Philips ventilator, and the government ordered 10,000 ventilators for delivery in mid-2020.<ref name=\"Aura\">{{cite news\n| author = Nicholas Kulish, Sarah Kliff and Jessica Silver-Greenberg\n| title = The U.S. Tried to Build a New Fleet of Ventilators. The Mission Failed. As the coronavirus spreads, the collapse of the project helps explain America\u2019s acute shortage.\n| quote = \n| newspaper = New York Times\n| date = March 29, 2020\n| pages = \n| url =  https://www.nytimes.com/2020/03/29/business/coronavirus-us-ventilator-shortage.html\n}}</ref> \n\nFifty-four governments, including many in Europe and Asia, imposed restrictions on medical supply exports in response to the coronavirus pandemic.<ref>{{Cite web|url=https://www.politico.com/newsletters/morning-trade/2020/03/24/export-restrictions-threaten-ventilator-availability-786327|title=Export restrictions threaten ventilator availability|date=2020-03-24|website=politico.com|language=en-US|access-date=2020-03-25}}</ref>\n\n==See also==\n{{Commons category|Ventilators}}\n{{wiktionary}}\n* [[Artificial ventilation]]\n* [[Intensive care unit]]\n* [[Joseph Stoddart]]\n* [[Mechanical ventilation]]\n* [[Open-source hardware]]\n* [[Respirator]]\n* [[Respiratory therapy]]\n* [[Robert Martensen]]\n\n==References==\n{{Reflist}}\n{{Mechanical ventilation}}\n{{Authority control}}\n\n[[Category:Respiratory therapy]]\n[[Category:Medical pumps]]\n", "name_user": "FredWineHouse", "label": "safe", "comment": "\u2192\u200eSee also", "url_page": "//en.wikipedia.org/wiki/Ventilator"}
{"title_page": "CPU cache", "text_new": "A '''CPU cache''' is a [[hardware cache]] used by the [[central processing unit]] (CPU) of a [[computer]] to reduce the average cost (time or energy) to access [[data (computing)|data]] from the [[main memory]].<ref>{{cite web|url=http://www.hardwaresecrets.com/how-the-cache-memory-works/|title=How The Cache Memory Works|author=Gabriel Torres|date=September 12, 2007}}</ref> A cache is a smaller, faster memory, located closer to a [[processor core]], which stores copies of the data from frequently used main [[memory location]]s.  Most CPUs have different independent caches, including [[instruction cache|instruction]] and [[data cache]]s, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).\n\nAll modern (fast) CPUs (with few specialized exceptions{{refn|A few specialized CPUs, accelerators or microcontrollers do not have a cache. To be fast, if needed/wanted, they still have an on-chip [[scratchpad memory]] that has a similar function, while software managed. In, for example, microcontrollers, it can be better for hard real-time use, to have that or at least no cache, as with one level of memory latencies of loads are predictable.}}) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). Split L1 cache started in 1985 with the R2000 MIPS CPU, achieved mainstream in 1993 with the Intel Pentium and in 1997 the embedded CPU market with the ARMv5TE. In 2015, even sub-dollar SoC split the L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a [[multi-core processor]] has a dedicated L1 cache and is usually not shared between the cores. The L2 cache, and higher-level caches, may be shared between the cores. L4 cache is currently uncommon, and is generally on (a form of) [[dynamic random-access memory]] (DRAM), rather than on [[static random-access memory]] (SRAM), on a separate die or chip (exceptionally, the form, [[eDRAM]] is used for all levels of cache, down to L1). That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and optimized differently.\n\nOther types of caches exist (that are not counted towards the \"cache size\" of the most important caches mentioned above), such as the [[translation lookaside buffer]] (TLB) which is part of the [[memory management unit]] (MMU) which most CPUs have.\n\nCaches (like for RAM historically) have generally been sized in powers of: 2, 4, 8, 16 etc. [[Kibibyte|KiB]]; when up to [[Mebibyte|MiB]] sizes (i.e. for larger non-L1), very early on the pattern broke down, to allow for larger caches without being forced into the doubling-in-size paradigm, with e.g. [[Intel Core 2 Duo]] with 3&nbsp;MiB L2 cache in April 2008. Much later however for L1 sizes, that still only count in small number of KiB, however [[IBM zEC12 (microprocessor)|IBM zEC12]] from 2012 is an exception, to gain unusually large 96&nbsp;KiB L1 data cache for its time, and e.g. the [[IBM z13 (microprocessor)|IBM z13]] having a 96&nbsp;KiB L1 instruction cache (and 128&nbsp;KiB L1 data cache),<ref>{{cite web|url=http://www.redbooks.ibm.com/redbooks/pdfs/sg248250.pdf|title=IBM z13 and IBM z13s Technical Introduction|page=20|date=March 2016|publisher=[[IBM]]}}</ref> and Intel [[Ice Lake (microprocessor)|Ice Lake]]-based processors from 2018, having 48&nbsp;KiB L1 data cache and 48&nbsp;KiB L1 instruction cache. In 2020, some [[Intel Atom]] CPUs (with up to 24 cores) have (multiple of) 4.5&nbsp;MiB and 15&nbsp;MiB cache sizes.<ref>{{Cite press|url=https://newsroom.intel.com/news/product-fact-sheet-accelerating-5g-network-infrastructure-core-edge/|title=Product Fact Sheet: Accelerating 5G Network Infrastructure, from the Core to the Edge|website=Intel Newsroom|quote=L1 cache of 32KB/core, L2 cache of 4.5MB per 4-core cluster and shared LLC cache up to 15MB.|language=en-US|access-date=2020-04-12}}</ref><ref>{{Cite web|url=https://www.anandtech.com/show/15544/intel-launches-atom-p5900-a-10nm-atom-for-radio-access-networks|title=Intel Launches Atom P5900: A 10nm Atom for Radio Access Networks|last=Smith|first=Ryan|website=www.anandtech.com|access-date=2020-04-12}}</ref>\n\n=={{Anchor|ICACHE|DCACHE|instruction cache|data cache}}Overview==\nWhen trying to read from or write to a location in main memory, the processor checks whether the data from that location is already in the cache. If so, the processor will read from or write to the cache instead of the much slower main memory. \n\nMost modern [[Desktop computer|desktop]] and [[Server (computing)|server]] CPUs have at least three independent caches: an '''instruction cache''' to speed up executable instruction fetch, a '''data cache''' to speed up data fetch and store, and a [[translation lookaside buffer]] (TLB) used to speed up virtual-to-physical address translation for both executable instructions and data.  A single TLB can be provided for access to both instructions and data, or a separate Instruction TLB (ITLB) and data TLB (DTLB) can be provided.<ref name=\"tlbsurvey\">\"[https://www.academia.edu/29585076/A_Survey_of_Techniques_for_Architecting_TLBs A Survey of Techniques for Architecting TLBs]\", Concurrency and Computation, 2016.</ref> The data cache is usually organized as a hierarchy of more cache levels (L1, L2, etc.; see also [[#Multi-level caches|multi-level caches]] below). However, the TLB cache is part of the [[memory management unit]] (MMU) and not directly related to the CPU caches.\n\n==={{Anchor|cache lines|CACHE-LINES}}Cache entries===\nData is transferred between memory and cache in blocks of fixed size, called ''cache lines'' or ''cache blocks''. When a cache line is copied from memory into the cache, a cache entry is created. The cache entry will include the copied data as well as the requested memory location (called a tag).\n\nWhen the processor needs to read or write a location in memory, it first checks for a corresponding entry in the cache. The cache checks for the contents of the requested memory location in any cache lines that might contain that address. If the processor finds that the memory location is in the cache, a cache hit has occurred. However, if the processor does not find the memory location in the cache, a cache miss has occurred. In the case of a cache hit, the processor immediately reads or writes the data in the cache line.  For a cache miss, the cache allocates a new entry and copies data from main memory, then the request is fulfilled from the contents of the cache.\n\n===Policies===\n\n===={{Anchor|EVICTION}}Replacement policies====\n{{Main article|Cache algorithms}}\n\nTo make room for the new entry on a cache miss, the cache may have to evict one of the existing entries. The heuristic it uses to choose the entry to evict is called the replacement policy. The fundamental problem with any replacement policy is that it must predict which existing cache entry is least likely to be used in the future. Predicting the future is difficult, so there is no perfect method to choose among the variety of replacement policies available. One popular replacement policy, [[Least Recently Used|least-recently used]] (LRU), replaces the least recently accessed entry.\n\nMarking some memory ranges as non-cacheable can improve performance, by avoiding caching of memory regions that are rarely re-accessed. This avoids the overhead of loading something into the cache without having any reuse.  Cache entries may also be disabled or locked depending on the context.\n\n====Write policies====\n{{Main article|Cache (computing)#WRITEPOLICIES|l1=Cache (computing) \u00a7 Writing policies}}\n\nIf data is written to the cache, at some point it must also be written to main memory; the timing of this write is known as the write policy. In a [[write-through]] cache, every write to the cache causes a write to main memory.  Alternatively, in a [[write-back]] or copy-back cache, writes are not immediately mirrored to the main memory, and the cache instead tracks which locations have been written over, marking them as [[Dirty bit|dirty]]. The data in these locations is written back to the main memory only when that data is evicted from the cache. For this reason, a read miss in a write-back cache may sometimes require two memory accesses to service: one to first write the dirty location to main memory, and then another to read the new location from memory. Also, a write to a main memory location that is not yet mapped in a write-back cache may evict an already dirty location, thereby freeing that cache space for the new memory location.\n\nThere are intermediate policies as well. The cache may be write-through, but the writes may be held in a store data queue temporarily, usually so multiple stores can be processed together (which can reduce bus turnarounds and improve bus utilization).\n\nCached data from the main memory may be changed by other entities (e.g., peripherals using [[direct memory access]] (DMA) or another core in a [[multi-core processor]]), in which case the copy in the cache may become out-of-date or stale. Alternatively, when a CPU in a [[multiprocessor]] system updates data in the cache, copies of data in caches associated with other CPUs become stale. Communication protocols between the cache managers that keep the data consistent are known as [[cache coherence]] protocols.\n\n=== Cache performance ===\n[[Cache performance measurement and metric|Cache performance measurement]] has become important in recent times where the speed gap between the memory performance and the processor performance is increasing exponentially. The cache was introduced to reduce this speed gap. Thus knowing how well the cache is able to bridge the gap in the speed of processor and memory becomes important, especially in high-performance systems. The cache hit rate and the cache miss rate play an important role in determining this performance. To improve the cache performance, reducing the miss rate becomes one of the necessary steps among other steps. Decreasing the access time to the cache also gives a boost to its performance.\n\n====CPU stalls====\nThe time taken to fetch one cache line from memory (read [[Latency (engineering)|latency]] due to a cache miss) matters because the CPU will run out of things to do while waiting for the cache line. When a CPU reaches this state, it is called a stall.  As CPUs become faster compared to main memory, stalls due to cache misses displace more potential computation; modern CPUs can execute hundreds of instructions in the time taken to fetch a single cache line from main memory.\n\nVarious techniques have been employed to keep the CPU busy during this time, including [[out-of-order execution]] in which the CPU attempts to execute independent instructions after the instruction that is waiting for the cache miss data.  Another technology, used by many processors, is [[simultaneous multithreading]] (SMT), which allows an alternate thread to use the CPU core while the first thread waits for required CPU resources to become available.\n\n==Associativity==\n[[Image:Cache,associative-fill-both.png|thumb|450px|An illustration of different ways in which memory locations can be cached by particular cache locations]]\n{{Main|Cache placement policies}}\nThe [[Cache placement policies|placement policy]] decides where in the cache a copy of a particular entry of main memory will go. If the placement policy is free to choose any entry in the cache to hold the copy, the cache is called ''fully associative''. At the other extreme, if each entry in main memory can go in just one place in the cache, the cache is ''direct mapped''. Many caches implement a compromise in which each entry in main memory can go to any one of N places in the cache, and are described as N-way set associative.<ref>{{cite web|url=http://cseweb.ucsd.edu/classes/fa10/cse240a/pdf/08/CSE240A-MBT-L15-Cache.ppt.pdf|title=Cache design|date=2010-12-02|website=ucsd.edu|page=10&ndash;15|accessdate=2014-02-24}}</ref> For example, the level-1 data cache in an [[AMD Athlon]] is two-way set associative, which means that any particular location in main memory can be cached in either of two locations in the level-1 data cache.\n\nChoosing the right value of associativity involves a [[trade-off]]. If there are ten places to which the placement policy could have mapped a memory location, then to check if that location is in the cache, ten cache entries must be searched. Checking more places takes more power and chip area, and potentially more time. On the other hand, caches with more associativity suffer fewer misses (see conflict misses, below), so that the CPU wastes less time reading from the slow main memory. The general guideline is that doubling the associativity, from direct mapped to two-way, or from two-way to four-way, has about the same effect on raising the hit rate as doubling the cache size. However, increasing associativity more than four does not improve hit rate as much,<ref>[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5234663 IEEE Xplore - Phased set associative cache design for reduced power consumption]. Ieeexplore.ieee.org (2009-08-11). Retrieved on 2013-07-30.</ref> and are generally done for other reasons (see virtual aliasing, below).  Some CPUs can dynamically reduce the associativity of their caches in low-power states, which acts as a power-saving measure.<ref>{{cite web|url=http://hotchips.org/wp-content/uploads/hc_archives/hc24/HC24-1-Microprocessor/HC24.28.117-HotChips_IvyBridge_Power_04.pdf#page=18|title=Power Management of the Third Generation Intel Core Micro Architecture formerly codenamed Ivy Bridge|author1=Sanjeev Jahagirdar|author2=Varghese George|year=2012|website=hotchips.org|page=18|format=PDF|accessdate=2015-12-16|author3=Inder Sodhi|author4=Ryan Wells}}</ref>\n\n<!-- where does \"pseudo-associative cache\" go in this spectrum? -->\nIn order of worse but simple to better but complex:\n\n* Direct mapped cache{{snd}} good best-case time, but unpredictable in worst case\n* Two-way set associative cache\n* Two-way skewed associative cache<ref name=\"Seznec\">{{cite journal|author=Andr\u00e9 Seznec|year=1993|title=A Case for Two-Way Skewed-Associative Caches|journal=ACM Sigarch Computer Architecture News|volume=21|issue=2|pages=169\u2013178|doi=10.1145/173682.165152}}</ref>\n* Four-way set associative cache\n* Eight-way set associative cache, a common choice for later implementations\n* 12-way set associative cache, similar to eight-way\n* Fully associative cache{{snd}} the best miss rates, but practical only for a small number of entries\n\n===Direct-mapped cache===\nIn this cache organization, each location in main memory can go in only one entry in the cache.  Therefore, a direct-mapped cache can also be called a \"one-way set associative\" cache.  It does not have a placement policy as such, since there is no choice of which cache entry's contents to evict.  This means that if two locations map to the same entry, they may continually knock each other out.  Although simpler, a direct-mapped cache needs to be much larger than an associative one to give comparable performance, and it is more unpredictable.  Let {{mvar|x}} be block number in cache, {{mvar|y}} be block number of memory, and {{mvar|n}} be number of blocks in cache, then mapping is done with the help of the equation {{math|''x'' {{=}} ''y'' mod ''n''}}.\n\n===Two-way set associative cache===\nIf each location in main memory can be cached in either of two locations in the cache, one logical question is: ''which one of the two?'' The simplest and most commonly used scheme, shown in the right-hand diagram above, is to use the least significant bits of the memory location's index as the index for the cache memory, and to have two entries for each index. One benefit of this scheme is that the tags stored in the cache do not have to include that part of the main memory address which is implied by the cache memory's index. Since the cache tags have fewer bits, they require fewer transistors, take less space on the processor circuit board or on the microprocessor chip, and can be read and compared faster. Also [[Cache algorithms|LRU]] is especially simple since only one bit needs to be stored for each pair.\n\n===Speculative Execution===\nOne of the advantages of a direct mapped cache is that it allows simple and fast [[speculative execution|speculation]]. Once the address has been computed, the one cache index which might have a copy of that location in memory is known. That cache entry can be read, and the processor can continue to work with that data before it finishes checking that the tag actually matches the requested address.\n\nThe idea of having the processor use the cached data before the tag match completes can be applied to associative caches as well. A subset of the tag, called a ''hint'', can be used to pick just one of the possible cache entries mapping to the requested address. The entry selected by the hint can then be used in parallel with checking the full tag. The hint technique works best when used in the context of address translation, as explained below.\n\n===Two-way skewed associative cache===\nOther schemes have been suggested, such as the ''skewed cache'',<ref name=\"Seznec\" /> where the index for way 0 is direct, as above, but the index for way 1 is formed with a [[hash function]]. A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function, and so it is less likely that a program will suffer from an unexpectedly large number of conflict misses due to a pathological access pattern. The downside is extra latency from computing the hash function.<ref name=\"CK\">{{cite web|url=http://www.stanford.edu/class/ee282/08_handouts/L03-Cache.pdf|title=Lecture 3: Advanced Caching Techniques|author=C. Kozyrakis|archive-url=https://web.archive.org/web/20120907012034/http://www.stanford.edu/class/ee282/08_handouts/L03-Cache.pdf|archive-date=September 7, 2012}}</ref> Additionally, when it comes time to load a new line and evict an old line, it may be difficult to determine which existing line was least recently used, because the new line conflicts with data at different indexes in each way; [[Cache algorithms|LRU]] tracking for non-skewed caches is usually done on a per-set basis. Nevertheless, skewed-associative caches have major advantages over conventional set-associative ones.<ref>\n[http://www.irisa.fr/caps/PROJECTS/Architecture/ Micro-Architecture] \"Skewed-associative caches have ... major advantages over conventional set-associative caches.\"\n</ref>\n\n===Pseudo-associative cache===\nA true set-associative cache tests all the possible ways simultaneously, using something like a [[content addressable memory]]. A pseudo-associative cache tests each possible way one at a time. A hash-rehash cache and a column-associative cache are examples of a pseudo-associative cache.\n\nIn the common case of finding a hit in the first way tested, a pseudo-associative cache is as fast as a direct-mapped cache, but it has a much lower conflict miss rate than a direct-mapped cache, closer to the miss rate of a fully associative cache.\n<ref name=\"CK\" />\n\n==Cache entry structure==\nCache row entries usually have the following structure:\n\n{| style=\"width:30%; text-align:center\" border=\"1\"\n|-\n| tag || data block || flag bits\n|}\n\nThe ''data block'' (cache line) contains the actual data fetched from the main memory.  The ''tag'' contains (part of) the address of the actual data fetched from the main memory.  The flag bits are [[#Flag_bits|discussed below]].\n\nThe \"size\" of the cache is the amount of main memory data it can hold.  This size can be calculated as the number of bytes stored in each data block times the number of blocks stored in the cache.  (The tag, flag and [[ECC memory#cache|error correction code]] bits are not included in the size,<ref>{{cite web|author=Nathan N. Sadler|author2=Daniel J. Sorin|url=http://people.ee.duke.edu/~sorin/papers/iccd06_perc.pdf|title=Choosing an Error Protection Scheme for a Microprocessor's L1 Data Cache\"|year=2006|page=4}}</ref> although they do affect the physical area of a cache.)\n\nAn effective memory address which goes along with the cache line (memory block) is split ([[Most significant bit|MSB]] to [[Least significant bit|LSB]]) into the tag, the index and the block offset.<ref>{{cite book|author1=John L. Hennessy|author2=David A. Patterson|title=Computer Architecture: A Quantitative Approach|year=2011|isbn=978-0-12-383872-8|at=p. B-9|url=https://books.google.com/?id=v3-1hVwHnHwC&pg=PA120&lpg=PA120&dq=Hennessey+%22block+offset%22#v=onepage&q=%22block%20offset%22&f=false}}</ref><ref>{{cite book|author1=David A. Patterson|author2=John L. Hennessy|title=Computer Organization and Design: The Hardware/Software Interface|year=2009|isbn=978-0-12-374493-7|page=484|url=https://books.google.com/?id=3b63x-0P3_UC&pg=PA484&lpg=PA484&dq=Hennessey+%22block+offset%22#v=onepage&q=Hennessey%20%22block%20offset%22&f=false}}</ref>\n\n{| style=\"width:30%; text-align:center\" border=\"1\"\n|-\n| tag || index || block offset\n|}\n\nThe index describes which cache set that the data has been put in. The index length is <math>\\lceil \\log_2(s) \\rceil</math> bits for {{mvar|s}} cache sets.\n\nThe block offset specifies the desired data within the stored data block within the cache row. Typically the effective address is in bytes, so the block offset length is <math>\\lceil \\log_2(b) \\rceil</math> bits, where {{mvar|b}} is the number of bytes per data block.\nThe tag contains the most significant bits of the address, which are checked against all rows in the current set (the set has been retrieved by index) to see if this set contains the requested address. If it does, a cache hit occurs. The tag length in bits is as follows:\n\n::<code>tag_length = address_length - index_length - block_offset_length</code>\n\nSome authors refer to the block offset as simply the \"offset\"<ref name=\"ccs.neu.edu\">{{cite web|author=Gene Cooperman|title=Cache Basics|year=2003|url=http://www.ccs.neu.edu/course/com3200/parent/NOTES/cache-basics.html}}</ref> or the \"displacement\".<ref>{{cite web|author=Ben Dugan|title=Concerning Cache|year=2002|url=http://www.cs.washington.edu/education/courses/cse378/02sp/sections/section9-1.html}}</ref><ref>Harvey G. Cragon.\n\"Memory systems and pipelined processors\".\n1996. {{ISBN|0-86720-474-5}}, {{ISBN|978-0-86720-474-2}}.\n\"Chapter 4.1: Cache Addressing, Virtual or Real\"\np. 209\n[https://books.google.com/books?id=q2w3JSFD7l4C&pg=PA209&lpg=PA209&dq=displacement+tag+cache&source=bl&ots=i3HOLDymZk&sig=VOnTozBRVPb8BTcphIPSPvvFNSU&hl=en&sa=X&ei=spTwTsm0KtHMsQK-poW-AQ&ved=0CEkQ6AEwBQ#v=onepage&q=displacement%20tag%20cache&f=false]\n</ref>\n\n===Example===\nThe original [[Pentium 4]] processor had a four-way set associative L1 data cache of 8&nbsp;[[Kibibyte|KiB]] in size, with 64-byte cache blocks. Hence, there are 8&nbsp;KiB&nbsp;/&nbsp;64&nbsp;=&nbsp;128 cache blocks. The number of sets is equal to the number of cache blocks divided by the number of ways of associativity, what leads to 128&nbsp;/&nbsp;4&nbsp;=&nbsp;32 sets, and hence 2<sup>5</sup>&nbsp;=&nbsp;32 different indices. There are 2<sup>6</sup>&nbsp;=&nbsp;64 possible offsets. Since the CPU address is 32 bits wide, this implies 32&nbsp;-&nbsp;5&nbsp;-&nbsp;6&nbsp;=&nbsp;21 bits for the tag field.\n\nThe original Pentium&nbsp;4 processor also had an eight-way set associative L2 integrated cache 256&nbsp;KiB in size, with 128-byte cache blocks. This implies 32&nbsp;-&nbsp;8&nbsp;-&nbsp;7&nbsp;=&nbsp;17 bits for the tag field.<ref name=\"ccs.neu.edu\" />\n\n==={{Anchor|Flag_bits}} Flag bits===\nAn instruction cache requires only one flag bit per cache row entry: a valid bit.  The valid bit indicates whether or not a cache block has been loaded with valid data.\n\nOn power-up, the hardware sets all the valid bits in all the caches to \"invalid\".  Some systems also set a valid bit to \"invalid\" at other times, such as when multi-master [[bus snooping]] hardware in the cache of one processor hears an address broadcast from some other processor, and realizes that certain data blocks in the local cache are now stale and should be marked invalid.\n\nA data cache typically requires two flag bits per cache line{{snd}} a valid bit and a [[dirty bit]].  Having a dirty bit set indicates that the associated cache line has been changed since it was read from main memory (\"dirty\"), meaning that the processor has written data to that line and the new value has not propagated all the way to main memory.\n\n=={{Anchor|CACHE-MISS}}Cache miss==\nA cache miss is a failed attempt to read or write a piece of data in the cache, which results in a main memory access with much longer latency. There are three kinds of cache misses: instruction read miss, data read miss, and data write miss.\n\n''Cache read misses'' from an ''instruction'' cache generally cause the largest delay, because the processor, or at least the [[Simultaneous multithreading|thread of execution]], has to wait (stall) until the instruction is fetched from main memory.  ''Cache read misses'' from a ''data'' cache usually cause a smaller delay, because instructions not dependent on the cache read can be issued and continue execution until the data is returned from main memory, and the dependent instructions can resume execution.  ''Cache write misses'' to a ''data'' cache generally cause the shortest delay, because the write can be queued and there are few limitations on the execution of subsequent instructions; the processor can continue until the queue is full. For a detailed introduction to the types of misses, see [[cache performance measurement and metric]].\n\n=={{Anchor|ADDRTRANS}}Address translation==\nMost general purpose CPUs implement some form of [[virtual memory]]. To summarize, either each program running on the machine sees its own simplified [[address space]], which contains code and data for that program only, or all programs run in a common virtual address space. A program executes by calculating, comparing, reading and writing to addresses of its virtual address space, rather than addresses of physical address space, making programs simpler and thus easier to write.\n\nVirtual memory requires the processor to translate virtual addresses generated by the program into physical addresses in main memory. The portion of the processor that does this translation is known as the [[memory management unit]] (MMU). The fast path through the MMU can perform those translations stored in the [[translation lookaside buffer]] (TLB), which is a cache of mappings from the operating system's [[page table]], segment table, or both.\n\nFor the purposes of the present discussion, there are three important features of address translation:\n\n* ''Latency:'' The physical address is available from the MMU some time, perhaps a few cycles, after the virtual address is available from the address generator.\n* ''Aliasing:'' Multiple virtual addresses can map to a single physical address. Most processors guarantee that all updates to that single physical address will happen in program order. To deliver on that guarantee, the processor must ensure that only one copy of a physical address resides in the cache at any given time.\n* ''Granularity:'' The virtual address space is broken up into pages. For instance, a 4&nbsp;[[Gibibyte|GiB]] virtual address space might be cut up into 1,048,576 pages of 4&nbsp;KiB size, each of which can be independently mapped. There may be multiple page sizes supported; see [[virtual memory]] for elaboration.\n\nSome early virtual memory systems were very slow because they required an access to the page table (held in main memory) before every programmed access to main memory.{{refn|The very first paging machine, the [[Ferranti]] [[Atlas Computer (Manchester)|Atlas]]<ref name=AtlasCPU/><ref name=AtlasSup/> had no page tables in main memory; there was an associative memory with one entry for every 512 word page frame of core.|group=NB}} With no caches, this effectively cut the speed of memory access in half. The first hardware cache used in a computer system was not actually a data or instruction cache, but rather a TLB.<ref name=AtlasSup/>\n\nCaches can be divided into four types, based on whether the index or tag correspond to physical or virtual addresses:\n\n* ''Physically indexed, physically tagged'' (PIPT) caches use the physical address for both the index and the tag. While this is simple and avoids problems with aliasing, it is also slow, as the physical address must be looked up (which could involve a TLB miss and access to main memory) before that address can be looked up in the cache.\n* ''Virtually indexed, virtually tagged'' (VIVT) caches use the virtual address for both the index and the tag. This caching scheme can result in much faster lookups, since the MMU does not need to be consulted first to determine the physical address for a given virtual address. However, VIVT suffers from aliasing problems, where several different virtual addresses may refer to the same physical address. The result is that such addresses would be cached separately despite referring to the same memory, causing coherency problems. Although solutions to this problem exist <ref>{{cite book|last1=Kaxiras|first1=Stefanos|last2=Ros|first2=Alberto|title=A New Perspective for Efficient Virtual-Cache Coherence|journal=40th International Symposium on Computer Architecture (ISCA)|date=2013|pages=535\u2013547|doi=10.1145/2485922.2485968|isbn=9781450320795|citeseerx=10.1.1.307.9125}}</ref> they do not work for standard coherence protocols. Another problem is homonyms, where the same virtual address maps to several different physical addresses. It is not possible to distinguish these mappings merely by looking at the virtual index itself, though potential solutions include: flushing the cache after a [[context switch]], forcing address spaces to be non-overlapping, tagging the virtual address with an address space ID (ASID). Additionally, there is a problem that virtual-to-physical mappings can change, which would require flushing cache lines, as the VAs would no longer be valid.  All these issues are absent if tags use physical addresses (VIPT).\n* ''Virtually indexed, physically tagged'' (VIPT) caches use the virtual address for the index and the physical address in the tag. The advantage over PIPT is lower latency, as the cache line can be looked up in parallel with the TLB translation, however the tag cannot be compared until the physical address is available. The advantage over VIVT is that since the tag has the physical address, the cache can detect homonyms.  Theoretically, VIPT requires more tags bits because some of the index bits could differ between the virtual and physical addresses (for example bit 12 and above for 4 KiB pages) and would have to be included both in the virtual index and in the physical tag. In practice this is not an issue because, in order to avoid coherency problems, VIPT caches are designed to have no such index bits (e.g., by limiting the total number of bits for the index and the block offset to 12 for 4 KiB pages); this limits the size of VIPT caches to the page size times the associativity of the cache.\n* ''Physically indexed, virtually tagged'' (PIVT) caches are often claimed in literature to be useless and non-existing.<ref>{{cite magazine |url=http://www.linuxjournal.com/article/7105 |title=Understanding Caching |magazine=Linux Journal |accessdate=2010-05-02}}</ref>  However, the [[MIPS architecture|MIPS]] [[R6000]] uses this cache type as the sole known implementation.<ref>{{cite journal|first1=George|last1=Taylor|first2=Peter|last2=Davies|first3=Michael|last3=Farmwald|title=The TLB Slice - A Low-Cost High-Speed Address Translation Mechanism|year=1990|id=CH2887-8/90/0000/0355$01.OO}}</ref>  The R6000 is implemented in [[emitter-coupled logic]], which is an extremely fast technology not suitable for large memories such as a [[Translation lookaside buffer|TLB]].  The R6000 solves the issue by putting the TLB memory into a reserved part of the second-level cache having a tiny, high-speed TLB \"slice\" on chip.  The cache is indexed by the physical address obtained from the TLB slice.  However, since the TLB slice only translates those virtual address bits that are necessary to index the cache and does not use any tags, false cache hits may occur, which is solved by tagging with the virtual address.\n\nThe speed of this recurrence (the ''load latency'') is crucial to CPU performance, and so most modern level-1 caches are virtually indexed, which at least allows the MMU's TLB lookup to proceed in parallel with fetching the data from the cache RAM.\n\nBut virtual indexing is not the best choice for all cache levels. The cost of dealing with virtual aliases grows with cache size, and as a result most level-2 and larger caches are physically indexed.\n\nCaches have historically used both virtual and physical addresses for the cache tags, although virtual tagging is now uncommon. If the TLB lookup can finish before the cache RAM lookup, then the physical address is available in time for tag compare, and there is no need for virtual tagging. Large caches, then, tend to be physically tagged, and only small, very low latency caches are virtually tagged. In recent general-purpose CPUs, virtual tagging has been superseded by vhints, as described below.\n\n===Homonym and synonym problems===\nA cache that relies on virtual indexing and tagging becomes inconsistent after the same virtual address is mapped into different physical addresses ([[homonym]]), which can be solved by using physical address for tagging, or by storing the address space identifier in the cache line. However, the latter approach does not help against the [[synonym]] problem, in which several cache lines end up storing data for the same physical address. Writing to such locations may update only one location in the cache, leaving the others with inconsistent data. This issue may be solved by using non-overlapping memory layouts for different address spaces, or otherwise the cache (or a part of it) must be flushed when the mapping changes.<ref>{{cite web\n | url = http://www.systems.ethz.ch/education/courses/fs09/aos/lectures/wk3-print.pdf\n | archiveurl = https://web.archive.org/web/20111007150424/http://www.systems.ethz.ch/education/past-courses/fs09/aos/lectures/wk3-print.pdf\n | title = Advanced Operating Systems Caches and TLBs (263-3800-00L)\n | date = 2009-03-03 | accessdate = 2016-02-14 | archivedate = 2011-10-07\n | author1 = Timothy Roscoe | author2 = Andrew Baumann\n | website = systems.ethz.ch }}</ref>\n\n===Virtual tags and vhints===\nThe great advantage of virtual tags is that, for associative caches, they allow the tag match to proceed before the virtual to physical translation is done. However, coherence probes and evictions present a physical address for action. The hardware must have some means of converting the physical addresses into a cache index, generally by storing physical tags as well as virtual tags. For comparison, a physically tagged cache does not need to keep virtual tags, which is simpler. When a virtual to physical mapping is deleted from the TLB, cache entries with those virtual addresses will have to be flushed somehow. Alternatively, if cache entries are allowed on pages not mapped by the TLB, then those entries will have to be flushed when the access rights on those pages are changed in the page table.\n\nIt is also possible for the operating system to ensure that no virtual aliases are simultaneously resident in the cache. The operating system makes this guarantee by enforcing page coloring, which is described below. Some early RISC processors (SPARC, RS/6000) took this approach. It has not been used recently, as the hardware cost of detecting and evicting virtual aliases has fallen and the software complexity and performance penalty of perfect page coloring has risen.\n\nIt can be useful to distinguish the two functions of tags in an associative cache: they are used to determine which way of the entry set to select, and they are used to determine if the cache hit or missed. The second function must always be correct, but it is permissible for the first function to guess, and get the wrong answer occasionally.\n\nSome processors (e.g. early SPARCs) have caches with both virtual and physical tags. The virtual tags are used for way selection, and the physical tags are used for determining hit or miss. This kind of cache enjoys the latency advantage of a virtually tagged cache, and the simple software interface of a physically tagged cache. It bears the added cost of duplicated tags, however. Also, during miss processing, the alternate ways of the cache line indexed have to be probed for virtual aliases and any matches evicted.\n\nThe extra area (and some latency) can be mitigated by keeping ''virtual hints'' with each cache entry instead of virtual tags. These hints are a subset or hash of the virtual tag, and are used for selecting the way of the cache from which to get data and a physical tag. Like a virtually tagged cache, there may be a virtual hint match but physical tag mismatch, in which case the cache entry with the matching hint must be evicted so that cache accesses after the cache fill at this address will have just one hint match. Since virtual hints have fewer bits than virtual tags distinguishing them from one another, a virtually hinted cache suffers more conflict misses than a virtually tagged cache.\n\nPerhaps the ultimate reduction of virtual hints can be found in the Pentium&nbsp;4 (Willamette and Northwood cores). In these processors the virtual hint is effectively two bits, and the cache is four-way set associative. Effectively, the hardware maintains a simple permutation from virtual address to cache index, so that no [[content-addressable memory]] (CAM) is necessary to select the right one of the four ways fetched.\n\n===Page coloring===\n{{Main article|Cache coloring}}\n\nLarge physically indexed caches (usually secondary caches) run into a problem: the operating system rather than the application controls which pages collide with one another in the cache. Differences in page allocation from one program run to the next lead to differences in the cache collision patterns, which can lead to very large differences in program performance. These differences can make it very difficult to get a consistent and repeatable timing for a benchmark run.\n\nTo understand the problem, consider a CPU with a 1&nbsp;MiB physically indexed direct-mapped level-2 cache and 4&nbsp;KiB virtual memory pages. Sequential physical pages map to sequential locations in the cache until after 256 pages the pattern wraps around. We can label each physical page with a color of 0\u2013255 to denote where in the cache it can go. Locations within physical pages with different colors cannot conflict in the cache.\n\nProgrammers attempting to make maximum use of the cache may arrange their programs' access patterns so that only 1&nbsp;MiB of data need be cached at any given time, thus avoiding capacity misses. But they should also ensure that the access patterns do not have conflict misses. One way to think about this problem is to divide up the virtual pages the program uses and assign them virtual colors in the same way as physical colors were assigned to physical pages before. Programmers can then arrange the access patterns of their code so that no two pages with the same virtual color are in use at the same time. There is a wide literature on such optimizations (e.g. [[loop nest optimization]]), largely coming from the [[High Performance Computing|High Performance Computing (HPC)]] community.\n\nThe snag is that while all the pages in use at any given moment may have different virtual colors, some may have the same physical colors. In fact, if the operating system assigns physical pages to virtual pages randomly and uniformly, it is extremely likely that some pages will have the same physical color, and then locations from those pages will collide in the cache (this is the [[birthday paradox]]).\n\nThe solution is to have the operating system attempt to assign different physical color pages to different virtual colors, a technique called ''page coloring''. Although the actual mapping from virtual to physical color is irrelevant to system performance, odd mappings are difficult to keep track of and have little benefit, so most approaches to page coloring simply try to keep physical and virtual page colors the same.\n\nIf the operating system can guarantee that each physical page maps to only one virtual color, then there are no virtual aliases, and the processor can use virtually indexed caches with no need for extra virtual alias probes during miss handling. Alternatively, the OS can flush a page from the cache whenever it changes from one virtual color to another. As mentioned above, this approach was used for some early SPARC and RS/6000 designs.\n\n=={{Anchor|Cache hierarchy}}Cache hierarchy in a modern processor==\n[[File:Hwloc.png|thumb|right|300px|Memory hierarchy of an AMD Bulldozer server]]\n\nModern processors have multiple interacting on-chip caches.  The operation of a particular cache can be completely specified by the cache size, the cache block size, the number of blocks in a set, the cache set replacement policy, and the cache write policy (write-through or write-back).<ref name=\"ccs.neu.edu\" />\n\nWhile all of the cache blocks in a particular cache are the same size and have the same associativity, typically the \"lower-level\" caches (called Level 1 cache) have a smaller number of blocks, smaller block size, and fewer blocks in a set, but have very short access times. \"Higher-level\" caches (i.e. Level 2 and above) have progressively larger numbers of blocks, larger block size, more blocks in a set, and relatively longer access times, but are still much faster than main memory.\n\nCache entry replacement policy is determined by a [[cache algorithm]] selected to be implemented by the processor designers.  In some cases, multiple algorithms are provided for different kinds of work loads.\n\n===Specialized caches===\n\nPipelined CPUs access memory from multiple points in the [[Instruction pipeline|pipeline]]: instruction fetch, [[virtual memory|virtual-to-physical]] address translation, and data fetch (see [[classic RISC pipeline]]). The natural design is to use different physical caches for each of these points, so that no one physical resource has to be scheduled to service two points in the pipeline. Thus the pipeline naturally ends up with at least three separate caches (instruction, [[translation lookaside buffer|TLB]], and data), each specialized to its particular role.\n\n====Victim cache====\n{{ main | victim cache }}\nA '''victim cache''' is a cache used to hold blocks evicted from a CPU cache upon replacement. The victim cache lies between the main cache and its refill path, and holds only those blocks of data that were evicted from the main cache. The victim cache is usually fully associative, and is intended to reduce the number of conflict misses. Many commonly used programs do not require an associative mapping for all the accesses. In fact, only a small fraction of the memory accesses of the program require high associativity. The victim cache exploits this property by providing high associativity to only these accesses. It was introduced by [[Norman Jouppi]] from DEC in 1990.<ref>N.P.Jouppi. \"Improving direct-mapped cache performance by the addition of a small {{Sic|hide=y|fully|-}}associative cache and prefetch buffers.\" - 17th Annual International Symposium on Computer Architecture, 1990. Proceedings., {{doi|10.1109/ISCA.1990.134547}}</ref>\n\nIntel's ''[[Crystalwell]]''<ref name=\"intel-ark-crystal-well\">{{cite web\n | url = http://ark.intel.com/products/codename/51802/Crystal-Well\n | title = Products (Formerly Crystal Well)\n | publisher = [[Intel]]\n | accessdate = 2013-09-15\n}}</ref> variant of its [[Haswell (microarchitecture)|Haswell]] processors introduced an on-package 128&nbsp;MB [[eDRAM]] Level 4 cache which serves as a victim cache to the processors' Level 3 cache.<ref name=\"anandtech-i74950hq\">{{cite web\n | url = http://www.anandtech.com/show/6993/intel-iris-pro-5200-graphics-review-core-i74950hq-tested/3\n | title =  Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested\n | publisher = [[AnandTech]]\n | accessdate = 2013-09-16\n}}</ref> In the [[Skylake (microarchitecture)|Skylake]] microarchitecture the Level 4 cache no longer works as a victim cache.<ref>{{cite web\n | url = http://www.anandtech.com/show/9582/intel-skylake-mobile-desktop-launch-architecture-analysis/5\n | title = The Intel Skylake Mobile and Desktop Launch, with Architecture Analysis\n | author = Ian Cutress\n | date = September 2, 2015\n | publisher = AnandTech\n}}</ref>\n\n===={{Anchor|TRACE-CACHE}}Trace cache====\n{{Main article|Trace Cache}}\n\nOne of the more extreme examples of cache specialization is the '''trace cache''' (also known as ''execution trace cache'') found in the [[Intel]] [[Pentium&nbsp;4]] microprocessors.  A trace cache is a mechanism for increasing the instruction fetch bandwidth and decreasing power consumption (in the case of the Pentium&nbsp;4) by storing traces of [[instruction (computer science)|instruction]]s that have already been fetched and decoded.<ref>{{cite web\n | url = http://www.anandtech.com/show/661/5\n | title = The Pentium 4's Cache \u2013 Intel Pentium&nbsp;4 1.4&nbsp;GHz & 1.5&nbsp;GHz\n | date = 2000-11-20 | accessdate = 2015-11-30\n | author = Anand Lal Shimpi | publisher = [[AnandTech]]\n}}</ref>\n\nA trace cache stores instructions either after they have been decoded, or as they are retired. Generally, instructions are added to trace caches in groups representing either individual [[basic block]]s or dynamic instruction traces. The Pentium&nbsp;4's trace cache stores [[micro-operations]] resulting from decoding x86 instructions, providing also the functionality of a micro-operation cache.  Having this, the next time an instruction is needed, it does not have to be decoded into micro-ops again.<ref name=\"agner.org\" />{{rp|63&ndash;68}}\n\n==== Write Coalescing Cache (WCC) ====\nWrite Coalescing Cache<ref>{{cite web|url=http://www.realworldtech.com/bulldozer/9/|title=AMD's Bulldozer Microarchitecture - Memory Subsystem Continued|date=August 26, 2010|author=David Kanter|website=Real World Technologies}}</ref> is a special cache that is part of L2 cache in [[AMD]]'s [[Bulldozer (microarchitecture)|Bulldozer microarchitecture]]. Stores from both L1D caches in the module go through the WCC, where they are buffered and coalesced.\nThe WCC's task is reducing number of writes to the L2 cache.\n\n===={{Anchor|UOP-CACHE}}Micro-operation (\u03bcop or uop) cache====\nA '''micro-operation cache''' ('''\u03bcop cache''', '''uop cache''' or '''UC''')<ref>{{cite web|url=http://www.realworldtech.com/sandy-bridge/4/|title=Intel's Sandy Bridge Microarchitecture - Instruction Decode and uop Cache|date=September 25, 2010|author=David Kanter|website=Real World Technologies}}</ref> is a specialized cache that stores [[micro-operation]]s of decoded instructions, as received directly from the [[instruction decoder]]s or from the instruction cache.  When an instruction needs to be decoded, the \u03bcop cache is checked for its decoded form which is re-used if cached; if it is not available, the instruction is decoded and then cached.\n\nOne of the early works describing \u03bcop cache as an alternative frontend for the Intel [[P6 (microarchitecture)|P6 processor family]] is the 2001 paper ''\"Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA\"''.<ref name=\"uop-intel\">{{cite book\n | chapter-url = http://cecs.uci.edu/~papers/compendium94-03/papers/2001/islped01/pdffiles/p004.pdf\n | chapter = Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA\n | author1 = Baruch Solomon | title = ISLPED'01: Proceedings of the 2001 International Symposium on Low Power Electronics and Design (IEEE Cat. No.01TH8581)\n | pages = 4\u20139\n | author2 = Avi Mendelson | author3 = Doron Orenstein | author4 = Yoav Almog | author5 = Ronny Ronen\n | date = August 2001 | accessdate = 2013-10-06\n | publisher = [[Intel]] | doi = 10.1109/LPE.2001.945363\n| isbn = 978-1-58113-371-4\n }}</ref>  Later, Intel included \u03bcop caches in its [[Sandy Bridge]] processors and in successive microarchitectures like [[Ivy Bridge (microarchitecture)|Ivy Bridge]] and [[Haswell (microarchitecture)|Haswell]].<ref name=\"agner.org\">{{cite web\n | url = http://www.agner.org/optimize/microarchitecture.pdf\n | title = The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers\n | date = 2014-02-19 | accessdate = 2014-03-21\n | author = Agner Fog | website = agner.org\n }}</ref>{{rp|121&ndash;123}}<ref name=\"anandtech-haswell\">{{cite web\n | url = http://www.anandtech.com/show/6355/intels-haswell-architecture/6\n | title = Intel's Haswell Architecture Analyzed\n | date = 2012-10-05 | accessdate = 2013-10-20\n | author = Anand Lal Shimpi | publisher = [[AnandTech]]\n}}</ref> AMD implemented a \u03bcop cache in their [[Zen (microarchitecture)|Zen microarchitecture]].<ref>{{cite web\n | url = http://www.anandtech.com/show/10578/amd-zen-microarchitecture-dual-schedulers-micro-op-cache-memory-hierarchy-revealed\n | title = AMD Zen Microarchitecture: Dual Schedulers, Micro-Op Cache and Memory Hierarchy Revealed\n | date = 2016-08-18 | access-date = 2017-04-03\n | author = Ian Cutress | publisher = AnandTech}}</ref>\n\nFetching complete pre-decoded instructions eliminates the need to repeatedly decode variable length complex instructions into simpler fixed-length micro-operations, and simplifies the process of predicting, fetching, rotating and aligning fetched instructions. A \u03bcop cache effectively offloads the fetch and decode hardware, thus decreasing [[power consumption]] and improving the frontend supply of decoded micro-operations.  The \u03bcop cache also increases performance by more consistently delivering decoded micro-operations to the backend and eliminating various bottlenecks in the CPU's fetch and decode logic.<ref name=\"uop-intel\" /><ref name=\"anandtech-haswell\" />\n\nA \u03bcop cache has many similarities with a trace cache, although a \u03bcop cache is much simpler thus providing better power efficiency; this makes it better suited for implementations on battery-powered devices. The main disadvantage of the trace cache, leading to its power inefficiency, is the hardware complexity required for its [[heuristic]] deciding on caching and reusing dynamically created instruction traces.<ref name=\"tc-slides\">{{cite web|url=https://www.cs.cmu.edu/afs/cs/academic/class/15740-f03/www/lectures/TraceCache_slides.pdf|title=Trace Cache|date=October 2003|accessdate=2013-10-06|author1=Leon Gu|author2=Dipti Motiani}}</ref>\n\n==== Branch target cache ====\nA '''branch target cache''' or '''branch target instruction cache''', the name used on ARM microprocessors,<ref>{{cite web |title=How does the BTIC (branch target instruction cache) work? |url=https://community.arm.com/processors/f/discussions/5320/how-does-the-btic-branch-target-instruction-cache-works |date=28 May 2015 |author=Kun Niu |accessdate=7 April 2018}}</ref> is a specialized cache which holds the first few instructions at the destination of a taken branch.  This is used by low-powered processors which do not need a normal instruction cache because the memory system is capable of delivering instructions fast enough to satisfy the CPU without one.  However, this only applies to consecutive instructions in sequence; it still takes several cycles of latency to restart instruction fetch at a new address, causing a few cycles of pipeline bubble after a control transfer.  A branch target cache provides instructions for those few cycles avoiding a delay after most taken branches.\n\nThis allows full-speed operation with a much smaller cache than a traditional full-time instruction cache.\n\n==== Smart cache ====\n'''Smart cache''' is a [[L2 cache|level 2]] or [[L3 cache|level 3]] [[CPU cache|caching]] method for multiple execution cores, developed by [[Intel]].\n\nSmart Cache shares the actual cache memory between the cores of a [[multi-core processor]]. In comparison to a dedicated per-core cache, the overall [[cache miss]] rate decreases when not all cores need equal parts of the cache space. Consequently, a single core can use the full level 2 or level 3 cache, if the other cores are inactive.<ref>{{cite web|url=http://www.intel.com/content/www/us/en/architecture-and-technology/intel-smart-cache.html|title=Intel Smart Cache: Demo|publisher=[[Intel]]|accessdate=2012-01-26}}</ref> Furthermore, the shared cache makes it faster to share memory among different execution cores.<ref>{{cite web |url=http://software.intel.com/file/18374/ |archiveurl=https://web.archive.org/web/20111229193036/http://software.intel.com/file/18374/ |title=Inside Intel Core Microarchitecture and Smart Memory Access |format=PDF |page=5 |publisher=[[Intel]] |year=2006 |accessdate=2012-01-26 |archivedate=2011-12-29}}</ref>\n\n==={{Anchor|MULTILEVEL}}Multi-level caches===\n{{See also|Cache hierarchy}}\nAnother issue is the fundamental tradeoff between cache latency and hit rate. Larger caches have better hit rates but longer latency. To address this tradeoff, many computers use multiple levels of cache, with small fast caches backed up by larger, slower caches.  Multi-level caches generally operate by checking the fastest, ''level 1'' ('''L1''') cache first; if it hits, the processor proceeds at high speed. If that smaller cache misses, the next fastest cache (''level 2'', '''L2''') is checked, and so on, before accessing external memory.\n\nAs the latency difference between main memory and the fastest cache has become larger, some processors have begun to utilize as many as three levels of on-chip cache.  Price-sensitive designs used this to pull the entire cache hierarchy on-chip, but by the 2010s some of the highest-performance designs returned to having large off-chip caches, which is often implemented in [[eDRAM]] and mounted on a [[multi-chip module]], as a fourth cache level. In rare cases, as in latest IBM mainframe CPU, [[IBM z15 (microprocessor)|IBM z15]] from 2019, all levels down to L1 are implemented by eDRAM, replacing [[static random-access memory|SRAM]] entirely (for caches, i.g. it's still used for registers) for 128&nbsp;KiB L1 for instructions and for data, or combined 256&nbsp;KiB.\n\nThe benefits of L3 and L4 caches depend on the application's access patterns.  Examples of products incorporating L3 and L4 caches include the following:\n\n* [[Alpha 21164]] (1995) has 1 to 64&nbsp;MB off-chip L3 cache.\n* IBM [[POWER4]] (2001) has off-chip L3 caches of 32&nbsp;MB per processor, shared among several processors.\n* [[Itanium 2]] (2003) has a 6&nbsp;MB [[unified cache|unified]] level 3 (L3) cache on-die; the [[Itanium 2]] (2003) MX&nbsp;2 module incorporates two Itanium&nbsp;2 processors along with a shared 64&nbsp;MB L4 cache on a [[multi-chip module]] that was pin compatible with a Madison processor.\n* Intel's [[Xeon]] MP product codenamed \"Tulsa\" (2006) features 16&nbsp;MB of on-die L3 cache shared between two processor cores.\n* AMD [[Phenom&nbsp;II]] (2008) has up to 6&nbsp;MB on-die unified L3 cache.\n* [[Intel Core i7]] (2008) has an 8&nbsp;MB on-die unified L3 cache that is inclusive, shared by all cores.\n* Intel [[Haswell (microarchitecture)|Haswell]] CPUs with integrated [[Intel Iris Pro Graphics]] have 128&nbsp;MB of eDRAM acting essentially as an L4 cache.<ref>{{cite web|url=http://www.anandtech.com/show/6993/intel-iris-pro-5200-graphics-review-core-i74950hq-tested/3 |title=Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested |publisher=AnandTech |accessdate=2014-02-25}}</ref>\n\nFinally, at the other end of the memory hierarchy, the CPU [[register file]] itself can be considered the smallest, fastest cache in the system, with the special characteristic that it is scheduled in software\u2014typically by a compiler, as it allocates registers to hold values retrieved from main memory for, as an example, [[loop nest optimization]]. However, with [[register renaming]] most compiler register assignments are reallocated dynamically by hardware at runtime into a register bank, allowing the CPU to break false data dependencies and thus easing pipeline hazards.\n\nRegister files sometimes also have hierarchy: The [[Cray-1]] (circa 1976) had eight address \"A\" and eight scalar data \"S\" registers that were generally usable. There was also a set of 64 address \"B\" and 64 scalar data \"T\" registers that took longer to access, but were faster than main memory. The \"B\" and \"T\" registers were provided because the Cray-1 did not have a data cache. (The Cray-1 did, however, have an instruction cache.)\n\n===={{Anchor|LLC}}Multi-core chips====\nWhen considering a chip with [[Multi-core processor|multiple cores]], there is a question of whether the caches should be shared or local to each core. Implementing shared cache inevitably introduces more wiring and complexity. But then, having one cache per ''chip'', rather than ''core'', greatly reduces the amount of space needed, and thus one can include a larger cache.\n\nTypically, sharing the L1 cache is undesirable because the resulting increase in latency would make each core run considerably slower than a single-core chip.  However, for the highest-level cache, the last one called before accessing memory, having a global cache is desirable for several reasons, such as allowing a single core to use the whole cache, reducing data redundancy by making it possible for different processes or threads to share cached data, and reducing the complexity of utilized cache coherency protocols.<ref>{{cite web\n | url = https://software.intel.com/en-us/articles/software-techniques-for-shared-cache-multi-core-systems\n | title = Software Techniques for Shared-Cache Multi-Core Systems\n | date = 2012-03-08 | accessdate = 2015-11-24\n | author1 = Tian Tian | author2 = Chiu-Pi Shih\n | publisher = [[Intel]]\n}}</ref>  For example, an eight-core chip with three levels may include an L1 cache for each core, one intermediate L2 cache for each pair of cores, and one L3 cache shared between all cores.\n\nShared highest-level cache, which is called before accessing memory, is usually referred to as the ''last level cache'' (LLC).  Additional techniques are used for increasing the level of parallelism when LLC is shared between multiple cores, including slicing it into multiple pieces which are addressing certain ranges of memory addresses, and can be accessed independently.<ref>{{cite web\n | url = http://www.hotchips.org/wp-content/uploads/hc_archives/hc23/HC23.19.9-Desktop-CPUs/HC23.19.911-Sandy-Bridge-Lempel-Intel-Rev%207.pdf\n | title = 2nd Generation Intel Core Processor Family: Intel Core i7, i5 and i3\n | date = 2013-07-28 | accessdate = 2014-01-21\n | author = Oded Lempel | website = hotchips.org\n | page = 7&ndash;10,31&ndash;45\n}}</ref>\n\n====Separate versus unified====\nIn a separate cache structure, instructions and data are cached separately, meaning that a cache line is used to cache either instructions or data, but not both; various benefits have been demonstrated with separate data and instruction [[translation lookaside buffer]]s.<ref>{{cite journal |author1=Chen, J. Bradley |author2=Borg, Anita |author3=Jouppi, Norman P. |title=A Simulation Based Study of TLB Performance |journal=SIGARCH Computer Architecture News |volume=20 |issue=2 |year=1992 |page=114&ndash;123 |doi=10.1145/146628.139708}}</ref>  In a unified structure, this constraint is not present, and cache lines can be used to cache both instructions and data.\n\n===={{Anchor|INCLUSIVE|EXCLUSIVE}}Exclusive versus inclusive====\nMulti-level caches introduce new design decisions.  For instance, in some processors, all data in the L1 cache must also be somewhere in the L2 cache.  These caches are called ''strictly inclusive''. Other processors (like the [[AMD Athlon]]) have ''exclusive'' caches: data is guaranteed to be in at most one of the L1 and L2 caches, never in both.  Still other processors (like the Intel [[Pentium II]], [[Pentium III|III]], and [[Pentium 4|4]]) do not require that data in the L1 cache also reside in the L2 cache, although it may often do so. There is no universally accepted name for this intermediate policy;<ref>{{cite web\n | url = http://www.amecomputers.com/explanation-of-the-l1-and-l2-cache.html\n | title = Explanation of the L1 and L2 Cache \n | accessdate = 2014-06-09\n | website = amecomputers.com\n}}</ref><ref name=\"ispass04\">{{cite web\n | url = http://mercury.pr.erau.edu/~davisb22/papers/ispass04.pdf\n | title = Performance Evaluation of Exclusive Cache Hierarchies\n | date = 2004-06-25 | accessdate = 2014-06-09\n | author1 = Ying Zheng | author2 = Brian T. Davis | author3 = Matthew Jordan\n | publisher = Michigan Technological University\n }}</ref>\ntwo common names are \"non-exclusive\" and \"partially-inclusive\".\n\nThe advantage of exclusive caches is that they store more data. This advantage is larger when the exclusive L1 cache is comparable to the L2 cache, and diminishes if the L2 cache is many times larger than the L1 cache. When the L1 misses and the L2 hits on an access, the hitting cache line in the L2 is exchanged with a line in the L1. This exchange is quite a bit more work than just copying a line from L2 to L1, which is what an inclusive cache does.<ref name=\"ispass04\" />\n\nOne advantage of strictly inclusive caches is that when external devices or other processors in a multiprocessor system wish to remove a cache line from the processor, they need only have the processor check the L2 cache. In cache hierarchies which do not enforce inclusion, the L1 cache must be checked as well. As a drawback, there is a correlation between the associativities of L1 and L2 caches: if the L2 cache does not have at least as many ways as all L1 caches together, the effective associativity of the L1 caches is restricted. Another disadvantage of inclusive cache is that whenever there is an eviction in L2 cache, the (possibly) corresponding lines in L1 also have to get evicted in order to maintain inclusiveness. This is quite a bit of work, and would result in a higher L1 miss rate.<ref name=\"ispass04\" />\n\nAnother advantage of inclusive caches is that the larger cache can use larger cache lines, which reduces the size of the secondary cache tags. (Exclusive caches require both caches to have the same size cache lines, so that cache lines can be swapped on a L1 miss, L2 hit.) If the secondary cache is an order of magnitude larger than the primary, and the cache data is an order of magnitude larger than the cache tags, this tag area saved can be comparable to the incremental area needed to store the L1 cache data in the L2.<ref>{{cite web\n | url = http://www.jaleels.org/ajaleel/publications/micro2010-tla.pdf\n | title = Achieving Non-Inclusive Cache Performance with Inclusive Caches\n | date = 2010-09-27 | accessdate = 2014-06-09\n | author1 = Aamer Jaleel | author2 = Eric Borch | author3 = Malini Bhandaru\n | author4 = Simon C. Steely Jr. | author5 = Joel Emer\n | website = jaleels.org }}</ref>\n\n===Example: the K8===\nTo illustrate both specialization and multi-level caching, here is the cache hierarchy of the K8 core in the AMD [[Athlon 64]] CPU.<ref>{{cite web|url=http://www.sandpile.org/impl/k8.htm |title=AMD K8 |accessdate=2007-06-02 |website=Sandpile.org |url-status=dead |archiveurl=https://web.archive.org/web/20070515052223/http://www.sandpile.org/impl/k8.htm |archivedate=2007-05-15 }}</ref>\n\n[[File:Cache,hierarchy-example.svg|thumb|center|500px|Cache hierarchy of the K8 core in the AMD Athlon 64 CPU.]]\n\nThe K8 has four specialized caches: an instruction cache, an instruction [[translation lookaside buffer|TLB]], a data TLB, and a data cache. Each of these caches is specialized:\n\n* The instruction cache keeps copies of 64-byte lines of memory, and fetches 16 bytes each cycle. Each byte in this cache is stored in ten bits rather than eight, with the extra bits marking the boundaries of instructions (this is an example of predecoding). The cache has only [[parity bit|parity]] protection rather than [[Error-correcting code|ECC]], because parity is smaller and any damaged data can be replaced by fresh data fetched from memory (which always has an up-to-date copy of instructions).\n* The instruction TLB keeps copies of page table entries (PTEs). Each cycle's instruction fetch has its virtual address translated through this TLB into a physical address. Each entry is either four or eight bytes in memory. Because the K8 has a variable page size, each of the TLBs is split into two sections, one to keep PTEs that map 4&nbsp;KB pages, and one to keep PTEs that map 4&nbsp;MB or 2&nbsp;MB pages. The split allows the fully associative match circuitry in each section to be simpler. The operating system maps different sections of the virtual address space with different size PTEs.\n* The data TLB has two copies which keep identical entries. The two copies allow two data accesses per cycle to translate virtual addresses to physical addresses. Like the instruction TLB, this TLB is split into two kinds of entries.\n* The data cache keeps copies of 64-byte lines of memory. It is split into 8 banks (each storing 8&nbsp;KB of data), and can fetch two 8-byte data each cycle so long as those data are in different banks. There are two copies of the tags, because each 64-byte line is spread among all eight banks. Each tag copy handles one of the two accesses per cycle.\n\nThe K8 also has multiple-level caches. There are second-level instruction and data TLBs, which store only PTEs mapping 4&nbsp;KB. Both instruction and data caches, and the various TLBs, can fill from the large '''unified''' L2 cache. This cache is exclusive to both the L1 instruction and data caches, which means that any 8-byte line can only be in one of the L1 instruction cache, the L1 data cache, or the L2 cache. It is, however, possible for a line in the data cache to have a PTE which is also in one of the TLBs\u2014the operating system is responsible for keeping the TLBs coherent by flushing portions of them when the page tables in memory are updated.\n\nThe K8 also caches information that is never stored in memory\u2014prediction information. These caches are not shown in the above diagram. As is usual for this class of CPU, the K8 has fairly complex\n[[branch prediction]], with tables that help predict whether branches are taken and other tables which predict the targets of branches and jumps. Some of this information is associated with instructions, in both the level 1 instruction cache and the unified secondary cache.\n\nThe K8 uses an interesting trick to store prediction information with instructions in the secondary cache. Lines in the secondary cache are protected from accidental data corruption (e.g. by an [[alpha particle]] strike) by either [[Error-correcting code|ECC]] or [[parity (telecommunication)|parity]], depending on whether those lines were evicted from the data or instruction primary caches. Since the parity code takes fewer bits than the ECC code, lines from the instruction cache have a few spare bits. These bits are used to cache branch prediction information associated with those instructions. The net result is that the branch predictor has a larger effective history table, and so has better accuracy.\n\n===More hierarchies===\n<!-- (This section should be rewritten.) -->\nOther processors have other kinds of predictors (e.g., the store-to-load bypass predictor in the [[Digital Equipment Corporation|DEC]] [[Alpha 21264]]), and various specialized predictors are likely to flourish in future processors.\n\nThese predictors are caches in that they store information that is costly to compute. Some of the terminology used when discussing predictors is the same as that for caches (one speaks of a '''hit''' in a branch predictor), but predictors are not generally thought of as part of the cache hierarchy.\n\nThe K8 keeps the instruction and data caches '''[[cache coherency|coherent]]''' in hardware, which means that a store into an instruction closely following the store instruction will change that following instruction. Other processors, like those in the Alpha and MIPS family, have relied on software to keep the instruction cache coherent. Stores are not guaranteed to show up in the instruction stream until a program calls an operating system facility to ensure coherency.\n\n===Tag RAM===\nIn computer engineering, a ''tag RAM'' is used to specify which of the possible memory locations is currently stored in a CPU cache.<ref>{{cite web\n | url = http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0363g/Chdijaed.html\n | title = Cortex-R4 and Cortex-R4F Technical Reference Manual\n | accessdate = 2013-09-28\n | publisher = arm.com\n}}</ref><ref>{{cite web\n | url = http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0284g/Ebddefci.html\n | title = L210 Cache Controller Technical Reference Manual\n | accessdate = 2013-09-28\n | publisher = arm.com\n}}</ref>  For a simple, direct-mapped design fast [[static random-access memory|SRAM]] can be used.   Higher [[CPU cache#Associativity|associative caches]] usually employ [[content-addressable memory]].\n\n==Implementation==\n{{Main article|Cache algorithms}}\n\nCache '''reads''' are the most common CPU operation that takes more than a single cycle. Program execution time tends to be very sensitive to the latency of a level-1 data cache hit. A great deal of design effort, and often power and silicon area are expended making the caches as fast as possible.\n\nThe simplest cache is a virtually indexed direct-mapped cache. The virtual address is calculated with an adder, the relevant portion of the address extracted and used to index an SRAM, which returns the loaded data. The data is byte aligned in a byte shifter, and from there is bypassed to the next operation. There is no need for any tag checking in the inner loop{{snd}} in fact, the tags need not even be read. Later in the pipeline, but before the load instruction is retired, the tag for the loaded data must be read, and checked against the virtual address to make sure there was a cache hit. On a miss, the cache is updated with the requested cache line and the pipeline is restarted.\n\nAn associative cache is more complicated, because some form of tag must be read to determine which entry of the cache to select. An N-way set-associative level-1 cache usually reads all N possible tags and N data in parallel, and then chooses the data associated with the matching tag. Level-2 caches sometimes save power by reading the tags first, so that only one data element is read from the data SRAM.\n\n[[Image:Cache,associative-read.svg|thumb|right|upright=1.8|Read path for a 2-way associative cache]]\n\nThe adjacent diagram is intended to clarify the manner in which the various fields of the address are used. Address bit 31 is most significant, bit 0 is least significant. The diagram shows the SRAMs, indexing, and [[multiplexing]] for a 4&nbsp;KB, 2-way set-associative, virtually indexed and virtually tagged cache with 64&nbsp;byte (B) lines, a 32-bit read width and 32-bit virtual address.\n\nBecause the cache is 4&nbsp;KB and has 64&nbsp;B lines, there are just 64 lines in the cache, and we read two at a time from a Tag SRAM which has 32 rows, each with a pair of 21 bit tags. Although any function of virtual address bits 31 through 6 could be used to index the tag and data SRAMs, it is simplest to use the least significant bits.\n\nSimilarly, because the cache is 4&nbsp;KB and has a 4&nbsp;B read path, and reads two ways for each access, the Data SRAM is 512 rows by 8 bytes wide.\n\nA more modern cache might be 16&nbsp;KB, 4-way set-associative, virtually indexed, virtually hinted, and physically tagged, with 32&nbsp;B lines, 32-bit read width and 36-bit physical addresses. The read path recurrence for such a cache looks very similar to the path above. Instead of tags, vhints are read, and matched against a subset of the virtual address. Later on in the pipeline, the virtual address is translated into a physical address by the TLB, and the physical tag is read (just one, as the vhint supplies which way of the cache to read). Finally the physical address is compared to the physical tag to determine if a hit has occurred.\n\nSome SPARC designs have improved the speed of their L1 caches by a few gate delays by collapsing the virtual address adder into the SRAM decoders. See [[Sum addressed decoder]].\n\n===History===\nThe early history of cache technology is closely tied to the invention and use of virtual memory.{{Citation needed|date=March 2008}} <!-- this *is* a truly interesting observation, but are there any sources? Also why was the \"CPU-mam speed\" part deleted? ~~~~ --> Because of scarcity and cost of semi-conductor memories, early mainframe computers in the 1960s used a complex hierarchy of physical memory, mapped onto a flat virtual memory space used by programs. The memory technologies would span semi-conductor, magnetic core, drum and disc. Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access. Extensive studies were done to optimize the cache sizes. Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest cache sizes.{{Disputed inline|Talk:CPU cache#Dispute sequence of events for paging|reason=sequence of events wrong|date=December 2010}}\n\nIn the early days of microcomputer technology, memory access was only slightly slower than [[processor register|register]] access. But since the 1980s<ref>{{cite journal| url=https://epic.hpi.uni-potsdam.de/pub/Home/TrendsAndConceptsII2010/HW_Trends_The_Processor-Memory_bottleneck___Problems_and_Solutions..pdf | title=The processor-memory bottleneck: problems and solutions | journal=Crossroads | volume=5 | issue=3es | pages=2\u2013es | first1=Nihar R. | last1=Mahapatra |first2=Balakrishna | last2=Venkatrao | accessdate=2013-03-05 | doi=10.1145/357783.331677 | year=1999 }}</ref> the performance gap between processor and memory has been growing. Microprocessors have advanced much faster than memory, especially in terms of their operating [[frequency]], so memory became a performance [[Von Neumann architecture#Von Neumann bottleneck|bottleneck]]. While it was technically possible to have all the main memory as fast as the CPU, a more economically viable path has been taken: use plenty of low-speed memory, but also introduce a small high-speed cache memory to alleviate the performance gap. This provided an order of magnitude more capacity\u2014for the same price\u2014with only a slightly reduced combined performance.\n\n====First TLB implementations====\nThe first documented uses of a TLB were on the [[General Electric|GE]] [[GE 645|645]]<ref>{{cite book\n |    author = GE\n |     title = GE-645 System Manual\n |      date = January 1968\n |       url = http://bitsavers.org/pdf/ge/GE-645/GE-645_SystemMan_Jan68.pdf}}\n</ref> and the [[IBM]] [[IBM System/360 Model 67|360/67]],<ref>\n{{cite book\n |    author = IBM\n |     title = IBM System/360 Model 67 Functional Characteristics\n |         id = GA27-2719-2\n |        url = http://www.bitsavers.org/pdf/ibm/360/funcChar/GA27-2719-2_360-67_funcChar.pdf\n |   version = Third Edition\n |      date = February 1972}}</ref> both of which used an associative memory as a TLB.\n\n====First data cache====\nThe first documented use of a data cache was on the [[IBM]] System/360 Model 85.<ref>{{cite book\n |     author = IBM\n |      title = IBM System/360 Model 85 Functional Characteristics\n |         id = A22-6916-1\n |        url = http://www.bitsavers.org/pdf/ibm/360/funcChar/A22-6916-1_360-85_funcChar_Jun68.pdf\n |    version = SECOND EDITION\n |       date = June 1968}}</ref>\n\n====In 68k microprocessors====\nThe [[68010]], released in 1982, has a \"loop mode\" which can be considered a tiny and special-case instruction cache that accelerates loops that consist of only two instructions. The [[68020]], released in 1984, replaced that with a typical instruction cache of 256 bytes, being the first 68k series processor to feature true on-chip cache memory.\n\nThe [[68030]], released in 1987, is basically a 68020 core with an additional 256-byte data cache, a process shrink, and added burst mode for the caches. The [[Motorola 68040|68040]], released in 1990, has split instruction and data caches of four kilobytes each. The [[68060]], released in 1994, has the following: 8&nbsp;KB data cache (four-way associative), 8&nbsp;KB instruction cache (four-way associative), 96-byte FIFO instruction buffer, 256-entry branch cache, and 64-entry address translation cache MMU buffer (four-way associative).\n\n====In x86 microprocessors====\nAs the [[x86]] microprocessors reached clock rates of 20&nbsp;MHz and above in the [[Intel 80386|386]], small amounts of fast cache memory began to be featured in systems to improve performance. This was because the [[DRAM]] used for main memory had significant latency, up to 120&nbsp;ns, as well as refresh cycles. The cache was constructed from more expensive, but significantly faster, [[Static random access memory|SRAM]] [[Memory cell (computing)|memory cells]], which at the time had latencies around 10&nbsp;ns - 25&nbsp;ns. The early caches were external to the processor and typically located on the motherboard in the form of eight or nine [[Dual in-line package|DIP]] devices placed in sockets to enable the cache as an optional extra or upgrade feature.\n\nSome versions of the Intel 386 processor could support 16 to 256&nbsp;KB of external cache.\n\nWith the [[Intel 80486|486]] processor, an 8&nbsp;KB cache was integrated directly into the CPU die. This cache was termed Level 1 or L1 cache to differentiate it from the slower on-motherboard, or Level 2 (L2) cache. These on-motherboard caches were much larger, with the most common size being 256&nbsp;KB. The popularity of on-motherboard cache continued through the [[Intel P5|Pentium MMX]] era but was made obsolete by the introduction of [[SDRAM]] and the growing disparity between bus clock rates and CPU clock rates, which caused on-motherboard cache to be only slightly faster than main memory.\n\nThe next development in cache implementation in the x86 microprocessors began with the [[Pentium Pro]], which brought the secondary cache onto the same package as the microprocessor, clocked at the same frequency as the microprocessor.\n\nOn-motherboard caches enjoyed prolonged popularity thanks to the [[AMD K6-2]] and [[AMD K6-III]] processors that still used [[Socket 7]], which was previously used by Intel with on-motherboard caches. K6-III included 256&nbsp;KB on-die L2 cache and took advantage of the on-board cache as a third level cache, named L3 (motherboards with up to 2&nbsp;MB of on-board cache were produced). After the Socket&nbsp;7 became obsolete, on-motherboard cache disappeared from the x86 systems.\n\nThe three-level caches were used again first with the introduction of multiple processor cores, where the L3 cache was added to the CPU die.  It became common for the total cache sizes to be increasingly larger in newer processor generations, and recently (as of 2011) it is not uncommon to find Level 3 cache sizes of tens of megabytes.<ref>{{cite web\n | url = http://ark.intel.com/products/family/59139/Intel-Xeon-Processor-E7-Family/server\n | title = Intel\u00ae Xeon\u00ae Processor E7 Family\n | accessdate = 2013-10-10\n | publisher = [[Intel]]\n}}</ref>\n\n[[Intel]] introduced a Level 4 on-package cache with the [[Haswell (microarchitecture)|Haswell]] [[microarchitecture]]. ''[[Crystalwell]]''<ref name=\"intel-ark-crystal-well\" /> Haswell CPUs, equipped with the [[GT3e]] variant of Intel's integrated Iris Pro graphics, effectively feature 128&nbsp;MB of embedded DRAM ([[eDRAM]]) on the same package.  This L4 cache is shared dynamically between the on-die GPU and CPU, and serves as a [[victim cache]] to the CPU's L3 cache.<ref name=\"anandtech-i74950hq\" />\n\n====Current research====\nEarly cache designs focused entirely on the direct cost of cache and [[RAM]] and average execution speed.\nMore recent cache designs also consider [[low-power electronics|energy efficiency]],<ref>{{cite journal|url=https://www.academia.edu/5010517|title=A Survey of Architectural Techniques For Improving Cache Power Efficiency|author=Sparsh Mittal|journal=Sustainable Computing: Informatics and Systems|volume=4|issue=1|pages=33\u201343|date=March 2014|doi=10.1016/j.suscom.2013.11.001}}</ref> fault tolerance, and other goals.<ref>{{cite journal|url=https://spectrum.ieee.org/computing/hardware/chip-design-thwarts-sneak-attack-on-data|title=Chip Design Thwarts Sneak Attack on Data|author=Sally Adee|year=2009}}</ref><ref>{{cite conference|url=http://palms.princeton.edu/system/files/Micro08_Newcache.pdf|title=A novel cache architecture with enhanced performance and security|author1=Zhenghong Wang|author2=Ruby B. Lee|conference=41st annual IEEE/ACM International Symposium on Microarchitecture|pages=83\u201393|date=November 8-12, 2008|archive-url=https://web.archive.org/web/20120306225926/http://palms.princeton.edu/system/files/Micro08_Newcache.pdf|archive-date=March 6, 2012}}</ref> Researchers have also explored use of emerging memory technologies such as [[eDRAM]] (embedded DRAM) and NVRAM (non-volatile RAM) for designing caches.<ref>{{cite journal|url=https://www.academia.edu/6988421|title=A Survey Of Architectural Approaches for Managing Embedded DRAM and Non-volatile On-chip Caches|author1=Sparsh Mittal|author2=Jeffrey S. Vetter|author3=Dong Li|journal=IEEE Transactions on Parallel and Distributed Systems|volume=26|issue=6|date=June 2015|pages=1524\u20131537|doi=10.1109/TPDS.2014.2324563}}</ref>\n\nThere are several tools available to computer architects to help explore tradeoffs between the cache cycle time, energy, and area; the CACTI cache simulator<ref>{{cite web|url=http://www.hpl.hp.com/research/cacti/ |title=CACTI |website=Hpl.hp.com |accessdate=2010-05-02}}</ref> and the SimpleScalar instruction set simulator are two open-source options. Modeling of 2D and 3D [[static random-access memory|SRAM]], [[eDRAM]], [[spin-transfer torque|STT-RAM]], [[resistive random-access memory|ReRAM]] and [[phase-change memory|PCM]] caches can be done using the DESTINY tool.<ref>{{cite web|url=https://code.ornl.gov/3d_cache_modeling_tool/destiny/blob/master/README|title=3d_cache_modeling_tool / destiny|website=code.ornl.gov|access-date=2015-02-26}}</ref>\n\n===Multi-ported cache===\nA multi-ported cache is a cache which can serve more than one request at a time. When accessing a traditional cache we normally use a single memory address, whereas in a multi-ported cache we may request N addresses at a time{{snd}} where N is the number of ports that connected through the processor and the cache. The benefit of this is that a pipelined processor may access memory from different phases in its pipeline. Another benefit is that it allows the concept of super-scalar processors through different cache levels.\n\n==See also==\n{{Div col|colwidth=30em}}\n* [[Cache (computing)]]\n* [[Cache algorithms]]\n* [[Cache coherency]]\n* [[Cache control instruction]]s\n* [[Cache hierarchy]]\n* [[Cache prefetching]]\n* [[Dinero (cache simulator)|Dinero]] (Cache simulator by [[University of Wisconsin System]])\n* [[Instruction unit]]\n* [[Locality of reference]]\n* [[Memoization]]\n* [[Memory hierarchy]]\n* [[Micro-operation]]\n* [[No-write allocation]]\n* [[Scratchpad RAM]]\n* [[Sum addressed decoder]]\n* [[Write buffer]]\n{{Div col end}}\n\n==Notes==\n{{Reflist|group=NB}}\n\n==References==\n{{Reflist|30em|refs=\n<ref name=AtlasSup>\n{{cite book\n |      last1 = Kilburn\n |     first1 = T.\n |      last2 = Payne\n |     first2 = R. B.\n |      last3 = Howarth\n |     first3 = D. J.\n |      title = Computers - Key to Total Systems Control\n |     series = Conferences Proceedings\n |      pages = 279\u2013294\n |    chapter = The Atlas Supervisor\n | chapterurl = http://www.chilton-computing.org.uk/acl/technology/atlas/p019.htm\n |     volume = 20 Proceedings of the Eastern Joint Computer Conference Washington, D.C.\n |date=December 1961\n |  publisher = Macmillan}}\n</ref>\n\n<ref name=AtlasCPU>\n{{cite book\n |      last1 = Sumner\n |     first1 = F. H.\n |      last2 = Haley\n |     first2 = G.\n |      last3 = Chenh\n |     first3 = E. C. Y.\n |      title = Information Processing 1962\n |     series = IFIP Congress Proceedings\n |    chapter = The Central Control Unit of the 'Atlas' Computer\n |     volume = Proceedings of IFIP Congress 62\n |       year = 1962\n |  publisher = Spartan}}\n</ref>\n}}\n\n==External links==\n{{wikibooks\n |1= Microprocessor Design\n |2= Cache\n}}\n* [https://lwn.net/Articles/252125/ Memory part 2: CPU caches]{{snd}} an article on lwn.net by Ulrich Drepper describing CPU caches in detail\n* [ftp://ftp.cs.wisc.edu/markhill/Papers/toc89_cpu_cache_associativity.pdf Evaluating Associativity in CPU Caches] \u2013 Hill and Smith (1989) \u2013 introduces capacity, conflict, and compulsory classification\n* [http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/ Cache Performance for SPEC CPU2000 Benchmarks] \u2013 Hill and Cantin (2003) \u2013 This reference paper has been updated several times. It has thorough and lucidly presented simulation results for a reasonably wide set of benchmarks and cache organizations.\n* [http://www.sun.com/blueprints/1102/817-0742.pdf Memory Hierarchy in Cache-Based Systems] \u2013 by Ruud van der Pas, 2002, Sun Microsystems \u2013 a nice introductory article to CPU memory caching\n* [http://www.freescale.com/files/32bit/doc/app_note/AN2663.pdf A Cache Primer] \u2013 by Paul Genua, P.E., 2004, Freescale Semiconductor, another introductory article\n* [https://web.archive.org/web/20110718154522/http://www.zipcores.com/skin1/zipdocs/datasheets/cache_8way_set.pdf An 8-way set-associative cache]{{snd}} written in [[VHDL]]\n* [https://arstechnica.com/old/content/2002/07/caching.ars Understanding CPU caching and performance]{{snd}} an article on Ars Technica by Jon Stokes\n* [http://ixbtlabs.com/articles/ibmpower4/ IBM POWER4 processor review]{{snd}} an article on ixbtlabs by Pavel Danilov\n* [https://www.cs.princeton.edu/courses/archive/fall15/cos375/lectures/16-Cache-2x2.pdf Memory Caching]{{snd}} a Princeton University lecture\n\n{{CPU technologies|state=collapsed}}\n\n{{DEFAULTSORT:Cpu Cache}}\n[[Category:Central processing unit]]\n[[Category:Computer memory]]\n[[Category:Cache (computing)]]\n", "text_old": "A '''CPU cache''' is a [[hardware cache]] used by the [[central processing unit]] (CPU) of a [[computer]] to reduce the average cost (time or energy) to access [[data (computing)|data]] from the [[main memory]].<ref>{{cite web|url=http://www.hardwaresecrets.com/how-the-cache-memory-works/|title=How The Cache Memory Works|author=Gabriel Torres|date=September 12, 2007}}</ref> A cache is a smaller, faster memory, located closer to a [[processor core]], which stores copies of the data from frequently used main [[memory location]]s.  Most CPUs have different independent caches, including [[instruction cache|instruction]] and [[data cache]]s, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).\n\nAll modern (fast) CPUs (with few specialized exceptions{{refn|A few specialized CPUs, accelerators or microcontrollers do not have a cache. To be fast, if needed/wanted, they still have an on-chip [[scratchpad memory]] that has a similar function, while software managed. In, for example, microcontrollers, it can be better for hard real-time use, to have that or at least no cache, as with one level of memory latencies of loads are predictable.}}) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). \nSplit L1 cache started in 1985 with the R2000 MIPS CPU, achieved mainstream in 1993 with the Intel Pentium and in 1997 the embedded CPU market with the ARMv5TE. In 2015, even sub-dollar SoC split the L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a [[multi-core processor]] has a dedicated L1 cache and is usually not shared between the cores. The L2 cache, and higher-level caches, may be shared between the cores. L4 cache is currently uncommon, and is generally on (a form of) [[dynamic random-access memory]] (DRAM), rather than on [[static random-access memory]] (SRAM), on a separate die or chip (exceptionally, the form, [[eDRAM]] is used for all levels of cache, down to L1). That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and optimized differently.\n\nOther types of caches exist (that are not counted towards the \"cache size\" of the most important caches mentioned above), such as the [[translation lookaside buffer]] (TLB) which is part of the [[memory management unit]] (MMU) which most CPUs have.\n\nCaches (like for RAM historically) have generally been sized in powers of: 2, 4, 8, 16 etc. [[Kibibyte|KiB]]; when up to [[Mebibyte|MiB]] sizes (i.e. for larger non-L1), very early on the pattern broke down, to allow for larger caches without being forced into the doubling-in-size paradigm, with e.g. [[Intel Core 2 Duo]] with 3&nbsp;MiB L2 cache in April 2008. Much later however for L1 sizes, that still only count in small number of KiB, however [[IBM zEC12 (microprocessor)|IBM zEC12]] from 2012 is an exception, to gain unusually large 96&nbsp;KiB L1 data cache for its time, and e.g. the [[IBM z13 (microprocessor)|IBM z13]] having a 96&nbsp;KiB L1 instruction cache (and 128&nbsp;KiB L1 data cache),<ref>{{cite web|url=http://www.redbooks.ibm.com/redbooks/pdfs/sg248250.pdf|title=IBM z13 and IBM z13s Technical Introduction|page=20|date=March 2016|publisher=[[IBM]]}}</ref> and Intel [[Ice Lake (microprocessor)|Ice Lake]]-based processors from 2018, having 48&nbsp;KiB L1 data cache and 48&nbsp;KiB L1 instruction cache. In 2020, some [[Intel Atom]] CPUs (with up to 24 cores) have (multiple of) 4.5&nbsp;MiB and 15&nbsp;MiB cache sizes.<ref>{{Cite press|url=https://newsroom.intel.com/news/product-fact-sheet-accelerating-5g-network-infrastructure-core-edge/|title=Product Fact Sheet: Accelerating 5G Network Infrastructure, from the Core to the Edge|website=Intel Newsroom|quote=L1 cache of 32KB/core, L2 cache of 4.5MB per 4-core cluster and shared LLC cache up to 15MB.|language=en-US|access-date=2020-04-12}}</ref><ref>{{Cite web|url=https://www.anandtech.com/show/15544/intel-launches-atom-p5900-a-10nm-atom-for-radio-access-networks|title=Intel Launches Atom P5900: A 10nm Atom for Radio Access Networks|last=Smith|first=Ryan|website=www.anandtech.com|access-date=2020-04-12}}</ref>\n\n=={{Anchor|ICACHE|DCACHE|instruction cache|data cache}}Overview==\nWhen trying to read from or write to a location in main memory, the processor checks whether the data from that location is already in the cache. If so, the processor will read from or write to the cache instead of the much slower main memory. \n\nMost modern [[Desktop computer|desktop]] and [[Server (computing)|server]] CPUs have at least three independent caches: an '''instruction cache''' to speed up executable instruction fetch, a '''data cache''' to speed up data fetch and store, and a [[translation lookaside buffer]] (TLB) used to speed up virtual-to-physical address translation for both executable instructions and data.  A single TLB can be provided for access to both instructions and data, or a separate Instruction TLB (ITLB) and data TLB (DTLB) can be provided.<ref name=\"tlbsurvey\">\"[https://www.academia.edu/29585076/A_Survey_of_Techniques_for_Architecting_TLBs A Survey of Techniques for Architecting TLBs]\", Concurrency and Computation, 2016.</ref> The data cache is usually organized as a hierarchy of more cache levels (L1, L2, etc.; see also [[#Multi-level caches|multi-level caches]] below). However, the TLB cache is part of the [[memory management unit]] (MMU) and not directly related to the CPU caches.\n\n==={{Anchor|cache lines|CACHE-LINES}}Cache entries===\nData is transferred between memory and cache in blocks of fixed size, called ''cache lines'' or ''cache blocks''. When a cache line is copied from memory into the cache, a cache entry is created. The cache entry will include the copied data as well as the requested memory location (called a tag).\n\nWhen the processor needs to read or write a location in memory, it first checks for a corresponding entry in the cache. The cache checks for the contents of the requested memory location in any cache lines that might contain that address. If the processor finds that the memory location is in the cache, a cache hit has occurred. However, if the processor does not find the memory location in the cache, a cache miss has occurred. In the case of a cache hit, the processor immediately reads or writes the data in the cache line.  For a cache miss, the cache allocates a new entry and copies data from main memory, then the request is fulfilled from the contents of the cache.\n\n===Policies===\n\n===={{Anchor|EVICTION}}Replacement policies====\n{{Main article|Cache algorithms}}\n\nTo make room for the new entry on a cache miss, the cache may have to evict one of the existing entries. The heuristic it uses to choose the entry to evict is called the replacement policy. The fundamental problem with any replacement policy is that it must predict which existing cache entry is least likely to be used in the future. Predicting the future is difficult, so there is no perfect method to choose among the variety of replacement policies available. One popular replacement policy, [[Least Recently Used|least-recently used]] (LRU), replaces the least recently accessed entry.\n\nMarking some memory ranges as non-cacheable can improve performance, by avoiding caching of memory regions that are rarely re-accessed. This avoids the overhead of loading something into the cache without having any reuse.  Cache entries may also be disabled or locked depending on the context.\n\n====Write policies====\n{{Main article|Cache (computing)#WRITEPOLICIES|l1=Cache (computing) \u00a7 Writing policies}}\n\nIf data is written to the cache, at some point it must also be written to main memory; the timing of this write is known as the write policy. In a [[write-through]] cache, every write to the cache causes a write to main memory.  Alternatively, in a [[write-back]] or copy-back cache, writes are not immediately mirrored to the main memory, and the cache instead tracks which locations have been written over, marking them as [[Dirty bit|dirty]]. The data in these locations is written back to the main memory only when that data is evicted from the cache. For this reason, a read miss in a write-back cache may sometimes require two memory accesses to service: one to first write the dirty location to main memory, and then another to read the new location from memory. Also, a write to a main memory location that is not yet mapped in a write-back cache may evict an already dirty location, thereby freeing that cache space for the new memory location.\n\nThere are intermediate policies as well. The cache may be write-through, but the writes may be held in a store data queue temporarily, usually so multiple stores can be processed together (which can reduce bus turnarounds and improve bus utilization).\n\nCached data from the main memory may be changed by other entities (e.g., peripherals using [[direct memory access]] (DMA) or another core in a [[multi-core processor]]), in which case the copy in the cache may become out-of-date or stale. Alternatively, when a CPU in a [[multiprocessor]] system updates data in the cache, copies of data in caches associated with other CPUs become stale. Communication protocols between the cache managers that keep the data consistent are known as [[cache coherence]] protocols.\n\n=== Cache performance ===\n[[Cache performance measurement and metric|Cache performance measurement]] has become important in recent times where the speed gap between the memory performance and the processor performance is increasing exponentially. The cache was introduced to reduce this speed gap. Thus knowing how well the cache is able to bridge the gap in the speed of processor and memory becomes important, especially in high-performance systems. The cache hit rate and the cache miss rate play an important role in determining this performance. To improve the cache performance, reducing the miss rate becomes one of the necessary steps among other steps. Decreasing the access time to the cache also gives a boost to its performance.\n\n====CPU stalls====\nThe time taken to fetch one cache line from memory (read [[Latency (engineering)|latency]] due to a cache miss) matters because the CPU will run out of things to do while waiting for the cache line. When a CPU reaches this state, it is called a stall.  As CPUs become faster compared to main memory, stalls due to cache misses displace more potential computation; modern CPUs can execute hundreds of instructions in the time taken to fetch a single cache line from main memory.\n\nVarious techniques have been employed to keep the CPU busy during this time, including [[out-of-order execution]] in which the CPU attempts to execute independent instructions after the instruction that is waiting for the cache miss data.  Another technology, used by many processors, is [[simultaneous multithreading]] (SMT), which allows an alternate thread to use the CPU core while the first thread waits for required CPU resources to become available.\n\n==Associativity==\n[[Image:Cache,associative-fill-both.png|thumb|450px|An illustration of different ways in which memory locations can be cached by particular cache locations]]\n{{Main|Cache placement policies}}\nThe [[Cache placement policies|placement policy]] decides where in the cache a copy of a particular entry of main memory will go. If the placement policy is free to choose any entry in the cache to hold the copy, the cache is called ''fully associative''. At the other extreme, if each entry in main memory can go in just one place in the cache, the cache is ''direct mapped''. Many caches implement a compromise in which each entry in main memory can go to any one of N places in the cache, and are described as N-way set associative.<ref>{{cite web|url=http://cseweb.ucsd.edu/classes/fa10/cse240a/pdf/08/CSE240A-MBT-L15-Cache.ppt.pdf|title=Cache design|date=2010-12-02|website=ucsd.edu|page=10&ndash;15|accessdate=2014-02-24}}</ref> For example, the level-1 data cache in an [[AMD Athlon]] is two-way set associative, which means that any particular location in main memory can be cached in either of two locations in the level-1 data cache.\n\nChoosing the right value of associativity involves a [[trade-off]]. If there are ten places to which the placement policy could have mapped a memory location, then to check if that location is in the cache, ten cache entries must be searched. Checking more places takes more power and chip area, and potentially more time. On the other hand, caches with more associativity suffer fewer misses (see conflict misses, below), so that the CPU wastes less time reading from the slow main memory. The general guideline is that doubling the associativity, from direct mapped to two-way, or from two-way to four-way, has about the same effect on raising the hit rate as doubling the cache size. However, increasing associativity more than four does not improve hit rate as much,<ref>[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5234663 IEEE Xplore - Phased set associative cache design for reduced power consumption]. Ieeexplore.ieee.org (2009-08-11). Retrieved on 2013-07-30.</ref> and are generally done for other reasons (see virtual aliasing, below).  Some CPUs can dynamically reduce the associativity of their caches in low-power states, which acts as a power-saving measure.<ref>{{cite web|url=http://hotchips.org/wp-content/uploads/hc_archives/hc24/HC24-1-Microprocessor/HC24.28.117-HotChips_IvyBridge_Power_04.pdf#page=18|title=Power Management of the Third Generation Intel Core Micro Architecture formerly codenamed Ivy Bridge|author1=Sanjeev Jahagirdar|author2=Varghese George|year=2012|website=hotchips.org|page=18|format=PDF|accessdate=2015-12-16|author3=Inder Sodhi|author4=Ryan Wells}}</ref>\n\n<!-- where does \"pseudo-associative cache\" go in this spectrum? -->\nIn order of worse but simple to better but complex:\n\n* Direct mapped cache{{snd}} good best-case time, but unpredictable in worst case\n* Two-way set associative cache\n* Two-way skewed associative cache<ref name=\"Seznec\">{{cite journal|author=Andr\u00e9 Seznec|year=1993|title=A Case for Two-Way Skewed-Associative Caches|journal=ACM Sigarch Computer Architecture News|volume=21|issue=2|pages=169\u2013178|doi=10.1145/173682.165152}}</ref>\n* Four-way set associative cache\n* Eight-way set associative cache, a common choice for later implementations\n* 12-way set associative cache, similar to eight-way\n* Fully associative cache{{snd}} the best miss rates, but practical only for a small number of entries\n\n===Direct-mapped cache===\nIn this cache organization, each location in main memory can go in only one entry in the cache.  Therefore, a direct-mapped cache can also be called a \"one-way set associative\" cache.  It does not have a placement policy as such, since there is no choice of which cache entry's contents to evict.  This means that if two locations map to the same entry, they may continually knock each other out.  Although simpler, a direct-mapped cache needs to be much larger than an associative one to give comparable performance, and it is more unpredictable.  Let {{mvar|x}} be block number in cache, {{mvar|y}} be block number of memory, and {{mvar|n}} be number of blocks in cache, then mapping is done with the help of the equation {{math|''x'' {{=}} ''y'' mod ''n''}}.\n\n===Two-way set associative cache===\nIf each location in main memory can be cached in either of two locations in the cache, one logical question is: ''which one of the two?'' The simplest and most commonly used scheme, shown in the right-hand diagram above, is to use the least significant bits of the memory location's index as the index for the cache memory, and to have two entries for each index. One benefit of this scheme is that the tags stored in the cache do not have to include that part of the main memory address which is implied by the cache memory's index. Since the cache tags have fewer bits, they require fewer transistors, take less space on the processor circuit board or on the microprocessor chip, and can be read and compared faster. Also [[Cache algorithms|LRU]] is especially simple since only one bit needs to be stored for each pair.\n\n===Speculative Execution===\nOne of the advantages of a direct mapped cache is that it allows simple and fast [[speculative execution|speculation]]. Once the address has been computed, the one cache index which might have a copy of that location in memory is known. That cache entry can be read, and the processor can continue to work with that data before it finishes checking that the tag actually matches the requested address.\n\nThe idea of having the processor use the cached data before the tag match completes can be applied to associative caches as well. A subset of the tag, called a ''hint'', can be used to pick just one of the possible cache entries mapping to the requested address. The entry selected by the hint can then be used in parallel with checking the full tag. The hint technique works best when used in the context of address translation, as explained below.\n\n===Two-way skewed associative cache===\nOther schemes have been suggested, such as the ''skewed cache'',<ref name=\"Seznec\" /> where the index for way 0 is direct, as above, but the index for way 1 is formed with a [[hash function]]. A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function, and so it is less likely that a program will suffer from an unexpectedly large number of conflict misses due to a pathological access pattern. The downside is extra latency from computing the hash function.<ref name=\"CK\">{{cite web|url=http://www.stanford.edu/class/ee282/08_handouts/L03-Cache.pdf|title=Lecture 3: Advanced Caching Techniques|author=C. Kozyrakis|archive-url=https://web.archive.org/web/20120907012034/http://www.stanford.edu/class/ee282/08_handouts/L03-Cache.pdf|archive-date=September 7, 2012}}</ref> Additionally, when it comes time to load a new line and evict an old line, it may be difficult to determine which existing line was least recently used, because the new line conflicts with data at different indexes in each way; [[Cache algorithms|LRU]] tracking for non-skewed caches is usually done on a per-set basis. Nevertheless, skewed-associative caches have major advantages over conventional set-associative ones.<ref>\n[http://www.irisa.fr/caps/PROJECTS/Architecture/ Micro-Architecture] \"Skewed-associative caches have ... major advantages over conventional set-associative caches.\"\n</ref>\n\n===Pseudo-associative cache===\nA true set-associative cache tests all the possible ways simultaneously, using something like a [[content addressable memory]]. A pseudo-associative cache tests each possible way one at a time. A hash-rehash cache and a column-associative cache are examples of a pseudo-associative cache.\n\nIn the common case of finding a hit in the first way tested, a pseudo-associative cache is as fast as a direct-mapped cache, but it has a much lower conflict miss rate than a direct-mapped cache, closer to the miss rate of a fully associative cache.\n<ref name=\"CK\" />\n\n==Cache entry structure==\nCache row entries usually have the following structure:\n\n{| style=\"width:30%; text-align:center\" border=\"1\"\n|-\n| tag || data block || flag bits\n|}\n\nThe ''data block'' (cache line) contains the actual data fetched from the main memory.  The ''tag'' contains (part of) the address of the actual data fetched from the main memory.  The flag bits are [[#Flag_bits|discussed below]].\n\nThe \"size\" of the cache is the amount of main memory data it can hold.  This size can be calculated as the number of bytes stored in each data block times the number of blocks stored in the cache.  (The tag, flag and [[ECC memory#cache|error correction code]] bits are not included in the size,<ref>{{cite web|author=Nathan N. Sadler|author2=Daniel J. Sorin|url=http://people.ee.duke.edu/~sorin/papers/iccd06_perc.pdf|title=Choosing an Error Protection Scheme for a Microprocessor's L1 Data Cache\"|year=2006|page=4}}</ref> although they do affect the physical area of a cache.)\n\nAn effective memory address which goes along with the cache line (memory block) is split ([[Most significant bit|MSB]] to [[Least significant bit|LSB]]) into the tag, the index and the block offset.<ref>{{cite book|author1=John L. Hennessy|author2=David A. Patterson|title=Computer Architecture: A Quantitative Approach|year=2011|isbn=978-0-12-383872-8|at=p. B-9|url=https://books.google.com/?id=v3-1hVwHnHwC&pg=PA120&lpg=PA120&dq=Hennessey+%22block+offset%22#v=onepage&q=%22block%20offset%22&f=false}}</ref><ref>{{cite book|author1=David A. Patterson|author2=John L. Hennessy|title=Computer Organization and Design: The Hardware/Software Interface|year=2009|isbn=978-0-12-374493-7|page=484|url=https://books.google.com/?id=3b63x-0P3_UC&pg=PA484&lpg=PA484&dq=Hennessey+%22block+offset%22#v=onepage&q=Hennessey%20%22block%20offset%22&f=false}}</ref>\n\n{| style=\"width:30%; text-align:center\" border=\"1\"\n|-\n| tag || index || block offset\n|}\n\nThe index describes which cache set that the data has been put in. The index length is <math>\\lceil \\log_2(s) \\rceil</math> bits for {{mvar|s}} cache sets.\n\nThe block offset specifies the desired data within the stored data block within the cache row. Typically the effective address is in bytes, so the block offset length is <math>\\lceil \\log_2(b) \\rceil</math> bits, where {{mvar|b}} is the number of bytes per data block.\nThe tag contains the most significant bits of the address, which are checked against all rows in the current set (the set has been retrieved by index) to see if this set contains the requested address. If it does, a cache hit occurs. The tag length in bits is as follows:\n\n::<code>tag_length = address_length - index_length - block_offset_length</code>\n\nSome authors refer to the block offset as simply the \"offset\"<ref name=\"ccs.neu.edu\">{{cite web|author=Gene Cooperman|title=Cache Basics|year=2003|url=http://www.ccs.neu.edu/course/com3200/parent/NOTES/cache-basics.html}}</ref> or the \"displacement\".<ref>{{cite web|author=Ben Dugan|title=Concerning Cache|year=2002|url=http://www.cs.washington.edu/education/courses/cse378/02sp/sections/section9-1.html}}</ref><ref>Harvey G. Cragon.\n\"Memory systems and pipelined processors\".\n1996. {{ISBN|0-86720-474-5}}, {{ISBN|978-0-86720-474-2}}.\n\"Chapter 4.1: Cache Addressing, Virtual or Real\"\np. 209\n[https://books.google.com/books?id=q2w3JSFD7l4C&pg=PA209&lpg=PA209&dq=displacement+tag+cache&source=bl&ots=i3HOLDymZk&sig=VOnTozBRVPb8BTcphIPSPvvFNSU&hl=en&sa=X&ei=spTwTsm0KtHMsQK-poW-AQ&ved=0CEkQ6AEwBQ#v=onepage&q=displacement%20tag%20cache&f=false]\n</ref>\n\n===Example===\nThe original [[Pentium 4]] processor had a four-way set associative L1 data cache of 8&nbsp;[[Kibibyte|KiB]] in size, with 64-byte cache blocks. Hence, there are 8&nbsp;KiB&nbsp;/&nbsp;64&nbsp;=&nbsp;128 cache blocks. The number of sets is equal to the number of cache blocks divided by the number of ways of associativity, what leads to 128&nbsp;/&nbsp;4&nbsp;=&nbsp;32 sets, and hence 2<sup>5</sup>&nbsp;=&nbsp;32 different indices. There are 2<sup>6</sup>&nbsp;=&nbsp;64 possible offsets. Since the CPU address is 32 bits wide, this implies 32&nbsp;-&nbsp;5&nbsp;-&nbsp;6&nbsp;=&nbsp;21 bits for the tag field.\n\nThe original Pentium&nbsp;4 processor also had an eight-way set associative L2 integrated cache 256&nbsp;KiB in size, with 128-byte cache blocks. This implies 32&nbsp;-&nbsp;8&nbsp;-&nbsp;7&nbsp;=&nbsp;17 bits for the tag field.<ref name=\"ccs.neu.edu\" />\n\n==={{Anchor|Flag_bits}} Flag bits===\nAn instruction cache requires only one flag bit per cache row entry: a valid bit.  The valid bit indicates whether or not a cache block has been loaded with valid data.\n\nOn power-up, the hardware sets all the valid bits in all the caches to \"invalid\".  Some systems also set a valid bit to \"invalid\" at other times, such as when multi-master [[bus snooping]] hardware in the cache of one processor hears an address broadcast from some other processor, and realizes that certain data blocks in the local cache are now stale and should be marked invalid.\n\nA data cache typically requires two flag bits per cache line{{snd}} a valid bit and a [[dirty bit]].  Having a dirty bit set indicates that the associated cache line has been changed since it was read from main memory (\"dirty\"), meaning that the processor has written data to that line and the new value has not propagated all the way to main memory.\n\n=={{Anchor|CACHE-MISS}}Cache miss==\nA cache miss is a failed attempt to read or write a piece of data in the cache, which results in a main memory access with much longer latency. There are three kinds of cache misses: instruction read miss, data read miss, and data write miss.\n\n''Cache read misses'' from an ''instruction'' cache generally cause the largest delay, because the processor, or at least the [[Simultaneous multithreading|thread of execution]], has to wait (stall) until the instruction is fetched from main memory.  ''Cache read misses'' from a ''data'' cache usually cause a smaller delay, because instructions not dependent on the cache read can be issued and continue execution until the data is returned from main memory, and the dependent instructions can resume execution.  ''Cache write misses'' to a ''data'' cache generally cause the shortest delay, because the write can be queued and there are few limitations on the execution of subsequent instructions; the processor can continue until the queue is full. For a detailed introduction to the types of misses, see [[cache performance measurement and metric]].\n\n=={{Anchor|ADDRTRANS}}Address translation==\nMost general purpose CPUs implement some form of [[virtual memory]]. To summarize, either each program running on the machine sees its own simplified [[address space]], which contains code and data for that program only, or all programs run in a common virtual address space. A program executes by calculating, comparing, reading and writing to addresses of its virtual address space, rather than addresses of physical address space, making programs simpler and thus easier to write.\n\nVirtual memory requires the processor to translate virtual addresses generated by the program into physical addresses in main memory. The portion of the processor that does this translation is known as the [[memory management unit]] (MMU). The fast path through the MMU can perform those translations stored in the [[translation lookaside buffer]] (TLB), which is a cache of mappings from the operating system's [[page table]], segment table, or both.\n\nFor the purposes of the present discussion, there are three important features of address translation:\n\n* ''Latency:'' The physical address is available from the MMU some time, perhaps a few cycles, after the virtual address is available from the address generator.\n* ''Aliasing:'' Multiple virtual addresses can map to a single physical address. Most processors guarantee that all updates to that single physical address will happen in program order. To deliver on that guarantee, the processor must ensure that only one copy of a physical address resides in the cache at any given time.\n* ''Granularity:'' The virtual address space is broken up into pages. For instance, a 4&nbsp;[[Gibibyte|GiB]] virtual address space might be cut up into 1,048,576 pages of 4&nbsp;KiB size, each of which can be independently mapped. There may be multiple page sizes supported; see [[virtual memory]] for elaboration.\n\nSome early virtual memory systems were very slow because they required an access to the page table (held in main memory) before every programmed access to main memory.{{refn|The very first paging machine, the [[Ferranti]] [[Atlas Computer (Manchester)|Atlas]]<ref name=AtlasCPU/><ref name=AtlasSup/> had no page tables in main memory; there was an associative memory with one entry for every 512 word page frame of core.|group=NB}} With no caches, this effectively cut the speed of memory access in half. The first hardware cache used in a computer system was not actually a data or instruction cache, but rather a TLB.<ref name=AtlasSup/>\n\nCaches can be divided into four types, based on whether the index or tag correspond to physical or virtual addresses:\n\n* ''Physically indexed, physically tagged'' (PIPT) caches use the physical address for both the index and the tag. While this is simple and avoids problems with aliasing, it is also slow, as the physical address must be looked up (which could involve a TLB miss and access to main memory) before that address can be looked up in the cache.\n* ''Virtually indexed, virtually tagged'' (VIVT) caches use the virtual address for both the index and the tag. This caching scheme can result in much faster lookups, since the MMU does not need to be consulted first to determine the physical address for a given virtual address. However, VIVT suffers from aliasing problems, where several different virtual addresses may refer to the same physical address. The result is that such addresses would be cached separately despite referring to the same memory, causing coherency problems. Although solutions to this problem exist <ref>{{cite book|last1=Kaxiras|first1=Stefanos|last2=Ros|first2=Alberto|title=A New Perspective for Efficient Virtual-Cache Coherence|journal=40th International Symposium on Computer Architecture (ISCA)|date=2013|pages=535\u2013547|doi=10.1145/2485922.2485968|isbn=9781450320795|citeseerx=10.1.1.307.9125}}</ref> they do not work for standard coherence protocols. Another problem is homonyms, where the same virtual address maps to several different physical addresses. It is not possible to distinguish these mappings merely by looking at the virtual index itself, though potential solutions include: flushing the cache after a [[context switch]], forcing address spaces to be non-overlapping, tagging the virtual address with an address space ID (ASID). Additionally, there is a problem that virtual-to-physical mappings can change, which would require flushing cache lines, as the VAs would no longer be valid.  All these issues are absent if tags use physical addresses (VIPT).\n* ''Virtually indexed, physically tagged'' (VIPT) caches use the virtual address for the index and the physical address in the tag. The advantage over PIPT is lower latency, as the cache line can be looked up in parallel with the TLB translation, however the tag cannot be compared until the physical address is available. The advantage over VIVT is that since the tag has the physical address, the cache can detect homonyms.  Theoretically, VIPT requires more tags bits because some of the index bits could differ between the virtual and physical addresses (for example bit 12 and above for 4 KiB pages) and would have to be included both in the virtual index and in the physical tag. In practice this is not an issue because, in order to avoid coherency problems, VIPT caches are designed to have no such index bits (e.g., by limiting the total number of bits for the index and the block offset to 12 for 4 KiB pages); this limits the size of VIPT caches to the page size times the associativity of the cache.\n* ''Physically indexed, virtually tagged'' (PIVT) caches are often claimed in literature to be useless and non-existing.<ref>{{cite magazine |url=http://www.linuxjournal.com/article/7105 |title=Understanding Caching |magazine=Linux Journal |accessdate=2010-05-02}}</ref>  However, the [[MIPS architecture|MIPS]] [[R6000]] uses this cache type as the sole known implementation.<ref>{{cite journal|first1=George|last1=Taylor|first2=Peter|last2=Davies|first3=Michael|last3=Farmwald|title=The TLB Slice - A Low-Cost High-Speed Address Translation Mechanism|year=1990|id=CH2887-8/90/0000/0355$01.OO}}</ref>  The R6000 is implemented in [[emitter-coupled logic]], which is an extremely fast technology not suitable for large memories such as a [[Translation lookaside buffer|TLB]].  The R6000 solves the issue by putting the TLB memory into a reserved part of the second-level cache having a tiny, high-speed TLB \"slice\" on chip.  The cache is indexed by the physical address obtained from the TLB slice.  However, since the TLB slice only translates those virtual address bits that are necessary to index the cache and does not use any tags, false cache hits may occur, which is solved by tagging with the virtual address.\n\nThe speed of this recurrence (the ''load latency'') is crucial to CPU performance, and so most modern level-1 caches are virtually indexed, which at least allows the MMU's TLB lookup to proceed in parallel with fetching the data from the cache RAM.\n\nBut virtual indexing is not the best choice for all cache levels. The cost of dealing with virtual aliases grows with cache size, and as a result most level-2 and larger caches are physically indexed.\n\nCaches have historically used both virtual and physical addresses for the cache tags, although virtual tagging is now uncommon. If the TLB lookup can finish before the cache RAM lookup, then the physical address is available in time for tag compare, and there is no need for virtual tagging. Large caches, then, tend to be physically tagged, and only small, very low latency caches are virtually tagged. In recent general-purpose CPUs, virtual tagging has been superseded by vhints, as described below.\n\n===Homonym and synonym problems===\nA cache that relies on virtual indexing and tagging becomes inconsistent after the same virtual address is mapped into different physical addresses ([[homonym]]), which can be solved by using physical address for tagging, or by storing the address space identifier in the cache line. However, the latter approach does not help against the [[synonym]] problem, in which several cache lines end up storing data for the same physical address. Writing to such locations may update only one location in the cache, leaving the others with inconsistent data. This issue may be solved by using non-overlapping memory layouts for different address spaces, or otherwise the cache (or a part of it) must be flushed when the mapping changes.<ref>{{cite web\n | url = http://www.systems.ethz.ch/education/courses/fs09/aos/lectures/wk3-print.pdf\n | archiveurl = https://web.archive.org/web/20111007150424/http://www.systems.ethz.ch/education/past-courses/fs09/aos/lectures/wk3-print.pdf\n | title = Advanced Operating Systems Caches and TLBs (263-3800-00L)\n | date = 2009-03-03 | accessdate = 2016-02-14 | archivedate = 2011-10-07\n | author1 = Timothy Roscoe | author2 = Andrew Baumann\n | website = systems.ethz.ch }}</ref>\n\n===Virtual tags and vhints===\nThe great advantage of virtual tags is that, for associative caches, they allow the tag match to proceed before the virtual to physical translation is done. However, coherence probes and evictions present a physical address for action. The hardware must have some means of converting the physical addresses into a cache index, generally by storing physical tags as well as virtual tags. For comparison, a physically tagged cache does not need to keep virtual tags, which is simpler. When a virtual to physical mapping is deleted from the TLB, cache entries with those virtual addresses will have to be flushed somehow. Alternatively, if cache entries are allowed on pages not mapped by the TLB, then those entries will have to be flushed when the access rights on those pages are changed in the page table.\n\nIt is also possible for the operating system to ensure that no virtual aliases are simultaneously resident in the cache. The operating system makes this guarantee by enforcing page coloring, which is described below. Some early RISC processors (SPARC, RS/6000) took this approach. It has not been used recently, as the hardware cost of detecting and evicting virtual aliases has fallen and the software complexity and performance penalty of perfect page coloring has risen.\n\nIt can be useful to distinguish the two functions of tags in an associative cache: they are used to determine which way of the entry set to select, and they are used to determine if the cache hit or missed. The second function must always be correct, but it is permissible for the first function to guess, and get the wrong answer occasionally.\n\nSome processors (e.g. early SPARCs) have caches with both virtual and physical tags. The virtual tags are used for way selection, and the physical tags are used for determining hit or miss. This kind of cache enjoys the latency advantage of a virtually tagged cache, and the simple software interface of a physically tagged cache. It bears the added cost of duplicated tags, however. Also, during miss processing, the alternate ways of the cache line indexed have to be probed for virtual aliases and any matches evicted.\n\nThe extra area (and some latency) can be mitigated by keeping ''virtual hints'' with each cache entry instead of virtual tags. These hints are a subset or hash of the virtual tag, and are used for selecting the way of the cache from which to get data and a physical tag. Like a virtually tagged cache, there may be a virtual hint match but physical tag mismatch, in which case the cache entry with the matching hint must be evicted so that cache accesses after the cache fill at this address will have just one hint match. Since virtual hints have fewer bits than virtual tags distinguishing them from one another, a virtually hinted cache suffers more conflict misses than a virtually tagged cache.\n\nPerhaps the ultimate reduction of virtual hints can be found in the Pentium&nbsp;4 (Willamette and Northwood cores). In these processors the virtual hint is effectively two bits, and the cache is four-way set associative. Effectively, the hardware maintains a simple permutation from virtual address to cache index, so that no [[content-addressable memory]] (CAM) is necessary to select the right one of the four ways fetched.\n\n===Page coloring===\n{{Main article|Cache coloring}}\n\nLarge physically indexed caches (usually secondary caches) run into a problem: the operating system rather than the application controls which pages collide with one another in the cache. Differences in page allocation from one program run to the next lead to differences in the cache collision patterns, which can lead to very large differences in program performance. These differences can make it very difficult to get a consistent and repeatable timing for a benchmark run.\n\nTo understand the problem, consider a CPU with a 1&nbsp;MiB physically indexed direct-mapped level-2 cache and 4&nbsp;KiB virtual memory pages. Sequential physical pages map to sequential locations in the cache until after 256 pages the pattern wraps around. We can label each physical page with a color of 0\u2013255 to denote where in the cache it can go. Locations within physical pages with different colors cannot conflict in the cache.\n\nProgrammers attempting to make maximum use of the cache may arrange their programs' access patterns so that only 1&nbsp;MiB of data need be cached at any given time, thus avoiding capacity misses. But they should also ensure that the access patterns do not have conflict misses. One way to think about this problem is to divide up the virtual pages the program uses and assign them virtual colors in the same way as physical colors were assigned to physical pages before. Programmers can then arrange the access patterns of their code so that no two pages with the same virtual color are in use at the same time. There is a wide literature on such optimizations (e.g. [[loop nest optimization]]), largely coming from the [[High Performance Computing|High Performance Computing (HPC)]] community.\n\nThe snag is that while all the pages in use at any given moment may have different virtual colors, some may have the same physical colors. In fact, if the operating system assigns physical pages to virtual pages randomly and uniformly, it is extremely likely that some pages will have the same physical color, and then locations from those pages will collide in the cache (this is the [[birthday paradox]]).\n\nThe solution is to have the operating system attempt to assign different physical color pages to different virtual colors, a technique called ''page coloring''. Although the actual mapping from virtual to physical color is irrelevant to system performance, odd mappings are difficult to keep track of and have little benefit, so most approaches to page coloring simply try to keep physical and virtual page colors the same.\n\nIf the operating system can guarantee that each physical page maps to only one virtual color, then there are no virtual aliases, and the processor can use virtually indexed caches with no need for extra virtual alias probes during miss handling. Alternatively, the OS can flush a page from the cache whenever it changes from one virtual color to another. As mentioned above, this approach was used for some early SPARC and RS/6000 designs.\n\n=={{Anchor|Cache hierarchy}}Cache hierarchy in a modern processor==\n[[File:Hwloc.png|thumb|right|300px|Memory hierarchy of an AMD Bulldozer server]]\n\nModern processors have multiple interacting on-chip caches.  The operation of a particular cache can be completely specified by the cache size, the cache block size, the number of blocks in a set, the cache set replacement policy, and the cache write policy (write-through or write-back).<ref name=\"ccs.neu.edu\" />\n\nWhile all of the cache blocks in a particular cache are the same size and have the same associativity, typically the \"lower-level\" caches (called Level 1 cache) have a smaller number of blocks, smaller block size, and fewer blocks in a set, but have very short access times. \"Higher-level\" caches (i.e. Level 2 and above) have progressively larger numbers of blocks, larger block size, more blocks in a set, and relatively longer access times, but are still much faster than main memory.\n\nCache entry replacement policy is determined by a [[cache algorithm]] selected to be implemented by the processor designers.  In some cases, multiple algorithms are provided for different kinds of work loads.\n\n===Specialized caches===\n\nPipelined CPUs access memory from multiple points in the [[Instruction pipeline|pipeline]]: instruction fetch, [[virtual memory|virtual-to-physical]] address translation, and data fetch (see [[classic RISC pipeline]]). The natural design is to use different physical caches for each of these points, so that no one physical resource has to be scheduled to service two points in the pipeline. Thus the pipeline naturally ends up with at least three separate caches (instruction, [[translation lookaside buffer|TLB]], and data), each specialized to its particular role.\n\n====Victim cache====\n{{ main | victim cache }}\nA '''victim cache''' is a cache used to hold blocks evicted from a CPU cache upon replacement. The victim cache lies between the main cache and its refill path, and holds only those blocks of data that were evicted from the main cache. The victim cache is usually fully associative, and is intended to reduce the number of conflict misses. Many commonly used programs do not require an associative mapping for all the accesses. In fact, only a small fraction of the memory accesses of the program require high associativity. The victim cache exploits this property by providing high associativity to only these accesses. It was introduced by [[Norman Jouppi]] from DEC in 1990.<ref>N.P.Jouppi. \"Improving direct-mapped cache performance by the addition of a small {{Sic|hide=y|fully|-}}associative cache and prefetch buffers.\" - 17th Annual International Symposium on Computer Architecture, 1990. Proceedings., {{doi|10.1109/ISCA.1990.134547}}</ref>\n\nIntel's ''[[Crystalwell]]''<ref name=\"intel-ark-crystal-well\">{{cite web\n | url = http://ark.intel.com/products/codename/51802/Crystal-Well\n | title = Products (Formerly Crystal Well)\n | publisher = [[Intel]]\n | accessdate = 2013-09-15\n}}</ref> variant of its [[Haswell (microarchitecture)|Haswell]] processors introduced an on-package 128&nbsp;MB [[eDRAM]] Level 4 cache which serves as a victim cache to the processors' Level 3 cache.<ref name=\"anandtech-i74950hq\">{{cite web\n | url = http://www.anandtech.com/show/6993/intel-iris-pro-5200-graphics-review-core-i74950hq-tested/3\n | title =  Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested\n | publisher = [[AnandTech]]\n | accessdate = 2013-09-16\n}}</ref> In the [[Skylake (microarchitecture)|Skylake]] microarchitecture the Level 4 cache no longer works as a victim cache.<ref>{{cite web\n | url = http://www.anandtech.com/show/9582/intel-skylake-mobile-desktop-launch-architecture-analysis/5\n | title = The Intel Skylake Mobile and Desktop Launch, with Architecture Analysis\n | author = Ian Cutress\n | date = September 2, 2015\n | publisher = AnandTech\n}}</ref>\n\n===={{Anchor|TRACE-CACHE}}Trace cache====\n{{Main article|Trace Cache}}\n\nOne of the more extreme examples of cache specialization is the '''trace cache''' (also known as ''execution trace cache'') found in the [[Intel]] [[Pentium&nbsp;4]] microprocessors.  A trace cache is a mechanism for increasing the instruction fetch bandwidth and decreasing power consumption (in the case of the Pentium&nbsp;4) by storing traces of [[instruction (computer science)|instruction]]s that have already been fetched and decoded.<ref>{{cite web\n | url = http://www.anandtech.com/show/661/5\n | title = The Pentium 4's Cache \u2013 Intel Pentium&nbsp;4 1.4&nbsp;GHz & 1.5&nbsp;GHz\n | date = 2000-11-20 | accessdate = 2015-11-30\n | author = Anand Lal Shimpi | publisher = [[AnandTech]]\n}}</ref>\n\nA trace cache stores instructions either after they have been decoded, or as they are retired. Generally, instructions are added to trace caches in groups representing either individual [[basic block]]s or dynamic instruction traces. The Pentium&nbsp;4's trace cache stores [[micro-operations]] resulting from decoding x86 instructions, providing also the functionality of a micro-operation cache.  Having this, the next time an instruction is needed, it does not have to be decoded into micro-ops again.<ref name=\"agner.org\" />{{rp|63&ndash;68}}\n\n==== Write Coalescing Cache (WCC) ====\nWrite Coalescing Cache<ref>{{cite web|url=http://www.realworldtech.com/bulldozer/9/|title=AMD's Bulldozer Microarchitecture - Memory Subsystem Continued|date=August 26, 2010|author=David Kanter|website=Real World Technologies}}</ref> is a special cache that is part of L2 cache in [[AMD]]'s [[Bulldozer (microarchitecture)|Bulldozer microarchitecture]]. Stores from both L1D caches in the module go through the WCC, where they are buffered and coalesced.\nThe WCC's task is reducing number of writes to the L2 cache.\n\n===={{Anchor|UOP-CACHE}}Micro-operation (\u03bcop or uop) cache====\nA '''micro-operation cache''' ('''\u03bcop cache''', '''uop cache''' or '''UC''')<ref>{{cite web|url=http://www.realworldtech.com/sandy-bridge/4/|title=Intel's Sandy Bridge Microarchitecture - Instruction Decode and uop Cache|date=September 25, 2010|author=David Kanter|website=Real World Technologies}}</ref> is a specialized cache that stores [[micro-operation]]s of decoded instructions, as received directly from the [[instruction decoder]]s or from the instruction cache.  When an instruction needs to be decoded, the \u03bcop cache is checked for its decoded form which is re-used if cached; if it is not available, the instruction is decoded and then cached.\n\nOne of the early works describing \u03bcop cache as an alternative frontend for the Intel [[P6 (microarchitecture)|P6 processor family]] is the 2001 paper ''\"Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA\"''.<ref name=\"uop-intel\">{{cite book\n | chapter-url = http://cecs.uci.edu/~papers/compendium94-03/papers/2001/islped01/pdffiles/p004.pdf\n | chapter = Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA\n | author1 = Baruch Solomon | title = ISLPED'01: Proceedings of the 2001 International Symposium on Low Power Electronics and Design (IEEE Cat. No.01TH8581)\n | pages = 4\u20139\n | author2 = Avi Mendelson | author3 = Doron Orenstein | author4 = Yoav Almog | author5 = Ronny Ronen\n | date = August 2001 | accessdate = 2013-10-06\n | publisher = [[Intel]] | doi = 10.1109/LPE.2001.945363\n| isbn = 978-1-58113-371-4\n }}</ref>  Later, Intel included \u03bcop caches in its [[Sandy Bridge]] processors and in successive microarchitectures like [[Ivy Bridge (microarchitecture)|Ivy Bridge]] and [[Haswell (microarchitecture)|Haswell]].<ref name=\"agner.org\">{{cite web\n | url = http://www.agner.org/optimize/microarchitecture.pdf\n | title = The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers\n | date = 2014-02-19 | accessdate = 2014-03-21\n | author = Agner Fog | website = agner.org\n }}</ref>{{rp|121&ndash;123}}<ref name=\"anandtech-haswell\">{{cite web\n | url = http://www.anandtech.com/show/6355/intels-haswell-architecture/6\n | title = Intel's Haswell Architecture Analyzed\n | date = 2012-10-05 | accessdate = 2013-10-20\n | author = Anand Lal Shimpi | publisher = [[AnandTech]]\n}}</ref> AMD implemented a \u03bcop cache in their [[Zen (microarchitecture)|Zen microarchitecture]].<ref>{{cite web\n | url = http://www.anandtech.com/show/10578/amd-zen-microarchitecture-dual-schedulers-micro-op-cache-memory-hierarchy-revealed\n | title = AMD Zen Microarchitecture: Dual Schedulers, Micro-Op Cache and Memory Hierarchy Revealed\n | date = 2016-08-18 | access-date = 2017-04-03\n | author = Ian Cutress | publisher = AnandTech}}</ref>\n\nFetching complete pre-decoded instructions eliminates the need to repeatedly decode variable length complex instructions into simpler fixed-length micro-operations, and simplifies the process of predicting, fetching, rotating and aligning fetched instructions. A \u03bcop cache effectively offloads the fetch and decode hardware, thus decreasing [[power consumption]] and improving the frontend supply of decoded micro-operations.  The \u03bcop cache also increases performance by more consistently delivering decoded micro-operations to the backend and eliminating various bottlenecks in the CPU's fetch and decode logic.<ref name=\"uop-intel\" /><ref name=\"anandtech-haswell\" />\n\nA \u03bcop cache has many similarities with a trace cache, although a \u03bcop cache is much simpler thus providing better power efficiency; this makes it better suited for implementations on battery-powered devices. The main disadvantage of the trace cache, leading to its power inefficiency, is the hardware complexity required for its [[heuristic]] deciding on caching and reusing dynamically created instruction traces.<ref name=\"tc-slides\">{{cite web|url=https://www.cs.cmu.edu/afs/cs/academic/class/15740-f03/www/lectures/TraceCache_slides.pdf|title=Trace Cache|date=October 2003|accessdate=2013-10-06|author1=Leon Gu|author2=Dipti Motiani}}</ref>\n\n==== Branch target cache ====\nA '''branch target cache''' or '''branch target instruction cache''', the name used on ARM microprocessors,<ref>{{cite web |title=How does the BTIC (branch target instruction cache) work? |url=https://community.arm.com/processors/f/discussions/5320/how-does-the-btic-branch-target-instruction-cache-works |date=28 May 2015 |author=Kun Niu |accessdate=7 April 2018}}</ref> is a specialized cache which holds the first few instructions at the destination of a taken branch.  This is used by low-powered processors which do not need a normal instruction cache because the memory system is capable of delivering instructions fast enough to satisfy the CPU without one.  However, this only applies to consecutive instructions in sequence; it still takes several cycles of latency to restart instruction fetch at a new address, causing a few cycles of pipeline bubble after a control transfer.  A branch target cache provides instructions for those few cycles avoiding a delay after most taken branches.\n\nThis allows full-speed operation with a much smaller cache than a traditional full-time instruction cache.\n\n==== Smart cache ====\n'''Smart cache''' is a [[L2 cache|level 2]] or [[L3 cache|level 3]] [[CPU cache|caching]] method for multiple execution cores, developed by [[Intel]].\n\nSmart Cache shares the actual cache memory between the cores of a [[multi-core processor]]. In comparison to a dedicated per-core cache, the overall [[cache miss]] rate decreases when not all cores need equal parts of the cache space. Consequently, a single core can use the full level 2 or level 3 cache, if the other cores are inactive.<ref>{{cite web|url=http://www.intel.com/content/www/us/en/architecture-and-technology/intel-smart-cache.html|title=Intel Smart Cache: Demo|publisher=[[Intel]]|accessdate=2012-01-26}}</ref> Furthermore, the shared cache makes it faster to share memory among different execution cores.<ref>{{cite web |url=http://software.intel.com/file/18374/ |archiveurl=https://web.archive.org/web/20111229193036/http://software.intel.com/file/18374/ |title=Inside Intel Core Microarchitecture and Smart Memory Access |format=PDF |page=5 |publisher=[[Intel]] |year=2006 |accessdate=2012-01-26 |archivedate=2011-12-29}}</ref>\n\n==={{Anchor|MULTILEVEL}}Multi-level caches===\n{{See also|Cache hierarchy}}\nAnother issue is the fundamental tradeoff between cache latency and hit rate. Larger caches have better hit rates but longer latency. To address this tradeoff, many computers use multiple levels of cache, with small fast caches backed up by larger, slower caches.  Multi-level caches generally operate by checking the fastest, ''level 1'' ('''L1''') cache first; if it hits, the processor proceeds at high speed. If that smaller cache misses, the next fastest cache (''level 2'', '''L2''') is checked, and so on, before accessing external memory.\n\nAs the latency difference between main memory and the fastest cache has become larger, some processors have begun to utilize as many as three levels of on-chip cache.  Price-sensitive designs used this to pull the entire cache hierarchy on-chip, but by the 2010s some of the highest-performance designs returned to having large off-chip caches, which is often implemented in [[eDRAM]] and mounted on a [[multi-chip module]], as a fourth cache level. In rare cases, as in latest IBM mainframe CPU, [[IBM z15 (microprocessor)|IBM z15]] from 2019, all levels down to L1 are implemented by eDRAM, replacing [[static random-access memory|SRAM]] entirely (for caches, i.g. it's still used for registers) for 128&nbsp;KiB L1 for instructions and for data, or combined 256&nbsp;KiB.\n\nThe benefits of L3 and L4 caches depend on the application's access patterns.  Examples of products incorporating L3 and L4 caches include the following:\n\n* [[Alpha 21164]] (1995) has 1 to 64&nbsp;MB off-chip L3 cache.\n* IBM [[POWER4]] (2001) has off-chip L3 caches of 32&nbsp;MB per processor, shared among several processors.\n* [[Itanium 2]] (2003) has a 6&nbsp;MB [[unified cache|unified]] level 3 (L3) cache on-die; the [[Itanium 2]] (2003) MX&nbsp;2 module incorporates two Itanium&nbsp;2 processors along with a shared 64&nbsp;MB L4 cache on a [[multi-chip module]] that was pin compatible with a Madison processor.\n* Intel's [[Xeon]] MP product codenamed \"Tulsa\" (2006) features 16&nbsp;MB of on-die L3 cache shared between two processor cores.\n* AMD [[Phenom&nbsp;II]] (2008) has up to 6&nbsp;MB on-die unified L3 cache.\n* [[Intel Core i7]] (2008) has an 8&nbsp;MB on-die unified L3 cache that is inclusive, shared by all cores.\n* Intel [[Haswell (microarchitecture)|Haswell]] CPUs with integrated [[Intel Iris Pro Graphics]] have 128&nbsp;MB of eDRAM acting essentially as an L4 cache.<ref>{{cite web|url=http://www.anandtech.com/show/6993/intel-iris-pro-5200-graphics-review-core-i74950hq-tested/3 |title=Intel Iris Pro 5200 Graphics Review: Core i7-4950HQ Tested |publisher=AnandTech |accessdate=2014-02-25}}</ref>\n\nFinally, at the other end of the memory hierarchy, the CPU [[register file]] itself can be considered the smallest, fastest cache in the system, with the special characteristic that it is scheduled in software\u2014typically by a compiler, as it allocates registers to hold values retrieved from main memory for, as an example, [[loop nest optimization]]. However, with [[register renaming]] most compiler register assignments are reallocated dynamically by hardware at runtime into a register bank, allowing the CPU to break false data dependencies and thus easing pipeline hazards.\n\nRegister files sometimes also have hierarchy: The [[Cray-1]] (circa 1976) had eight address \"A\" and eight scalar data \"S\" registers that were generally usable. There was also a set of 64 address \"B\" and 64 scalar data \"T\" registers that took longer to access, but were faster than main memory. The \"B\" and \"T\" registers were provided because the Cray-1 did not have a data cache. (The Cray-1 did, however, have an instruction cache.)\n\n===={{Anchor|LLC}}Multi-core chips====\nWhen considering a chip with [[Multi-core processor|multiple cores]], there is a question of whether the caches should be shared or local to each core. Implementing shared cache inevitably introduces more wiring and complexity. But then, having one cache per ''chip'', rather than ''core'', greatly reduces the amount of space needed, and thus one can include a larger cache.\n\nTypically, sharing the L1 cache is undesirable because the resulting increase in latency would make each core run considerably slower than a single-core chip.  However, for the highest-level cache, the last one called before accessing memory, having a global cache is desirable for several reasons, such as allowing a single core to use the whole cache, reducing data redundancy by making it possible for different processes or threads to share cached data, and reducing the complexity of utilized cache coherency protocols.<ref>{{cite web\n | url = https://software.intel.com/en-us/articles/software-techniques-for-shared-cache-multi-core-systems\n | title = Software Techniques for Shared-Cache Multi-Core Systems\n | date = 2012-03-08 | accessdate = 2015-11-24\n | author1 = Tian Tian | author2 = Chiu-Pi Shih\n | publisher = [[Intel]]\n}}</ref>  For example, an eight-core chip with three levels may include an L1 cache for each core, one intermediate L2 cache for each pair of cores, and one L3 cache shared between all cores.\n\nShared highest-level cache, which is called before accessing memory, is usually referred to as the ''last level cache'' (LLC).  Additional techniques are used for increasing the level of parallelism when LLC is shared between multiple cores, including slicing it into multiple pieces which are addressing certain ranges of memory addresses, and can be accessed independently.<ref>{{cite web\n | url = http://www.hotchips.org/wp-content/uploads/hc_archives/hc23/HC23.19.9-Desktop-CPUs/HC23.19.911-Sandy-Bridge-Lempel-Intel-Rev%207.pdf\n | title = 2nd Generation Intel Core Processor Family: Intel Core i7, i5 and i3\n | date = 2013-07-28 | accessdate = 2014-01-21\n | author = Oded Lempel | website = hotchips.org\n | page = 7&ndash;10,31&ndash;45\n}}</ref>\n\n====Separate versus unified====\nIn a separate cache structure, instructions and data are cached separately, meaning that a cache line is used to cache either instructions or data, but not both; various benefits have been demonstrated with separate data and instruction [[translation lookaside buffer]]s.<ref>{{cite journal |author1=Chen, J. Bradley |author2=Borg, Anita |author3=Jouppi, Norman P. |title=A Simulation Based Study of TLB Performance |journal=SIGARCH Computer Architecture News |volume=20 |issue=2 |year=1992 |page=114&ndash;123 |doi=10.1145/146628.139708}}</ref>  In a unified structure, this constraint is not present, and cache lines can be used to cache both instructions and data.\n\n===={{Anchor|INCLUSIVE|EXCLUSIVE}}Exclusive versus inclusive====\nMulti-level caches introduce new design decisions.  For instance, in some processors, all data in the L1 cache must also be somewhere in the L2 cache.  These caches are called ''strictly inclusive''. Other processors (like the [[AMD Athlon]]) have ''exclusive'' caches: data is guaranteed to be in at most one of the L1 and L2 caches, never in both.  Still other processors (like the Intel [[Pentium II]], [[Pentium III|III]], and [[Pentium 4|4]]) do not require that data in the L1 cache also reside in the L2 cache, although it may often do so. There is no universally accepted name for this intermediate policy;<ref>{{cite web\n | url = http://www.amecomputers.com/explanation-of-the-l1-and-l2-cache.html\n | title = Explanation of the L1 and L2 Cache \n | accessdate = 2014-06-09\n | website = amecomputers.com\n}}</ref><ref name=\"ispass04\">{{cite web\n | url = http://mercury.pr.erau.edu/~davisb22/papers/ispass04.pdf\n | title = Performance Evaluation of Exclusive Cache Hierarchies\n | date = 2004-06-25 | accessdate = 2014-06-09\n | author1 = Ying Zheng | author2 = Brian T. Davis | author3 = Matthew Jordan\n | publisher = Michigan Technological University\n }}</ref>\ntwo common names are \"non-exclusive\" and \"partially-inclusive\".\n\nThe advantage of exclusive caches is that they store more data. This advantage is larger when the exclusive L1 cache is comparable to the L2 cache, and diminishes if the L2 cache is many times larger than the L1 cache. When the L1 misses and the L2 hits on an access, the hitting cache line in the L2 is exchanged with a line in the L1. This exchange is quite a bit more work than just copying a line from L2 to L1, which is what an inclusive cache does.<ref name=\"ispass04\" />\n\nOne advantage of strictly inclusive caches is that when external devices or other processors in a multiprocessor system wish to remove a cache line from the processor, they need only have the processor check the L2 cache. In cache hierarchies which do not enforce inclusion, the L1 cache must be checked as well. As a drawback, there is a correlation between the associativities of L1 and L2 caches: if the L2 cache does not have at least as many ways as all L1 caches together, the effective associativity of the L1 caches is restricted. Another disadvantage of inclusive cache is that whenever there is an eviction in L2 cache, the (possibly) corresponding lines in L1 also have to get evicted in order to maintain inclusiveness. This is quite a bit of work, and would result in a higher L1 miss rate.<ref name=\"ispass04\" />\n\nAnother advantage of inclusive caches is that the larger cache can use larger cache lines, which reduces the size of the secondary cache tags. (Exclusive caches require both caches to have the same size cache lines, so that cache lines can be swapped on a L1 miss, L2 hit.) If the secondary cache is an order of magnitude larger than the primary, and the cache data is an order of magnitude larger than the cache tags, this tag area saved can be comparable to the incremental area needed to store the L1 cache data in the L2.<ref>{{cite web\n | url = http://www.jaleels.org/ajaleel/publications/micro2010-tla.pdf\n | title = Achieving Non-Inclusive Cache Performance with Inclusive Caches\n | date = 2010-09-27 | accessdate = 2014-06-09\n | author1 = Aamer Jaleel | author2 = Eric Borch | author3 = Malini Bhandaru\n | author4 = Simon C. Steely Jr. | author5 = Joel Emer\n | website = jaleels.org }}</ref>\n\n===Example: the K8===\nTo illustrate both specialization and multi-level caching, here is the cache hierarchy of the K8 core in the AMD [[Athlon 64]] CPU.<ref>{{cite web|url=http://www.sandpile.org/impl/k8.htm |title=AMD K8 |accessdate=2007-06-02 |website=Sandpile.org |url-status=dead |archiveurl=https://web.archive.org/web/20070515052223/http://www.sandpile.org/impl/k8.htm |archivedate=2007-05-15 }}</ref>\n\n[[File:Cache,hierarchy-example.svg|thumb|center|500px|Cache hierarchy of the K8 core in the AMD Athlon 64 CPU.]]\n\nThe K8 has four specialized caches: an instruction cache, an instruction [[translation lookaside buffer|TLB]], a data TLB, and a data cache. Each of these caches is specialized:\n\n* The instruction cache keeps copies of 64-byte lines of memory, and fetches 16 bytes each cycle. Each byte in this cache is stored in ten bits rather than eight, with the extra bits marking the boundaries of instructions (this is an example of predecoding). The cache has only [[parity bit|parity]] protection rather than [[Error-correcting code|ECC]], because parity is smaller and any damaged data can be replaced by fresh data fetched from memory (which always has an up-to-date copy of instructions).\n* The instruction TLB keeps copies of page table entries (PTEs). Each cycle's instruction fetch has its virtual address translated through this TLB into a physical address. Each entry is either four or eight bytes in memory. Because the K8 has a variable page size, each of the TLBs is split into two sections, one to keep PTEs that map 4&nbsp;KB pages, and one to keep PTEs that map 4&nbsp;MB or 2&nbsp;MB pages. The split allows the fully associative match circuitry in each section to be simpler. The operating system maps different sections of the virtual address space with different size PTEs.\n* The data TLB has two copies which keep identical entries. The two copies allow two data accesses per cycle to translate virtual addresses to physical addresses. Like the instruction TLB, this TLB is split into two kinds of entries.\n* The data cache keeps copies of 64-byte lines of memory. It is split into 8 banks (each storing 8&nbsp;KB of data), and can fetch two 8-byte data each cycle so long as those data are in different banks. There are two copies of the tags, because each 64-byte line is spread among all eight banks. Each tag copy handles one of the two accesses per cycle.\n\nThe K8 also has multiple-level caches. There are second-level instruction and data TLBs, which store only PTEs mapping 4&nbsp;KB. Both instruction and data caches, and the various TLBs, can fill from the large '''unified''' L2 cache. This cache is exclusive to both the L1 instruction and data caches, which means that any 8-byte line can only be in one of the L1 instruction cache, the L1 data cache, or the L2 cache. It is, however, possible for a line in the data cache to have a PTE which is also in one of the TLBs\u2014the operating system is responsible for keeping the TLBs coherent by flushing portions of them when the page tables in memory are updated.\n\nThe K8 also caches information that is never stored in memory\u2014prediction information. These caches are not shown in the above diagram. As is usual for this class of CPU, the K8 has fairly complex\n[[branch prediction]], with tables that help predict whether branches are taken and other tables which predict the targets of branches and jumps. Some of this information is associated with instructions, in both the level 1 instruction cache and the unified secondary cache.\n\nThe K8 uses an interesting trick to store prediction information with instructions in the secondary cache. Lines in the secondary cache are protected from accidental data corruption (e.g. by an [[alpha particle]] strike) by either [[Error-correcting code|ECC]] or [[parity (telecommunication)|parity]], depending on whether those lines were evicted from the data or instruction primary caches. Since the parity code takes fewer bits than the ECC code, lines from the instruction cache have a few spare bits. These bits are used to cache branch prediction information associated with those instructions. The net result is that the branch predictor has a larger effective history table, and so has better accuracy.\n\n===More hierarchies===\n<!-- (This section should be rewritten.) -->\nOther processors have other kinds of predictors (e.g., the store-to-load bypass predictor in the [[Digital Equipment Corporation|DEC]] [[Alpha 21264]]), and various specialized predictors are likely to flourish in future processors.\n\nThese predictors are caches in that they store information that is costly to compute. Some of the terminology used when discussing predictors is the same as that for caches (one speaks of a '''hit''' in a branch predictor), but predictors are not generally thought of as part of the cache hierarchy.\n\nThe K8 keeps the instruction and data caches '''[[cache coherency|coherent]]''' in hardware, which means that a store into an instruction closely following the store instruction will change that following instruction. Other processors, like those in the Alpha and MIPS family, have relied on software to keep the instruction cache coherent. Stores are not guaranteed to show up in the instruction stream until a program calls an operating system facility to ensure coherency.\n\n===Tag RAM===\nIn computer engineering, a ''tag RAM'' is used to specify which of the possible memory locations is currently stored in a CPU cache.<ref>{{cite web\n | url = http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0363g/Chdijaed.html\n | title = Cortex-R4 and Cortex-R4F Technical Reference Manual\n | accessdate = 2013-09-28\n | publisher = arm.com\n}}</ref><ref>{{cite web\n | url = http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0284g/Ebddefci.html\n | title = L210 Cache Controller Technical Reference Manual\n | accessdate = 2013-09-28\n | publisher = arm.com\n}}</ref>  For a simple, direct-mapped design fast [[static random-access memory|SRAM]] can be used.   Higher [[CPU cache#Associativity|associative caches]] usually employ [[content-addressable memory]].\n\n==Implementation==\n{{Main article|Cache algorithms}}\n\nCache '''reads''' are the most common CPU operation that takes more than a single cycle. Program execution time tends to be very sensitive to the latency of a level-1 data cache hit. A great deal of design effort, and often power and silicon area are expended making the caches as fast as possible.\n\nThe simplest cache is a virtually indexed direct-mapped cache. The virtual address is calculated with an adder, the relevant portion of the address extracted and used to index an SRAM, which returns the loaded data. The data is byte aligned in a byte shifter, and from there is bypassed to the next operation. There is no need for any tag checking in the inner loop{{snd}} in fact, the tags need not even be read. Later in the pipeline, but before the load instruction is retired, the tag for the loaded data must be read, and checked against the virtual address to make sure there was a cache hit. On a miss, the cache is updated with the requested cache line and the pipeline is restarted.\n\nAn associative cache is more complicated, because some form of tag must be read to determine which entry of the cache to select. An N-way set-associative level-1 cache usually reads all N possible tags and N data in parallel, and then chooses the data associated with the matching tag. Level-2 caches sometimes save power by reading the tags first, so that only one data element is read from the data SRAM.\n\n[[Image:Cache,associative-read.svg|thumb|right|upright=1.8|Read path for a 2-way associative cache]]\n\nThe adjacent diagram is intended to clarify the manner in which the various fields of the address are used. Address bit 31 is most significant, bit 0 is least significant. The diagram shows the SRAMs, indexing, and [[multiplexing]] for a 4&nbsp;KB, 2-way set-associative, virtually indexed and virtually tagged cache with 64&nbsp;byte (B) lines, a 32-bit read width and 32-bit virtual address.\n\nBecause the cache is 4&nbsp;KB and has 64&nbsp;B lines, there are just 64 lines in the cache, and we read two at a time from a Tag SRAM which has 32 rows, each with a pair of 21 bit tags. Although any function of virtual address bits 31 through 6 could be used to index the tag and data SRAMs, it is simplest to use the least significant bits.\n\nSimilarly, because the cache is 4&nbsp;KB and has a 4&nbsp;B read path, and reads two ways for each access, the Data SRAM is 512 rows by 8 bytes wide.\n\nA more modern cache might be 16&nbsp;KB, 4-way set-associative, virtually indexed, virtually hinted, and physically tagged, with 32&nbsp;B lines, 32-bit read width and 36-bit physical addresses. The read path recurrence for such a cache looks very similar to the path above. Instead of tags, vhints are read, and matched against a subset of the virtual address. Later on in the pipeline, the virtual address is translated into a physical address by the TLB, and the physical tag is read (just one, as the vhint supplies which way of the cache to read). Finally the physical address is compared to the physical tag to determine if a hit has occurred.\n\nSome SPARC designs have improved the speed of their L1 caches by a few gate delays by collapsing the virtual address adder into the SRAM decoders. See [[Sum addressed decoder]].\n\n===History===\nThe early history of cache technology is closely tied to the invention and use of virtual memory.{{Citation needed|date=March 2008}} <!-- this *is* a truly interesting observation, but are there any sources? Also why was the \"CPU-mam speed\" part deleted? ~~~~ --> Because of scarcity and cost of semi-conductor memories, early mainframe computers in the 1960s used a complex hierarchy of physical memory, mapped onto a flat virtual memory space used by programs. The memory technologies would span semi-conductor, magnetic core, drum and disc. Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access. Extensive studies were done to optimize the cache sizes. Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest cache sizes.{{Disputed inline|Talk:CPU cache#Dispute sequence of events for paging|reason=sequence of events wrong|date=December 2010}}\n\nIn the early days of microcomputer technology, memory access was only slightly slower than [[processor register|register]] access. But since the 1980s<ref>{{cite journal| url=https://epic.hpi.uni-potsdam.de/pub/Home/TrendsAndConceptsII2010/HW_Trends_The_Processor-Memory_bottleneck___Problems_and_Solutions..pdf | title=The processor-memory bottleneck: problems and solutions | journal=Crossroads | volume=5 | issue=3es | pages=2\u2013es | first1=Nihar R. | last1=Mahapatra |first2=Balakrishna | last2=Venkatrao | accessdate=2013-03-05 | doi=10.1145/357783.331677 | year=1999 }}</ref> the performance gap between processor and memory has been growing. Microprocessors have advanced much faster than memory, especially in terms of their operating [[frequency]], so memory became a performance [[Von Neumann architecture#Von Neumann bottleneck|bottleneck]]. While it was technically possible to have all the main memory as fast as the CPU, a more economically viable path has been taken: use plenty of low-speed memory, but also introduce a small high-speed cache memory to alleviate the performance gap. This provided an order of magnitude more capacity\u2014for the same price\u2014with only a slightly reduced combined performance.\n\n====First TLB implementations====\nThe first documented uses of a TLB were on the [[General Electric|GE]] [[GE 645|645]]<ref>{{cite book\n |    author = GE\n |     title = GE-645 System Manual\n |      date = January 1968\n |       url = http://bitsavers.org/pdf/ge/GE-645/GE-645_SystemMan_Jan68.pdf}}\n</ref> and the [[IBM]] [[IBM System/360 Model 67|360/67]],<ref>\n{{cite book\n |    author = IBM\n |     title = IBM System/360 Model 67 Functional Characteristics\n |         id = GA27-2719-2\n |        url = http://www.bitsavers.org/pdf/ibm/360/funcChar/GA27-2719-2_360-67_funcChar.pdf\n |   version = Third Edition\n |      date = February 1972}}</ref> both of which used an associative memory as a TLB.\n\n====First data cache====\nThe first documented use of a data cache was on the [[IBM]] System/360 Model 85.<ref>{{cite book\n |     author = IBM\n |      title = IBM System/360 Model 85 Functional Characteristics\n |         id = A22-6916-1\n |        url = http://www.bitsavers.org/pdf/ibm/360/funcChar/A22-6916-1_360-85_funcChar_Jun68.pdf\n |    version = SECOND EDITION\n |       date = June 1968}}</ref>\n\n====In 68k microprocessors====\nThe [[68010]], released in 1982, has a \"loop mode\" which can be considered a tiny and special-case instruction cache that accelerates loops that consist of only two instructions. The [[68020]], released in 1984, replaced that with a typical instruction cache of 256 bytes, being the first 68k series processor to feature true on-chip cache memory.\n\nThe [[68030]], released in 1987, is basically a 68020 core with an additional 256-byte data cache, a process shrink, and added burst mode for the caches. The [[Motorola 68040|68040]], released in 1990, has split instruction and data caches of four kilobytes each. The [[68060]], released in 1994, has the following: 8&nbsp;KB data cache (four-way associative), 8&nbsp;KB instruction cache (four-way associative), 96-byte FIFO instruction buffer, 256-entry branch cache, and 64-entry address translation cache MMU buffer (four-way associative).\n\n====In x86 microprocessors====\nAs the [[x86]] microprocessors reached clock rates of 20&nbsp;MHz and above in the [[Intel 80386|386]], small amounts of fast cache memory began to be featured in systems to improve performance. This was because the [[DRAM]] used for main memory had significant latency, up to 120&nbsp;ns, as well as refresh cycles. The cache was constructed from more expensive, but significantly faster, [[Static random access memory|SRAM]] [[Memory cell (computing)|memory cells]], which at the time had latencies around 10&nbsp;ns - 25&nbsp;ns. The early caches were external to the processor and typically located on the motherboard in the form of eight or nine [[Dual in-line package|DIP]] devices placed in sockets to enable the cache as an optional extra or upgrade feature.\n\nSome versions of the Intel 386 processor could support 16 to 256&nbsp;KB of external cache.\n\nWith the [[Intel 80486|486]] processor, an 8&nbsp;KB cache was integrated directly into the CPU die. This cache was termed Level 1 or L1 cache to differentiate it from the slower on-motherboard, or Level 2 (L2) cache. These on-motherboard caches were much larger, with the most common size being 256&nbsp;KB. The popularity of on-motherboard cache continued through the [[Intel P5|Pentium MMX]] era but was made obsolete by the introduction of [[SDRAM]] and the growing disparity between bus clock rates and CPU clock rates, which caused on-motherboard cache to be only slightly faster than main memory.\n\nThe next development in cache implementation in the x86 microprocessors began with the [[Pentium Pro]], which brought the secondary cache onto the same package as the microprocessor, clocked at the same frequency as the microprocessor.\n\nOn-motherboard caches enjoyed prolonged popularity thanks to the [[AMD K6-2]] and [[AMD K6-III]] processors that still used [[Socket 7]], which was previously used by Intel with on-motherboard caches. K6-III included 256&nbsp;KB on-die L2 cache and took advantage of the on-board cache as a third level cache, named L3 (motherboards with up to 2&nbsp;MB of on-board cache were produced). After the Socket&nbsp;7 became obsolete, on-motherboard cache disappeared from the x86 systems.\n\nThe three-level caches were used again first with the introduction of multiple processor cores, where the L3 cache was added to the CPU die.  It became common for the total cache sizes to be increasingly larger in newer processor generations, and recently (as of 2011) it is not uncommon to find Level 3 cache sizes of tens of megabytes.<ref>{{cite web\n | url = http://ark.intel.com/products/family/59139/Intel-Xeon-Processor-E7-Family/server\n | title = Intel\u00ae Xeon\u00ae Processor E7 Family\n | accessdate = 2013-10-10\n | publisher = [[Intel]]\n}}</ref>\n\n[[Intel]] introduced a Level 4 on-package cache with the [[Haswell (microarchitecture)|Haswell]] [[microarchitecture]]. ''[[Crystalwell]]''<ref name=\"intel-ark-crystal-well\" /> Haswell CPUs, equipped with the [[GT3e]] variant of Intel's integrated Iris Pro graphics, effectively feature 128&nbsp;MB of embedded DRAM ([[eDRAM]]) on the same package.  This L4 cache is shared dynamically between the on-die GPU and CPU, and serves as a [[victim cache]] to the CPU's L3 cache.<ref name=\"anandtech-i74950hq\" />\n\n====Current research====\nEarly cache designs focused entirely on the direct cost of cache and [[RAM]] and average execution speed.\nMore recent cache designs also consider [[low-power electronics|energy efficiency]],<ref>{{cite journal|url=https://www.academia.edu/5010517|title=A Survey of Architectural Techniques For Improving Cache Power Efficiency|author=Sparsh Mittal|journal=Sustainable Computing: Informatics and Systems|volume=4|issue=1|pages=33\u201343|date=March 2014|doi=10.1016/j.suscom.2013.11.001}}</ref> fault tolerance, and other goals.<ref>{{cite journal|url=https://spectrum.ieee.org/computing/hardware/chip-design-thwarts-sneak-attack-on-data|title=Chip Design Thwarts Sneak Attack on Data|author=Sally Adee|year=2009}}</ref><ref>{{cite conference|url=http://palms.princeton.edu/system/files/Micro08_Newcache.pdf|title=A novel cache architecture with enhanced performance and security|author1=Zhenghong Wang|author2=Ruby B. Lee|conference=41st annual IEEE/ACM International Symposium on Microarchitecture|pages=83\u201393|date=November 8-12, 2008|archive-url=https://web.archive.org/web/20120306225926/http://palms.princeton.edu/system/files/Micro08_Newcache.pdf|archive-date=March 6, 2012}}</ref> Researchers have also explored use of emerging memory technologies such as [[eDRAM]] (embedded DRAM) and NVRAM (non-volatile RAM) for designing caches.<ref>{{cite journal|url=https://www.academia.edu/6988421|title=A Survey Of Architectural Approaches for Managing Embedded DRAM and Non-volatile On-chip Caches|author1=Sparsh Mittal|author2=Jeffrey S. Vetter|author3=Dong Li|journal=IEEE Transactions on Parallel and Distributed Systems|volume=26|issue=6|date=June 2015|pages=1524\u20131537|doi=10.1109/TPDS.2014.2324563}}</ref>\n\nThere are several tools available to computer architects to help explore tradeoffs between the cache cycle time, energy, and area; the CACTI cache simulator<ref>{{cite web|url=http://www.hpl.hp.com/research/cacti/ |title=CACTI |website=Hpl.hp.com |accessdate=2010-05-02}}</ref> and the SimpleScalar instruction set simulator are two open-source options. Modeling of 2D and 3D [[static random-access memory|SRAM]], [[eDRAM]], [[spin-transfer torque|STT-RAM]], [[resistive random-access memory|ReRAM]] and [[phase-change memory|PCM]] caches can be done using the DESTINY tool.<ref>{{cite web|url=https://code.ornl.gov/3d_cache_modeling_tool/destiny/blob/master/README|title=3d_cache_modeling_tool / destiny|website=code.ornl.gov|access-date=2015-02-26}}</ref>\n\n===Multi-ported cache===\nA multi-ported cache is a cache which can serve more than one request at a time. When accessing a traditional cache we normally use a single memory address, whereas in a multi-ported cache we may request N addresses at a time{{snd}} where N is the number of ports that connected through the processor and the cache. The benefit of this is that a pipelined processor may access memory from different phases in its pipeline. Another benefit is that it allows the concept of super-scalar processors through different cache levels.\n\n==See also==\n{{Div col|colwidth=30em}}\n* [[Cache (computing)]]\n* [[Cache algorithms]]\n* [[Cache coherency]]\n* [[Cache control instruction]]s\n* [[Cache hierarchy]]\n* [[Cache prefetching]]\n* [[Dinero (cache simulator)|Dinero]] (Cache simulator by [[University of Wisconsin System]])\n* [[Instruction unit]]\n* [[Locality of reference]]\n* [[Memoization]]\n* [[Memory hierarchy]]\n* [[Micro-operation]]\n* [[No-write allocation]]\n* [[Scratchpad RAM]]\n* [[Sum addressed decoder]]\n* [[Write buffer]]\n{{Div col end}}\n\n==Notes==\n{{Reflist|group=NB}}\n\n==References==\n{{Reflist|30em|refs=\n<ref name=AtlasSup>\n{{cite book\n |      last1 = Kilburn\n |     first1 = T.\n |      last2 = Payne\n |     first2 = R. B.\n |      last3 = Howarth\n |     first3 = D. J.\n |      title = Computers - Key to Total Systems Control\n |     series = Conferences Proceedings\n |      pages = 279\u2013294\n |    chapter = The Atlas Supervisor\n | chapterurl = http://www.chilton-computing.org.uk/acl/technology/atlas/p019.htm\n |     volume = 20 Proceedings of the Eastern Joint Computer Conference Washington, D.C.\n |date=December 1961\n |  publisher = Macmillan}}\n</ref>\n\n<ref name=AtlasCPU>\n{{cite book\n |      last1 = Sumner\n |     first1 = F. H.\n |      last2 = Haley\n |     first2 = G.\n |      last3 = Chenh\n |     first3 = E. C. Y.\n |      title = Information Processing 1962\n |     series = IFIP Congress Proceedings\n |    chapter = The Central Control Unit of the 'Atlas' Computer\n |     volume = Proceedings of IFIP Congress 62\n |       year = 1962\n |  publisher = Spartan}}\n</ref>\n}}\n\n==External links==\n{{wikibooks\n |1= Microprocessor Design\n |2= Cache\n}}\n* [https://lwn.net/Articles/252125/ Memory part 2: CPU caches]{{snd}} an article on lwn.net by Ulrich Drepper describing CPU caches in detail\n* [ftp://ftp.cs.wisc.edu/markhill/Papers/toc89_cpu_cache_associativity.pdf Evaluating Associativity in CPU Caches] \u2013 Hill and Smith (1989) \u2013 introduces capacity, conflict, and compulsory classification\n* [http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/ Cache Performance for SPEC CPU2000 Benchmarks] \u2013 Hill and Cantin (2003) \u2013 This reference paper has been updated several times. It has thorough and lucidly presented simulation results for a reasonably wide set of benchmarks and cache organizations.\n* [http://www.sun.com/blueprints/1102/817-0742.pdf Memory Hierarchy in Cache-Based Systems] \u2013 by Ruud van der Pas, 2002, Sun Microsystems \u2013 a nice introductory article to CPU memory caching\n* [http://www.freescale.com/files/32bit/doc/app_note/AN2663.pdf A Cache Primer] \u2013 by Paul Genua, P.E., 2004, Freescale Semiconductor, another introductory article\n* [https://web.archive.org/web/20110718154522/http://www.zipcores.com/skin1/zipdocs/datasheets/cache_8way_set.pdf An 8-way set-associative cache]{{snd}} written in [[VHDL]]\n* [https://arstechnica.com/old/content/2002/07/caching.ars Understanding CPU caching and performance]{{snd}} an article on Ars Technica by Jon Stokes\n* [http://ixbtlabs.com/articles/ibmpower4/ IBM POWER4 processor review]{{snd}} an article on ixbtlabs by Pavel Danilov\n* [https://www.cs.princeton.edu/courses/archive/fall15/cos375/lectures/16-Cache-2x2.pdf Memory Caching]{{snd}} a Princeton University lecture\n\n{{CPU technologies|state=collapsed}}\n\n{{DEFAULTSORT:Cpu Cache}}\n[[Category:Central processing unit]]\n[[Category:Computer memory]]\n[[Category:Cache (computing)]]\n", "name_user": "Comp.arch", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/CPU_cache"}
