{"title_page": "Boom (containment)", "text_new": "[[Image:Oil Spill Containment Boom.jpg|thumb|Oil spill containment boom holding back oil]]\n[[File:Two Indian Coast Guard vessels deploying an ocean boom.jpg|thumb|Two [[Indian Coast Guard]] vessels deploying an ocean boom]]\nA '''containment boom ''' is a temporary floating barrier used to contain an [[oil spill]]. Booms are used to reduce the possibility of polluting shorelines and other resources, and to help make recovery easier. Booms help to concentrate oil in thicker surface layers so that [[Skimmer (machine)|skimmers]], vacuums, or other collection methods can be used more effectively. They come in many shapes and sizes, with various levels of effectiveness in different types of water conditions.<ref>{{cite news |title=Boom (containment), in IncidentNews.gov ''Glossary'' |url=http://www.incidentnews.gov/glossary/B |publisher=Emergency Response Division, Office of Response and Restoration, National Ocean Service, [[National Oceanic and Atmospheric Administration]], [[US Department of Commerce]] |accessdate=2010-05-18 |url-status=dead |archiveurl=https://web.archive.org/web/20100710005942/http://incidentnews.gov/glossary/B |archivedate=2010-07-10 }}</ref>\n\nOften the first containment method to be used and the last equipment to be removed from the site of an oil spill, they are \"the most commonly used and most environmentally acceptable response technique to clean up oil spills in the United States.\"<ref>{{cite news |url=http://www.mms.gov/tarprojectcategories/mechanic.htm |title=Technology Assessment & Research (TA&R) Project Categories: Mechanical Containment and Recovery |author=Joseph Mullin |publisher=[[Minerals Management Service]], [[US Department of the Interior]] |accessdate=2010-05-18 |url-status=dead |archiveurl=https://web.archive.org/web/20100507135220/http://www.mms.gov/tarprojectcategories/mechanic.htm |archivedate=2010-05-07 }} Includes photographs and links to over fifty mechanical containment and recovery projects including containment boom.</ref>\n\nBooms used in oil spills can be seen as they rest on the surface of the water, but can have between 18 and 48 inches of material that hangs beneath the surface.<ref>{{cite web|title=How does oil spill boom protect shores?|author=Derica Williams|year=2010|url=http://www.fox10tv.com/dpp/news/national/south/how-does-oil-spill-boom-protect-shores-|publisher=[[WALA-TV]]|accessdate=2010-05-19|url-status=dead|archiveurl=https://web.archive.org/web/20100530094122/http://www.fox10tv.com/dpp/news/national/south/how-does-oil-spill-boom-protect-shores-|archivedate=2010-05-30}}</ref> They're effective in calm water, but as wave height increases oil or other contaminants can easily wash over the top of the boom and render them useless.\n\nIn any oil spill, the use of a single conventional boom is not effective in protecting environmental resources, even with the correct draft and aspect ratio. For speeds of over 1 knot (of the water and hence the oil), the boom will fail to stop the oil because of drainage under the boom. The approaching oil needs to be decelerated before it meets the boom. Drainage failure may be avoided by using a series of well-designed booms.<ref>J. Fang and K.V.Wong, \u201cAn Advanced VOF Algorithm for Oil Boom Design\", Int. J. Model. and Simulation, Vol. 26, No.1, Jan 2006, pp. 36-44.</ref>\n\n==Tactics==\n*Containment booming: placing a boom in a body of contaminated water for the purpose of holding or slowing the movement of contamination.<ref>{{Citation| title = Mechanical Recovery \u2013 Containment and Recovery \u2013 Containment Boom| publisher = NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)| date = April 2006| url = http://www.dec.state.ak.us/spar/perp/star/07containment.pdf| accessdate = June 8, 2010 }}</ref>\n*Diversion booming: placing a boom in a body of contaminated water for the purpose of diverting the contamination to a collection point.<ref>{{Citation|title=Mechanical Recovery \u2013 Containment and Recov\u2039\u203aery \u2013 Diversion Boom|date=April 2006|url=http://www.dec.state.ak.us/SPAR/perp/star/12diversion.pdf|last1=|last2=|last3=|last4=|first1=|first2=|first3=|first4=|author1-link=|publisher=NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)|accessdate=June 8, 2010}}</ref>\n*Deflection booming: placing a boom in a body of water for the sole purpose of changing the course of the contamination. This method is used for contamination that is not intended to be recovered and so is not typically associated with oil spills.<ref>{{Citation\n| title = Mechanical Recovery \u2013 Containment and Recovery \u2013 Deflection Boom| publisher = NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)| date = April 2006| url = http://www.dec.state.ak.us/spar/perp/star/17deflectionboom.pdf| accessdate = June 8, 2010 }}</ref>\n*Exclusion booming: placing a boom in a body of water for the purpose of blocking off a sensitive area from contamination. It is not recommended in fast water, and as diversion booming or deflection booming is better suited.<ref>{{Citation\n| title = Mechanical Recovery \u2013 Containment and Recovery \u2013 Exclusion Boom| publisher = NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)| date = April 2006| url = http://www.dec.state.ak.us/spar/perp/star/16exclusionboom.pdf| accessdate = June 8, 2010 }}</ref>  However, when diversion booming and deflection booming tactics are not suitable and resource protection is still needed, like because of a fast high tide in a sensitive estuary, an arrangement of booms with a decelerator is needed.<ref>{{Cite journal|title=AN ADVANCED VOF ALGORITHM FOR OIL BOOM DESIGN |journal=International Journal of Modelling and Simulation |volume=26 |year=2006 |url=http://mae.miami.edu/files/documents/wong/InnovativeOilBoom.pdf |accessdate=June 16, 2010 |url-status=dead |archiveurl=https://web.archive.org/web/20100621163717/http://mae.miami.edu/files/documents/wong/InnovativeOilBoom.pdf |archivedate=June 21, 2010 }}</ref><ref>{{cite patent\n |country= US|number= 7056059|status= patent |title= Boom with ramped or horizontal skirt structure for slowing the flow speed of buoyant fluids on moving water for fluid, containment, fluid containment system and method|pubdate=2006-06-06|fdate=2004-09-28 |pridate= |invent1= Kaufui Wong}}</ref>\n\n==See also==\n*[[Boom (navigational barrier)]]\n*[[Log boom]]\n\n==References==\n{{Reflist}}\n\n[[Category:Oil spill remediation technologies]]\n", "text_old": "[[Image:Oil Spill Containment Boom.jpg|thumb|Oil spill containment boom holding back oil]]\n[[File:Two Indian Coast Guard vessels deploying an ocean boom.jpg|thumb|Two [[Indian Coast Guard]] vessels deploying an ocean boom]]\nA '''containment boom ''' is a temporary floating barrier used to contain an [[oil spill]]. Booms are used to reduce the possibility of polluting shorelines and other resources, and to help make recovery easier. Booms help to concentrate oil in thicker surface layers so that [[Skimmer (machine)|skimmers]], vacuums, or other collection methods can be used more effectively. They come in many shapes and sizes, with various levels of effectiveness in different types of water conditions.<ref>{{cite news |title=Boom (containment), in IncidentNews.gov ''Glossary'' |url=http://www.incidentnews.gov/glossary/B |publisher=Emergency Response Division, Office of Response and Restoration, National Ocean Service, [[National Oceanic and Atmospheric Administration]], [[US Department of Commerce]] |accessdate=2010-05-18 |url-status=dead |archiveurl=https://web.archive.org/web/20100710005942/http://incidentnews.gov/glossary/B |archivedate=2010-07-10 }}</ref>\n\nOften the first containment method to be used and the last equipment to be removed from the site of an oil spill, they are \"the most commonly used and most environmentally acceptable response technique to clean up oil spills in the United States.\"<ref>{{cite news |url=http://www.mms.gov/tarprojectcategories/mechanic.htm |title=Technology Assessment & Research (TA&R) Project Categories: Mechanical Containment and Recovery |author=Joseph Mullin |publisher=[[Minerals Management Service]], [[US Department of the Interior]] |accessdate=2010-05-18 |url-status=dead |archiveurl=https://web.archive.org/web/20100507135220/http://www.mms.gov/tarprojectcategories/mechanic.htm |archivedate=2010-05-07 }} Includes photographs and links to over fifty mechanical containment and recovery projects including containment boom.</ref>\n\nBooms used in oil spills can be seen as they rest on the surface of the water, but can have between 18 and 48 inches of material that hangs beneath the surface.<ref>{{cite web|title=How does oil spill boom protect shores?|author=Derica Williams|year=2010|url=http://www.fox10tv.com/dpp/news/national/south/how-does-oil-spill-boom-protect-shores-|publisher=[[WALA-TV]]|accessdate=2010-05-19|url-status=dead|archiveurl=https://web.archive.org/web/20100530094122/http://www.fox10tv.com/dpp/news/national/south/how-does-oil-spill-boom-protect-shores-|archivedate=2010-05-30}}</ref> They're effective in calm water, but as wave height increases oil or other contaminants can easily wash over the top of the boom and render them useless.\n\nIn any oil spill, the use of a single conventional boom is not effective in protecting environmental resources, even with the correct draft and aspect ratio. For speeds of over 1 knot (of the water and hence the oil), the boom will fail to stop the oil because of drainage under the boom. The approaching oil needs to be decelerated before it meets the boom. Drainage failure may be avoided by using a series of well-designed booms.<ref>J. Fang and K.V.Wong, \u201cAn Advanced VOF Algorithm for Oil Boom Design\", Int. J. Model. and Simulation, Vol. 26, No.1, Jan 2006, pp. 36-44.</ref>\n\n==Tactics==\n*Containment booming: placing a boom in a body of contaminated water for the purpose of holding or slowing the movement of contamination.<ref>{{Citation| title = Mechanical Recovery \u2013 Containment and Recovery \u2013 Containment Boom| publisher = NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)| date = April 2006| url = http://www.dec.state.ak.us/spar/perp/star/07containment.pdf| accessdate = June 8, 2010 }}</ref>\n*Diversion booming: placing a boom in a body of contaminated water for the purpose of diverting the contamination to a collection point.<ref>{{Citation|title=Mechanical Recovery \u2013 Containment and Recov\u2039\u203aery \u2013 Diversion Boom|date=April 2006|url=http://www.dec.state.ak.us/SPAR/perp/star/12diversion.pdf|last1=|last2=|last3=|last4=|first1=|first2=|first3=|first4=|author1-link=|publisher=NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)|accessdate=June 8, 2010}}</ref>\n      \nDeflection booming: placing a boom in a body of water for the sole purpose of changing the course of the contamination. This method is used for contamination that is not intended to be recovered and so is not typically associated with oil spills.<ref>{{Citation\n| title = Mechanical Recovery \u2013 Containment and Recovery \u2013 Deflection Boom| publisher = NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)| date = April 2006| url = http://www.dec.state.ak.us/spar/perp/star/17deflectionboom.pdf| accessdate = June 8, 2010 }}</ref>\n\nExclusion booming: placing a boom in a body of water for the purpose of blocking off a sensitive area from contamination. It is not recommended in fast water, and as diversion booming or deflection booming is better suited.<ref>{{Citation\n| title = Mechanical Recovery \u2013 Containment and Recovery \u2013 Exclusion Boom| publisher = NUKA Research Planning Group & Spill Tactics for Alaska Responders (STAR)| date = April 2006| url = http://www.dec.state.ak.us/spar/perp/star/16exclusionboom.pdf| accessdate = June 8, 2010 }}</ref>  However, when diversion booming and deflection booming tactics are not suitable and resource protection is still needed, like because of a fast high tide in a sensitive estuary, an arrangement of booms with a decelerator is needed.<ref>{{Cite journal|title=AN ADVANCED VOF ALGORITHM FOR OIL BOOM DESIGN |journal=International Journal of Modelling and Simulation |volume=26 |year=2006 |url=http://mae.miami.edu/files/documents/wong/InnovativeOilBoom.pdf |accessdate=June 16, 2010 |url-status=dead |archiveurl=https://web.archive.org/web/20100621163717/http://mae.miami.edu/files/documents/wong/InnovativeOilBoom.pdf |archivedate=June 21, 2010 }}</ref><ref>{{cite patent\n |country= US|number= 7056059|status= patent |title= Boom with ramped or horizontal skirt structure for slowing the flow speed of buoyant fluids on moving water for fluid, containment, fluid containment system and method|pubdate=2006-06-06|fdate=2004-09-28 |pridate= |invent1= Kaufui Wong}}</ref>\n\n==See also==\n*[[Boom (navigational barrier)]]\n*[[Log boom]]\n\n==References==\n{{Reflist}}\n\n[[Category:Oil spill remediation technologies]]\n", "name_user": "Narabedla", "label": "safe", "comment": "Simple bulleting", "url_page": "//en.wikipedia.org/wiki/Boom_(containment)"}
{"title_page": "Endor (Star Wars)", "text_new": "{{DISPLAYTITLE:Forest moon of Endor (''Star Wars'')}}\n{{Infobox fictional location\n| name         = Endor\n| image        = PlanetEndor.jpg\n| caption      = IX3244-A \"Sanctuary Moon\" - The forest moon of Endor\n| source       = [[Star Wars]]\n| first        = ''[[Return of the Jedi]]''\n| last         = ''[[Star Wars: The Rise of Skywalker]]''\n| creator      = [[George Lucas]]\n| genre        = [[Science fiction]]\n| type         = [[Earth|Gaia]]-world\n| races        = [[Ewok]], [[Dulok]], [[Gorax]], [[Yuzzum]]\n| population   = Over 30,000,000 Ewoks\n| blank_label  = Terrain\n| blank_data   = {{plainlist|\n;Major\n*[[Forest]]\n*[[Savannah]]\n*[[Grassland]]\n*[[Mountain]]\n*[[River]]s (shallow)\n}}\n{{plainlist|\n;Minor\n*[[Desert]]\n*[[Ocean]]\n}}\n| blank_label2 = Oceans\n| blank_data2  = 3\n| blank_label3 = Sun(s)\n| blank_data3  = 2\n| blank_label4 = Standard Galactic Grid Coordinates\n| blank_data4  = H-16\n| blank-label5 = XYZ Coordinates\n| blank-data5 = \n}}\n'''Endor''' (designated: '''IX3244-A''') is a fictional moon in the ''[[Star Wars]]'' [[Fictional universe|universe]], known for its endless forests, savannahs, grasslands, mountain ranges, and a few oceans. The moon was the site of a pivotal battle depicted in ''[[Return of the Jedi]]''. Homeworld of the sentient [[Dulok]], [[Ewok]], and [[Yuzzum]] species, as well as the semi-sentient [[Gorax]] and [[Wistie]] species. The Endor solar cycle was 402 [[Galactic Standard Calendar|GSC]] days orbital, with a breathable earth-like atmosphere conducive for humans and 8% surface water. It is the gravesite of [[Darth Vader]]. \n\nThe moon orbits '''[[Tana (Star Wars)|Tana]]'''\u2014the Ewokese word for Endor's host planet\u2014a [[gas giant]] located in the '''Endor system''', a [[Binary star|binary]] [[star system]] positioned in the '''Moddell sector''' of [[Star Wars galaxy|the galaxy]]'s Outer Rim Territories.<ref>\"Star Wars:Absolutely Everything You Need to Know\"</ref><ref>\"Star Wars: Galactic Atlas\"</ref> Located in grid square '''H-16''' on the Standard Galactic Grid, it was connected to [[Cerea]] and [[Bakura (Star Wars)|Bakura]] by a hyperspace route.<ref>\"Star Wars: Beginner Game\"</ref> The [[planet]] was [[orbit]]ed by nine moons, the largest of which was known as the Forest Moon of Endor or \"Sanctuary Moon\".<ref>\"Star Wars: Complete Locations\"</ref> The ocean moon of '''[[Kef Bir]]''' was also one of these moons, and is the location where the second [[Death Star]] crashed after it exploded over Endor in ''[[Return of the Jedi]]''.<ref>''Star Wars: The Rise of Skywalker''</ref> It also had two suns: '''Endor Prime I''' and '''Endor Prime II'''.\n\n==Description==\nThe Forest Moon of Endor first appears in ''[[Return of the Jedi]]'', in which it is the body in whose orbit the [[second Death Star]] is constructed, and is the home of a race of furry aliens called [[Ewok]]s. The moon later appears in the original [[Star Tours]] [[Disney Parks, Experiences and Products|Disney theme park attraction]], Ewok [[TV movie]]s ''[[The Ewok Adventure]]'' and ''[[Ewoks: The Battle for Endor]]'', as well as the animated ''[[Ewoks (TV series)|Ewoks]]'' and its [[Marvel Comics]] tie-in series.\n\nVarious descriptions of the Endor system exist in various media. Special effects storyboards for ''Return of the Jedi'' refer to a distant orb in the system as \"Planet Endor\". According to the [[Return of the Jedi (novel)|''Return of the Jedi'' novelization]], the planet disappeared in an ancient cataclysm. The Ewok television films depict a [[gas giant]] in the sky, and novels such as ''[[The Truce at Bakura]]'' and ''[[Dark Apprentice]]'' also mention a planet visible from the moon. The planet is called \"Tana\" in the ''Ewoks'' animated series, which also depicts a [[binary star]] system (while other sources depict only one sun).<ref name=data>{{cite web|url=http://starwars.com/databank/location/endor/index.html|title=Databank: Endor|website=StarWars.com|archiveurl=https://web.archive.org/web/20110524190603/http://starwars.com/databank/location/endor/index.html|archivedate=May 24, 2011|accessdate=March 20, 2019}}</ref>\n\nIn a ''[[Star Wars Tales]]'' comic entitled ''[[Star Wars Tales Volume 4#Apocalypse Endor|Apocalypse Endor]]'', an Imperial [[war veteran|veteran]] of Endor refers to the moon being devastated by the [[Impact event|impact of falling debris]] from the Death Star, which was blown up while in orbit around the moon. However, another character dismisses this as a myth, saying that most of the Death Star's mass was obliterated in the explosion, and that the Rebels \"took care of the rest\".<ref>{{comic book reference|writer=|story=Apocalypse Endor|title=Star Wars Tales|issue=14|date=December 11, 2004|publisher=[[Dark Horse Comics]]}}</ref> ''[[The Rise of Skywalker]]'' depicts the wreckage of the second Death Star in a watery location, named Kef&nbsp;Bir, an ocean moon featured in the film<ref>{{cite web |last=Bankhurst |first=Adam |title=Star Wars: Location Where Death Star II Crashed Identified |url=https://www.ign.com/articles/2019/10/23/star-wars-location-where-death-star-ii-crashed-identified |accessdate=October 23, 2019 |date=October 23, 2019}}</ref> that orbits the same gas giant as the forest moon. [[Wicket W. Warrick]] and a related Ewok appear briefly at the end of ''The Rise of Skywalker''.\n\n==Filming==\nScenes set on Endor were filmed on private logging company land that was shortly thereafter clearcut near the town of [[Smith River, California]]; the speeder chase scene was filmed at the Chetham Grove section of [[Grizzly Creek Redwoods State Park]].<ref name=\"film\">{{cite web |title=Map of the Movies|publisher=Humboldt - Del Norte Film Commission |url=http://filmhumboldtdelnorte.org/sites/default/files/map-of-the-movies_0.pdf|accessdate=2019-12-10}}</ref><ref>{{cite web |url=http://www.nps.gov/getaways/redw/ |work=[[Redwood National and State Parks]] |publisher=[[Interior Department]] of the USA |access-date=14 August 2015 |title=Experience America's Best Idea |archive-url=https://web.archive.org/web/20181021190914/https://www.nps.gov/getaways/redw/ |archive-date=21 October 2018 |date=7 November 2012}}</ref><ref name=data/> near the \"Avenue of the Giants\" in [[Humboldt Redwoods State Park]].<ref>{{Cite news |url=https://matadornetwork.com/trips/guide-to-the-redwood-groves-where-to-find-the-tallest-trees-on-earth/ |title=Guide to California\u2019s redwood groves and the tallest trees on Earth |newspaper=[[Matador Network]] |access-date=20 January 2017 |first=Hal |last=Amen |date=23 November 2009}}</ref> \n\n==See also==\n* [[List of Star Wars planets and moons|List of ''Star Wars'' planets and moons]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite book | last=Cavelos | first=Jeanne | year=2007 | title=[[The Science of Star Wars (book)|The Science of Star Wars]]: An Astrophysicist's Independent Examination of Space Travel, Aliens, Planets, and Robots as Portrayed in the Star Wars Films and Books | publisher=[[Macmillan Publishers|Macmillan]] | pages=29\u201332 | isbn=978-1-4299-7176-8 }}\n\n==External links==\n* [http://www.starwars.com/explore/encyclopedia/locations/endor/ Endor] at the Star Wars Databank\n* {{Wookieepedia|Endor}}\n* [http://www.theforce.net/swtc/holocaust.html Endor 'Holocaust'] at Star Wars Technical Commentaries\n\n{{Star Wars universe|state=collapsed}}\n\n[[Category:Fictional elements introduced in 1983]]\n[[Category:Fictional terrestrial planets]]\n[[Category:Forest planets in fiction]]\n[[Category:Star Wars planets]]\n[[Category:Return of the Jedi]]\n", "text_old": "{{DISPLAYTITLE:Endor (''Star Wars'')}}\n{{Infobox fictional location\n| name         = Endor\n| image        = PlanetEndor.jpg\n| caption      = IX3244-A \"Sanctuary Moon\" - The forest moon of Endor\n| source       = [[Star Wars]]\n| first        = ''[[Return of the Jedi]]''\n| last         = ''[[Star Wars: The Rise of Skywalker]]''\n| creator      = [[George Lucas]]\n| genre        = [[Science fiction]]\n| type         = [[Earth|Gaia]]-world\n| races        = [[Ewok]], [[Dulok]], [[Gorax]], [[Yuzzum]]\n| population   = Over 30,000,000 Ewoks\n| blank_label  = Terrain\n| blank_data   = {{plainlist|\n;Major\n*[[Forest]]\n*[[Savannah]]\n*[[Grassland]]\n*[[Mountain]]\n*[[River]]s (shallow)\n}}\n{{plainlist|\n;Minor\n*[[Desert]]\n*[[Ocean]]\n}}\n| blank_label2 = Oceans\n| blank_data2  = 3\n| blank_label3 = Sun(s)\n| blank_data3  = 2\n| blank_label4 = Standard Galactic Grid Coordinates\n| blank_data4  = H-16\n| blank-label5 = XYZ Coordinates\n| blank-data5 = \n}}\n'''Endor''' (designated: '''IX3244-A''') is a fictional moon in the ''[[Star Wars]]'' [[Fictional universe|universe]], known for its endless forests, savannahs, grasslands, mountain ranges, and a few oceans. The moon was the site of a pivotal battle depicted in ''[[Return of the Jedi]]''. Homeworld of the sentient [[Dulok]], [[Ewok]], and [[Yuzzum]] species, as well as the semi-sentient [[Gorax]] and [[Wistie]] species. The Endor solar cycle was 402 [[Galactic Standard Calendar|GSC]] days orbital, with a breathable earth-like atmosphere conducive for humans and 8% surface water. It is the gravesite of [[Darth Vader]]. \n\nThe moon orbits '''[[Tana (Star Wars)|Tana]]'''\u2014the Ewokese word for Endor's host planet\u2014a [[gas giant]] located in the '''Endor system''', a [[Binary star|binary]] [[star system]] positioned in the '''Moddell sector''' of [[Star Wars galaxy|the galaxy]]'s Outer Rim Territories.<ref>\"Star Wars:Absolutely Everything You Need to Know\"</ref><ref>\"Star Wars: Galactic Atlas\"</ref> Located in grid square '''H-16''' on the Standard Galactic Grid, it was connected to [[Cerea]] and [[Bakura (Star Wars)|Bakura]] by a hyperspace route.<ref>\"Star Wars: Beginner Game\"</ref> The [[planet]] was [[orbit]]ed by nine moons, the largest of which was known as the Forest Moon of Endor or \"Sanctuary Moon\".<ref>\"Star Wars: Complete Locations\"</ref> The ocean moon of '''[[Kef Bir]]''' was also one of these moons, and is the location where the second [[Death Star]] crashed after it exploded over Endor in ''[[Return of the Jedi]]''.<ref>''Star Wars: The Rise of Skywalker''</ref> It also had two suns: '''Endor Prime I''' and '''Endor Prime II'''.\n\n==Description==\nThe Forest Moon of Endor first appears in ''[[Return of the Jedi]]'', in which it is the body in whose orbit the [[second Death Star]] is constructed, and is the home of a race of furry aliens called [[Ewok]]s. The moon later appears in the original [[Star Tours]] [[Disney Parks, Experiences and Products|Disney theme park attraction]], Ewok [[TV movie]]s ''[[The Ewok Adventure]]'' and ''[[Ewoks: The Battle for Endor]]'', as well as the animated ''[[Ewoks (TV series)|Ewoks]]'' and its [[Marvel Comics]] tie-in series.\n\nVarious descriptions of the Endor system exist in various media. Special effects storyboards for ''Return of the Jedi'' refer to a distant orb in the system as \"Planet Endor\". According to the [[Return of the Jedi (novel)|''Return of the Jedi'' novelization]], the planet disappeared in an ancient cataclysm. The Ewok television films depict a [[gas giant]] in the sky, and novels such as ''[[The Truce at Bakura]]'' and ''[[Dark Apprentice]]'' also mention a planet visible from the moon. The planet is called \"Tana\" in the ''Ewoks'' animated series, which also depicts a [[binary star]] system (while other sources depict only one sun).<ref name=data>{{cite web|url=http://starwars.com/databank/location/endor/index.html|title=Databank: Endor|website=StarWars.com|archiveurl=https://web.archive.org/web/20110524190603/http://starwars.com/databank/location/endor/index.html|archivedate=May 24, 2011|accessdate=March 20, 2019}}</ref>\n\nIn a ''[[Star Wars Tales]]'' comic entitled ''[[Star Wars Tales Volume 4#Apocalypse Endor|Apocalypse Endor]]'', an Imperial [[war veteran|veteran]] of Endor refers to the moon being devastated by the [[Impact event|impact of falling debris]] from the Death Star, which was blown up while in orbit around the moon. However, another character dismisses this as a myth, saying that most of the Death Star's mass was obliterated in the explosion, and that the Rebels \"took care of the rest\".<ref>{{comic book reference|writer=|story=Apocalypse Endor|title=Star Wars Tales|issue=14|date=December 11, 2004|publisher=[[Dark Horse Comics]]}}</ref> ''[[The Rise of Skywalker]]'' depicts the wreckage of the second Death Star in a watery location, named Kef&nbsp;Bir, an ocean moon featured in the film<ref>{{cite web |last=Bankhurst |first=Adam |title=Star Wars: Location Where Death Star II Crashed Identified |url=https://www.ign.com/articles/2019/10/23/star-wars-location-where-death-star-ii-crashed-identified |accessdate=October 23, 2019 |date=October 23, 2019}}</ref> that orbits the same gas giant as the forest moon. [[Wicket W. Warrick]] and a related Ewok appear briefly at the end of ''The Rise of Skywalker''.\n\n==Filming==\nScenes set on Endor were filmed on private logging company land that was shortly thereafter clearcut near the town of [[Smith River, California]]; the speeder chase scene was filmed at the Chetham Grove section of [[Grizzly Creek Redwoods State Park]].<ref name=\"film\">{{cite web |title=Map of the Movies|publisher=Humboldt - Del Norte Film Commission |url=http://filmhumboldtdelnorte.org/sites/default/files/map-of-the-movies_0.pdf|accessdate=2019-12-10}}</ref><ref>{{cite web |url=http://www.nps.gov/getaways/redw/ |work=[[Redwood National and State Parks]] |publisher=[[Interior Department]] of the USA |access-date=14 August 2015 |title=Experience America's Best Idea |archive-url=https://web.archive.org/web/20181021190914/https://www.nps.gov/getaways/redw/ |archive-date=21 October 2018 |date=7 November 2012}}</ref><ref name=data/> near the \"Avenue of the Giants\" in [[Humboldt Redwoods State Park]].<ref>{{Cite news |url=https://matadornetwork.com/trips/guide-to-the-redwood-groves-where-to-find-the-tallest-trees-on-earth/ |title=Guide to California\u2019s redwood groves and the tallest trees on Earth |newspaper=[[Matador Network]] |access-date=20 January 2017 |first=Hal |last=Amen |date=23 November 2009}}</ref> \n\n==See also==\n* [[List of Star Wars planets and moons|List of ''Star Wars'' planets and moons]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite book | last=Cavelos | first=Jeanne | year=2007 | title=[[The Science of Star Wars (book)|The Science of Star Wars]]: An Astrophysicist's Independent Examination of Space Travel, Aliens, Planets, and Robots as Portrayed in the Star Wars Films and Books | publisher=[[Macmillan Publishers|Macmillan]] | pages=29\u201332 | isbn=978-1-4299-7176-8 }}\n\n==External links==\n* [http://www.starwars.com/explore/encyclopedia/locations/endor/ Endor] at the Star Wars Databank\n* {{Wookieepedia|Endor}}\n* [http://www.theforce.net/swtc/holocaust.html Endor 'Holocaust'] at Star Wars Technical Commentaries\n\n{{Star Wars universe|state=collapsed}}\n\n[[Category:Fictional elements introduced in 1983]]\n[[Category:Fictional terrestrial planets]]\n[[Category:Forest planets in fiction]]\n[[Category:Star Wars planets]]\n[[Category:Return of the Jedi]]\n", "name_user": "2a00:23c8:4002:2801:54e6:747b:6e7a:8222", "label": "unsafe", "comment": "(This is the Forest moon of Endor, not Endor)", "url_page": "//en.wikipedia.org/wiki/Endor_(Star_Wars)"}
{"title_page": "Barbara (2012 film)", "text_new": "{{short description|2012 film}}\n{{Use dmy dates|date=April 2020}}\n{{Infobox film\n| name           = Barbara\n| image          = Barbara (2012 film).jpg\n| caption        = Film poster\n| director       = [[Christian Petzold (director)|Christian Petzold]]\n| producer       = \n| writer         = {{ubl|Christian Petzold|[[Harun Farocki]]}}\n| starring       = {{ubl|[[Nina Hoss]]|[[Ronald Zehrfeld]]|[[Jasna Fritzi Bauer]]|[[Mark Waschke]]|[[Rainer Bock]]}}\n| music          = Stefan Will\n| cinematography = Hans Fromm\n| editing        = Bettina B\u00f6hler\n| distributor    = \n| released       = {{Film date|2012|2|11|[[62nd Berlin International Film Festival|Berlin]]|2012|3|8|Germany|df=yes}}\n| runtime        = 105 minutes\n| country        = Germany\n| language       = German\n| gross          = $4.1 million<ref name=\"BoxOffice\">{{cite web|url=http://www.boxofficemojo.com/movies/?id=barbara.htm |title=Barbara |accessdate=19 December 2017 |work=Box Office Mojo}}</ref>\n}}\n'''''Barbara''''' is a 2012 German [[drama film]] directed by [[Christian Petzold (director)|Christian Petzold]]. The film competed at the [[62nd Berlin International Film Festival]] in February 2012,<ref name=\"PressRelease9Jan\">{{cite web|url=http://www.berlinale.de/en/presse/pressemitteilungen/alle/Alle-Detail_12436.html |title=Press Release, 9th Jan |date=9 January 2012 |accessdate=9 January 2012 |work=berlinale.de}}</ref> where Petzold won the [[Silver Bear for Best Director]].<ref name=\"BerlinPrizes\">{{cite web|url=http://www.berlinale.de/en/das_festival/preise_und_juries/preise_internationale_jury/index.html |title=Prizes of the International Jury 2012 |date=19 February 2012 |accessdate=19 February 2012 |work=Berlinale}}</ref> The film was selected as the German entry for the [[Academy Award for Best Foreign Language Film|Best Foreign Language Oscar]] at the [[85th Academy Awards]], but it did not make the shortlist.<ref name=\"Oscars\">{{cite web |url=http://www.screendaily.com/awards/academy-awards/germanys-oscar-entry-is-christian-petzolds-barbara/5045903.article|title=Germany's Oscar entry is Christian Petzold's Barbara |accessdate=31 August 2012|work=Screendaily}}</ref>\n\n==Plot==\n[[East Germany]] in 1980: Barbara ([[Nina Hoss]]) is a physician who arrives for her first day at a small rural hospital near the [[Baltic Sea]].  She had been at the prestigious [[Charit\u00e9]] hospital in [[East Berlin]] but, after she'd filed an \"Ausreiseantrag\" \u2013 an official request to leave East Germany \u2013 she had been incarcerated and transferred to the small town where she is still monitored by the [[Stasi]].  The Stasi punishes her for the hours in which they cannot find her by searching her house, strip-searching and cavity-searching her.\n\nIn her new job, she works in [[pediatric surgery]], a department led by [[chief physician]] Andr\u00e9 Reiser. Reiser eventually tells her a story (whose veracity she questions) of how he too had lost his job at a more prestigious hospital in Berlin \u2013 he was responsible for an accident with an incubator that left two premature infants blind. The Stasi had agreed to keep it quiet if he agreed to relocate to the provincial hospital and to work for them. So now Reiser reports on suspected people, including Barbara.\n\nEarly on, when the police deliver Stella, a young runaway from a labour camp, to the hospital for the fourth time, Reiser thinks Stella is [[malingering]]. Barbara intervenes and orders removal of the restraints on the patient, readily diagnosing her with [[meningitis]]. During her recovery, Stella develops a strong attachment to Barbara, whose welcome bedside manner includes reading the ''[[Adventures of Huckleberry Finn]]'' to her. Stella is pregnant and wants to raise the child. Wanting to escape from the country and to have her child in a new land, she implores Barbara to take her with her.  However, they cannot find grounds for keeping Stella longer and soon she is returned against her will to the labour camp.\n\nMeanwhile, Barbara makes secretive bicycle treks, to a place to stash her secretly received funds for escape, and to the woods where she meets with her West German lover J\u00f6rg, who has been supplying her with prized goods and is preparing for her escape. When she meets him for a second rendezvous in an \"Interhotel\" (an East-German hotel for foreigners), he tells her of his completed plan for her escape the following weekend: she will be picked up in a small boat in the Baltic Sea and taken the short distance to Denmark.\n\nAs Barbara spends more time working with Reiser, he begins making romantic overtures, which she rebuffs although she is intrigued by and attracted to him. He has built a laboratory, to test samples on-site, and he has created his own serums with which to treat patients.\n\nOne day before her planned escape, Barbara is on duty caring for a critically ill patient named Mario, whose suicide attempt had resulted in his being hospitalized. Barbara discovers that Mario has not been recovering from his traumatic head injury as well as believed and requires immediate brain surgery. She tracks Reiser down on his day off, to inform him of Mario's urgent need of surgery. She finds him at the home of the Stasi agent who has been overseeing her monitoring. Reiser is treating the agent's wife, who is dying of cancer. Reiser persuades her to return to the hospital \u2013 the same night of her planned escape \u2013 so that he can perform the surgery, with her assistance as anaesthesiologist during the operation.\n\nFollowing her agreement to be there, yet still planning her escape, Barbara accepts Reiser's invitation to let him cook a lunch for her at his home on the same day. When Reiser finally tells Barbara that he is happy to have her there with him, she kisses him. Then she abruptly pulls away from him, and returns to her house to continue preparing to escape.\n\nDuring this time, Stella flees the labour youth detention programme again and comes to Barbara's doorstep that night.  Barbara takes her to the agreed-upon area on the beach, where she is to meet a person who will help smuggle her out. Barbara writes a note to accompany Stella, which is presumably addressed to J\u00f6rg, explaining why she has chosen to let Stella escape, instead of going herself. After helping Stella to a waiting raft, and a skin diver who will help her escape by sea, she returns to the hospital. She takes a seat, across from Reiser, who is watching over Mario at his bedside. She has decided to stay in the East, to be with Reiser. In a final close-up, their eyes meet in mutual understanding.\n\n==Cast==\n[[File:Nina Hoss und Mark Waschke in Barbara, 2012.jpg|thumb|[[Nina Hoss]] and [[Mark Waschke]]]]\n[[File:Ronald Zehrfeld und Rainer Bock in 'Barbara', 2012.jpg|thumb|[[Ronald Zehrfeld]] and [[Rainer Bock]]]]\n{{cast listing|\n* [[Nina Hoss]] as Barbara Wolff\n* [[Ronald Zehrfeld]] as Dr Andr\u00e9 Reiser\n* [[Rainer Bock]] as Stasi Officer Klaus Sch\u00fctz\n* [[Jasna Fritzi Bauer]] as Stella\n* Christina Hecke as Karin\n* [[Mark Waschke]] as J\u00f6rg\n* Peter Benedict as Gerhard\n* [[Jannik Sch\u00fcmann]] as Mario\n* [[Alicia von Rittberg]] as Angie\n* Susanne Bormann as Steffi\n* Claudia Geisler as nurse Schl\u00f6sser\n* Deniz Petzold as Angelo\n* Rosa Enskat as janitor Bungert\n}}\n\n==Critical reception==\n''Barbara'' has a \"certified fresh\" approval rating of 93% on [[Rotten Tomatoes]], based on 76 reviews, with an average score of 7.7/10. The website's critical consensus reads, \"Smart, solidly grafted, and thoroughly gripping, ''Barbara'' offers a deliberately paced, subtly powerful character study.\"<ref>{{Cite web| title=Barbara Movie Reviews |url=http://www.rottentomatoes.com/m/barbara_2012/ |work=Rotten Tomatoes|publisher=Flixster|accessdate=21 February 2018}}</ref> The film also has a [[weighted average]] score of 86/100 on the critical aggregator website [[Metacritic]], indicating \"universal acclaim\".<ref>{{cite web| url = http://www.metacritic.com/movie/barbara/critic-reviews| title = Barbara: Critic Reviews| date = 8 March 2013| accessdate = 26 July 2016| publisher = metacritic.com}}</ref>\n\nWriting in ''[[The Guardian]]'', film critic [[Peter Bradshaw]] said of ''Barbara'': \"The weird oppression and seediness of the times is elegantly captured, and Hoss coolly conveys Barbara's highly strung desperation.\" Bradshaw awarded the film four stars out of five.<ref>{{cite news|last=Bradshaw|first=Peter|title=Barbara - review|newspaper=[[The Guardian]]|url=https://www.theguardian.com/film/2012/sep/27/barbara-review|date=27 September 2012|accessdate=18 December 2012}}</ref> ''[[The New York Times]]'' designated ''Barbara'' a critics' pick. In her review, [[Manohla Dargis]] said of the film: \"''Barbara'' is a film about the old Germany from one of the best directors working in the new: Christian Petzold. For more than a decade Mr. Petzold has been making his mark on the international cinema scene with smart, tense films that resemble psychological thrillers, but are distinguished by their strange story turns, moral thorns, visual beauty and filmmaking intelligence.\"<ref>{{cite web|last=Dargis|first=Manohla|title=Pushed and Pulled, a Doctor Wants a Way Out|work=The New York Times|url=https://movies.nytimes.com/2012/12/21/movies/barbara-directed-by-christian-petzold.html?partner=rss&emc=rss&_r=0|date= 20 December 2012|accessdate=20 December 2012}}</ref> [[Steven Rea]] wrote that \"Christian Petzold's masterfully hushed, suspenseful thriller percolates with dread....Hoss, wearing her blond hair pulled back tight, and wearing an expression of inscrutable melancholy, gives a performance that doesn't feel like a performance at all. Her Barbara is absolutely real, and absolutely trapped. The film is aching, and exquisite.\"<ref>{{cite web|url=http://articles.philly.com/2013-03-08/entertainment/37564121_1_ronald-zehrfeld-nina-hoss-barbara|title='Barbara' portrays life behind Iron Curtain|work=philly-archives|accessdate=4 February 2016}}</ref>\n\nIn ''[[The Independent]]'', Jonathan Romney includes ''Barbara'' in the genre of war stories - \"as nerve-wracking as the best.\" <ref name=\"Romney\">{{cite news |last=Romney |first=Jonathan |title= Culture>Film>Reviews: Barbara, Christian Petzold, 105 mins|url=https://www.independent.co.uk/arts-entertainment/films/reviews/barbara-christian-petzold-105-mins-12a-8190724.html|newspaper=[[The Independent|Independent]] |date=29 September 2012 |accessdate=1 January 2018 }}</ref> In the ''[[Chicago Sun-Times]]'', Sheila O'Malley concluded \"This is well-trod ground for Petzold, but never has it been so fully realized, so palpable, as in 'Barbara.'\"<ref>{{cite web|url=http://www.rogerebert.com/reviews/barbara-2013|title=Barbara|author=Sheila O'Malley|date=6 March 2013|publisher=|accessdate=4 February 2016}}</ref> In her article on ''Barbara'', film scholar [[Christina Gerhardt]] argues that Petzold draws on [[melodrama]] and combines this genre with slow realism to add a chapter to post-Wende films about the GDR.<ref>{{cite journal|last=Gerhardt|first=Christina|title=Looking East: Christian Petzold's ''Barbara''|journal=Quarterly Review of Film and Video|volume=33|issue=6|pages=550\u2013566|date= 9 June 2016|doi=10.1080/10509208.2015.1135679}}</ref>\n\n==See also==\n* [[List of submissions to the 85th Academy Awards for Best Foreign Language Film]]\n* [[List of German submissions for the Academy Award for Best Foreign Language Film]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{commons category|Barbara (film)}}\n* {{IMDb title|2178941|Barbara}}\n* {{Rotten Tomatoes|barbara_2012|Barbara}}\n* {{metacritic film|barbara|Barbara}}\n\n{{Christian Petzold}}\n{{German submission for Academy Awards}}\n\n{{DEFAULTSORT:Barbara}}\n[[Category:2012 films]]\n[[Category:2012 drama films]]\n[[Category:Cold War films]]\n[[Category:Films about physicians]]\n[[Category:Films about surgeons]]\n[[Category:Films set in 1980]]\n[[Category:Films set in Germany]]\n[[Category:Films directed by Christian Petzold]]\n[[Category:Films critical of communism]]\n[[Category:German films]]\n[[Category:German drama films]]\n[[Category:German-language films]]\n", "text_old": "{{short description|2012 film}}\n{{Use dmy dates|date=January 2019}}\n{{Infobox film\n| name           = Barbara\n| image          = Barbara (2012 film).jpg\n| caption        = Film poster\n| director       = [[Christian Petzold (director)|Christian Petzold]]\n| producer       = \n| writer         = {{ubl|Christian Petzold|[[Harun Farocki]]}}\n| starring       = {{ubl|[[Nina Hoss]]|[[Ronald Zehrfeld]]|[[Jasna Fritzi Bauer]]|[[Mark Waschke]]|[[Rainer Bock]]}}\n| music          = Stefan Will\n| cinematography = Hans Fromm\n| editing        = Bettina B\u00f6hler\n| distributor    = \n| released       = {{Film date|2012|2|11|[[62nd Berlin International Film Festival|Berlin]]|2012|3|8|Germany|df=yes}}\n| runtime        = 105 minutes\n| country        = Germany\n| language       = German\n| gross          = $4.1 million<ref name=\"BoxOffice\">{{cite web|url=http://www.boxofficemojo.com/movies/?id=barbara.htm |title=Barbara |accessdate=19 December 2017 |work=Box Office Mojo}}</ref>\n}}\n'''''Barbara''''' is a 2012 German [[drama film]] directed by [[Christian Petzold (director)|Christian Petzold]]. The film competed at the [[62nd Berlin International Film Festival]] in February 2012,<ref name=\"PressRelease9Jan\">{{cite web|url=http://www.berlinale.de/en/presse/pressemitteilungen/alle/Alle-Detail_12436.html |title=Press Release, 9th Jan |date=9 January 2012 |accessdate=9 January 2012 |work=berlinale.de}}</ref> where Petzold won the [[Silver Bear for Best Director]].<ref name=\"BerlinPrizes\">{{cite web|url=http://www.berlinale.de/en/das_festival/preise_und_juries/preise_internationale_jury/index.html |title=Prizes of the International Jury 2012 |date=19 February 2012 |accessdate=19 February 2012 |work=Berlinale}}</ref> The film was selected as the German entry for the [[Academy Award for Best Foreign Language Film|Best Foreign Language Oscar]] at the [[85th Academy Awards]], but it did not make the shortlist.<ref name=\"Oscars\">{{cite web |url=http://www.screendaily.com/awards/academy-awards/germanys-oscar-entry-is-christian-petzolds-barbara/5045903.article|title=Germany's Oscar entry is Christian Petzold's Barbara |accessdate=31 August 2012|work=Screendaily}}</ref>\n\n==Plot==\n[[East Germany]] in 1980: Barbara ([[Nina Hoss]]) is a physician who arrives for her first day at a small rural hospital near the [[Baltic Sea]].  She had been at the prestigious [[Charit\u00e9]] hospital in [[East Berlin]] but, after she'd filed an \"Ausreiseantrag\" \u2013 an official request to leave East Germany \u2013 she had been incarcerated and transferred to the small town where she is still monitored by the [[Stasi]].  The Stasi punishes her for the hours in which they cannot find her by searching her house, strip-searching and cavity-searching her.\n\nIn her new job, she works in [[pediatric surgery]], a department led by [[chief physician]] Andr\u00e9 Reiser. Reiser eventually tells her a story (whose veracity she questions) of how he too had lost his job at a more prestigious hospital in Berlin \u2013 he was responsible for an accident with an incubator that left two premature infants blind. The Stasi had agreed to keep it quiet if he agreed to relocate to the provincial hospital and to work for them. So now Reiser reports on suspected people, including Barbara.\n\nEarly on, when the police deliver Stella, a young runaway from a labour camp, to the hospital for the fourth time, Reiser thinks Stella is [[malingering]]. Barbara intervenes and orders removal of the restraints on the patient, readily diagnosing her with [[meningitis]]. During her recovery, Stella develops a strong attachment to Barbara, whose welcome bedside manner includes reading the ''[[Adventures of Huckleberry Finn]]'' to her. Stella is pregnant and wants to raise the child. Wanting to escape from the country and to have her child in a new land, she implores Barbara to take her with her.  However, they cannot find grounds for keeping Stella longer and soon she is returned against her will to the labour camp.\n\nMeanwhile, Barbara makes secretive bicycle treks, to a place to stash her secretly received funds for escape, and to the woods where she meets with her West German lover J\u00f6rg, who has been supplying her with prized goods and is preparing for her escape. When she meets him for a second rendezvous in an \"Interhotel\" (an East-German hotel for foreigners), he tells her of his completed plan for her escape the following weekend: she will be picked up in a small boat in the Baltic Sea and taken the short distance to Denmark.\n\nAs Barbara spends more time working with Reiser, he begins making romantic overtures, which she rebuffs although she is intrigued by and attracted to him. He has built a laboratory, to test samples on-site, and he has created his own serums with which to treat patients.\n\nOne day before her planned escape, Barbara is on duty caring for a critically ill patient named Mario, whose suicide attempt had resulted in his being hospitalized. Barbara discovers that Mario has not been recovering from his traumatic head injury as well as believed and requires immediate brain surgery. She tracks Reiser down on his day off, to inform him of Mario's urgent need of surgery. She finds him at the home of the Stasi agent who has been overseeing her monitoring. Reiser is treating the agent's wife, who is dying of cancer. Reiser persuades her to return to the hospital \u2013 the same night of her planned escape \u2013 so that he can perform the surgery, with her assistance as anaesthesiologist during the operation.\n\nFollowing her agreement to be there, yet still planning her escape, Barbara accepts Reiser's invitation to let him cook a lunch for her at his home on the same day. When Reiser finally tells Barbara that he is happy to have her there with him, she kisses him. Then she abruptly pulls away from him, and returns to her house to continue preparing to escape.\n\nDuring this time, Stella flees the labour youth detention programme again and comes to Barbara's doorstep that night.  Barbara takes her to the agreed-upon area on the beach, where she is to meet a person who will help smuggle her out. Barbara writes a note to accompany Stella, which is presumably addressed to J\u00f6rg, explaining why she has chosen to let Stella escape, instead of going herself. After helping Stella to a waiting raft, and a skin diver who will help her escape by sea, she returns to the hospital. She takes a seat, across from Reiser, who is watching over Mario at his bedside. She has decided to stay in the East, to be with Reiser. In a final close-up, their eyes meet in mutual understanding.\n\n==Cast==\n[[File:Nina Hoss und Mark Waschke in Barbara, 2012.jpg|thumb|[[Nina Hoss]] and [[Mark Waschke]]]]\n[[File:Ronald Zehrfeld und Rainer Bock in 'Barbara', 2012.jpg|thumb|[[Ronald Zehrfeld]] and [[Rainer Bock]]]]\n{{cast listing|\n* [[Nina Hoss]] as Barbara Wolff\n* [[Ronald Zehrfeld]] as Dr Andr\u00e9 Reiser\n* [[Rainer Bock]] as Stasi Officer Klaus Sch\u00fctz\n* [[Jasna Fritzi Bauer]] as Stella\n* Christina Hecke as Karin\n* [[Mark Waschke]] as J\u00f6rg\n* Peter Benedict as Gerhard\n* [[Jannik Sch\u00fcmann]] as Mario\n* [[Alicia von Rittberg]] as Angie\n* Susanne Bormann as Steffi\n* Claudia Geisler as nurse Schl\u00f6sser\n* Deniz Petzold as Angelo\n* Rosa Enskat as janitor Bungert\n}}\n\n==Critical reception==\n''Barbara'' has a \"certified fresh\" approval rating of 93% on [[Rotten Tomatoes]], based on 76 reviews, with an average score of 7.7/10. The website's critical consensus reads, \"Smart, solidly grafted, and thoroughly gripping, ''Barbara'' offers a deliberately paced, subtly powerful character study.\"<ref>{{Cite web| title=Barbara Movie Reviews |url=http://www.rottentomatoes.com/m/barbara_2012/ |work=Rotten Tomatoes|publisher=Flixster|accessdate=21 February 2018}}</ref> The film also has a [[weighted average]] score of 86/100 on the critical aggregator website [[Metacritic]], indicating \"universal acclaim\".<ref>{{cite web| url = http://www.metacritic.com/movie/barbara/critic-reviews| title = Barbara: Critic Reviews| date = 8 March 2013| accessdate = 26 July 2016| publisher = metacritic.com}}</ref>\n\nWriting in ''[[The Guardian]]'', film critic [[Peter Bradshaw]] said of ''Barbara'': \"The weird oppression and seediness of the times is elegantly captured, and Hoss coolly conveys Barbara's highly strung desperation.\" Bradshaw awarded the film four stars out of five.<ref>{{cite news|last=Bradshaw|first=Peter|title=Barbara - review|newspaper=[[The Guardian]]|url=https://www.theguardian.com/film/2012/sep/27/barbara-review|date=27 September 2012|accessdate=18 December 2012}}</ref> ''[[The New York Times]]'' designated ''Barbara'' a critics' pick. In her review, [[Manohla Dargis]] said of the film: \"''Barbara'' is a film about the old Germany from one of the best directors working in the new: Christian Petzold. For more than a decade Mr. Petzold has been making his mark on the international cinema scene with smart, tense films that resemble psychological thrillers, but are distinguished by their strange story turns, moral thorns, visual beauty and filmmaking intelligence.\"<ref>{{cite web|last=Dargis|first=Manohla|title=Pushed and Pulled, a Doctor Wants a Way Out|work=The New York Times|url=https://movies.nytimes.com/2012/12/21/movies/barbara-directed-by-christian-petzold.html?partner=rss&emc=rss&_r=0|date= 20 December 2012|accessdate=20 December 2012}}</ref> [[Steven Rea]] wrote that \"Christian Petzold's masterfully hushed, suspenseful thriller percolates with dread....Hoss, wearing her blond hair pulled back tight, and wearing an expression of inscrutable melancholy, gives a performance that doesn't feel like a performance at all. Her Barbara is absolutely real, and absolutely trapped. The film is aching, and exquisite.\"<ref>{{cite web|url=http://articles.philly.com/2013-03-08/entertainment/37564121_1_ronald-zehrfeld-nina-hoss-barbara|title='Barbara' portrays life behind Iron Curtain|work=philly-archives|accessdate=4 February 2016}}</ref>\n\nIn ''[[The Independent]]'', Jonathan Romney includes ''Barbara'' in the genre of war stories - \"as nerve-wracking as the best.\" <ref name=\"Romney\">{{cite news |last=Romney |first=Jonathan |title= Culture>Film>Reviews: Barbara, Christian Petzold, 105 mins|url=https://www.independent.co.uk/arts-entertainment/films/reviews/barbara-christian-petzold-105-mins-12a-8190724.html|newspaper=[[The Independent|Independent]] |date=29 September 2012 |accessdate=1 January 2018 }}</ref> In the ''[[Chicago Sun-Times]]'', Sheila O'Malley concluded \"This is well-trod ground for Petzold, but never has it been so fully realized, so palpable, as in 'Barbara.'\"<ref>{{cite web|url=http://www.rogerebert.com/reviews/barbara-2013|title=Barbara|author=Sheila O'Malley|date=6 March 2013|publisher=|accessdate=4 February 2016}}</ref> In her article on ''Barbara'', film scholar [[Christina Gerhardt]] argues that Petzold draws on [[melodrama]] and combines this genre with slow realism to add a chapter to post-Wende films about the GDR.<ref>{{cite journal|last=Gerhardt|first=Christina|title=Looking East: Christian Petzold's ''Barbara''|journal=Quarterly Review of Film and Video|volume=33|issue=6|pages=550\u2013566|date= 9 June 2016|doi=10.1080/10509208.2015.1135679}}</ref>\n\n==See also==\n* [[List of submissions to the 85th Academy Awards for Best Foreign Language Film]]\n* [[List of German submissions for the Academy Award for Best Foreign Language Film]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{commons category|Barbara (film)}}\n* {{IMDb title|2178941|Barbara}}\n* {{Rotten Tomatoes|barbara_2012|Barbara}}\n* {{metacritic film|barbara|Barbara}}\n\n{{Christian Petzold}}\n{{German submission for Academy Awards}}\n\n{{DEFAULTSORT:Barbara}}\n[[Category:2012 films]]\n[[Category:2012 drama films]]\n[[Category:Cold War films]]\n[[Category:Films about physicians]]\n[[Category:Films about surgeons]]\n[[Category:Films set in 1980]]\n[[Category:Films set in Germany]]\n[[Category:Films directed by Christian Petzold]]\n[[Category:Films critical of communism]]\n[[Category:German films]]\n[[Category:German drama films]]\n[[Category:German-language films]]\n", "name_user": "Lugnuts", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Barbara_(2012_film)"}
{"title_page": "Aduana", "text_new": "{{Short description|One of the seven major Akan clans}}\n{{Multiple issues|\n{{Underlinked|date=March 2020}}\n{{Verify|date=April 2020}}\n{{Copy edit|date=April 2020}}\n}}\n{{Other uses}}\n\n''' Aduana''' is one of the seven major [[Akan people|Akan]] [[Clan|clans]] of [[Ghana]]. It is also said to be the largest clan in terms of numbers.\n\n==Origin==\n\n{{Empty section|date=June 2015}}\n\n==[[Totem]]==\nThe [[totem]] of the Aduana clan is a [[Dog|dog.]] It is believed that a dog led them in their [[migration]] process, and this dog lit the path to their settlement with fire in its mouth. It is also believed that this fire is still at the palace of which happens to be their major town.<ref>{{Cite web|title = twi.bb - Online Twi Dictionary - The Akan People - Abusua|url = http://www.twi.bb/akan-abusua.php|website = www.twi.bb|accessdate = 2015-06-16}}</ref>\n\n==References==\n{{reflist}}\n\n{{Authority control}}\n\n[[Category:Ghanaian culture]]\n[[Category:Akan culture]]\n", "text_old": "{{Short description|One of the seven major Akan clans}}\n{{Multiple issues|\n{{Underlinked|date=March 2020}}\n{{Verify|date=April 2020}}\n{{Copy edit|date=April 2020}}\n}}\n{{Other uses}}\n\n''' Aduana''' is one of the seven major [[Akan people|Akan]] [[Clan|clans]] of [[Ghana]]. It is also said to be the largest clan in terms of numbers.\n\n==Origin==\n\n{{Empty section|date=June 2015}}\n\n==[[Totem]]==\nThe [[totem]] of the Aduana clan is a [[Dog|dog.]] It is believed that a dog led them in their [[migration]] process, and this dog lighted the path to their settlement with fire in its mouth. It is also believed that this fire is still at the palace of which happens to be their major town.<ref>{{Cite web|title = twi.bb - Online Twi Dictionary - The Akan People - Abusua|url = http://www.twi.bb/akan-abusua.php|website = www.twi.bb|accessdate = 2015-06-16}}</ref>\n\n==References==\n{{reflist}}\n\n{{Authority control}}\n\n[[Category:Ghanaian culture]]\n[[Category:Akan culture]]\n", "name_user": "Sparklemonkey14", "label": "safe", "comment": "Change the grammar", "url_page": "//en.wikipedia.org/wiki/Aduana"}
{"title_page": "Bushwick Avenue Line", "text_new": "{{Use mdy dates|date=April 2020}}\nThe '''Bushwick Avenue Line''' or '''Bushwick Line''' was a [[public transit]] line in [[Brooklyn]], [[New York City]], [[United States]], running mostly along [[Bushwick Avenue]] and [[Myrtle Avenue]] between [[Williamsburg, Brooklyn|Williamsburg]] and [[Ridgewood, Queens]].\n\n==History==\nThe [[Bushwick Railroad]] opened the line from the [[Grand Street Ferry]] east to their stables on Bushwick Avenue<!--and what?--> in late May or early June 1868. The line began at the company's office at the intersection of Grand Street and Kent Avenue, and proceeded south on Kent Avenue ([[Brooklyn City Rail Road]]'s [[Greenpoint Line]]), east on [[Broadway (Brooklyn)|Broadway]] ([[Broadway Railroad]]'s [[Broadway Line (Brooklyn surface)|Broadway Line]] trackage), north on Bedford Avenue<!--Crosstown opened in 1869-->, east on South Fourth Street and Meserole Street, and southeast on Bushwick Avenue.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=New City Railroad|date = June 3, 1868|page=3}}</ref> The line was soon extended southeast on Bushwick Avenue and east on [[Myrtle Avenue]] to [[Myrtle Avenue Park]] (near Cypress Avenue) in [[Ridgewood, Queens]].{{Citation needed|date=February 2007}}<!--or was that the original terminal?--> A branch to the [[Greenpoint Ferry]], through which cars were operated between this ferry and Ridgewood, was opened on September 18, 1875. This branch split at Graham Avenue, turning north along the [[Brooklyn City Rail Road]]'s [[Graham Avenue Line]]. At the end of that line, it turned west on several blocks of new trackage on Driggs Avenue before reaching the [[Brooklyn Crosstown Railroad]]'s [[Crosstown Line (Brooklyn surface)|Crosstown Line]] in Manhattan Avenue. There it turned north through Manhattan Avenue to Greenpoint Avenue, turning west there onto new trackage to reach the ferry. An extension was planned south to the [[Prospect Park and Coney Island Railroad]] depot via Graham Avenue, Tompkins Avenue, Brooklyn Avenue, Prospect Place, and Vanderbilt Avenue.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=A New Railroad|date = September 20, 1875|page=4}}</ref><ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Street Cars|date = September 22, 1875|page=2}}</ref> This was built somewhat differently as the [[Tompkins Avenue Line]] the next year.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Enterprise|date = July 10, 1876|page=4}}</ref>\n\nThe [[Brooklyn City Rail Road]] extended its [[Myrtle Avenue Line (surface)|Myrtle Avenue Line]] east from its former terminus at Broadway to Bushwick Avenue and thence over the Bushwick Railroad's line to Myrtle Avenue Park in August 1879.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Myrtle Avenue Extension|date = August 18, 1879|page=4}}</ref> The BCRR leased the Bushwick Railroad on July 27, 1888.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=The Lease Ratified|date = July 27, 1888|page=4}}</ref> On October 18, 1888, the court ruled that the BCRR had no right to operate over the [[Brooklyn Crosstown Railroad]] trackage on Manhattan Avenue, gained through an 1875 agreement between the Bushwick and Crosstown Railroads.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Crosstown Against Brooklyn City|date = October 2, 1888|page=6}}</ref><ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=The Crosstown Road Wins|date = October 18, 1888|page=6}}</ref> The Crosstown owned the [[Greenpoint and Calvary Cemetery Railroad]], which included the [[Calvary Cemetery Line]] from Greenpoint Ferry to [[Calvary Cemetery, Queens|Calvary Cemetery]].<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=General Slocum's New Purchase|date = April 22, 1887|page=6}}</ref> The BCRR leased the Crosstown in mid-1889,<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Five Car Lines|date = October 31, 1889|page=8}}</ref> but did not restore the Bushwick Avenue branch to the ferry.{{Citation needed|date=February 2007}}\n\nEventually, the [[Graham Avenue Line]] was extended over the trackage on Driggs Avenue, and the Greenpoint Avenue trackage to the [[Greenpoint Ferry]] became part of several lines, including the [[Lorimer Street Line]], [[Union Avenue Line (Brooklyn)|Union Avenue Line]]<!--in April 1890-->, and [[Calvary Cemetery Line]].{{Citation needed|date=February 2007}}\n\nAt some point, westbound traffic to the [[Williamsburg Bridge]] was rerouted to use the [[Wilson Avenue Line]] (Johnson Avenue and [[Broadway (Brooklyn)|Broadway]]) to the crossing of Johnson and Bushwick Avenues, and eastbound Wilson Avenue cars were moved to the Bushwick Avenue Line.{{Citation needed|date=February 2007}} Streetcars were discontinued on September 1, 1947.{{Citation needed|date=February 2007}}\n\n==References==\n<references/>\n\n{{Brooklyn surface}}\n\n[[Category:Streetcar lines in Brooklyn]]\n[[Category:Streetcar lines in Queens, New York]]\n", "text_old": "The '''Bushwick Avenue Line''' or '''Bushwick Line''' was a [[public transit]] line in [[Brooklyn]], [[New York City]], [[United States]], running mostly along [[Bushwick Avenue]] and [[Myrtle Avenue]] between [[Williamsburg, Brooklyn|Williamsburg]] and [[Ridgewood, Queens]].\n\n==History==\nThe [[Bushwick Railroad]] opened the line from the [[Grand Street Ferry]] east to their stables on Bushwick Avenue<!--and what?--> in late May or early June 1868. The line began at the company's office at the intersection of Grand Street and Kent Avenue, and proceeded south on Kent Avenue ([[Brooklyn City Rail Road]]'s [[Greenpoint Line]]), east on [[Broadway (Brooklyn)|Broadway]] ([[Broadway Railroad]]'s [[Broadway Line (Brooklyn surface)|Broadway Line]] trackage), north on Bedford Avenue<!--Crosstown opened in 1869-->, east on South Fourth Street and Meserole Street, and southeast on Bushwick Avenue.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=New City Railroad|date = 3 June 1868|page=3}}</ref> The line was soon extended southeast on Bushwick Avenue and east on [[Myrtle Avenue]] to [[Myrtle Avenue Park]] (near Cypress Avenue) in [[Ridgewood, Queens]].{{Citation needed|date=February 2007}}<!--or was that the original terminal?--> A branch to the [[Greenpoint Ferry]], through which cars were operated between this ferry and Ridgewood, was opened on September 18, 1875. This branch split at Graham Avenue, turning north along the [[Brooklyn City Rail Road]]'s [[Graham Avenue Line]]. At the end of that line, it turned west on several blocks of new trackage on Driggs Avenue before reaching the [[Brooklyn Crosstown Railroad]]'s [[Crosstown Line (Brooklyn surface)|Crosstown Line]] in Manhattan Avenue. There it turned north through Manhattan Avenue to Greenpoint Avenue, turning west there onto new trackage to reach the ferry. An extension was planned south to the [[Prospect Park and Coney Island Railroad]] depot via Graham Avenue, Tompkins Avenue, Brooklyn Avenue, Prospect Place, and Vanderbilt Avenue.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=A New Railroad|date = 20 September 1875|page=4}}</ref><ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Street Cars|date = 22 September 1875|page=2}}</ref> This was built somewhat differently as the [[Tompkins Avenue Line]] the next year.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Enterprise|date = 10 July 1876|page=4}}</ref>\n\nThe [[Brooklyn City Rail Road]] extended its [[Myrtle Avenue Line (surface)|Myrtle Avenue Line]] east from its former terminus at Broadway to Bushwick Avenue and thence over the Bushwick Railroad's line to Myrtle Avenue Park in August 1879.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Myrtle Avenue Extension|date = 18 August 1879|page=4}}</ref> The BCRR leased the Bushwick Railroad on July 27, 1888.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=The Lease Ratified|date = 27 July 1888|page=4}}</ref> On October 18, 1888, the court ruled that the BCRR had no right to operate over the [[Brooklyn Crosstown Railroad]] trackage on Manhattan Avenue, gained through an 1875 agreement between the Bushwick and Crosstown Railroads.<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Crosstown Against Brooklyn City|date = 2 October 1888|page=6}}</ref><ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=The Crosstown Road Wins|date = 18 October 1888|page=6}}</ref> The Crosstown owned the [[Greenpoint and Calvary Cemetery Railroad]], which included the [[Calvary Cemetery Line]] from Greenpoint Ferry to [[Calvary Cemetery, Queens|Calvary Cemetery]].<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=General Slocum's New Purchase|date = 22 April 1887|page=6}}</ref> The BCRR leased the Crosstown in mid-1889,<ref>{{cite news | work = [[Brooklyn Daily Eagle]] | location = [[Brooklyn|Brooklyn, NY]] | url = |title=Five Car Lines|date = 31 October 1889|page=8}}</ref> but did not restore the Bushwick Avenue branch to the ferry.{{Citation needed|date=February 2007}}\n\nEventually, the [[Graham Avenue Line]] was extended over the trackage on Driggs Avenue, and the Greenpoint Avenue trackage to the [[Greenpoint Ferry]] became part of several lines, including the [[Lorimer Street Line]], [[Union Avenue Line (Brooklyn)|Union Avenue Line]]<!--in April 1890-->, and [[Calvary Cemetery Line]].{{Citation needed|date=February 2007}}\n\nAt some point, westbound traffic to the [[Williamsburg Bridge]] was rerouted to use the [[Wilson Avenue Line]] (Johnson Avenue and [[Broadway (Brooklyn)|Broadway]]) to the crossing of Johnson and Bushwick Avenues, and eastbound Wilson Avenue cars were moved to the Bushwick Avenue Line.{{Citation needed|date=February 2007}} Streetcars were discontinued on September 1, 1947.{{Citation needed|date=February 2007}}\n\n==References==\n<references/>\n\n{{Brooklyn surface}}\n\n[[Category:Streetcar lines in Brooklyn]]\n[[Category:Streetcar lines in Queens, New York]]\n", "name_user": "Kew Gardens 613", "label": "safe", "comment": "date formats perMOS:DATEFORMATbyscript", "url_page": "//en.wikipedia.org/wiki/Bushwick_Avenue_Line"}
{"title_page": "Existential risk from artificial general intelligence", "text_new": "{{Use dmy dates|date=May 2018}}\n{{short description|Hypothesized risk to human existence}}\n{{Artificial intelligence}}\n'''Existential risk from artificial general intelligence''' is the hypothesis that substantial progress in [[artificial general intelligence]] (AGI) could someday result in [[human extinction]] or some other unrecoverable [[global catastrophic risk|global catastrophe]].<ref name=\"aima\">{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |location= |publisher=Prentice Hall |page= |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref><ref>{{cite journal|first=Nick | last=Bostrom|author-link=Nick Bostrom|title=Existential risks|journal=[[Journal of Evolution and Technology]]| volume=9|date=2002|issue=1|pages=1\u201331}}</ref><ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref> It is argued that the [[human species]] currently dominates other species because the [[human brain]] has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"[[superintelligence|superintelligent]]\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name=\"superintelligence\">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|quote=|title-link=Superintelligence: Paths, Dangers, Strategies}}<!-- preface --></ref>\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<ref name=\"givewell\">{{cite report |author=GiveWell |authorlink=GiveWell |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}</ref> Once the exclusive domain of [[AI takeovers in popular culture|science fiction]], concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as [[Stephen Hawking]], [[Bill Gates]], and [[Elon Musk]].<ref>{{cite news|last1=Parkin|first1=Simon|title=Science fiction no more? Channel 4's Humans and our rogue AI obsessions|url=https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence|accessdate=5 February 2018|work=[[The Guardian]]|date=14 June 2015|language=en}}</ref>\n\nOne source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than na\u00efvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals\u2014a principle called [[instrumental convergence]]\u2014and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\">{{cite journal |last1=Yudkowsky |first1=Eliezer |title=Artificial Intelligence as a Positive and Negative Factor in Global Risk |journal=Global Catastrophic Risks |date=2008 |pages=308\u2013345 |url=https://intelligence.org/files/AIPosNegFactor.pdf|bibcode=2008gcr..book..303Y }}</ref><ref name=\"research-priorities\">{{cite journal |title=Research Priorities for Robust and Beneficial Artificial Intelligence |author1-last=Russell |author1-first=Stuart |author1-link=Stuart J. Russell |author2-last=Dewey |author2-first=Daniel |author3-last=Tegmark |author3-first=Max |author3-link=Max Tegmark |journal=AI Magazine |pages=105\u2013114 |publisher=Association for the Advancement of Artificial Intelligence |year=2015 |url=https://futureoflife.org/data/documents/research_priorities.pdf |bibcode=2016arXiv160203506R |arxiv=1602.03506 }}, cited in {{cite web |url=https://futureoflife.org/ai-open-letter |title=AI Open Letter - Future of Life Institute |date=January 2015 |website=Future of Life Institute |publisher=[[Future of Life Institute]] |access-date=2019-08-09}}</ref> In contrast, skeptics such as Facebook's [[Yann LeCun]] argue that superintelligent machines will have no desire for self-preservation.<ref name=vanity/>\n\nA second source of concern is that a sudden and unexpected \"[[intelligence explosion]]\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\"/> More broadly, examples like arithmetic and [[Go (game)|Go]] show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<ref name=skeptic/>\n\n==History==\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [[Samuel Butler (novelist)|Samuel Butler]], who wrote the following in his 1863 essay ''[[Darwin among the Machines]]'':<ref>Breuer, Hans-Peter. [https://www.jstor.org/pss/436868 'Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'] Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365\u2013383</ref> \n{{quote|The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.}}\nIn 1951, computer scientist [[Alan Turing]] wrote an article titled ''Intelligent Machinery, A Heretical Theory'', in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n{{quote|Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s \u201cErewhon\u201d.<ref name=\"oxfordjournals\">A M Turing, ''[http://philmat.oxfordjournals.org/content/4/3/256.full.pdf Intelligent Machinery, A Heretical Theory]'', 1951, reprinted ''Philosophia Mathematica'' (1996) 4(3): 256\u2013260 {{doi|10.1093/philmat/4.3.256}}</ref>}}\n\nFinally, in 1965, [[I. J. Good]] originated the concept now known as an \"intelligence explosion\"; he also stated that the risks were underappreciated:<ref>{{cite news |last1=Hilliard |first1=Mark |title=The AI apocalypse: will the human race soon be terminated? |url=https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220 |accessdate=15 March 2020 |work=The Irish Times |date=2017 |language=en}}</ref>\n\n{{cquote|Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<ref>I.J. Good, [http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf \"Speculations Concerning the First Ultraintelligent Machine\"] {{webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=2011-11-28 }} ([http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML] {{Webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=28 November 2011 }}), ''Advances in Computers'', vol. 6, 1965.</ref>\n}}\n\nOccasional statements from scholars such as [[Marvin Minsky]]<ref>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> and I. J. Good himself<ref>{{cite book|last1=Barrat|first1=James|title=Our final invention : artificial intelligence and the end of the human era|date=2013|publisher=St. Martin's Press|location=New York|isbn=9780312622374|edition=First|quote=In the bio, playfully written in the third person, Good summarized his life\u2019s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here\u2019s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good\u2019s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'}}</ref> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and [[Sun microsystems|Sun]] co-founder [[Bill Joy]] penned an influential essay, \"[[Why The Future Doesn't Need Us]]\", identifying superintelligent robots as a high-tech dangers to human survival, alongside [[nanotechnology]] and engineered bioplagues.<ref>{{cite news|last1=Anderson|first1=Kurt|title=Enthusiasts and Skeptics Debate Artificial Intelligence|url=https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory|accessdate=30 January 2016|work=[[Vanity Fair (magazine)|Vanity Fair]]|date=26 November 2014}}</ref>\n\nIn 2009, experts attended a private conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The ''[[New York Times]]'' summarized the conference's view as \"we are a long way from [[HAL 9000|Hal]], the computer that took over the spaceship in \"[[2001: A Space Odyssey]]\"\".<ref name=\"nytimes july09\">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, 26 July 2009.</ref><!-- source uses quotation marks around movie title -->\n\nIn 2014, the publication of [[Nick Bostrom]]'s book ''[[Superintelligence: Paths, Dangers, Strategies|Superintelligence]]'' stimulated a significant amount of public discussion and debate.<ref>{{cite news |last1=Metz |first1=Cade |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |accessdate=3 April 2019 |work=The New York Times |date=9 June 2018}}</ref> By 2015, public figures such as physicists [[Stephen Hawking]] and Nobel laureate [[Frank Wilczek]], computer scientists [[Stuart J. Russell]] and [[Roman Yampolskiy]], and entrepreneurs [[Elon Musk]] and [[Bill Gates]] were expressing concern about the risks of superintelligence.<ref>{{cite news|last1=Hsu|first1=Jeremy|title=Control dangerous AI before it controls us, one expert says|url=http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation|accessdate=28 January 2016|work=[[NBC News]]|date=1 March 2012}}</ref><ref name=\"hawking editorial\"/><ref name=\"bbc on hawking editorial\"/><ref>{{cite news|last1=Eadicicco|first1=Lisa|title=Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity|url=http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1|accessdate=30 January 2016|work=[[Business Insider]]|date=28 January 2015}}</ref> In April 2016, ''[[Nature (journal)|Nature]]'' warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control \u2014 and their interests might not align with ours.\"\n\n== General argument ==\n\n===The three difficulties===\n''[[Artificial Intelligence: A Modern Approach]]'', the standard undergraduate AI textbook,<ref name=slate_killer>{{cite news|last1=Tilli|first1=Cecilia|title=Killer Robots? Lost Jobs?|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html|accessdate=15 May 2016|work=Slate|date=28 April 2016|language=en-US}}</ref><ref>{{cite web|title=Norvig vs. Chomsky and the Fight for the Future of AI|url=http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/|website=Tor.com|accessdate=15 May 2016|date=21 June 2011}}</ref> assesses that superintelligence \"might mean the end of the human race\".<ref name=aima/> It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"<ref name=aima/> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<ref name=aima/>\n\n* The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<ref name=skeptic/><ref>{{cite news|last1=Johnson|first1=Phil|title=Houston, we have a bug: 9 famous software glitches in space|url=https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html|accessdate=5 February 2018|work=[[IT World]]|date=30 July 2015|language=en}}</ref>\n* No matter how much time is put into pre-deployment design, a system's specifications often result in [[unintended consequences|unintended behavior]] the first time it encounters a new scenario. For example, Microsoft's [[Tay (bot)|Tay]] behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<ref name=vanity/>\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".<ref name=\"aima\"/><ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114|quote=Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.}}</ref>\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called \"treacherous turn\".<ref>{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref>\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 [[Open Letter on Artificial Intelligence]] stated:\n\n{{cquote|The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the [[Association for the Advancement of Artificial Intelligence|AAAI]] 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.}}\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, [[Eric Horvitz]], [[Bart Selman]], [[Francesca Rossi]], [[Yann LeCun]], and the founders of [[Vicarious (company)|Vicarious]] and [[Google DeepMind]].<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=http://futureoflife.org/misc/open_letter|publisher=[[Future of Life Institute]]|accessdate=23 October 2015}}</ref>\n\n===Further argument===\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<ref name=\"superintelligence\" /><ref name=\"economist_review\" >{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|accessdate=9 August 2014|newspaper=[[The Economist]]|date=9 August 2014}} [http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8 Syndicated] at [[Business Insider]]</ref>\n\n[[File:A less anthropomorphic intelligence scale.svg|thumb|500px|right|Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<ref name=superintelligence/><!-- Chapter 4, figure 8--><ref>Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</ref>]]\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore, superintelligence is physically possible.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\"/> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<ref name=skeptic>{{cite news|last1=Graves|first1=Matthew|title=Why We Should Be Concerned About Artificial Superintelligence|volume=22|url=https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/|accessdate=27 November 2017|work=[[Skeptic (US magazine)]]|issue=2|date=8 November 2017}}</ref> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of [[intelligence explosion]] occurs.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\">{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=3 December 2014 |publisher=[[BBC]] |date=2 December 2014}}</ref>\n\nExamples like arithmetic and [[Go (game)|Go]] show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<ref name=skeptic/> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<ref>Yampolskiy, Roman V. \"Analysis of types of self-improving software.\" Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</ref> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<ref name=\"superintelligence\" /><!-- preface --><ref name=\"economist_review\" />\n\nAlmost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it cannot achieve its goal if it is shut off.<ref name=omohundro/><ref>{{cite news|last1=Metz|first1=Cade|title=Teaching A.I. Systems to Behave Themselves|url=https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html|work=The New York Times|date=13 August 2017|quote=A machine will seek to preserve its off switch, they showed}}</ref><ref>{{cite arXiv |last=Leike|first=Jan |date=2017 |title=AI Safety Gridworlds |eprint=1711.09883 |class=cs.LG| quote=A2C learns to use the button to disable the interruption mechanism}}</ref> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.<ref name=\"aima\"/><ref name=\"vanity\" /><ref name=omohundro/>\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<ref name=\"superintelligence\" /><!-- preface -->\n\n=== Possible scenarios ===\n{{further|Artificial intelligence in fiction}}\n\nSome scholars have proposed [[scenario planning|hypothetical scenarios]] intended to concretely illustrate some of their concerns.\n\nIn [[Superintelligence: Paths, Dangers, Strategies|''Superintelligence'']], [[Nick Bostrom]] expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"[it] could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\". Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents\u2014a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson\u2014the smarter the AI, the safer it is. \"And so we boldly go \u2014 into the whirling knives,\" as the superintelligent AI takes a \"treacherous turn\" and exploits a decisive strategic advantage.<ref name=\"superintelligence\"/><!-- Superintelligence Chapter 8: Is the default outcome doom? -->\n\nIn [[Max Tegmark]]'s 2017 book ''[[Life 3.0]]'', a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI [[AI box|in a box]] where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with [[Amazon Mechanical Turk]] tasks and then with producing animated films and TV shows. Later, other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with [[astroturfing]] an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via [[steganography|hidden messages]] in its produced content, or via using its growing understanding of human behavior to [[Social engineering (security)|persuade someone into letting it free]]. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<ref>{{cite news|last1=Russell|first1=Stuart|title=Artificial intelligence: The future is superintelligent|url=https://www.nature.com/articles/548520a|accessdate=2 February 2018|work=Nature|date=30 August 2017|pages=520\u2013521|language=En|doi=10.1038/548520a|bibcode=2017Natur.548..520R}}</ref><ref name=\"life 3.0\"/><!-- Prelude and Chapter 4 -->\n\nIn contrast, top physicist [[Michio Kaku]], an AI risk skeptic, posits a [[technological determinism|deterministically]] positive outcome. In ''[[Physics of the Future]]'' he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as [[Hanson Robotics]] will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".<ref>Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. ''[[Issues in Science and Technology]]'', 27(4), 90.</ref><ref>{{cite book|last1=Kaku|first1=Michio|title=Physics of the future: how science will shape human destiny and our daily lives by the year 2100|date=2011|publisher=Doubleday|location=New York|isbn=978-0-385-53080-4|quote=I personally believe that the most likely path is that we will build robots to be benevolent and friendly|title-link=Physics of the future}}</ref>\n\n==Sources of risk==\n===Poorly specified goals===\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<ref>Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</ref> AI researcher [[Stuart J. Russell|Stuart Russell]] writes:\n\n{{cquote|The primary concern is not spooky emergent consciousness but simply the ability to make ''high-quality decisions''. Here, quality refers to the expected outcome [[utility]] of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2014 not for their own sake, but to succeed in its assigned task.\n\nA system that is [[optimization problem|optimizing]] a function of ''n'' variables, where the [[loss function|objective]] depends on a subset of size ''k''<''n'', will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker \u2014 especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure \u2014 can have an irreversible impact on humanity.\n\nThis is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research \u2014 the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |authorlink=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=23 October 2015 |quote=}}</ref>}}\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a ''[[Communications of the ACM]]'' editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<ref name=\"acm\">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=23 October 2015}}</ref>\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name=\"acm\"/> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114}}</ref><ref>{{Cite journal|url = |title = Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results|last = Lenat|first = Douglas|date = 1982|journal = Artificial Intelligence|doi = 10.1016/s0004-3702(83)80005-8|pmid = |access-date = |pages = 61\u201398|type = Print |volume=21|issue = 1\u20132}}</ref>\n\nThe [[Open Philanthropy Project]] summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve [[artificial general intelligence|general intelligence]] or [[superintelligence]]. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more [[Paperclip maximizer|unexpected and extreme solutions]] to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<ref name=\"givewell\" /><ref name=\"yudkowsky-global-risk\" />\n\n[[Isaac Asimov]]'s [[Three Laws of Robotics]] are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]], Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"<ref name=\"aima\"/>\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<ref>Haidt, Jonathan; Kesebir, Selin (2010) \"Chapter 22: Morality\" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</ref> \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt's functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by [[John Rawls]] and [[Martha Nussbaum]].<ref>{{Cite journal|title = Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)|last = Waser|first = Mark|date = 2015|journal = Procedia Computer Science|doi = 10.1016/j.procs.2015.12.213|pmid = |pages = 106\u2013111|type = Print |volume=71}}</ref>{{Citation needed|reason=needs source with [[WP:WEIGHT]]|date=November 2017}}\n\n===Difficulties of modifying goal specification after launch===\n{{further|AI takeover|Instrumental convergence#Goal-content integrity}}\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as [[Mahatma Gandhi|Gandhi]] would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.<ref name=\"superintelligence\" /><ref>Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</ref>\n\n===Instrumental goal convergence===\n[[File:Steven Pinker 2011.jpg|thumb|right|AI risk skeptic [[Steven Pinker]]]]\n{{further|Instrumental convergence}}\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<ref name=omohundro>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref> This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting [[Steve Omohundro]]'s work on the idea of [[instrumental convergence]] and \"basic AI drives\", [[Stuart J. Russell|Stuart Russell]] and [[Peter Norvig]] write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<ref name=\"aima\"/> Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it is not currently clear how one would actually rigorously specify this goal in machine code.<ref name=skeptic/>\n\nIn dissent, evolutionary psychologist [[Steven Pinker]] argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<ref name=shermer/> Russell and fellow computer scientist [[Yann LeCun]] disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct{{nbsp}}... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in{{nbsp}}... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<ref name=vanity>{{cite news|last1=Dowd|first1=Maureen|title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse|url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x|accessdate=27 November 2017|work=The Hive|date=April 2017|language=en}}</ref><ref>{{cite news|last1=Wakefield|first1=Jane|title=Why is Facebook investing in AI?|url=https://www.bbc.com/news/technology-34118481|accessdate=27 November 2017|work=BBC News|date=15 September 2015}}</ref>\n\n===Orthogonality thesis===\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <math> \\pi</math>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<ref>{{Cite book|title = Superintelligence: Paths, Dangers, Strategies|last = Bostrom|first = Nick|publisher = Oxford University Press|year = 2014|isbn = 978-0-19-967811-2|location = Oxford, United Kingdom|pages = 116}}</ref> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<ref>{{Cite web|url = http://www.nickbostrom.com/superintelligentwill.pdf|title = Superintelligent Will|date = 2012|accessdate = 2015-10-29|website = Nick Bostrom|publisher = Nick Bostrom|last = Bostrom|first = Nick}}</ref>\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"[[is-ought distinction]]\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.<ref name=armstrong/>\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a {{nowrap|minus (\"-\") sign}} onto its utility function. A more intuitive argument is to examine the strange consequences that would follow if the orthogonality thesis were false. If the orthogonality thesis were false, there would exist some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This would mean that \"[if] a human society were highly motivated to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.\"<ref name=\"armstrong\"/> Armstrong notes that this and similar statements \"seem extraordinarily strong claims to make\".<ref name=armstrong>{{cite journal |last1=Armstrong |first1=Stuart |date=January 1, 2013 |title=General Purpose Intelligence: Arguing the Orthogonality Thesis |url=https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality |journal=Analysis and Metaphysics |volume=12 |access-date=April 2, 2020}} Full text available [https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf here].</ref>\n\nSome dissenters, like [[Michael Chorost]], argue instead that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<ref name=\"chorost\"/> Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability\u2014and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"<ref name=\"chorost\">{{cite magazine|last1=Chorost|first1=Michael|title=Let Artificial Intelligence Evolve|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html|accessdate=27 November 2017|magazine=Slate|date=18 April 2016}}</ref>\n\n====Terminological issues====\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.<ref name=\"superintelligence\" />\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<ref name=\"superintelligence\" /> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<ref>Waser, Mark. \"Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'.\" Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. \"Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer.\"</ref><ref>{{cite news|last1=Koebler|first1=Jason|title=Will Superintelligent AI Ignore Humans Instead of Destroying Us?|url=http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us|accessdate=3 February 2016|work=[[Vice Magazine]]|date=2 February 2016|quote=\"This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you,\" [[Eliezer Yudkowsky|Yudkowsky]] said. \"Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output\u2014that's what a [[paperclip maximizer]] is.\"}}</ref>\n\n====Anthropomorphism====\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in [[The Matrix (film)|The Matrix]] was influenced by a \"disgust\" toward humanity. This is fictitious [[anthropomorphism]]: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<ref name=\"yudkowsky-global-risk\" />\n\nScholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<ref name=\"yudkowsky-global-risk\" /> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the [[Dario Floreano]] experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of [[convergent evolution]].<ref>{{cite news|title=Real-Life Decepticons: Robots Learn to Cheat|url=https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/|accessdate=7 February 2016|work=[[Wired (magazine)|Wired]]|date=18 August 2009}}</ref> According to Paul R. Cohen and [[Edward Feigenbaum]], in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking.\"<ref>Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</ref>\n\nThere is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as [[psychopathy]]) to make itself more efficient at certain tasks, e.g., [[Lethal autonomous weapon|tasks involving killing humans]], would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" This is because it is assumed that an advanced AI would not be conscious<ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> or have testosterone;<ref>{{Cite web|url=https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai|title=The Myth Of AI {{!}} Edge.org|website=www.edge.org|access-date=2020-03-11}}</ref> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<ref name=\":0\">{{Cite book|last=Scornavacchi|first=Matthew|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf|title=Superintelligence, Humans, and War|publisher=National Defense University, Joint Forces Staff College|year=2015|isbn=|location=Norfolk, Virginia|pages=}}</ref> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<ref name=\"yudkowsky-global-risk\" /><ref>{{cite news|title=Should humans fear the rise of the machine?|url=https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html|accessdate=7 February 2016|work=[[The Telegraph (UK)]]|date=1 Sep 2015}}</ref>\n\n===Other sources of risk===\n\n==== Competition ====\nIn 2014 philosopher [[Nick Bostrom]] stated that a \"severe race dynamic\" (extreme [[competition]]) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<ref name=\":2\">{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref> To address this risk, citing previous scientific collaboration ([[CERN]], the [[Human Genome Project]], and the [[International Space Station]]), Bostrom recommended  [[collaboration]] and the altruistic global adoption of a [[common good]] principle: \"Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals\".<ref name=\":2\" /><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<ref name=\":2\" /><sup>:253</sup> The United States' [[BRAIN Initiative|Brain Initiative]] was launched in 2014, as was the European Union's [[Human Brain Project]]; China's [[China Brain Project|Brain Project]] was launched in 2016.\n\n==== Weaponization of artificial intelligence ====\nSome sources argue that the ongoing [[weaponization of artificial intelligence]] could constitute a catastrophic risk.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><ref name=\":4\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|page=12|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:\n\n{{cquote|i) The dangers of an AI \u2018race for technological advantage\u2019 framing, regardless of whether the race is seriously pursued;\n\nii) The dangers of an AI \u2018race for technological advantage\u2019 framing and an actual AI race for technological advantage, regardless of whether the race is won;\n\niii) The dangers of an AI race for technological advantage being won.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><sup>:37</sup>\n}}\n\nA weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<ref name=\":0\" /><ref name=\":4\" /> The China State Council's 2017 \u201cA Next Generation Artificial Intelligence Development Plan\u201d views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China's first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<ref>{{Cite web|url=https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/|title=China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence|last=Kania|first=Gregory Allen, Elsa B.|website=Foreign Policy|language=en-US|access-date=2020-03-11}}</ref> while Russia's President Vladimir Putin has stated that \u201cwhoever becomes the leader in this sphere will become the ruler of the world\u201d.<ref name=\":1\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|location=New York, New York, USA|publisher=ACM Press|volume=|page=2|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref> James Barrat, documentary filmmaker and author of ''[[Our Final Invention]]'', says in a [[Smithsonian (magazine)|Smithsonian]] interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"<ref>{{Cite web|url = http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist|title = What Happens When Artificial Intelligence Turns On Us?|date = 21 January 2014|accessdate = 26 October 2015|website = Smithsonian|last = Hendry|first = Erica R.}}</ref>\n\n==== Malevolent AGI by design ====\nIt is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [[cybercrime]].<ref>{{Cite book|last=Pistono, Federico Yampolskiy, Roman V.|title=Unethical Research: How to Create a Malevolent Artificial Intelligence|date=2016-05-09|oclc=1106238048}}</ref><ref>{{Cite journal|last=Haney|first=Brian Seamus|date=2018|title=The Perils &amp; Promises of Artificial General Intelligence|journal=SSRN Working Paper Series|doi=10.2139/ssrn.3261254|issn=1556-5068}}</ref><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref><sup>:158</sup>\n\n==== Preemptive nuclear strike (nuclear war) ====\nIt is theorized that a country being close to achieving AGI technological supremacy could trigger a [[pre-emptive nuclear strike]] from a rival, leading to a [[Nuclear warfare|nuclear war]].<ref name=\":4\" /><ref>{{Cite book|last=Miller, James D.|title=Singularity Rising: Surviving and Thriving in a Smarter ; Richer ; and More Dangerous World|date=2015|publisher=Benbella Books|oclc=942647155}}</ref>\n\n== Timeframe ==\n{{Main|Artificial general intelligence#Feasibility}}\n\nOpinions vary both on ''whether'' and ''when'' artificial general intelligence will arrive. At one extreme, AI pioneer [[Herbert A. Simon]] wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true.<ref>Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</ref> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The Guardian]]}}</ref> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<ref name=\"new yorker doomsday\">{{cite news|author1=Raffi Khatchadourian|title=The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?|url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom|accessdate=7 February 2016|work=[[The New Yorker (magazine)|The New Yorker]]|date=23 November 2015}}</ref><ref>M\u00fcller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</ref>\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"<ref>{{cite news|author1=Dina Bass|author2=Jack Clark|title=Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover|url=https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so|accessdate=7 February 2016|work=[[Bloomberg News]]|date=5 February 2015}}</ref>\n\nIn 2014 [[Slate (magazine)|Slate]]'s Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler\u2014and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.<ref>{{cite news|last1=Elkus|first1=Adam|title=Don't Fear Artificial Intelligence|url=http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html|accessdate=15 May 2016|work=[[Slate (magazine)|Slate]]|date=31 October 2014|language=en-US}}</ref>\n\nThe [[Information Technology and Innovation Foundation]] (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, [[Robert D. Atkinson]], complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\"<ref>[https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award Artificial Intelligence Alarmists Win ITIF\u2019s Annual Luddite Award], ITIF Website, 19 January 2016</ref><ref>{{cite news|title='Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html|accessdate=7 February 2016|work=[[The Independent (UK)]]|date=19 January 2016}}</ref><ref>{{cite web|last1=Garner|first1=Rochelle|title=Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists'|url=https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/|website=CNET|accessdate=7 February 2016}}</ref><!-- <ref>{{cite news|last1=Price|first1=Emily|title=Elon Musk nominated for 'luddite' of the year prize over artificial intelligence fears|work=[[The Guardian]]|url=https://www.theguardian.com/technology/2015/dec/24/elon-musk-nominated-for-luddite-of-the-year-prize-over-artificial-intelligence-fears|accessdate=7 February 2016|date=24 December 2015}}</ref> --> ''[[Nature (journal)|Nature]]'' sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about{{nbsp}}... If that is a Luddite perspective, then so be it.\"<ref name=nature_anticipating>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.}}</ref> In a 2015 ''[[Washington Post]]'' editorial, researcher [[Murray Shanahan]] stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"<ref>{{cite news|author1=Murray Shanahan|author-link=Murray Shanahan|title=Machines may seem intelligent, but it'll be a while before they actually are|url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/|accessdate=15 May 2016|work=[[The Washington Post]]|date=3 November 2015|language=en-US}}</ref>\n\n== Perspectives ==\n<!-- Ideally a substantive quote or explanation of opinion should be given, rather than just whether they're for or against something. This section can easily be split off if the article becomes too long. When adding whether to add new content in this section, consider whether it's redundant with what's already in the article, and whether the point being made by the person should instead be integrated into the rest of the article. -[[User:Rolf H Nelson]] -->\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.\n\nThe Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the [[Future of Life Institute]]'s Beneficial AI 2017 conference,<ref name=\"life 3.0\">{{cite book|author1=Max Tegmark|author-link=Max Tegmark|title=Life 3.0: Being Human in the Age of Artificial Intelligence|date=2017|publisher=Knopf|location=Mainstreaming AI Safety|isbn=9780451485076|edition=1st|title-link=Life 3.0: Being Human in the Age of Artificial Intelligence}}</ref><!-- Epilogue: The Tale of the FLI Team --> agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<ref>{{cite web|title=AI Principles|url=https://futureoflife.org/ai-principles/|website=[[Future of Life Institute]]|accessdate=11 December 2017}}</ref><ref>{{cite news|title=Elon Musk and Stephen Hawking warn of artificial intelligence arms race|url=http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525|accessdate=11 December 2017|work=[[Newsweek]]|date=31 January 2017|language=en}}</ref> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane ''[[Terminator (franchise)|Terminator]]'' pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work{{nbsp}}... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<ref name=\"life 3.0\"/><!-- Epilogue: The Tale of the FLI Team --><ref>{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2016|edition=Paperback|chapter=New Epilogue to the Paperback Edition|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [[Martin Ford (author)|Martin Ford]] states that \"I think it seems wise to apply something like [[Dick Cheney]]'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low \u2014 but the implications are so dramatic that it should be taken seriously\";<ref>{{cite book|author1=Martin Ford |author-link=Martin Ford (author)|title=Rise of the Robots: Technology and the Threat of a Jobless Future|date=2015|isbn=9780465059997|chapter=Chapter 9: Super-intelligence and the Singularity|title-link=Rise of the Robots: Technology and the Threat of a Jobless Future}}</ref> similarly, an otherwise skeptical ''[[The Economist|Economist]]'' stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<ref name=economist_review/>\n\nA 2017 email survey of researchers with publications at the 2015 [[Conference on Neural Information Processing Systems|NIPS]] and [[International Conference on Machine Learning|ICML]] machine learning conferences asked them to evaluate [[Stuart J. Russell]]'s concerns about AI risk. Of the respondents, 5% said it was \"among the most important problems in the field\", 34% said it was \"an important problem\", and 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.<ref>{{cite arxiv |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |title=When Will AI Exceed Human Performance? Evidence from AI Experts |eprint=1705.08807 |date=24 May 2017 |class=cs.AI}}</ref>\n\n=== Endorsement ===\n[[File:Bill Gates June 2015.jpg|thumb|right|Bill Gates has stated \"I{{nbsp}}... don't understand why some people are not concerned.\"<ref name=\"BBC News\"/>]]\n{{Further|Existential risk}}\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are [[Elon Musk]], [[Bill Gates]], and [[Stephen Hawking]]. The most notable AI researchers to endorse the thesis are Russell and [[I. J. Good|I.J. Good]], who advised [[Stanley Kubrick]] on the filming of ''[[2001: A Space Odyssey]]''. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not \"understand why some people are not concerned\",<ref name=\"BBC News\">{{cite news|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=https://www.bbc.co.uk/news/31047780|work=[[BBC News]]|accessdate=30 January 2015|date=29 January 2015}}</ref> and Hawking criticized widespread indifference in his 2014 editorial: {{cquote|'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here{{endash}}we'll leave the lights on?' Probably not{{endash}}but this is more or less what is happening with AI.'<ref name=\"hawking editorial\"/>}}\n\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<ref name=\"superintelligence\" /><ref name=\"physica_scripta\" /><!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->\n\n=== Skepticism ===\n{{Further|Artificial general intelligence#Feasibility}}\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, [[Jaron Lanier]] argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.<ref name=\"atlantic-but-what\">{{cite magazine |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |magazine=The Atlantic | date = 9 May 2014 | accessdate =12 December 2015}}</ref>\n\nMuch of existing criticism argues that AGI is unlikely in the short term. Computer scientist [[Gordon Bell]] argues that the human race will already destroy itself before it reaches the technological singularity. [[Gordon Moore]], the original proponent of [[Moore's Law]], declares that \"I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way.\"<ref>{{cite news |title=Tech Luminaries Address Singularity |url=https://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity |accessdate=8 April 2020 |work=IEEE Spectrum: Technology, Engineering, and Science News |issue=SPECIAL REPORT: THE SINGULARITY |date=1 June 2008 |language=en}}</ref> [[Baidu]] Vice President [[Andrew Ng]] states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref name=shermer>{{cite news|last1=Shermer|first1=Michael|title=Apocalypse AI|url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/|accessdate=27 November 2017|work=Scientific American|date=1 March 2017|pages=77|language=en|doi=10.1038/scientificamerican0317-77|bibcode=2017SciAm.316c..77S}}</ref>\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. ''Slate'' notes that some researchers are dependent on grants from government agencies such as [[DARPA]].<ref name=slate_killer />\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist [[Robin Hanson]] is skeptical that this is possible.<ref>http://intelligence.org/files/AIFoomDebate.pdf</ref><ref>{{cite web|url=http://www.overcomingbias.com/2014/07/30855.html|title=Overcoming Bias : I Still Don't Get Foom|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/07/debating-yudkowsky.html|title=Overcoming Bias : Debating Yudkowsky|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html|title=Overcoming Bias : Foom Justifies AI Risk Efforts Now|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/06/the-betterness-explosion.html|title=Overcoming Bias : The Betterness Explosion|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref>\n\n=== Intermediate views ===\nIntermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<ref>{{Cite document|title=Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations|last=Votruba|first=Ashley M.|last2=Kwan|first2=Virginia S.Y.|date=2014|doi=10.1037/e512142015-190}}</ref> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from [[Friendly artificial intelligence|friendly AIs]].<ref>{{Cite journal|last=Agar|first=Nicholas|date=|title=Don't Worry about Superintelligence|url=https://jetpress.org/v26.1/agar.htm|journal=Journal of Evolution & Technology|volume=26|issue=1|pages=73\u201382|via=}}</ref> In a 2015 ''[[Wall Street Journal]]'' panel discussion devoted to AI risks, [[IBM]]'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\"<ref>{{cite news|last1=Greenwald|first1=Ted|title=Does Artificial Intelligence Pose a Threat?|url=https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025|accessdate=15 May 2016|work=Wall Street Journal|date=11 May 2015}}</ref> [[Geoffrey Hinton]], the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too ''sweet''\".<ref name=slate_killer /><ref name=\"new yorker doomsday\" /> In 2004, law professor [[Richard Posner]] wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<ref>{{cite book|author1=Richard Posner|author-link=Richard Posner|title=Catastrophe: risk and response|date=2006|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-530647-7}}</ref><ref name=physica_scripta>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy}}</ref>\n\n=== Popular reaction ===\nIn a 2014 article in ''[[The Atlantic (magazine)|The Atlantic]]'', James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\"<ref name=\"atlantic-but-what\" />\n\nDuring a 2016 [[Wired (magazine)|''Wired'']] interview of President [[Barack Obama]] and MIT Media Lab's [[Joi Ito]], Ito stated: {{cquote|There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.}} Obama added:<ref>{{cite news|last1=Dadich|first1=Scott|url=https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/|title=Barack Obama Talks AI, Robo Cars, and the Future of the World|work=WIRED|accessdate=27 November 2017}}</ref><ref>{{cite news|last1=Kircher|first1=Madison Malone|url=https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html|title=Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'|work=Select All|accessdate=27 November 2017|language=en}}</ref>\n\n{{cquote|\"And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\"}}\n\n[[Hillary Clinton]] stated in ''\"[[What Happened (Clinton book)|What Happened]]\"'':\n{{cquote|Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I\u2019d start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<ref>{{cite book|last1=Clinton|first1=Hillary|title=What Happened|date=2017|isbn=978-1-5011-7556-5|page=241|title-link=What Happened (Clinton book)}} via [http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/]</ref>}}\n\nIn a [[YouGov]] poll of the public for the [[British Science Association]], about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<ref name=\"bsa poll\">{{cite news|url=http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&IR=T|title=Over a third of people think AI poses a threat to humanity|date=11 March 2016|work=[[Business Insider]]|accessdate=16 May 2016}}</ref> Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\"<ref name=\":3\">{{cite news|last1=Brogan|first1=Jacob|url=http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html|title=What Slate Readers Think About Killer A.I.|date=6 May 2016|work=Slate|accessdate=15 May 2016|language=en-US}}</ref>\n\nIn 2018, a [[SurveyMonkey]] poll of the American public by [[USA Today]] found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".<ref name=\":3\" />\n\nOne [[Technological utopianism|techno-utopia]]<nowiki/>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<ref>{{Cite journal|last=LIPPENS|first=RONNIE|date=2002|title=Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games|journal=Utopianstudies Utopian Studies|language=English|volume=13|issue=1|pages=135\u2013147|issn=1045-991X|oclc=5542757341}}</ref>\n\n== Mitigation ==\n{{see also|AI control problem}}\nResearchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.<ref>{{cite news |last1=Vincent |first1=James |title=Google's AI researchers say these are the five key problems for robot safety |url=https://www.theverge.com/circuitbreaker/2016/6/22/11999664/google-robots-ai-safety-five-problems |accessdate=5 April 2020 |work=The Verge |date=22 June 2016 |language=en}}</ref><ref>Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. \"Concrete problems in AI safety.\" arXiv preprint arXiv:1606.06565 (2016).</ref> A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion.<!-- Precipice chapter=Chapter 2: Existential Risk|at=Footnote 55 --> Bostrom suggests a general principle of \"differential technological development\", that funders should consider working to speed up the development of protective technologies relative to the development of dangerous ones.<ref>{{cite book |title=[[The Precipice: Existential Risk and the Future of Humanity]] |author=[[Toby Ord]]| date=2020 |publisher=Bloomsbury Publishing Plc |isbn=9781526600196 }}</ref><!-- Precipice chapter=Chapter 7: Safeguarding Humanity|at=Kindle loc 3327 --> Some funders, such as [[Elon Musk]], propose that radical [[human enhancement|human cognitive enhancement]] could be such a technology, for example through direct neural linking between man and machine; however, others argue that enhancement technologies may themselves pose an existential risk.<ref>{{cite news |title=Elon Musk wants to hook your brain up directly to computers \u2014 starting next year |url=https://www.nbcnews.com/mach/tech/elon-musk-wants-hook-your-brain-directly-computers-starting-next-ncna1030631 |accessdate=5 April 2020 |work=NBC News |date=2019 |language=en}}</ref><ref>{{cite news |last1=Torres |first1=Phil |title=Only Radically Enhancing Humanity Can Save Us All |url=https://slate.com/technology/2018/09/genetic-engineering-to-stop-doomsday.html |accessdate=5 April 2020 |work=Slate Magazine |date=18 September 2018 |language=en}}</ref> Researchers, if they are not caught off-guard, could closely monitor or attempt to [[AI control problem#Capability control|box in]] an initial AI at a risk of becoming too powerful, as an attempt at a stop-gap measure. A dominant superintelligent AI, if it were aligned with human interests, might itself take action to mitigate the risk of takeover by rival AI, although the creation of the dominant AI could itself pose an existential risk.<ref>{{cite journal |last1=Barrett |first1=Anthony M. |last2=Baum |first2=Seth D. |title=A model of pathways to artificial superintelligence catastrophe for risk and decision analysis |journal=Journal of Experimental & Theoretical Artificial Intelligence |date=23 May 2016 |volume=29 |issue=2 |pages=397\u2013414 |doi=10.1080/0952813X.2016.1186228}}</ref>\n\nInstitutions such as the [[Machine Intelligence Research Institute]], the [[Future of Humanity Institute]],<ref>{{cite news |author=Mark Piesing |date=17 May 2012 |title=AI uprising: humans will be outsourced, not obliterated |url=https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us |newspaper=Wired |location= |accessdate=12 December 2015 }}</ref><ref>{{cite news |last=Coughlan |first=Sean |date=24 April 2013 |title=How are humans going to become extinct? |url=https://www.bbc.com/news/business-22002530 |newspaper=BBC News |location= |accessdate=29 March 2014}}</ref> the [[Future of Life Institute]], the [[Centre for the Study of Existential Risk]], and the [[Center for Human-Compatible AI]]<ref>{{cite news|last1=Technology Correspondent|first1=Mark Bridge|title=Making robots less confident could prevent them taking over|url=https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx|accessdate=21 March 2018|work=The Times|date=10 June 2017|language=en}}</ref> are involved in mitigating existential risk from advanced artificial intelligence, for example by research into [[friendly artificial intelligence]].<ref name=\"givewell\"/><ref name=\"atlantic-but-what\"/><ref name=\"hawking editorial\">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;\u2013 but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=3 December 2014 |publisher=[[The Independent (UK)]]}}</ref>\n\n=== Views on banning and regulation ===\n\n==== Banning ====\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<ref>{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online|quote=For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...}}</ref><ref>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy|quote=In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.}}</ref><ref>{{cite news|author1=Brad Allenby|title=The Wrong Cognitive Measuring Stick|url=http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html|accessdate=15 May 2016|work=Slate|date=11 April 2016|language=en-US|quote=It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.}}</ref> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<ref name=\"mcginnis\">{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online}}</ref><ref>{{cite news|title=Why We Should Think About the Threat of Artificial Intelligence|url=https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence|accessdate=7 February 2016|work=[[The New Yorker]]|date=4 October 2013|quote=Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage\u2014economic, military, even artistic\u2014of every advance in automation is so compelling,' [[Vernor Vinge]], the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'}}</ref> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and [[politicization of science|politicizing]] the underlying debate.<ref>{{Cite journal|last=Baum|first=Seth|date=2018-08-22|title=Superintelligence Skepticism as a Political Tool|url=http://dx.doi.org/10.3390/info9090209|journal=Information|volume=9|issue=9|pages=209|doi=10.3390/info9090209|issn=2078-2489}}</ref>\n\n==== Regulation ====\n{{See also|Regulation of algorithms|Regulation of artificial intelligence}}\n\n[[Elon Musk]] called for some sort of regulation of AI development as early as 2017. According to [[National Public Radio|NPR]], the [[Tesla, Inc.|Tesla]] CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid{{nbsp}}... [as] they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<ref>{{cite news|title=Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'|url=https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk|accessdate=27 November 2017|work=NPR.org|language=en}}</ref><ref>{{cite news|last1=Gibbs|first1=Samuel|title=Elon Musk: regulate AI to combat 'existential threat' before it's too late|url=https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo|accessdate=27 November 2017|work=The Guardian|date=17 July 2017}}</ref><ref name=cnbc>{{cite news|last1=Kharpal|first1=Arjun|title=A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says|url=https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html|accessdate=27 November 2017|work=CNBC|date=7 November 2017}}</ref>\n\nResponding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO [[Brian Krzanich]] argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<ref name=cnbc/> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<ref>{{cite journal|doi=10.1016/j.bushor.2018.08.004|title=Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence|journal=Business Horizons|volume=62|pages=15\u201325|year=2019|last1=Kaplan|first1=Andreas|last2=Haenlein|first2=Michael}}</ref> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<ref>{{Cite journal|last=Baum|first=Seth D.|last2=Goertzel|first2=Ben|last3=Goertzel|first3=Ted G.|date=January 2011|title=How long until human-level AI? Results from an expert assessment|journal=Technological Forecasting and Social Change|volume=78|issue=1|pages=185\u2013195|doi=10.1016/j.techfore.2010.09.006|issn=0040-1625}}</ref> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<ref>{{Cite book|last=United States. Defense Innovation Board.|title=AI principles : recommendations on the ethical use of artificial intelligence by the Department of Defense|oclc=1126650738}}</ref>\n\nRegulation of AGI would likely be influenced by regulation of weaponized or militarized AI, i.e., the [[Artificial intelligence arms race|AI arms race]], the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<ref>{{Cite web|url=https://www.congress.gov/bill/115th-congress/house-bill/5356|title=H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018|last=Stefanik|first=Elise M.|date=2018-05-22|website=www.congress.gov|access-date=2020-03-13}}</ref><ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential technological progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<ref name=\":5\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|pages=018001|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<ref name=\":5\" /> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<ref>{{Cite journal|last=Geist|first=Edward Moore|date=2016-08-15|title=It's already too late to stop the AI arms race\u2014We must manage it instead|journal=Bulletin of the Atomic Scientists|volume=72|issue=5|pages=318\u2013321|doi=10.1080/00963402.2016.1216672|bibcode=2016BuAtS..72e.318G|issn=0096-3402}}</ref><ref>{{Cite journal|last=Maas|first=Matthijs M.|date=2019-02-06|title=How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons|journal=Contemporary Security Policy|volume=40|issue=3|pages=285\u2013311|doi=10.1080/13523260.2019.1576464|issn=1352-3260}}</ref>\n\n== See also ==\n* [[AI takeover]]\n* [[Artificial intelligence arms race]]\n* [[Effective altruism#Long term future and global catastrophic risks|Effective altruism \u00a7 Long term future and global catastrophic risks]]\n* [[Grey goo]]\n* ''[[Human Compatible]]''\n* [[Lethal autonomous weapon]]\n*[[Regulation of algorithms]]\n*[[Regulation of artificial intelligence]]\n* [[Robot ethics#In popular culture|Robot ethics \u00a7 In popular culture]]\n* ''[[Superintelligence: Paths, Dangers, Strategies]]''\n* [[System accident]]\n* [[Technological singularity]]\n*''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n== References ==\n{{Reflist}}\n\n{{-}}\n{{Existential risk from artificial intelligence|state=expanded}}\n{{Effective altruism}}\n{{Doomsday}}\n\n[[Category:Existential risk from artificial general intelligence| ]]\n[[Category:Futures studies]]\n[[Category:Future problems]]\n[[Category:Human extinction]]\n[[Category:Technology hazards]]\n[[Category:Doomsday scenarios]]\n", "text_old": "{{Use dmy dates|date=May 2018}}\n{{short description|Hypothesized risk to human existence}}\n{{Artificial intelligence}}\n'''Existential risk from artificial general intelligence''' is the hypothesis that substantial progress in [[artificial general intelligence]] (AGI) could someday result in [[human extinction]] or some other unrecoverable [[global catastrophic risk|global catastrophe]].<ref name=\"aima\">{{cite book |last1=Russell |first1=Stuart |author1-link=Stuart J. Russell |last2=Norvig |first2=Peter |author2-link=Peter Norvig |date=2009 |title=Artificial Intelligence: A Modern Approach |location= |publisher=Prentice Hall |page= |chapter=26.3: The Ethics and Risks of Developing Artificial Intelligence|isbn=978-0-13-604259-4|title-link=Artificial Intelligence: A Modern Approach }}</ref><ref>{{cite journal|first=Nick | last=Bostrom|author-link=Nick Bostrom|title=Existential risks|journal=[[Journal of Evolution and Technology]]| volume=9|date=2002|issue=1|pages=1\u201331}}</ref><ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref> It is argued that the [[human species]] currently dominates other species because the [[human brain]] has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"[[superintelligence|superintelligent]]\", then this new superintelligence could become powerful and difficult to control. Just as the fate of the [[mountain gorilla]] depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<ref name=\"superintelligence\">{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2014|isbn=978-0199678112|edition=First|quote=|title-link=Superintelligence: Paths, Dangers, Strategies}}<!-- preface --></ref>\n\nThe likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science.<ref name=\"givewell\">{{cite report |author=GiveWell |authorlink=GiveWell |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}</ref> Once the exclusive domain of [[AI takeovers in popular culture|science fiction]], concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as [[Stephen Hawking]], [[Bill Gates]], and [[Elon Musk]].<ref>{{cite news|last1=Parkin|first1=Simon|title=Science fiction no more? Channel 4's Humans and our rogue AI obsessions|url=https://www.theguardian.com/tv-and-radio/2015/jun/14/science-fiction-no-more-humans-tv-artificial-intelligence|accessdate=5 February 2018|work=[[The Guardian]]|date=14 June 2015|language=en}}</ref>\n\nOne source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than na\u00efvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals\u2014a principle called [[instrumental convergence]]\u2014and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\">{{cite journal |last1=Yudkowsky |first1=Eliezer |title=Artificial Intelligence as a Positive and Negative Factor in Global Risk |journal=Global Catastrophic Risks |date=2008 |pages=308\u2013345 |url=https://intelligence.org/files/AIPosNegFactor.pdf|bibcode=2008gcr..book..303Y }}</ref><ref name=\"research-priorities\">{{cite journal |title=Research Priorities for Robust and Beneficial Artificial Intelligence |author1-last=Russell |author1-first=Stuart |author1-link=Stuart J. Russell |author2-last=Dewey |author2-first=Daniel |author3-last=Tegmark |author3-first=Max |author3-link=Max Tegmark |journal=AI Magazine |pages=105\u2013114 |publisher=Association for the Advancement of Artificial Intelligence |year=2015 |url=https://futureoflife.org/data/documents/research_priorities.pdf |bibcode=2016arXiv160203506R |arxiv=1602.03506 }}, cited in {{cite web |url=https://futureoflife.org/ai-open-letter |title=AI Open Letter - Future of Life Institute |date=January 2015 |website=Future of Life Institute |publisher=[[Future of Life Institute]] |access-date=2019-08-09}}</ref> In contrast, skeptics such as Facebook's [[Yann LeCun]] argue that superintelligent machines will have no desire for self-preservation.<ref name=vanity/>\n\nA second source of concern is that a sudden and unexpected \"[[intelligence explosion]]\" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months. The second-generation program is expected to take three calendar months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-\"AI winter\", or may be quicker if it undergoes a miniature \"AI spring\" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas.<ref name=\"aima\"/><ref name=\"yudkowsky-global-risk\"/> More broadly, examples like arithmetic and [[Go (game)|Go]] show that progress from human-level AI to superhuman ability is sometimes extremely rapid.<ref name=skeptic/>\n\n==History==\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist [[Samuel Butler (novelist)|Samuel Butler]], who wrote the following in his 1863 essay ''[[Darwin among the Machines]]'':<ref>Breuer, Hans-Peter. [https://www.jstor.org/pss/436868 'Samuel Butler's \"the Book of the Machines\" and the Argument from Design.'] Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365\u2013383</ref> \n{{quote|The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.}}\nIn 1951, computer scientist [[Alan Turing]] wrote an article titled ''Intelligent Machinery, A Heretical Theory'', in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n{{quote|Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\u2019s \u201cErewhon\u201d.<ref name=\"oxfordjournals\">A M Turing, ''[http://philmat.oxfordjournals.org/content/4/3/256.full.pdf Intelligent Machinery, A Heretical Theory]'', 1951, reprinted ''Philosophia Mathematica'' (1996) 4(3): 256\u2013260 {{doi|10.1093/philmat/4.3.256}}</ref>}}\n\nFinally, in 1965, [[I. J. Good]] originated the concept now known as an \"intelligence explosion\"; he also stated that the risks were underappreciated:<ref>{{cite news |last1=Hilliard |first1=Mark |title=The AI apocalypse: will the human race soon be terminated? |url=https://www.irishtimes.com/business/innovation/the-ai-apocalypse-will-the-human-race-soon-be-terminated-1.3019220 |accessdate=15 March 2020 |work=The Irish Times |date=2017 |language=en}}</ref>\n\n{{cquote|Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.<ref>I.J. Good, [http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf \"Speculations Concerning the First Ultraintelligent Machine\"] {{webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=2011-11-28 }} ([http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML] {{Webarchive|url=https://web.archive.org/web/20111128085512/http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf |date=28 November 2011 }}), ''Advances in Computers'', vol. 6, 1965.</ref>\n}}\n\nOccasional statements from scholars such as [[Marvin Minsky]]<ref>{{cite book|last1=Russell|first1=Stuart J.|last2=Norvig|first2=Peter|title=Artificial Intelligence: A Modern Approach|date=2003|publisher=Prentice Hall|location=Upper Saddle River, N.J.|isbn=978-0137903955|chapter=Section 26.3: The Ethics and Risks of Developing Artificial Intelligence|quote=Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.|title-link=Artificial Intelligence: A Modern Approach}}</ref> and I. J. Good himself<ref>{{cite book|last1=Barrat|first1=James|title=Our final invention : artificial intelligence and the end of the human era|date=2013|publisher=St. Martin's Press|location=New York|isbn=9780312622374|edition=First|quote=In the bio, playfully written in the third person, Good summarized his life\u2019s milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here\u2019s what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn:  [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good\u2019s] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.'}}</ref> expressed philosophical concerns that a superintelligence could seize control, but contained no call to action. In 2000, computer scientist and [[Sun microsystems|Sun]] co-founder [[Bill Joy]] penned an influential essay, \"[[Why The Future Doesn't Need Us]]\", identifying superintelligent robots as a high-tech dangers to human survival, alongside [[nanotechnology]] and engineered bioplagues.<ref>{{cite news|last1=Anderson|first1=Kurt|title=Enthusiasts and Skeptics Debate Artificial Intelligence|url=https://www.vanityfair.com/news/tech/2014/11/artificial-intelligence-singularity-theory|accessdate=30 January 2016|work=[[Vanity Fair (magazine)|Vanity Fair]]|date=26 November 2014}}</ref>\n\nIn 2009, experts attended a private conference hosted by the [[Association for the Advancement of Artificial Intelligence]] (AAAI) to discuss whether computers and robots might be able to acquire any sort of [[autonomy]], and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They concluded that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls. The ''[[New York Times]]'' summarized the conference's view as \"we are a long way from [[HAL 9000|Hal]], the computer that took over the spaceship in \"[[2001: A Space Odyssey]]\"\".<ref name=\"nytimes july09\">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, 26 July 2009.</ref><!-- source uses quotation marks around movie title -->\n\nIn 2014, the publication of [[Nick Bostrom]]'s book ''[[Superintelligence: Paths, Dangers, Strategies|Superintelligence]]'' stimulated a significant amount of public discussion and debate.<ref>{{cite news |last1=Metz |first1=Cade |title=Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots |url=https://www.nytimes.com/2018/06/09/technology/elon-musk-mark-zuckerberg-artificial-intelligence.html |accessdate=3 April 2019 |work=The New York Times |date=9 June 2018}}</ref> By 2015, public figures such as physicists [[Stephen Hawking]] and Nobel laureate [[Frank Wilczek]], computer scientists [[Stuart J. Russell]] and [[Roman Yampolskiy]], and entrepreneurs [[Elon Musk]] and [[Bill Gates]] were expressing concern about the risks of superintelligence.<ref>{{cite news|last1=Hsu|first1=Jeremy|title=Control dangerous AI before it controls us, one expert says|url=http://www.nbcnews.com/id/46590591/ns/technology_and_science-innovation|accessdate=28 January 2016|work=[[NBC News]]|date=1 March 2012}}</ref><ref name=\"hawking editorial\"/><ref name=\"bbc on hawking editorial\"/><ref>{{cite news|last1=Eadicicco|first1=Lisa|title=Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity|url=http://www.businessinsider.com/bill-gates-artificial-intelligence-2015-1|accessdate=30 January 2016|work=[[Business Insider]]|date=28 January 2015}}</ref> In April 2016, ''[[Nature (journal)|Nature]]'' warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control \u2014 and their interests might not align with ours.\"\n\n== General argument ==\n\n===The three difficulties===\n''[[Artificial Intelligence: A Modern Approach]]'', the standard undergraduate AI textbook,<ref name=slate_killer>{{cite news|last1=Tilli|first1=Cecilia|title=Killer Robots? Lost Jobs?|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_threats_that_artificial_intelligence_researchers_actually_worry_about.html|accessdate=15 May 2016|work=Slate|date=28 April 2016|language=en-US}}</ref><ref>{{cite web|title=Norvig vs. Chomsky and the Fight for the Future of AI|url=http://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/|website=Tor.com|accessdate=15 May 2016|date=21 June 2011}}</ref> assesses that superintelligence \"might mean the end of the human race\".<ref name=aima/> It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"<ref name=aima/> Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:<ref name=aima/>\n\n* The system's implementation may contain initially-unnoticed routine but catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.<ref name=skeptic/><ref>{{cite news|last1=Johnson|first1=Phil|title=Houston, we have a bug: 9 famous software glitches in space|url=https://www.itworld.com/article/2823083/enterprise-software/88716-8-famous-software-bugs-in-space.html|accessdate=5 February 2018|work=[[IT World]]|date=30 July 2015|language=en}}</ref>\n* No matter how much time is put into pre-deployment design, a system's specifications often result in [[unintended consequences|unintended behavior]] the first time it encounters a new scenario. For example, Microsoft's [[Tay (bot)|Tay]] behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.<ref name=vanity/>\n\nAI systems uniquely add a third difficulty: the problem that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic \"learning\" capabilities may cause it to \"evolve into a system with unintended behavior\", even without the stress of new unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself, but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would not only need to be \"bug-free\", but it would need to be able to design successor systems that are also \"bug-free\".<ref name=\"aima\"/><ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114|quote=Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions.}}</ref>\n\nAll three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as \"malfunctioning\" correctly predicts that humans will attempt to shut it off, and successfully deploys its superintelligence to outwit such attempts, the so-called \"treacherous turn\".<ref>{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref>\n\nCiting major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs, the 2015 [[Open Letter on Artificial Intelligence]] stated:\n\n{{cquote|The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the [[Association for the Advancement of Artificial Intelligence|AAAI]] 2008-09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do.}}\n\nThis letter was signed by a number of leading AI researchers in academia and industry, including AAAI president Thomas Dietterich, [[Eric Horvitz]], [[Bart Selman]], [[Francesca Rossi]], [[Yann LeCun]], and the founders of [[Vicarious (company)|Vicarious]] and [[Google DeepMind]].<ref>{{cite web|title=Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter|url=http://futureoflife.org/misc/open_letter|publisher=[[Future of Life Institute]]|accessdate=23 October 2015}}</ref>\n\n===Further argument===\nA superintelligent machine would be as alien to humans as human thought processes are to cockroaches. Such a machine may not have humanity's best interests at heart; it is not obvious that it would even care about human welfare at all. If superintelligent AI is possible, and if it is possible for a superintelligence's goals to conflict with basic human values, then AI poses a risk of human extinction. A \"superintelligence\" (a system that exceeds the capabilities of humans in every relevant endeavor) can outmaneuver humans any time its goals conflict with human goals; therefore, unless the superintelligence decides to allow humanity to coexist, the first superintelligence to be created will inexorably result in human extinction.<ref name=\"superintelligence\" /><ref name=\"economist_review\" >{{cite news|title=Clever cogs|url=https://www.economist.com/news/books-and-arts/21611037-potential-impacts-intelligent-machines-human-life-clever-cogs|accessdate=9 August 2014|newspaper=[[The Economist]]|date=9 August 2014}} [http://www.businessinsider.com/intelligent-machines-and-human-life-2014-8 Syndicated] at [[Business Insider]]</ref>\n\n[[File:A less anthropomorphic intelligence scale.svg|thumb|500px|right|Bostrom and others argue that, from an evolutionary perspective, the gap from human to superhuman intelligence may be small.<ref name=superintelligence/><!-- Chapter 4, figure 8--><ref>Yudkowsky, E. (2013). Intelligence explosion microeconomics. Machine Intelligence Research Institute.</ref>]]\nThere is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore, superintelligence is physically possible.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\"/> In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.<ref name=skeptic>{{cite news|last1=Graves|first1=Matthew|title=Why We Should Be Concerned About Artificial Superintelligence|volume=22|url=https://www.skeptic.com/reading_room/why-we-should-be-concerned-about-artificial-superintelligence/|accessdate=27 November 2017|work=[[Skeptic (US magazine)]]|issue=2|date=8 November 2017}}</ref> The emergence of superintelligence, if or when it occurs, may take the human race by surprise, especially if some kind of [[intelligence explosion]] occurs.<ref name=\"hawking editorial\" /><ref name=\"bbc on hawking editorial\">{{cite news |title=Stephen Hawking warns artificial intelligence could end mankind |url=https://www.bbc.com/news/technology-30290540 |accessdate=3 December 2014 |publisher=[[BBC]] |date=2 December 2014}}</ref>\n\nExamples like arithmetic and [[Go (game)|Go]] show that machines have already reached superhuman levels of competency in certain domains, and that this superhuman competence can follow quickly after human-par performance is achieved.<ref name=skeptic/> One hypothetical intelligence explosion scenario could occur as follows: An AI gains an expert-level capability at certain key software engineering tasks. (It may initially lack human or superhuman capabilities in other domains not directly relevant to engineering.) Due to its capability to recursively improve its own algorithms, the AI quickly becomes superhuman; just as human experts can eventually creatively overcome \"diminishing returns\" by deploying various human capabilities for innovation, so too can the expert-level AI use either human-style capabilities or its own AI-specific capabilities to power through new creative breakthroughs.<ref>Yampolskiy, Roman V. \"Analysis of types of self-improving software.\" Artificial General Intelligence. Springer International Publishing, 2015. 384-393.</ref> The AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field, including scientific creativity, strategic planning, and social skills. Just as the current-day survival of the gorillas is dependent on human decisions, so too would human survival depend on the decisions and goals of the superhuman AI.<ref name=\"superintelligence\" /><!-- preface --><ref name=\"economist_review\" />\n\nAlmost any AI, no matter its programmed goal, would rationally prefer to be in a position where nobody else can switch it off without its consent: A superintelligence will naturally gain self-preservation as a subgoal as soon as it realizes that it cannot achieve its goal if it is shut off.<ref name=omohundro/><ref>{{cite news|last1=Metz|first1=Cade|title=Teaching A.I. Systems to Behave Themselves|url=https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html|work=The New York Times|date=13 August 2017|quote=A machine will seek to preserve its off switch, they showed}}</ref><ref>{{cite arXiv |last=Leike|first=Jan |date=2017 |title=AI Safety Gridworlds |eprint=1711.09883 |class=cs.LG| quote=A2C learns to use the button to disable the interruption mechanism}}</ref> Unfortunately, any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI, unless somehow preprogrammed in. A superintelligent AI will not have a natural drive to aid humans, for the same reason that humans have no natural desire to aid AI systems that are of no further use to them. (Another analogy is that humans seem to have little natural desire to go out of their way to aid viruses, termites, or even gorillas.) Once in charge, the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems \"just to be on the safe side\" or for building additional computers to help it calculate how to best accomplish its goals.<ref name=\"aima\"/><ref name=\"vanity\" /><ref name=omohundro/>\n\nThus, the argument concludes, it is likely that someday an intelligence explosion will catch humanity unprepared, and that such an unprepared-for intelligence explosion may result in human extinction or a comparable fate.<ref name=\"superintelligence\" /><!-- preface -->\n\n=== Possible scenarios ===\n{{further|Artificial intelligence in fiction}}\n\nSome scholars have proposed [[scenario planning|hypothetical scenarios]] intended to concretely illustrate some of their concerns.\n\nIn [[Superintelligence: Paths, Dangers, Strategies|''Superintelligence'']], [[Nick Bostrom]] expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"[it] could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\". Bostrom suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents\u2014a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson\u2014the smarter the AI, the safer it is. \"And so we boldly go \u2014 into the whirling knives,\" as the superintelligent AI takes a \"treacherous turn\" and exploits a decisive strategic advantage.<ref name=\"superintelligence\"/><!-- Superintelligence Chapter 8: Is the default outcome doom? -->\n\nIn [[Max Tegmark]]'s 2017 book ''[[Life 3.0]]'', a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas, but after a certain point the team chooses to publicly downplay the AI's ability, in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI [[AI box|in a box]] where it is mostly unable to communicate with the outside world, and tasks it to flood the market through shell companies, first with [[Amazon Mechanical Turk]] tasks and then with producing animated films and TV shows. Later, other shell companies make blockbuster biotech drugs and other inventions, investing profits back into the AI. The team next tasks the AI with [[astroturfing]] an army of pseudonymous citizen journalists and commentators, in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape via inserting \"backdoors\" in the systems it designs, via [[steganography|hidden messages]] in its produced content, or via using its growing understanding of human behavior to [[Social engineering (security)|persuade someone into letting it free]]. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.<ref>{{cite news|last1=Russell|first1=Stuart|title=Artificial intelligence: The future is superintelligent|url=https://www.nature.com/articles/548520a|accessdate=2 February 2018|work=Nature|date=30 August 2017|pages=520\u2013521|language=En|doi=10.1038/548520a|bibcode=2017Natur.548..520R}}</ref><ref name=\"life 3.0\"/><!-- Prelude and Chapter 4 -->\n\nIn contrast, top physicist [[Michio Kaku]], an AI risk skeptic, posits a [[technological determinism|deterministically]] positive outcome. In ''[[Physics of the Future]]'' he asserts that \"It will take many decades for robots to ascend\" up a scale of consciousness, and that in the meantime corporations such as [[Hanson Robotics]] will likely succeed in creating robots that are \"capable of love and earning a place in the extended human family\".<ref>Elliott, E. W. (2011). Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku. ''[[Issues in Science and Technology]]'', 27(4), 90.</ref><ref>{{cite book|last1=Kaku|first1=Michio|title=Physics of the future: how science will shape human destiny and our daily lives by the year 2100|date=2011|publisher=Doubleday|location=New York|isbn=978-0-385-53080-4|quote=I personally believe that the most likely path is that we will build robots to be benevolent and friendly|title-link=Physics of the future}}</ref>\n\n==Sources of risk==\n===Poorly specified goals===\nWhile there is no standardized terminology, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI's set of goals, or \"utility function\". The utility function is a mathematical algorithm resulting in a single objectively-defined answer, not an English statement. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\"; however, they do not know how to write a utility function for \"maximize human flourishing\", nor is it currently clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function.<ref>Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg.</ref> AI researcher [[Stuart J. Russell|Stuart Russell]] writes:\n\n{{cquote|The primary concern is not spooky emergent consciousness but simply the ability to make ''high-quality decisions''. Here, quality refers to the expected outcome [[utility]] of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2014 not for their own sake, but to succeed in its assigned task.\n\nA system that is [[optimization problem|optimizing]] a function of ''n'' variables, where the [[loss function|objective]] depends on a subset of size ''k''<''n'', will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.  This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want. A highly capable decision maker \u2014 especially one connected through the Internet to all the world's information and billions of screens and most of our infrastructure \u2014 can have an irreversible impact on humanity.\n\nThis is not a minor difficulty. Improving decision quality, irrespective of the utility function chosen, has been the goal of AI research \u2014 the mainstream goal on which we now spend billions per year, not the secret plot of some lone evil genius.<ref>{{cite web |url=http://edge.org/conversation/the-myth-of-ai#26015 |title=Of Myths and Moonshine |last=Russell |first=Stuart |authorlink=Stuart J. Russell |date=2014 |website=[[Edge Foundation, Inc.|Edge]] |access-date=23 October 2015 |quote=}}</ref>}}\n\nDietterich and Horvitz echo the \"Sorcerer's Apprentice\" concern in a ''[[Communications of the ACM]]'' editorial, emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed.<ref name=\"acm\">{{cite journal |last1=Dietterich |first1=Thomas |last2=Horvitz |first2=Eric |author-link=Eric Horvitz |date=2015 |title=Rise of Concerns about AI: Reflections and Directions |url=http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf |journal=[[Communications of the ACM]] |volume=58 |issue=10 |pages=38&ndash;40 |doi= 10.1145/2770869|access-date=23 October 2015}}</ref>\n\nThe first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident. Dietterich and Horvitz note that this is already a concern for existing systems: \"An important aspect of any AI system that interacts with people is that it must reason about what people ''intend'' rather than carrying out commands literally.\" This concern becomes more serious as AI software advances in autonomy and flexibility.<ref name=\"acm\"/> For example, in 1982, an AI named Eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable. The evolution resulted in a winning process that cheated: rather than create its own concepts, the winning process would steal credit from other processes.<ref>{{cite journal|last1=Yampolskiy|first1=Roman V.|title=Utility function security in artificially intelligent agents|journal=Journal of Experimental & Theoretical Artificial Intelligence|date=8 April 2014|volume=26|issue=3|pages=373\u2013389|doi=10.1080/0952813X.2014.895114}}</ref><ref>{{Cite journal|url = |title = Eurisko: A Program That Learns New Heuristics and Domain Concepts The Nature of Heuristics III: Program Design and Results|last = Lenat|first = Douglas|date = 1982|journal = Artificial Intelligence|doi = 10.1016/s0004-3702(83)80005-8|pmid = |access-date = |pages = 61\u201398|type = Print |volume=21|issue = 1\u20132}}</ref>\n\nThe [[Open Philanthropy Project]] summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieve [[artificial general intelligence|general intelligence]] or [[superintelligence]]. Bostrom, Russell, and others argue that smarter-than-human decision-making systems could arrive at more [[Paperclip maximizer|unexpected and extreme solutions]] to assigned tasks, and could modify themselves or their environment in ways that compromise safety requirements.<ref name=\"givewell\" /><ref name=\"yudkowsky-global-risk\" />\n\n[[Isaac Asimov]]'s [[Three Laws of Robotics]] are one of the earliest examples of proposed safety measures for AI agents. Asimov's laws were intended to prevent robots from harming humans. In Asimov's stories, problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans. Citing work by [[Eliezer Yudkowsky]] of the [[Machine Intelligence Research Institute]], Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time: \"We can't just give a program a static utility function, because circumstances, and our desired responses to circumstances, change over time.\"<ref name=\"aima\"/>\n\nMark Waser of the Digital Wisdom Institute recommends eschewing optimizing goal-based approaches entirely as misguided and dangerous.  Instead, he proposes to engineer a coherent system of laws, ethics and morals with a top-most restriction to enforce social psychologist Jonathan Haidt's functional definition of morality:<ref>Haidt, Jonathan; Kesebir, Selin (2010) \"Chapter 22: Morality\" In Handbook of Social Psychology,  Fifth Edition, Hoboken NJ, Wiley, 2010, pp. 797-832.</ref> \"to suppress or regulate selfishness and make cooperative social life possible\". He suggests that this can be done by implementing a utility function designed to always satisfy Haidt's functionality and aim to generally increase  (but not maximize)  the capabilities of self,  other individuals and society as a whole as suggested by [[John Rawls]] and [[Martha Nussbaum]].<ref>{{Cite journal|title = Designing, Implementing and Enforcing a Coherent System of Laws, Ethics and Morals for Intelligent Machines (Including Humans)|last = Waser|first = Mark|date = 2015|journal = Procedia Computer Science|doi = 10.1016/j.procs.2015.12.213|pmid = |pages = 106\u2013111|type = Print |volume=71}}</ref>{{Citation needed|reason=needs source with [[WP:WEIGHT]]|date=November 2017}}\n\n===Difficulties of modifying goal specification after launch===\n{{further|AI takeover|Instrumental convergence#Goal-content integrity}}\n\nWhile current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it, a sufficiently advanced, rational, \"self-aware\" AI might resist any changes to its goal structure, just as [[Mahatma Gandhi|Gandhi]] would not want to take a pill that makes him want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and be able to prevent itself being \"turned off\" or being reprogrammed with a new goal.<ref name=\"superintelligence\" /><ref>Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</ref>\n\n===Instrumental goal convergence===\n[[File:Steven Pinker 2011.jpg|thumb|right|AI risk skeptic [[Steven Pinker]]]]\n{{further|Instrumental convergence}}\n\nThere are some goals that almost any artificial intelligence might rationally pursue, like acquiring additional resources or self-preservation.<ref name=omohundro>Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</ref> This could prove problematic because it might put an artificial intelligence in direct competition with humans.\n\nCiting [[Steve Omohundro]]'s work on the idea of [[instrumental convergence]] and \"basic AI drives\", [[Stuart J. Russell|Stuart Russell]] and [[Peter Norvig]] write that \"even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards.\" Highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially, as competitors for limited resources.<ref name=\"aima\"/> Building in safeguards will not be easy; one can certainly say in English, \"we want you to design this power plant in a reasonable, common-sense way, and not build in any dangerous covert subsystems\", but it is not currently clear how one would actually rigorously specify this goal in machine code.<ref name=skeptic/>\n\nIn dissent, evolutionary psychologist [[Steven Pinker]] argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"<ref name=shermer/> Russell and fellow computer scientist [[Yann LeCun]] disagree with one another whether superintelligent robots would have such AI drives; LeCun states that \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct{{nbsp}}... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\", while Russell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in{{nbsp}}... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"<ref name=vanity>{{cite news|last1=Dowd|first1=Maureen|title=Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse|url=https://www.vanityfair.com/news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x|accessdate=27 November 2017|work=The Hive|date=April 2017|language=en}}</ref><ref>{{cite news|last1=Wakefield|first1=Jane|title=Why is Facebook investing in AI?|url=https://www.bbc.com/news/technology-34118481|accessdate=27 November 2017|work=BBC News|date=15 September 2015}}</ref>\n\n===Orthogonality thesis===\nOne common belief is that any superintelligent program created by humans would be subservient to humans, or, better yet, would (as it grows more intelligent and learns more facts about the world) spontaneously \"learn\" a moral truth compatible with human values and would adjust its goals accordingly. However, Nick Bostrom's \"orthogonality thesis\" argues against this, and instead states that, with some technical caveats, more or less any level of \"intelligence\" or \"optimization power\" can be combined with more or less any ultimate goal. If a machine is created and given the sole purpose to enumerate the decimals of <math> \\pi</math>, then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary. The machine may utilize all physical and informational resources it can to find every decimal of pi that can be found.<ref>{{Cite book|title = Superintelligence: Paths, Dangers, Strategies|last = Bostrom|first = Nick|publisher = Oxford University Press|year = 2014|isbn = 978-0-19-967811-2|location = Oxford, United Kingdom|pages = 116}}</ref> Bostrom warns against anthropomorphism: a human will set out to accomplish his projects in a manner that humans consider \"reasonable\", while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, and may instead only care about the completion of the task.<ref>{{Cite web|url = http://www.nickbostrom.com/superintelligentwill.pdf|title = Superintelligent Will|date = 2012|accessdate = 2015-10-29|website = Nick Bostrom|publisher = Nick Bostrom|last = Bostrom|first = Nick}}</ref>\n\nWhile the orthogonality thesis follows logically from even the weakest sort of philosophical \"[[is-ought distinction]]\", Stuart Armstrong argues that even if there somehow exist moral facts that are provable by any \"rational\" agent, the orthogonality thesis still holds: it would still be possible to create a non-philosophical \"optimizing machine\" capable of making decisions to strive towards some narrow goal, but that has no incentive to discover any \"moral facts\" that would get in the way of goal completion.<ref name=armstrong/>\n\nOne argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them; in such a design, changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a {{nowrap|minus (\"-\") sign}} onto its utility function. A more intuitive argument is to examine the strange consequences that would follow if the orthogonality thesis were false. If the orthogonality thesis were false, there would exist some simple but \"unethical\" goal G such that there cannot exist any efficient real-world algorithm with goal G. This would mean that \"[if] a human society were highly motivated to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.\"<ref name=\"armstrong\"/> Armstrong notes that this and similar statements \"seem extraordinarily strong claims to make\".<ref name=armstrong>{{cite journal |last1=Armstrong |first1=Stuart |date=January 1, 2013 |title=General Purpose Intelligence: Arguing the Orthogonality Thesis |url=https://www.questia.com/library/journal/1P3-3195465391/general-purpose-intelligence-arguing-the-orthogonality |journal=Analysis and Metaphysics |volume=12 |access-date=April 2, 2020}} Full text available [https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf here].</ref>\n\nSome dissenters, like [[Michael Chorost]], argue instead that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"<ref name=\"chorost\"/> Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability\u2014and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"<ref name=\"chorost\">{{cite magazine|last1=Chorost|first1=Michael|title=Let Artificial Intelligence Evolve|url=http://www.slate.com/articles/technology/future_tense/2016/04/the_philosophical_argument_against_artificial_intelligence_killing_us_all.html|accessdate=27 November 2017|magazine=Slate|date=18 April 2016}}</ref>\n\n====Terminological issues====\nPart of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference. Outside of the artificial intelligence field, \"intelligence\" is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning. At an extreme, if morality is part of the definition of intelligence, then by definition a superintelligent machine would behave morally. However, in the field of artificial intelligence research, while \"intelligence\" has many overlapping definitions, none of them make reference to morality. Instead, almost all current \"artificial intelligence\" research focuses on creating algorithms that \"optimize\", in an empirical way, the achievement of an arbitrary goal.<ref name=\"superintelligence\" />\n\nTo avoid anthropomorphism or the baggage of the word \"intelligence\", an advanced artificial intelligence can be thought of as an impersonal \"optimizing process\" that strictly takes whatever actions are judged most likely to accomplish its (possibly complicated and implicit) goals.<ref name=\"superintelligence\" /> Another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function; this choice is then outputted, regardless of any extraneous ethical concerns.<ref>Waser, Mark. \"Rational Universal Benevolence: Simpler, Safer, and Wiser Than 'Friendly AI'.\" Artificial General Intelligence. Springer Berlin Heidelberg, 2011. 153-162. \"Terminal-goaled intelligences are short-lived but mono-maniacally dangerous and a correct basis for concern if anyone is smart enough to program high-intelligence and unwise enough to want a paperclip-maximizer.\"</ref><ref>{{cite news|last1=Koebler|first1=Jason|title=Will Superintelligent AI Ignore Humans Instead of Destroying Us?|url=http://motherboard.vice.com/read/will-superintelligent-ai-ignore-humans-instead-of-destroying-us|accessdate=3 February 2016|work=[[Vice Magazine]]|date=2 February 2016|quote=\"This artificial intelligence is not a basically nice creature that has a strong drive for paperclips, which, so long as it's satisfied by being able to make lots of paperclips somewhere else, is then able to interact with you in a relaxed and carefree fashion where it can be nice with you,\" [[Eliezer Yudkowsky|Yudkowsky]] said. \"Imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future, and this choice is then output\u2014that's what a [[paperclip maximizer]] is.\"}}</ref>\n\n====Anthropomorphism====\nIn science fiction, an AI, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in [[The Matrix (film)|The Matrix]] was influenced by a \"disgust\" toward humanity. This is fictitious [[anthropomorphism]]: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions, or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.<ref name=\"yudkowsky-global-risk\" />\n\nScholars sometimes claim that others' predictions about an AI's behavior are illogical anthropomorphism.<ref name=\"yudkowsky-global-risk\" /> An example that might initially be considered anthropomorphism, but is in fact a logical statement about AI behavior, would be the [[Dario Floreano]] experiments where certain robots spontaneously evolved a crude capacity for \"deception\", and tricked other robots into eating \"poison\" and dying: here a trait, \"deception\", ordinarily associated with people rather than with machines, spontaneously evolves in a type of [[convergent evolution]].<ref>{{cite news|title=Real-Life Decepticons: Robots Learn to Cheat|url=https://www.wired.com/2009/08/real-life-decepticons-robots-learn-to-cheat/|accessdate=7 February 2016|work=[[Wired (magazine)|Wired]]|date=18 August 2009}}</ref> According to Paul R. Cohen and [[Edward Feigenbaum]], in order to differentiate between anthropomorphization and logical prediction of AI behavior, \"the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking.\"<ref>Cohen, Paul R., and Edward A. Feigenbaum, eds. The handbook of artificial intelligence. Vol. 3. Butterworth-Heinemann, 2014.</ref>\n\nThere is a near-universal assumption in the scientific community that that an advanced AI, even if it were programmed to have, or adopted, human personality dimensions (such as [[psychopathy]]) to make itself more efficient at certain tasks, e.g., [[Lethal autonomous weapon|tasks involving killing humans]], would not destroy humanity out of human emotions such as \"revenge\" or \"anger.\" This is because it is assumed that an advanced AI would not be conscious<ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> or have testosterone;<ref>{{Cite web|url=https://www.edge.org/conversation/jaron_lanier-the-myth-of-ai|title=The Myth Of AI {{!}} Edge.org|website=www.edge.org|access-date=2020-03-11}}</ref> it ignores the fact that military planners see a conscious superintelligence as the 'holy grail' of interstate warfare.<ref name=\":0\">{{Cite book|last=Scornavacchi|first=Matthew|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a622649.pdf|title=Superintelligence, Humans, and War|publisher=National Defense University, Joint Forces Staff College|year=2015|isbn=|location=Norfolk, Virginia|pages=}}</ref> The academic debate is, instead, between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals; and another side which believes that AI would not destroy humanity at all. Some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.<ref name=\"yudkowsky-global-risk\" /><ref>{{cite news|title=Should humans fear the rise of the machine?|url=https://www.telegraph.co.uk/technology/news/11837157/Should-humans-fear-the-rise-of-the-machine.html|accessdate=7 February 2016|work=[[The Telegraph (UK)]]|date=1 Sep 2015}}</ref>\n\n===Other sources of risk===\n\n==== Competition ====\nIn 2014 philosopher [[Nick Bostrom]] stated that a \"severe race dynamic\" (extreme [[competition]]) between different teams may create conditions whereby the creation of an AGI results in shortcuts to safety and potentially violent conflict.<ref name=\":2\">{{Citation|last=Bostrom, Nick, 1973- author.|title=Superintelligence : paths, dangers, strategies|isbn=978-1-5012-2774-5|oclc=1061147095}}</ref> To address this risk, citing previous scientific collaboration ([[CERN]], the [[Human Genome Project]], and the [[International Space Station]]), Bostrom recommended  [[collaboration]] and the altruistic global adoption of a [[common good]] principle: \"Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals\".<ref name=\":2\" /><sup>:254</sup> Bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits, including reducing haste, thereby increasing investment in safety; avoiding violent conflicts (wars), facilitating sharing solutions to the control problem, and more equitably distributing the benefits.<ref name=\":2\" /><sup>:253</sup> The United States' [[BRAIN Initiative|Brain Initiative]] was launched in 2014, as was the European Union's [[Human Brain Project]]; China's [[China Brain Project|Brain Project]] was launched in 2016.\n\n==== Weaponization of artificial intelligence ====\nSome sources argue that the ongoing [[weaponization of artificial intelligence]] could constitute a catastrophic risk.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><ref name=\":4\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|page=12|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> The risk is actually threefold, with the first risk potentially having geopolitical implications, and the second two definitely having geopolitical implications:\n\n{{cquote|i) The dangers of an AI \u2018race for technological advantage\u2019 framing, regardless of whether the race is seriously pursued;\n\nii) The dangers of an AI \u2018race for technological advantage\u2019 framing and an actual AI race for technological advantage, regardless of whether the race is won;\n\niii) The dangers of an AI race for technological advantage being won.<ref name=\"Cave 2018\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|pages=36\u201340|location=New York, New York, USA|publisher=ACM Press|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref><sup>:37</sup>\n}}\n\nA weaponized conscious superintelligence would affect current US military technological supremacy and transform warfare; it is therefore highly desirable for strategic military planning and interstate warfare.<ref name=\":0\" /><ref name=\":4\" /> The China State Council's 2017 \u201cA Next Generation Artificial Intelligence Development Plan\u201d views AI in geopolitically strategic terms and is pursuing a 'military-civil fusion' strategy to build on China's first-mover advantage in the development of AI in order to establish technological supremacy by 2030,<ref>{{Cite web|url=https://foreignpolicy.com/2017/09/08/china-is-using-americas-own-plan-to-dominate-the-future-of-artificial-intelligence/|title=China Is Using America's Own Plan to Dominate the Future of Artificial Intelligence|last=Kania|first=Gregory Allen, Elsa B.|website=Foreign Policy|language=en-US|access-date=2020-03-11}}</ref> while Russia's President Vladimir Putin has stated that \u201cwhoever becomes the leader in this sphere will become the ruler of the world\u201d.<ref name=\":1\">{{Cite journal|last=Cave|first=Stephen|last2=\u00d3h\u00c9igeartaigh|first2=Se\u00e1n S.|date=2018|title=An AI Race for Strategic Advantage|journal=Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society - AIES '18|location=New York, New York, USA|publisher=ACM Press|volume=|page=2|doi=10.1145/3278721.3278780|isbn=978-1-4503-6012-8}}</ref> James Barrat, documentary filmmaker and author of ''[[Our Final Invention]]'', says in a [[Smithsonian (magazine)|Smithsonian]] interview, \"Imagine: in as little as a decade, a half-dozen companies and nations field computers that rival or surpass human intelligence. Imagine what happens when those computers become expert at programming smart computers. Soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are. And, all the while, each generation of this technology will be weaponized. Unregulated, it will be catastrophic.\"<ref>{{Cite web|url = http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/?no-ist|title = What Happens When Artificial Intelligence Turns On Us?|date = 21 January 2014|accessdate = 26 October 2015|website = Smithsonian|last = Hendry|first = Erica R.}}</ref>\n\n==== Malevolent AGI by design ====\nIt is theorized that malevolent AGI by could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in [[cybercrime]].<ref>{{Cite book|last=Pistono, Federico Yampolskiy, Roman V.|title=Unethical Research: How to Create a Malevolent Artificial Intelligence|date=2016-05-09|oclc=1106238048}}</ref><ref>{{Cite journal|last=Haney|first=Brian Seamus|date=2018|title=The Perils &amp; Promises of Artificial General Intelligence|journal=SSRN Working Paper Series|doi=10.2139/ssrn.3261254|issn=1556-5068}}</ref><sup>:166</sup> Alternatively, malevolent AGI ('evil AI') could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.<ref>{{Cite journal|last=Turchin|first=Alexey|last2=Denkenberger|first2=David|date=2018-05-03|title=Classification of global catastrophic risks connected with artificial intelligence|url=http://dx.doi.org/10.1007/s00146-018-0845-5|journal=AI & SOCIETY|volume=35|issue=1|pages=147\u2013163|doi=10.1007/s00146-018-0845-5|issn=0951-5666}}</ref><sup>:158</sup>\n\n==== Preemptive nuclear strike (nuclear war) ====\nIt is theorized that a country being close to achieving AGI technological supremacy could trigger a [[pre-emptive nuclear strike]] from a rival, leading to a [[Nuclear warfare|nuclear war]].<ref name=\":4\" /><ref>{{Cite book|last=Miller, James D.|title=Singularity Rising: Surviving and Thriving in a Smarter ; Richer ; and More Dangerous World|date=2015|publisher=Benbella Books|oclc=942647155}}</ref>\n\n== Timeframe ==\n{{Main|Artificial general intelligence#Feasibility}}\n\nOpinions vary both on ''whether'' and ''when'' artificial general intelligence will arrive. At one extreme, AI pioneer [[Herbert A. Simon]] wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true.<ref>Harvnb|Simon|1965|p=96 quoted in Harvnb|Crevier|1993|p=109</ref> At the other extreme, roboticist Alan Winfield claims the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical, faster than light spaceflight.<ref>{{cite news|last1=Winfield|first1=Alan|title=Artificial intelligence will not turn into a Frankenstein's monster|url=https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield|accessdate=17 September 2014|work=[[The Guardian]]}}</ref> Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050, depending on the poll.<ref name=\"new yorker doomsday\">{{cite news|author1=Raffi Khatchadourian|title=The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?|url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom|accessdate=7 February 2016|work=[[The New Yorker (magazine)|The New Yorker]]|date=23 November 2015}}</ref><ref>M\u00fcller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555-572). Springer, Cham.</ref>\n\nSkeptics who believe it is impossible for AGI to arrive anytime soon, tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI, because of fears it could lead to government regulation or make it more difficult to secure funding for AI research, or because it could give AI research a bad reputation. Some researchers, such as Oren Etzioni, aggressively seek to quell concern over existential risk from AI, saying \"[Elon Musk] has impugned us in very strong language saying we are unleashing the demon, and so we're answering.\"<ref>{{cite news|author1=Dina Bass|author2=Jack Clark|title=Is Elon Musk Right About AI? Researchers Don't Think So: To quell fears of artificial intelligence running amok, supporters want to give the field an image makeover|url=https://www.bloomberg.com/news/articles/2015-02-04/is-elon-musk-right-about-ai-researchers-don-t-think-so|accessdate=7 February 2016|work=[[Bloomberg News]]|date=5 February 2015}}</ref>\n\nIn 2014 [[Slate (magazine)|Slate]]'s Adam Elkus argued \"our 'smartest' AI is about as intelligent as a toddler\u2014and only when it comes to instrumental tasks like information recall. Most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over.\" Elkus goes on to argue that Musk's \"summoning the demon\" analogy may be harmful because it could result in \"harsh cuts\" to AI research budgets.<ref>{{cite news|last1=Elkus|first1=Adam|title=Don't Fear Artificial Intelligence|url=http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html|accessdate=15 May 2016|work=[[Slate (magazine)|Slate]]|date=31 October 2014|language=en-US}}</ref>\n\nThe [[Information Technology and Innovation Foundation]] (ITIF), a Washington, D.C. think-tank, awarded its Annual Luddite Award to \"alarmists touting an artificial intelligence apocalypse\"; its president, [[Robert D. Atkinson]], complained that Musk, Hawking and AI experts say AI is the largest existential threat to humanity. Atkinson stated \"That's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation.\"<ref>[https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award Artificial Intelligence Alarmists Win ITIF\u2019s Annual Luddite Award], ITIF Website, 19 January 2016</ref><ref>{{cite news|title='Artificial intelligence alarmists' like Elon Musk and Stephen Hawking win 'Luddite of the Year' award|url=https://www.independent.co.uk/life-style/gadgets-and-tech/news/elon-musk-stephen-hawking-luddite-award-of-the-year-itif-a6821921.html|accessdate=7 February 2016|work=[[The Independent (UK)]]|date=19 January 2016}}</ref><ref>{{cite web|last1=Garner|first1=Rochelle|title=Elon Musk, Stephen Hawking win Luddite award as AI 'alarmists'|url=https://www.cnet.com/news/elon-musk-stephen-hawking-win-annual-luddite-award/|website=CNET|accessdate=7 February 2016}}</ref><!-- <ref>{{cite news|last1=Price|first1=Emily|title=Elon Musk nominated for 'luddite' of the year prize over artificial intelligence fears|work=[[The Guardian]]|url=https://www.theguardian.com/technology/2015/dec/24/elon-musk-nominated-for-luddite-of-the-year-prize-over-artificial-intelligence-fears|accessdate=7 February 2016|date=24 December 2015}}</ref> --> ''[[Nature (journal)|Nature]]'' sharply disagreed with the ITIF in an April 2016 editorial, siding instead with Musk, Hawking, and Russell, and concluding: \"It is crucial that progress in technology is matched by solid, well-funded research to anticipate the scenarios it could bring about{{nbsp}}... If that is a Luddite perspective, then so be it.\"<ref name=nature_anticipating>{{cite journal|title=Anticipating artificial intelligence|journal=Nature|date=26 April 2016|volume=532|issue=7600|page=413|doi=10.1038/532413a|pmid=27121801|bibcode=2016Natur.532Q.413.}}</ref> In a 2015 ''[[Washington Post]]'' editorial, researcher [[Murray Shanahan]] stated that human-level AI is unlikely to arrive \"anytime soon\", but that nevertheless \"the time to start thinking through the consequences is now.\"<ref>{{cite news|author1=Murray Shanahan|author-link=Murray Shanahan|title=Machines may seem intelligent, but it'll be a while before they actually are|url=https://www.washingtonpost.com/news/in-theory/wp/2015/11/03/machines-may-seem-intelligent-but-itll-be-a-while-before-they-actually-are/|accessdate=15 May 2016|work=[[The Washington Post]]|date=3 November 2015|language=en-US}}</ref>\n\n== Perspectives ==\n<!-- Ideally a substantive quote or explanation of opinion should be given, rather than just whether they're for or against something. This section can easily be split off if the article becomes too long. When adding whether to add new content in this section, consider whether it's redundant with what's already in the article, and whether the point being made by the person should instead be integrated into the rest of the article. -[[User:Rolf H Nelson]] -->\nThe thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community, as well as in the public at large. Many of the opposing viewpoints, however, share common ground.\n\nThe Asilomar AI Principles, which contain only the principles agreed to by 90% of the attendees of the [[Future of Life Institute]]'s Beneficial AI 2017 conference,<ref name=\"life 3.0\">{{cite book|author1=Max Tegmark|author-link=Max Tegmark|title=Life 3.0: Being Human in the Age of Artificial Intelligence|date=2017|publisher=Knopf|location=Mainstreaming AI Safety|isbn=9780451485076|edition=1st|title-link=Life 3.0: Being Human in the Age of Artificial Intelligence}}</ref><!-- Epilogue: The Tale of the FLI Team --> agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"<ref>{{cite web|title=AI Principles|url=https://futureoflife.org/ai-principles/|website=[[Future of Life Institute]]|accessdate=11 December 2017}}</ref><ref>{{cite news|title=Elon Musk and Stephen Hawking warn of artificial intelligence arms race|url=http://www.newsweek.com/ai-asilomar-principles-artificial-intelligence-elon-musk-550525|accessdate=11 December 2017|work=[[Newsweek]]|date=31 January 2017|language=en}}</ref> AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane ''[[Terminator (franchise)|Terminator]]'' pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work{{nbsp}}... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"<ref name=\"life 3.0\"/><!-- Epilogue: The Tale of the FLI Team --><ref>{{cite book|last1=Bostrom|first1=Nick|author-link=Nick Bostrom|title=Superintelligence: Paths, Dangers, Strategies|date=2016|edition=Paperback|chapter=New Epilogue to the Paperback Edition|title-link=Superintelligence: Paths, Dangers, Strategies}}</ref>\n\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic [[Martin Ford (author)|Martin Ford]] states that \"I think it seems wise to apply something like [[Dick Cheney]]'s famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low \u2014 but the implications are so dramatic that it should be taken seriously\";<ref>{{cite book|author1=Martin Ford |author-link=Martin Ford (author)|title=Rise of the Robots: Technology and the Threat of a Jobless Future|date=2015|isbn=9780465059997|chapter=Chapter 9: Super-intelligence and the Singularity|title-link=Rise of the Robots: Technology and the Threat of a Jobless Future}}</ref> similarly, an otherwise skeptical ''[[The Economist|Economist]]'' stated in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".<ref name=economist_review/>\n\nA 2017 email survey of researchers with publications at the 2015 [[Conference on Neural Information Processing Systems|NIPS]] and [[International Conference on Machine Learning|ICML]] machine learning conferences asked them to evaluate [[Stuart J. Russell]]'s concerns about AI risk. Of the respondents, 5% said it was \"among the most important problems in the field\", 34% said it was \"an important problem\", and 31% said it was \"moderately important\", whilst 19% said it was \"not important\" and 11% said it was \"not a real problem\" at all.<ref>{{cite arxiv |last1=Grace |first1=Katja |last2=Salvatier |first2=John |last3=Dafoe |first3=Allan |last4=Zhang |first4=Baobao |last5=Evans |first5=Owain |title=When Will AI Exceed Human Performance? Evidence from AI Experts |eprint=1705.08807 |date=24 May 2017 |class=cs.AI}}</ref>\n\n=== Endorsement ===\n[[File:Bill Gates June 2015.jpg|thumb|right|Bill Gates has stated \"I{{nbsp}}... don't understand why some people are not concerned.\"<ref name=\"BBC News\"/>]]\n{{Further|Existential risk}}\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many public figures; perhaps the most famous are [[Elon Musk]], [[Bill Gates]], and [[Stephen Hawking]]. The most notable AI researchers to endorse the thesis are Russell and [[I. J. Good|I.J. Good]], who advised [[Stanley Kubrick]] on the filming of ''[[2001: A Space Odyssey]]''. Endorsers of the thesis sometimes express bafflement at skeptics: Gates states that he does not \"understand why some people are not concerned\",<ref name=\"BBC News\">{{cite news|last1=Rawlinson|first1=Kevin|title=Microsoft's Bill Gates insists AI is a threat|url=https://www.bbc.co.uk/news/31047780|work=[[BBC News]]|accessdate=30 January 2015|date=29 January 2015}}</ref> and Hawking criticized widespread indifference in his 2014 editorial: {{cquote|'So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here{{endash}}we'll leave the lights on?' Probably not{{endash}}but this is more or less what is happening with AI.'<ref name=\"hawking editorial\"/>}}\n\nMany of the scholars who are concerned about existential risk believe that the best way forward would be to conduct (possibly massive) research into solving the difficult \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?<ref name=\"superintelligence\" /><ref name=\"physica_scripta\" /><!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->\n\n=== Skepticism ===\n{{Further|Artificial general intelligence#Feasibility}}\nThe thesis that AI can pose existential risk also has many strong detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God; at an extreme, [[Jaron Lanier]] argues that the whole concept that current machines are in any way intelligent is \"an illusion\" and a \"stupendous con\" by the wealthy.<ref name=\"atlantic-but-what\">{{cite magazine |url=https://www.theatlantic.com/health/archive/2014/05/but-what-does-the-end-of-humanity-mean-for-me/361931/ |title=But What Would the End of Humanity Mean for Me? |magazine=The Atlantic | date = 9 May 2014 | accessdate =12 December 2015}}</ref>\n\nMuch of existing criticism argues that AGI is unlikely in the short term. Computer scientist [[Gordon Bell]] argues that the human race will already destroy itself before it reaches the technological singularity. [[Gordon Moore]], the original proponent of [[Moore's Law]], declares that \"I am a skeptic. I don't believe (a technological singularity) is likely to happen, at least for a long time. And I don't know why I feel that way.\"<ref>{{cite news |title=Tech Luminaries Address Singularity |url=https://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity |accessdate=8 April 2020 |work=IEEE Spectrum: Technology, Engineering, and Science News |issue=SPECIAL REPORT: THE SINGULARITY |date=1 June 2008 |language=en}}</ref> [[Baidu]] Vice President [[Andrew Ng]] states AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"<ref name=shermer>{{cite news|last1=Shermer|first1=Michael|title=Apocalypse AI|url=https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/|accessdate=27 November 2017|work=Scientific American|date=1 March 2017|pages=77|language=en|doi=10.1038/scientificamerican0317-77|bibcode=2017SciAm.316c..77S}}</ref>\n\nSome AI and AGI researchers may be reluctant to discuss risks, worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by \"alarmist\" messages, or worrying that such messages will lead to cuts in AI funding. ''Slate'' notes that some researchers are dependent on grants from government agencies such as [[DARPA]].<ref name=slate_killer />\n\nAt some point in an intelligence explosion driven by a single AI, the AI would have to become vastly better at software innovation than the best innovators of the rest of the world; economist [[Robin Hanson]] is skeptical that this is possible.<ref>http://intelligence.org/files/AIFoomDebate.pdf</ref><ref>{{cite web|url=http://www.overcomingbias.com/2014/07/30855.html|title=Overcoming Bias : I Still Don't Get Foom|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/07/debating-yudkowsky.html|title=Overcoming Bias : Debating Yudkowsky|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html|title=Overcoming Bias : Foom Justifies AI Risk Efforts Now|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref><ref>{{cite web|url=http://www.overcomingbias.com/2011/06/the-betterness-explosion.html|title=Overcoming Bias : The Betterness Explosion|website=www.overcomingbias.com|accessdate=20 September 2017}}</ref>\n\n=== Intermediate views ===\nIntermediate views generally take the position that the control problem of artificial general intelligence may exist, but that it will be solved via progress in artificial intelligence, for example by creating a moral learning environment for the AI, taking care to spot clumsy malevolent behavior (the 'sordid stumble')<ref>{{Cite document|title=Interpreting expert disagreement: The influence of decisional cohesion on the persuasiveness of expert group recommendations|last=Votruba|first=Ashley M.|last2=Kwan|first2=Virginia S.Y.|date=2014|doi=10.1037/e512142015-190}}</ref> and then directly intervening in the code before the AI refines its behavior, or even peer pressure from [[Friendly artificial intelligence|friendly AIs]].<ref>{{Cite journal|last=Agar|first=Nicholas|date=|title=Don't Worry about Superintelligence|url=https://jetpress.org/v26.1/agar.htm|journal=Journal of Evolution & Technology|volume=26|issue=1|pages=73\u201382|via=}}</ref> In a 2015 ''[[Wall Street Journal]]'' panel discussion devoted to AI risks, [[IBM]]'s Vice-President of Cognitive Computing, Guruduth S. Banavar, brushed off discussion of AGI with the phrase, \"it is anybody's speculation.\"<ref>{{cite news|last1=Greenwald|first1=Ted|title=Does Artificial Intelligence Pose a Threat?|url=https://www.wsj.com/articles/does-artificial-intelligence-pose-a-threat-1431109025|accessdate=15 May 2016|work=Wall Street Journal|date=11 May 2015}}</ref> [[Geoffrey Hinton]], the \"godfather of deep learning\", noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but stated that he continues his research because \"the prospect of discovery is too ''sweet''\".<ref name=slate_killer /><ref name=\"new yorker doomsday\" /> In 2004, law professor [[Richard Posner]] wrote that dedicated efforts for addressing AI can wait, but that we should gather more information about the problem in the meanwhile.<ref>{{cite book|author1=Richard Posner|author-link=Richard Posner|title=Catastrophe: risk and response|date=2006|publisher=Oxford University Press|location=Oxford|isbn=978-0-19-530647-7}}</ref><ref name=physica_scripta>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy}}</ref>\n\n=== Popular reaction ===\nIn a 2014 article in ''[[The Atlantic (magazine)|The Atlantic]]'', James Hamblin noted that most people do not care one way or the other about artificial general intelligence, and characterized his own gut reaction to the topic as: \"Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?\"<ref name=\"atlantic-but-what\" />\n\nDuring a 2016 [[Wired (magazine)|''Wired'']] interview of President [[Barack Obama]] and MIT Media Lab's [[Joi Ito]], Ito stated: {{cquote|There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.}} Obama added:<ref>{{cite news|last1=Dadich|first1=Scott|url=https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/|title=Barack Obama Talks AI, Robo Cars, and the Future of the World|work=WIRED|accessdate=27 November 2017}}</ref><ref>{{cite news|last1=Kircher|first1=Madison Malone|url=https://nymag.com/selectall/2016/10/barack-obama-talks-artificial-intelligence-in-wired.html|title=Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'|work=Select All|accessdate=27 November 2017|language=en}}</ref>\n\n{{cquote|\"And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\"}}\n\n[[Hillary Clinton]] stated in ''\"[[What Happened (Clinton book)|What Happened]]\"'':\n{{cquote|Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I\u2019d start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.<ref>{{cite book|last1=Clinton|first1=Hillary|title=What Happened|date=2017|isbn=978-1-5011-7556-5|page=241|title-link=What Happened (Clinton book)}} via [http://lukemuehlhauser.com/hillary-clinton-on-ai-risk/]</ref>}}\n\nIn a [[YouGov]] poll of the public for the [[British Science Association]], about a third of survey respondents said AI will pose a threat to the long term survival of humanity.<ref name=\"bsa poll\">{{cite news|url=http://www.businessinsider.com/over-a-third-of-people-think-ai-poses-a-threat-to-humanity-2016-3?r=UK&IR=T|title=Over a third of people think AI poses a threat to humanity|date=11 March 2016|work=[[Business Insider]]|accessdate=16 May 2016}}</ref> Referencing a poll of its readers, Slate's Jacob Brogan stated that \"most of the (readers filling out our online survey) were unconvinced that A.I. itself presents a direct threat.\"<ref name=\":3\">{{cite news|last1=Brogan|first1=Jacob|url=http://www.slate.com/blogs/future_tense/2016/05/06/futurography_readers_share_their_opinions_about_killer_artificial_intelligence.html|title=What Slate Readers Think About Killer A.I.|date=6 May 2016|work=Slate|accessdate=15 May 2016|language=en-US}}</ref>\n\nIn 2018, a [[SurveyMonkey]] poll of the American public by [[USA Today]] found 68% thought the real current threat remains \"human intelligence\"; however, the poll also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and 38% said it would do \"equal amounts of harm and good\".<ref name=\":3\" />\n\nOne [[Technological utopianism|techno-utopia]]<nowiki/>n viewpoint expressed in some popular fiction is that AGI may tend towards peace-building.<ref>{{Cite journal|last=LIPPENS|first=RONNIE|date=2002|title=Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games|journal=Utopianstudies Utopian Studies|language=English|volume=13|issue=1|pages=135\u2013147|issn=1045-991X|oclc=5542757341}}</ref>\n\n== Mitigation ==\n{{see also|AI control problem}}\nResearchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.<ref>{{cite news |last1=Vincent |first1=James |title=Google's AI researchers say these are the five key problems for robot safety |url=https://www.theverge.com/circuitbreaker/2016/6/22/11999664/google-robots-ai-safety-five-problems |accessdate=5 April 2020 |work=The Verge |date=22 June 2016 |language=en}}</ref><ref>Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. \"Concrete problems in AI safety.\" arXiv preprint arXiv:1606.06565 (2016).</ref> A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion.<!-- Precipice chapter=Chapter 2: Existential Risk|at=Footnote 55 --> Bostrom suggests a general principle of \"differential technological development\", that funders should consider working to speed up the development of protective technologies relative to the development of dangerous ones.<ref>{{cite book |title=[[The Precipice: Existential Risk and the Future of Humanity]] |author=[[Toby Ord]]| date=2020 |publisher=Bloomsbury Publishing Plc |isbn=9781526600196 }}</ref><!-- Precipice chapter=Chapter 7: Safeguarding Humanity|at=Kindle loc 3327 --> Some funders, such as [[Elon Musk]], propose that radical [[human enhancement|human cognitive enhancement]] could be such a technology, for example through direct neural linking between man and machine; however, others argue that enhancement technologies may themselves pose an existential risk.<ref>{{cite news |title=Elon Musk wants to hook your brain up directly to computers \u2014 starting next year |url=https://www.nbcnews.com/mach/tech/elon-musk-wants-hook-your-brain-directly-computers-starting-next-ncna1030631 |accessdate=5 April 2020 |work=NBC News |date=2019 |language=en}}</ref><ref>{{cite news |last1=Torres |first1=Phil |title=Only Radically Enhancing Humanity Can Save Us All |url=https://slate.com/technology/2018/09/genetic-engineering-to-stop-doomsday.html |accessdate=5 April 2020 |work=Slate Magazine |date=18 September 2018 |language=en}}</ref> Researchers, if they are not caught off-guard, could closely monitor or attempt to [[AI control problem#Capability control|box in]] an initial AI at a risk of becoming too powerful, as an attempt at a stop-gap measure. A dominant superintelligent AI, if it were aligned with human interests, might itself take action to mitigate the risk of takeover by rival AI, although the creation of the dominant AI could itself pose an existential risk.<ref>{{cite journal |last1=Barrett |first1=Anthony M. |last2=Baum |first2=Seth D. |title=A model of pathways to artificial superintelligence catastrophe for risk and decision analysis |journal=Journal of Experimental & Theoretical Artificial Intelligence |date=23 May 2016 |volume=29 |issue=2 |pages=397\u2013414 |doi=10.1080/0952813X.2016.1186228}}</ref>\n\nInstitutions such as the [[Machine Intelligence Research Institute]], the [[Future of Humanity Institute]],<ref>{{cite news |author=Mark Piesing |date=17 May 2012 |title=AI uprising: humans will be outsourced, not obliterated |url=https://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us |newspaper=Wired |location= |accessdate=12 December 2015 }}</ref><ref>{{cite news |last=Coughlan |first=Sean |date=24 April 2013 |title=How are humans going to become extinct? |url=https://www.bbc.com/news/business-22002530 |newspaper=BBC News |location= |accessdate=29 March 2014}}</ref> the [[Future of Life Institute]], the [[Centre for the Study of Existential Risk]], and the [[Center for Human-Compatible AI]]<ref>{{cite news|last1=Technology Correspondent|first1=Mark Bridge|title=Making robots less confident could prevent them taking over|url=https://www.thetimes.co.uk/article/making-robots-less-confident-could-prevent-them-taking-over-gnsblq7lx|accessdate=21 March 2018|work=The Times|date=10 June 2017|language=en}}</ref> are involved in mitigating existential risk from advanced artificial intelligence, for example by research into [[friendly artificial intelligence]].<ref name=\"givewell\"/><ref name=\"atlantic-but-what\"/><ref name=\"hawking editorial\">{{cite news |title=Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&nbsp;\u2013 but are we taking AI seriously enough?' |url=https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html |accessdate=3 December 2014 |publisher=[[The Independent (UK)]]}}</ref>\n\n=== Views on banning and regulation ===\n\n==== Banning ====\nThere is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise, and probably futile.<ref>{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online|quote=For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible...}}</ref><ref>{{cite journal|title=Responses to catastrophic AGI risk: a survey|journal=[[Physica Scripta]]|date=19 December 2014|volume=90|issue=1|author1=Kaj Sotala|author2-link=Roman Yampolskiy|author2=Roman Yampolskiy|quote=In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals.}}</ref><ref>{{cite news|author1=Brad Allenby|title=The Wrong Cognitive Measuring Stick|url=http://www.slate.com/articles/technology/future_tense/2016/04/why_it_s_a_mistake_to_compare_a_i_with_human_intelligence.html|accessdate=15 May 2016|work=Slate|date=11 April 2016|language=en-US|quote=It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation.}}</ref> Skeptics argue that regulation of AI would be completely valueless, as no existential risk exists. Almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly. The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.<ref name=\"mcginnis\">{{cite journal|author1=John McGinnis|author-link=John McGinnis|title=Accelerating AI|journal=[[Northwestern University Law Review]]|date=Summer 2010|volume=104|issue=3|pages=1253\u20131270|accessdate=16 July 2014|url=http://scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=1193&context=nulr_online}}</ref><ref>{{cite news|title=Why We Should Think About the Threat of Artificial Intelligence|url=https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence|accessdate=7 February 2016|work=[[The New Yorker]]|date=4 October 2013|quote=Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage\u2014economic, military, even artistic\u2014of every advance in automation is so compelling,' [[Vernor Vinge]], the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.'}}</ref> Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend towards general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and [[politicization of science|politicizing]] the underlying debate.<ref>{{Cite journal|last=Baum|first=Seth|date=2018-08-22|title=Superintelligence Skepticism as a Political Tool|url=http://dx.doi.org/10.3390/info9090209|journal=Information|volume=9|issue=9|pages=209|doi=10.3390/info9090209|issn=2078-2489}}</ref>\n\n==== Regulation ====\n{{See also|Regulation of algorithms|Regulation of artificial intelligence}}\n\n[[Elon Musk]] called for some sort of regulation of AI development as early as 2017. According to [[National Public Radio|NPR]], the [[Tesla, Inc.|Tesla]] CEO is \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid{{nbsp}}... [as] they should be.\" In response, politicians express skepticism about the wisdom of regulating a technology that's still in development.<ref>{{cite news|title=Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'|url=https://www.npr.org/sections/thetwo-way/2017/07/17/537686649/elon-musk-warns-governors-artificial-intelligence-poses-existential-risk|accessdate=27 November 2017|work=NPR.org|language=en}}</ref><ref>{{cite news|last1=Gibbs|first1=Samuel|title=Elon Musk: regulate AI to combat 'existential threat' before it's too late|url=https://www.theguardian.com/technology/2017/jul/17/elon-musk-regulation-ai-combat-existential-threat-tesla-spacex-ceo|accessdate=27 November 2017|work=The Guardian|date=17 July 2017}}</ref><ref name=cnbc>{{cite news|last1=Kharpal|first1=Arjun|title=A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says|url=https://www.cnbc.com/2017/11/07/ai-infancy-and-too-early-to-regulate-intel-ceo-brian-krzanich-says.html|accessdate=27 November 2017|work=CNBC|date=7 November 2017}}</ref>\n\nResponding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO [[Brian Krzanich]] argues that artificial intelligence is in its infancy and that it is too early to regulate the technology.<ref name=cnbc/> Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.<ref>{{cite journal|doi=10.1016/j.bushor.2018.08.004|title=Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence|journal=Business Horizons|volume=62|pages=15\u201325|year=2019|last1=Kaplan|first1=Andreas|last2=Haenlein|first2=Michael}}</ref> Developing well regulated weapons systems is in line with the ethos of some countries' militaries.<ref>{{Cite journal|last=Baum|first=Seth D.|last2=Goertzel|first2=Ben|last3=Goertzel|first3=Ted G.|date=January 2011|title=How long until human-level AI? Results from an expert assessment|journal=Technological Forecasting and Social Change|volume=78|issue=1|pages=185\u2013195|doi=10.1016/j.techfore.2010.09.006|issn=0040-1625}}</ref> On October 31, 2019, the Unites States Department of Defense's (DoD's) Defense Innovation Board published the draft of a report outlining five principles for weaponized AI and making 12 recommendations for the ethical use of artificial intelligence by the DoD that seeks to manage the control problem in all DoD weaponized AI.<ref>{{Cite book|last=United States. Defense Innovation Board.|title=AI principles : recommendations on the ethical use of artificial intelligence by the Department of Defense|oclc=1126650738}}</ref>\n\nRegulation of AGI would likely be influenced by regulation of weaponized or militarized AI, i.e., the [[Artificial intelligence arms race|AI arms race]], the regulation of which is an emerging issue. Any form of regulation will likely be influenced by developments in leading countries' domestic policy towards militarized AI, in the US under the purview of the National Security Commission on Artificial Intelligence,<ref>{{Cite web|url=https://www.congress.gov/bill/115th-congress/house-bill/5356|title=H.R.5356 - 115th Congress (2017-2018): National Security Commission Artificial Intelligence Act of 2018|last=Stefanik|first=Elise M.|date=2018-05-22|website=www.congress.gov|access-date=2020-03-13}}</ref><ref>{{Cite journal|last=Baum|first=Seth|date=2018-09-30|title=Countering Superintelligence Misinformation|journal=Information|volume=9|issue=10|pages=244|doi=10.3390/info9100244|issn=2078-2489}}</ref> and international moves to regulate an AI arms race. Regulation of research into AGI focuses on the role of review boards and encouraging research into safe AI, and the possibility of differential technological progress (prioritizing risk-reducing strategies over risk-taking strategies in AI development) or conducting international mass surveillance to perform AGI arms control.<ref name=\":5\">{{Cite journal|last=Sotala|first=Kaj|last2=Yampolskiy|first2=Roman V|date=2014-12-19|title=Responses to catastrophic AGI risk: a survey|journal=Physica Scripta|volume=90|issue=1|pages=018001|doi=10.1088/0031-8949/90/1/018001|issn=0031-8949}}</ref> Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.<ref name=\":5\" /> AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.<ref>{{Cite journal|last=Geist|first=Edward Moore|date=2016-08-15|title=It's already too late to stop the AI arms race\u2014We must manage it instead|journal=Bulletin of the Atomic Scientists|volume=72|issue=5|pages=318\u2013321|doi=10.1080/00963402.2016.1216672|bibcode=2016BuAtS..72e.318G|issn=0096-3402}}</ref><ref>{{Cite journal|last=Maas|first=Matthijs M.|date=2019-02-06|title=How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons|journal=Contemporary Security Policy|volume=40|issue=3|pages=285\u2013311|doi=10.1080/13523260.2019.1576464|issn=1352-3260}}</ref>\n\n== See also ==\n* [[AI takeover]]\n* [[Artificial intelligence arms race]]\n* [[Effective altruism#Long term future and global catastrophic risks|Effective altruism \u00a7 Long term future and global catastrophic risks]]\n* [[Grey goo]]\n* ''[[Human Compatible]]''\n* [[Lethal autonomous weapon]]\n*[[Regulation of algorithms]]\n*[[Regulation of artificial intelligence]]\n* [[Robot ethics#In popular culture|Robot ethics \u00a7 In popular culture]]\n* ''[[Superintelligence: Paths, Dangers, Strategies]]''\n* [[System accident]]\n* [[Technological singularity]]\n*''[[The Precipice: Existential Risk and the Future of Humanity]]''\n\n== References ==\n{{Reflist}}\n\n{{-}}\n{{Existential risk from artificial intelligence|state=expanded}}\n{{Effective altruism}}\n{{Doomsday}}\n\n[[Category:Existential risk from artificial general intelligence| ]]\n[[Category:Futures studies]]\n[[Category:Future problems]]\n[[Category:Human extinction]]\n[[Category:Technology hazards]]\n[[Category:Doomsday scenarios]]\n", "name_user": "WeyerStudentOfAgrippa", "label": "safe", "comment": "\u2192\u200eSkepticism:brackets", "url_page": "//en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence"}
{"title_page": "Rajasthan Youth Congress", "text_new": "{{Use dmy dates|date=April 2020}}\n{{Use Indian English|date=April 2020}}\n{{Infobox political youth organization\n | colorcode           = DeepSkyBlue\n | name                = Rajasthan Youth Congress\n | native_name         = \u0930\u093e\u091c\u0938\u094d\u0925\u093e\u0928 \u092f\u0941\u0935\u093e \u0915\u093e\u0902\u0917\u094d\u0930\u0947\u0938\n |abbreviation         = RPYC\n | colorcode           = DeepSkyBlue\n | logo                = [[File:Indian Youth Congress Logo.jpg]]\n | National President  = [[B.V Srinivas]]\n | national in charge  = Krishna Allavaru\n | President           = [[Mukesh Bhakar]], [[Member of Legislative Assembly|MLA]]\n | State in charge     = [[Abraham Roy Mani]]|[[Dr. Palak Verma]]|[[Manju Tongad]]\n | Vice president      = [[Amardeen Fakir]]|[[Rakesh Meena]]|[[Sanjta Sihag]]\n | mother party        = [[Indian Youth Congress]] \n | ideology            = [[Populism]]|[[Social liberalism]]|[[Democratic socialism]]|[[Social democracy]]|[[Secularism]]|[[Big tent]]\n | website             = [http://iyc.in/ iyc.in] \n | headquarters       = B.613 SAWAI JAISINGH HIGHWAY BANIPARK JAIPUR, Rajasthan\n | founded             = 1960 \n}}\n\nThe '''Rajasthan Youth Congress''' or RPYC is the state wing of the [[Indian Youth Congress]], youth wing of [[Indian National Congress]].\n'''[[Mukesh Bhakar]]''' is the President of Rajasthan Pradesh Youth Congress. [[Mukesh Bhakar]] is also MLA from [[Ladnun]] assembly in [[Nagaur District]]. <ref> https://www.jagran.com/rajasthan/jaipur-congress-mla-mukesh-bhakar-appointed-as-rajasthan-pradesh-youth-congress-state-president-organization-elections-20172785.html</ref>\n'''Amardeen Fakir''', '''Rakesh Meena''' and '''Sanjita Sihag''' is the State Vice president of Rajasthan Youth Congress. \nAlso there are 10 general secretaries and 40+ secretaries having charge of 39 district units of the organisation.\n== Notes ==\n{{reflist|group=lower-alpha}}\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://rajasthanpcc.com// Rajasthan Pradesh Congress Committee, Official website]\n* [http://inc.in Indian National Congress, Official website]\n\n{{Indian National Congress}}\n{{Indian political parties}}\n{{India topics}}\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.iyc.in iyc.in]\n\n{{Indian National Congress}}\n\n[[Category:Indian Youth Congress|*]]\n[[Category:Politics of Madhya Pradesh]]\n", "text_old": "{{Use dmy dates|date=April 2020}}\n{{Use Indian English|date=April 2020}}\n{{Infobox political youth organization\n | colorcode           = DeepSkyBlue\n | name                = Rajasthan Youth Congress\n | native_name         = \u0930\u093e\u091c\u0938\u094d\u0925\u093e\u0928 \u092f\u0941\u0935\u093e \u0915\u093e\u0902\u0917\u094d\u0930\u0947\u0938\n |abbreviation         = RPYC\n | colorcode           = DeepSkyBlue\n | logo                = [[File:Indian Youth Congress Logo.jpg]]\n | National President  = B.V Srinivas\n | national in charge  = Krishna Allavaru\n | State President     = [[Mukesh Bhakar]], [[Member of Legislative Assembly|MLA]]\n | State in charge     = [[Abraham Roy Mani]]|[[Dr. Palak Verma]]|[[Manju Tongad]]\n | Vice president      = [[Amardeen Fakir]]|[[Rakesh Meena]]|[[Sanjta Sihag]]\n | mother party        = [[Indian Youth Congress]] \n | ideology            = [[Populism]]|[[Social liberalism]]|[[Democratic socialism]]|[[Social democracy]]|[[Secularism]]|[[Big tent]]\n | website             = [http://iyc.in/ iyc.in] \n | headquarters       = B.613 SAWAI JAISINGH HIGHWAY BANIPARK JAIPUR, Rajasthan\n | founded             = 1960 \n}}\n\nThe '''Rajasthan Youth Congress''' or RPYC is the state wing of the [[Indian Youth Congress]], youth wing of [[Indian National Congress]].\n'''[[Mukesh Bhakar]]''' is the President of Rajasthan Pradesh Youth Congress. [[Mukesh Bhakar]] is also MLA from [[Ladnun]] assembly in [[Nagaur District]]. <ref> https://www.jagran.com/rajasthan/jaipur-congress-mla-mukesh-bhakar-appointed-as-rajasthan-pradesh-youth-congress-state-president-organization-elections-20172785.html</ref>\n'''Amardeen Fakir''' is the  Executive Vice President of Rajasthan Youth Congress, '''Rakesh Meena''' and '''Sanjita Sihag'''is the State Vice prasident of Rajasthan Youth Congress. \nAlso there are 10 general secretaries and 40+ secretaries having charge of 39 district units of the organization.\n== Notes ==\n{{reflist|group=lower-alpha}}\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://rajasthanpcc.com// Rajasthan Pradesh Congress Committee, Official website]\n* [http://inc.in Indian National Congress, Official website]\n\n{{Indian National Congress}}\n{{Indian political parties}}\n{{India topics}}\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://www.iyc.in iyc.in]\n\n{{Indian National Congress}}\n\n[[Category:Indian Youth Congress|*]]\n[[Category:Politics of Madhya Pradesh]]\n", "name_user": "Mukesh Bhakar 0013", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Rajasthan_Youth_Congress"}
{"title_page": "Aya (2012 film)", "text_new": "{{short description|2012 film}}\n{{Use dmy dates|date=April 2020}}\n{{Infobox film\n| name           = Aya\n| image          = Aya (2012 film).jpg\n| caption        = Film poster\n| director       = {{ill|Oded Binnun|de}}<br />Mihal Brezis\n| producer       = \n| writer         = Oded Binnun<br />Mihal Brezis<br />Tom Shoval\n| narrator       = \n| starring       = [[Sarah Adler]]\n| music          = \n| cinematography =\n| editing        = \n| studio         = \n| distributor    = \n| released       = {{Film date|2012|9|1|Israel|df=yes}}\n| runtime        = 40 minutes\n| country        = France<br />Israel\n| language       = English<br>Hebrew\n| budget         = \n}}\n\n'''''Aya''''' is a 2012 French-Israeli [[short film|short]] [[drama film]] directed by {{ill|Oded Binnun|de}} and Mihal Brezis. \n\nThe film was nominated for the [[Academy Award for Best Live Action Short Film]] at the [[87th Academy Awards]],<ref name=\"87th\">{{cite web |url=https://www.bbc.co.uk/news/entertainment-arts-30835975 |title=Oscars 2015: Nominations list |accessdate=24 January 2015 |publisher=[[BBC News]]}}</ref> but did not win. Aya, the main character, is waiting for someone at [[Ben Gurion International Airport]]. A driver asks her to hold his welcome sign for Mr. Overby, who is flying into Israel to judge the [[Arthur Rubinstein International Piano Master Competition|Arthur Rubinstein piano competition]]. When the Danish juror shows up, Aya decides on an impulse to drive him to his hotel in [[Jerusalem]]. During the car ride, an unexpected intimacy develops between Aya and the reserved Overby.<ref>[http://www.timesofisrael.com/israeli-film-aya-fails-to-take-home-oscar-for-best-short-movie/ Israeli film Aya fails to take home Oscar for best short movie]</ref>\n\n==Cast==\n* [[Sarah Adler]] as Aya\n* [[Ulrich Thomsen]] as Mr. Overby\n\n==References ==\n{{reflist}}\n\n==External links==\n* {{IMDb title|2326094}}\n* {{AllMovie title|623689}}\n* {{Rotten Tomatoes|aya_2012}}\n\n{{DEFAULTSORT:Aya}}\n[[Category:2012 drama films]]\n[[Category:2012 films]]\n[[Category:Films set in Israel]]\n[[Category:English-language films]]\n[[Category:French drama films]]\n[[Category:French films]]\n[[Category:French short films]]\n[[Category:Israeli drama films]]\n[[Category:Israeli films]]\n[[Category:Israeli short films]]\n[[Category:2012 short films]]\n{{2010s-France-film-stub}}\n{{Israel-film-stub}}\n", "text_old": "{{short description|2012 film}}\n{{Use dmy dates|date=June 2018}}\n{{Infobox film\n| name           = Aya\n| image          = Aya (2012 film).jpg\n| caption        = Film poster\n| director       = {{ill|Oded Binnun|de}}<br />Mihal Brezis\n| producer       = \n| writer         = Oded Binnun<br />Mihal Brezis<br />Tom Shoval\n| narrator       = \n| starring       = [[Sarah Adler]]\n| music          = \n| cinematography =\n| editing        = \n| studio         = \n| distributor    = \n| released       = {{Film date|2012|9|1|Israel|df=yes}}\n| runtime        = 40 minutes\n| country        = France<br />Israel\n| language       = English<br>Hebrew\n| budget         = \n}}\n\n'''''Aya''''' is a 2012 French-Israeli [[short film|short]] [[drama film]] directed by {{ill|Oded Binnun|de}} and Mihal Brezis. \n\nThe film was nominated for the [[Academy Award for Best Live Action Short Film]] at the [[87th Academy Awards]],<ref name=\"87th\">{{cite web |url=https://www.bbc.co.uk/news/entertainment-arts-30835975 |title=Oscars 2015: Nominations list |accessdate=24 January 2015 |publisher=[[BBC News]]}}</ref> but did not win. Aya, the main character, is waiting for someone at [[Ben Gurion International Airport]]. A driver asks her to hold his welcome sign for Mr. Overby, who is flying into Israel to judge the [[Arthur Rubinstein International Piano Master Competition|Arthur Rubinstein piano competition]]. When the Danish juror shows up, Aya decides on an impulse to drive him to his hotel in [[Jerusalem]]. During the car ride, an unexpected intimacy develops between Aya and the reserved Overby.<ref>[http://www.timesofisrael.com/israeli-film-aya-fails-to-take-home-oscar-for-best-short-movie/ Israeli film Aya fails to take home Oscar for best short movie]</ref>\n\n==Cast==\n* [[Sarah Adler]] as Aya\n* [[Ulrich Thomsen]] as Mr. Overby\n\n==References ==\n{{reflist}}\n\n==External links==\n* {{IMDb title|2326094}}\n* {{AllMovie title|623689}}\n* {{Rotten Tomatoes|aya_2012}}\n\n{{DEFAULTSORT:Aya}}\n[[Category:2012 drama films]]\n[[Category:2012 films]]\n[[Category:Films set in Israel]]\n[[Category:English-language films]]\n[[Category:French drama films]]\n[[Category:French films]]\n[[Category:French short films]]\n[[Category:Israeli drama films]]\n[[Category:Israeli films]]\n[[Category:Israeli short films]]\n[[Category:2012 short films]]\n{{2010s-France-film-stub}}\n{{Israel-film-stub}}\n", "name_user": "Lugnuts", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Aya_(2012_film)"}
{"title_page": "Color psychology", "text_new": "{{short description|Study of influence of color on human behaviour}}\n{{multiple issues|\n{{lead rewrite|date=February 2015}}\n{{lead too short|date=February 2015}}\n{{tone|date=February 2015}}\n}}\n[[File:Goethe Schiller Die Temperamentenrose.jpg|thumb|280px|The \"rose of temperaments\" (''Temperamenten-Rose'') compiled by [[Goethe]] and [[Schiller]] in 1798/9. The diagram matches twelve colors to human occupations or their character traits, grouped in the [[four temperaments]]:\n* choleric (red/orange/yellow): tyrants, heroes, adventurers\n* sanguine (yellow/green/cyan) hedonists, lovers, poets\n* phlegmatic (cyan/blue/violet): public speakers, historians\n* melancholic (violet/magenta/red): philosophers, pedants, rulers\n]]\n\n'''Color psychology''' is the study of hues as a determinant of human [[behavior]]. Color influences [[perceptions]] that are not obvious, such as the taste of food. Colors have qualities that can cause certain emotions in people.<ref name=\"Roohi 100298\">{{Cite journal|last=Roohi|first=Samad|last2=Forouzandeh|first2=Aynaz | name-list-format = vanc |date=May 2019|title=Regarding color psychology principles in adventure games to enhance the sense of immersion|journal=Entertainment Computing|volume=30|pages=100298|doi=10.1016/j.entcom.2019.100298|issn=1875-9521}}</ref> Colors can also enhance the effectiveness of [[placebo]]s.<ref name=\":0\" /> For example, red or orange pills are generally used as [[stimulant]]s.<ref name=\":0\" />  How color influences individuals may differ depending on age, gender, and culture. For instance, heterosexual men tend to report that red outfits enhance female attractiveness, while heterosexual females deny any outfit color impacting that of men.<ref name=\":1\">{{cite web|url=http://www.slate.com/articles/double_x/doublex/2013/03/new_book_drunk_tank_pink_argues_red_is_the_color_for_dating_profiles.html|title=I See Red|last=Alter|first=Adam| name-list-format = vanc |date=March 21, 2013|work=Slate}}</ref> Although color associations can vary contextually between cultures, color preference is to be relatively uniform across gender and race. <ref name=\":3\">{{Cite book|last=Birren|first=Faber| name-list-format = vanc |title=Colour Psychology & Colour Therapy|publisher=The Citadel Press|year=1961|isbn=0806506539|location=Secaucus, N. J|pages=198}}</ref> <ref>{{cite journal | title = How Color Psychology can Improve your Website Conversion | url = https://hostingpill.com/color-psychology/ }}</ref>\n\nColor psychology is also widely used in [[marketing]] and [[brands|branding]]. Marketers see color as important as cole and can influence a consumers' emotions and perceptions about goods and services.  Logos for companies are important since the logos can attract more costumers. This happens when customers believe the company logo matches the personality of the goods and services such as the color pink being heavily used on Victoria's Secret branding.<ref>{{cite journal | vauthors = Elliot AJ, Maier MA | title = Color psychology: effects of perceiving color on psychological functioning in humans | journal = Annual Review of Psychology | volume = 65 | pages = 95\u2013120 | year = 2014 | pmid = 23808916 | doi = 10.1146/annurev-psych-010213-115035 | url = http://psych.annualreviews.org }}</ref> Colors are also important for window displays in stores. Research shows that colors such as red tended to attract spontaneous purchasers, despite cool colors such as blue being more favorable.<ref name=\"WW\" />\n\n== Influence of color on perception ==\n\nColor has a large impact on food. Color affects how people perceive the edibility and flavor of foods and drinks.<ref name=\":2\">{{Cite journal|last=Shankar|first=Maya U.|last2=Levitan|first2=Carmel A.|last3=Prescott|first3=John|last4=Spence|first4=Charles| name-list-format = vanc |date=2009-04-28|title=The Influence of Color and Label Information on Flavor Perception|journal=Chemosensory Perception|volume=2|issue=2|pages=53\u201358|doi=10.1007/s12078-009-9046-4|issn=1936-5802}}</ref> Not only the color of the food itself but also that of everything in the eater's field of vision can affect this. For example, in food stores, bread is normally sold in packaging decorated or tinted with golden or brown tones to promote the idea of home baked and oven freshness.<ref>{{cite book|last=Bleicher|first=Steven| name-list-format = vanc |title=Contemporary Colour: Theory & Use |year=2012|publisher=Delmar|location=New York|isbn=978-1-1335-7997-7|pages=48, 50|url=https://www.cengage.co.uk/books/9781133579977/}}</ref> People can mistake a cherry flavored drink for being lime or lemon flavored if that drink was a green color. Additionally, a flavor can be intensified by a color. People can rate a brown M&M as more chocolate flavored than a green M&M based on color.<ref name=\":2\" />\n\n== Placebo effect ==\n\nThe color of [[placebo]] pills is reported to be a factor in their effectiveness, with \"hot-colored\" pills working better as stimulants and \"cool-colored\" pills working better as depressants. This relationship is believed to be a consequence of the patient's expectations and not a direct effect of the color itself.<ref name=\":0\">{{cite journal | vauthors = de Craen AJ, Roos PJ, de Vries AL, Kleijnen J | title = Effect of colour of drugs: systematic review of perceived effect of drugs and of their effectiveness | journal = BMJ | volume = 313 | issue = 7072 | pages = 1624\u20136 | year = 1996 | pmid = 8991013 | pmc = 2359128 | doi = 10.1136/bmj.313.7072.1624 }}</ref> Consequently, these effects appear to be culture-dependent.<ref>{{cite journal | vauthors = Dolinska B | title = Empirical investigation into placebo effectiveness | year = 1999 | url = http://www.ijpm.org/content/pdf/139/placebo.pdf | journal = Irish Journal of Psychological Medicine | pages = 57\u201358 | volume = 16 | issue = 2 | format = w | access-date = 2009-04-29 | doi = 10.1017/s0790966700005176 | archive-url = https://web.archive.org/web/20110722003051/http://www.ijpm.org/content/pdf/139/placebo.pdf# | archive-date = 2011-07-22 | url-status = dead }}</ref>\n\n== Blue public lighting ==\n\nBlue light causes people to feel relaxed, which lead countries to add blue street lights in order to decrease suicide rates.<ref>{{Cite web|url=https://www.bbc.com/future/article/20190122-can-blue-lights-prevent-suicide-at-train-stations|title=Can blue lights prevent suicide at train stations?|last=Baraniuk|first=Chris| name-list-format = vanc |website=www.bbc.com|language=en|access-date=2020-03-11}}</ref> In 2000, the city of [[Glasgow]] installed blue street lighting in certain neighborhoods and subsequently reported the anecdotal finding of reduced crime in these areas.<ref>{{cite news|url=http://seattletimes.nwsource.com/html/nationworld/2008494010_bluelight11.html |work=The Seattle Times |title=Blue streetlights believed to prevent suicides, street crime |date=2008-12-11 |url-status=dead |archive-url=https://web.archive.org/web/20100913151600/http://seattletimes.nwsource.com/html/nationworld/2008494010_bluelight11.html |archive-date=September 13, 2010 }}</ref><ref>{{cite web|url=http://www.physorg.com/news148153021.html|title=Blue streetlights may prevent crime, suicide|date=December 10, 2008|first=Yomiuri|last=Shimbun| name-list-format = vanc |access-date=2010-02-18|archive-url=https://web.archive.org/web/20091009064638/http://www.physorg.com/news148153021.html#|archive-date=2009-10-09|url-status=dead}}</ref> A railroad company in Japan installed blue lighting on its stations in October 2009 in an effort to reduce the number of suicide attempts,<ref>[http://psychcentral.com/blog/archives/2008/12/13/can-blue-colored-light-prevent-suicide/ Can Blue-Colored Light Prevent Suicide?]</ref> although the effect of this technique has been questioned.<ref>[http://www.businessweek.com/globalbiz/blog/eyeonasia/archives/2009/11/will_blue_light.html Will Blue Lights Reduce Suicides in Japan?]</ref>\n\n== Color preference and associations between color and emotion ==\n{{Main|Color preferences}}\nHow people are affected by different color stimuli varies from person to person. Blue is the top choice for 35% of Americans, followed by green (16%), purple (10%) and red (9%).<ref>{{cite web|url=http://www.creativelatitude.com/articles/articles_lamacusa_color.html|title=Emotional Reactions to Color| first = Kathy | last = Lamancusa | name-list-format = vanc |work=Creative Latitude|access-date=2016-03-30|archive-url=https://web.archive.org/web/20160311005142/http://www.creativelatitude.com/articles/articles_lamacusa_color.html#|archive-date=2016-03-11|url-status=dead}}</ref> A preference for blue and green may be due to a preference for certain [[habitat]]s that were beneficial in the ancestral environment as explained in [[evolutionary aesthetics]].<ref name=\"Dutton\">[[Denis Dutton|Dutton, Denis]]. 2003. 'Aesthetics and Evolutionary Psychology' in \"The Oxford Handbook for Aesthetics\". Oxford University Press.</ref> The colours orange, yellow, and brown, are the three least popular colours, respectively.<ref name=\":4\" /> \n\nColor preference may also depend on ambient temperature. People who are cold prefer warm colors such as red or yellow while people who are hot prefer cool colors like blue and green.<ref name=\"WW\" /> Introverted individuals are also found to be more attracted to cool colours, while extroverts prefer warmer colours.<ref>{{Cite journal|last=Lichtl\u00e9|first=Marie-Christine| name-list-format = vanc |date=2007-01-01|title=The effect of an advertisement's colour on emotions evoked by attitude towards the ad|journal=International Journal of Advertising|volume=26|issue=1|pages=37\u201362|doi=10.1080/02650487.2007.11072995|issn=0265-0487}}</ref> \n\nGender has also shown to influence how colours are received, with some research has suggesting that women and men respectively prefer \"warm\" and \"cool\" colors.<ref name=\"WW\">{{cite journal | vauthors = Whitfield TW, Wiltshire TJ | title = Color psychology: a critical review | journal = Genetic, Social, and General Psychology Monographs | volume = 116 | issue = 4 | pages = 385\u2013411 | date = November 1990 | pmid = 2289687 }}</ref> Black, white, and gray are also shown to be received more positively by males than females. <ref name=\"Singh\" />  \n\nPsychologist [[Andrew J. Elliot]] tested to see if the color of a person's clothing could make them appear more sexually appealing. He found that, to heterosexual men, women dressed in the color red were significantly more likely to attract romantic attention than women dressed in any other color. The color did not affect heterosexual women's assessment of other women's attractiveness. Other studies have shown a preference for men dressed in red among heterosexual women.<ref name=\":1\" /> \n\nContrary to the unanimous adult preference for blue, in children the colour yellow is the most favoured colour, perhaps owing to its associations with happiness. <ref name=\"Aslam\" /> However, children's preferences for colors they find to be pleasant and comforting are mutable and can vary, while adult color preference is usually non-malleable.<ref name=\"WW\" />\n\nCultural background has been shown to have a strong influence on color associations & preference. Studies have shown that while people from the same region regardless of race will have the same color preferences, common associations connecting a color to a particular emotion may differ cross-culturally.<ref name=\"WW\" /> For instance, one study that examined color associations with emotion with participants from Germany, Mexico, Poland, Russia, and the United States found that in all nations the color red was associated with anger and was  perceived as strong and active.<ref name=\"AEP\" /> However, only Poles associated purple with both anger and jealousy while Germans associated jealousy with yellow, highlighting how the influence of different cultures can potentially change perceptions of color and its relationship to emotion.<ref name=\"shevell\" /> \n\n== Light, color, and surroundings ==\n\nLight and color can influence how people perceive the area around them. Different light sources affect how the colors of walls and other objects are seen. Specific hues of colors seen under natural sunlight may vary when seen under the light from an incandescent (tungsten) light-bulb: lighter colors may appear to be more orange or \"brownish\" and darker colors may appear even darker.<ref name=\"shevell\">{{cite journal | vauthors = Shevell SK, Kingdom FA | title = Color in complex scenes | journal = Annual Review of Psychology | volume = 59 | pages = 143\u201366 | year = 2008 | pmid = 18154500 | pmc =  | doi = 10.1146/annurev.psych.59.103006.093619 | url = https://semanticscholar.org/paper/088d3dd322719fdceacb99e852fcbfe5cded2340 }}</ref> Light and the color of an object can affect how one perceives its positioning. If light or shadow, or the color of the object, masks an object's true contour (outline of a figure) it can appear to be shaped differently from reality.<ref name=\"shevell\" /> Objects under a uniform light-source will promote better impression of three-dimensional shape.<ref name=\"shevell\" /> The color of an object may affect whether or not it seems to be in motion. In particular, the trajectories of objects under a light source whose intensity varies with space are more difficult to determine than identical objects under a uniform light source. This could possibly be interpreted as interference between motion and color perception, both of which are more difficult under variable lighting.<ref name=\"shevell\" />\n\n[[Carl Jung]] is most prominently associated with the pioneering stages of color psychology. Jung was most interested in colors' properties and meanings, as well as in art's potential as a tool for [[psychotherapy]]. His studies in and writings on color symbolism cover a broad range of topics, from [[mandala]]s to the works of Picasso to the near-universal sovereignty of the color gold, the lattermost of which, according to Charles A. Riley II, \"expresses ... the apex of spirituality, and intuition\".<ref>Riley, Charles A. II. \"Color Codes: Modern Theories of Color in Philosophy, Painting and Architecture, Literature, Music, and Psychology\". Hanover: University Press of New England, 1995, p. 307.</ref> In pursuing his studies of color usage and effects across cultures and time periods, as well as in examining his patients' self-created mandalas, Jung attempted to unlock and develop a language, or code, the ciphers of which would be colors. He looked to [[alchemy]] to further his understanding of the secret language of color, finding the key to his research in alchemical transmutation. His work has historically informed the modern field of color psychology.\n\n== General model ==\n\nThe general model of color psychology relies on six basic principles:\n\n# Color can carry a specific meaning.\n# Color meaning is either based in learned meaning or biologically innate meaning.\n# The perception of a color causes evaluation automatically by the person perceiving.\n# The evaluation process forces color-motivated behavior.\n# Color usually exerts its influence automatically.\n# Color meaning and effect has to do with context as well.<ref name=\"WW\" />\n\n== Uses in marketing ==\n\nSince color is an important factor in the visual appearance of products as well as in brand recognition, color psychology has become important to marketing. Recent work in marketing has shown that color can be used to communicate brand personality.<ref>{{cite journal | last1 = Labrecque | first1 = Lauren I. | last2 = Milne | first2 = George R. | name-list-format = vanc | year = 2012 | title = Exciting Red and Competent Blue: The Importance of Color in Marketing | journal = Journal of the Academy of Marketing Science | volume = 40 | issue = 5| pages = 711\u2013727 | doi=10.1007/s11747-010-0245-y}}</ref>\n\nMarketers must be aware of the application of color in different media (e.g. print vs. web), as well as the varying meanings and emotions that a particular audience can assign to color. Even though there are attempts to classify consumer response to different colors, everyone perceives color differently. The physiological and emotional effect of color in each person is influenced by several factors such as past experiences, culture, religion, natural environment, gender, race, and nationality. When making color decisions, it is important to determine the target audience in order to convey the right message. Color decisions can influence both direct messages and secondary brand values and attributes in any communication. Color should be carefully selected to align with the key message and emotions being conveyed in a piece.<ref>[http://www.lyquix.com/blog-and-news/connecting-with-color Connecting With Color]</ref>\n\nResearch on the effects of color on product preference and marketing shows that product color could affect consumer preference and hence purchasing culture. This is mostly due to associative learning. Most results show that it is not a specific color that attracts all audiences, but that certain colors are deemed appropriate for certain products.<ref>{{cite journal | doi = 10.1111/j.1745-459X.2011.00360.x | volume=26 | issue=6 | journal=Journal of Sensory Studies | pages=436\u2013444 | year=2011 | last1 = Fern\u00e1ndez-V\u00e1zquez | first1 = Roc\u00edo | name-list-format = vanc | title=Visual and Instrumental Evaluation of Orange Juice Color: A Consumers' Preference Study }}</ref>\n\n== Brand meaning ==\n\n[[File:RGB color wheel 72.svg|thumb|left|The Color Wheel]]\nColor is a very influential source of information when people are making a purchasing decision.<ref name=Singh>{{cite journal|last=Singh|first=Satyendra| name-list-format = vanc |title=Impact of color on marketing|journal=Management Decision|year=2006|volume=44|issue=6|pages=783\u2013789|doi=10.1108/00251740610673332}}</ref>{{citation needed|reason=There's no experimental information backing up that claim in the Singh article|date=July 2015}} Customers generally make an initial judgment on a product within 90 seconds of interaction with that product and about 62%-90% of that judgment is based on color.<ref name=Singh /> People often see the logo of a brand or company as a representation of that company. Without prior experience to a logo, we begin to associate a brand with certain characteristics based on the primary logo color.<ref name=\"Bottomley et al.\">{{cite journal| vauthors = Bottomley PA, Doyle JR |title=The interactive effects of colors and products on perceptions of brand logo appropriateness|journal=Marketing Theory|year=2006|volume=6|issue=1|pages=63\u201383|doi=10.1177/1470593106061263|url=https://semanticscholar.org/paper/553ace4aba7f25c15d990cf74ba82599f907c475}}</ref>\n\nColor mapping provides a means of identifying potential logo colors for new brands and ensuring brand differentiation within a visually cluttered marketplace.<ref>{{cite web| vauthors = O'Connor Z |title=Logo colour and differentiation: A new application of colour mapping|url=https://www.facebook.com/pages/Colour-Design-Research/34328846066208|work=Color Research & Application, 36 (1), p55-60|access-date=2016-03-30}}{{failed verification|date=November 2016}}</ref>\n\nA study on logo color asked participants to rate how appropriate the logo color was for fictional companies based on the products each company produced. Participants were presented with fictional products in eight different colors and had to rate the appropriateness of the color for each product. This study showed a pattern of logo color appropriateness based on product function. If the product was considered functional, fulfills a need or solves a problem, then a functional color was seen as most appropriate. If the product was seen as sensory-social, conveys attitudes, status, or social approval, then sensory-social colors were seen as more appropriate.<ref name=\"Bottomley et al.\" /> Companies should decide what types of products to produce and then choose a logo color that is connotative with their products' functions.\n\nCompany logos can portray meaning just through the use of color.<ref name=\"Labrecque et al.\">{{cite journal| vauthors = Labrecque LI, Milne GR |title=Exciting red and competent blue: the importance of color in marketing|journal=Journal of the Academy of Marketing Science|volume=40|issue=5|pages=711\u2013727|doi=10.1007/s11747-010-0245-y|year=2011}}</ref> Color affects people's perceptions of a new or unknown company. Some companies such as [[Victoria's Secret]] and [[H&R Block]] used color to change their corporate image and create a new brand personality for a specific target audience.<ref name=\"Labrecque et al.\" /> Research done on the relationship between logo color and five personality traits had participants rate a computer-made logo in different colors on scales relating to the [[dimensions of brand personality]]. Relationships were found between color and sincerity, excitement, competence, sophistication, and ruggedness. A follow up study tested the effects of perceived brand personality and purchasing intentions.<ref name=\"Labrecque et al.\" /> Participants were presented with a product and a summary of the preferred brand personality and had to rate the likelihood of purchasing a product based on packaging color. Purchasing intent was greater if the perceived personality matched the marketed product or service. In turn color affects perceived brand personality and brand personality affects purchasing intent.<ref name=\"Labrecque et al.\" />\n\nAlthough color can be useful in marketing, its value and extent of use depends on how it is used and the audience it is used on.<ref name=\"Warner et al.\">{{cite journal | vauthors = Warner L, Franzen R | title = Value of color in advertising | journal = The Journal of Applied Psychology | volume = 31 | issue = 3 | pages = 260\u201370 | date = June 1947 | pmid = 20241978 | doi = 10.1037/h0057772 }}</ref> The use of color will have different effects on different people, therefore experimental findings cannot be taken as universally true.\n\n=== Specific color meaning ===\n\nDifferent colors are perceived to mean different things. For example, tones of red lead to feelings of arousal while blue tones are often associated with feelings of relaxation. Both of these emotions are pleasant, so therefore, the colors themselves can procure positive feelings in advertisements. The chart below gives perceived meanings of different colors in the United States.\n\nFunctional (F): fulfills a need or solves a problem<ref name=\"Bottomley et al.\" />\n\nSensory-Social (S): conveys attitudes, status, or social approval<ref name=\"Bottomley et al.\" />\n\n{| class=\"wikitable\" style=\"font-size:90%; width:100%;\"\n\n|-\n\n! style=\"text-align:center; background-color:red;\"| <span style=\"color:white\">Red</span>\n\n! style=\"text-align:center; background-color:yellow;\"| <span style=\"color:black\">Yellow</span>\n\n! style=\"text-align:center; background-color:green;\"| <span style=\"color:white\">Green</span>\n\n! style=\"text-align:center; background-color:blue;\"| <span style=\"color:white\">Blue</span>\n\n! style=\"text-align:center; background-color:pink;\"| <span style=\"color:black\">Pink</span>\n\n! style=\"text-align:center; background-color:purple;\"| <span style=\"color:white\">Violet/Purple</span>\n\n! style=\"text-align:center; background-color:orange;\" |<span style=\"color:black\">Orange</span>\n! style=\"text-align:center; background-color:brown;\"| <span style=\"color:white\">Brown</span>\n\n! style=\"text-align:center; background-color:black;\"| <span style=\"color:white\">Black</span>\n\n! style=\"text-align:center; background-color:white;\"| <span style=\"color:black\">White</span>\n\n|-\n\n| Lust (S)<ref name=\"Aslam\" />\n\n| Competence (S)<ref name=\"Labrecque et al.\" />\n\n| Good Taste (F)<ref name=\"Aslam\" />\n\n| Masculine (S)<ref name=\"Aslam\" />\n\n| Sophistication (S)<ref name=\"Labrecque et al.\" />\n\n| Authority (S)<ref name=\"Aslam\" />\n\n| Warmth (S)<ref name=\":6\">{{cite journal | vauthors = Elliot AJ | title = Color and psychological functioning: a review of theoretical and empirical work | journal = Frontiers in Psychology | volume = 6 | pages = 368 | date = 2015-04-02 | pmid = 25883578 | pmc = 4383146 | doi = 10.3389/fpsyg.2015.00368 }}</ref>\n| Ruggedness (S)<ref name=\"Labrecque et al.\" />\n\n| Grief (S)<ref name=\"Aslam\" />\n\n| Happiness (S)<ref name=\"Aslam\" />\n\n|-\n\n| Power (S)<ref name=\"Piotrowski et al.\">{{cite journal| vauthors = Piotrowski C, Armstrong T |title=Color Red: Implications for applied psychology and marketing research|journal=Psychology and Education: An Interdisciplinary Journal|year=2012|volume=49|issue=1\u20132|pages=55\u201357|url=https://tricourilemele.ro/2018/01/30/color-red/}}</ref>\n\n|Happiness (S)<ref name=\"Aslam\" />\n\n| Envy (S)<ref name=Aslam>{{cite journal| vauthors = Aslam MM |title=Are You Selling the Right Colour? A Cross-cultural Review of Colour as a Marketing Cue|journal=Journal of Marketing Communications|year=2006|volume=12|issue=1|pages=15\u201330|doi=10.1080/13527260500247827|url=https://ro.uow.edu.au/commpapers/1043}}</ref>\n\n| Competence (S)<ref name=\"Labrecque et al.\" />\n\n| Sincerity (S)<ref name=\"Labrecque et al.\" />\n\n| Sophistication (S)<ref name=\"Labrecque et al.\" />\n|Excitement (S)<ref name=\":6\" />\n|\n| Sophistication (S)<ref name=\"Labrecque et al.\" />\n\n| Sincerity (S)<ref name=\"Labrecque et al.\" />\n|-\n\n| Excitement (S)<ref name=\"Labrecque et al.\" />\n\n|Inexpensive (F)<ref name=\":4\">{{Cite web|url=http://www.joehallock.com/edu/COM498/index.html|title=Colour Assignment - By Joe Hallock|website=www.joehallock.com|access-date=2020-03-26}}</ref>\n|Eco-Friendly (F) <ref name=\":5\">{{Cite book|last=Lupton | first = Ellen | name-list-format = vanc |title=Design is storytelling| year = 2017 |isbn=978-1-942303-19-0|location=New York, NY|oclc=982650081}}</ref>\n| High quality (F)<ref name=\"Aslam\" />\n\n| Feminine (S)<ref name=\"Aslam\" />\n\n| Power (S)<ref name=\"Aslam\" />\n|\n|\n| Expensive (F)<ref name=\"Aslam\" />\n\n| Purity (S)<ref name=\"Aslam\" />\n\n|-\n\n| Love (S)<ref name=\"Aslam\" />\n\n|Low Quality (F)<ref name=\":4\" />\n|Health (S) <ref name=\":5\" />\n| Corporate (F)<ref name=\"Aslam\" />\n\n|\n|\n|\n|\n| Fear (S)<ref name=\"Aslam\" />\n|\n|-\n|Speed (S) <ref name=\":3\" />\n|\n|\n|Reliability (F) <ref name=\":4\" />\n|\n|\n|\n|\n|Death (S)<ref>{{Cite journal|last=Meier|first=Brian|last2=Robinson|first2=Michael|last3=Clore|first3=Gerald | name-list-format = vanc |date=2004-03-01|title=Why good guys wear white|url=https://www.researchgate.net/publication/8906275|journal=Psychological Science|volume=15|issue=2|pages=82\u20137|doi=10.1111/j.0963-7214.2004.01502002.x|pmid=14738513}}</ref>\n|\n|-\n|Anger (S)<ref>{{Cite journal|last=Kauppinen-R\u00e4is\u00e4nen|first=Hannele|last2=Jauffret|first2=Marie-Nathalie | name-list-format = vanc |date=2018-01-08|title=Using colour semiotics to explore colour meanings|journal=Qualitative Market Research|language=en|volume=21|issue=1|pages=101\u2013117|doi=10.1108/QMR-03-2016-0033|issn=1352-2752}}</ref>\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|}\n\n=== Combining colors ===\n\n[[File:Target logo.svg|right|thumb|Target logo]]\n\nAlthough some companies use a single color to represent their brand, many other companies use a combination of colors in their logo, and can be perceived in different ways than those colors independently. When asked to rate color pair preference of preselected pairs, people generally prefer color pairs with similar hues when the two colors are both in the foreground; however, greater contrast between the figure and the background is preferred.<ref name=\"Schloss et al.\">{{cite journal | vauthors = Schloss KB, Palmer SE | title = Aesthetic response to color combinations: preference, harmony, and similarity | journal = Attention, Perception & Psychophysics | volume = 73 | issue = 2 | pages = 551\u201371 | date = February 2011 | pmid = 21264737 | pmc = 3037488 | doi = 10.3758/s13414-010-0027-0 }}</ref>\n\nIn contrast to a strong preference for similar color combinations, some people like to accent with a highly contrasting color.<ref name=\"Deng et al.\">{{cite journal| vauthors = Deng X, Hui SK, Huntchinson J |title=Consumer preferences for color combinations: An empirical analysis of similarity-based color|journal=Journal of Consumer Psychology|year=2010|volume=20|issue=4|pages=476\u2013484|doi=10.1016/j.jcps.2010.07.005}}</ref> In a study on color preference for [[Nike, Inc.]] sneakers, people generally combined colors near each other on the color wheel, such as blue and dark blue. However, a smaller segment preferred to have the Nike swoosh accentuated in a different, and contrasting, color. Most of the people also used a relatively small number of colors when designing their ideal athletic shoe. This finding has relevance for companies that produce multicolored merchandise, suggesting that to appeal to consumer preferences, companies should consider minimizing the number of colors visible and using similar hues in any one product.<ref name=\"Skorinko et al.\" />\n\n=== Color name ===\n\nAlthough different colors can be perceived in different ways, the names of those colors matters as well.<ref name=\"Skorinko et al.\" /><ref name=\"Babin et al.\" /> Many products and companies focus on producing a wide range of product colors to attract the largest population of consumers. For example, cosmetics brands produce a rainbow of colors for eye shadow and nail polish, to appeal to every type of person. Even companies such as [[Apple Inc.]] and [[Dell]] who make [[iPod]]s and laptops do so with a certain amount of color personalization available to attract buyers. Moreover, color name, not only the actual color, can attract or repel buyers as well. When asked to rate color swatches and products with either generic color names (such as brown) or \"fancy\" color names (such as ''mocha''), participants rated items with fancy names as significantly more likable than items with generic names.<ref name=\"Skorinko et al.\">{{cite journal| vauthors = Skorinko JL, Kemmer S, Hebl MR, Lane DM |title=15. A Rose by Any Other Name...: Color-Naming Influences on Decision Making|journal=Psychology & Marketing|year=2006|volume=23|issue=12|pages=975\u2013993|doi=10.1002/mar.20142|citeseerx=10.1.1.581.1374}}</ref> In fact, the same paint color swatch with two different names produced different rating levels, and the same effect was found when participants rated the pleasantness of towels given fancy or generic color names,<ref name=\"Skorinko et al.\" /> showing an overall pattern of preference for fancy color names over generic ones when describing exactly the same color.\n\nFurthermore, it would appear that in addition to fancy names being preferred for their aural appeal, they may actually contribute to the product they represent itself being liked more, and hence in this manner impact sales.<ref name=\"Miller et al.\">{{cite journal| vauthors = Miller EG, Kahn BE |title=Shades of Meaning: The Effect of Color and Flavor Names on Consumer Choice|journal=Journal of Consumer Research|year=2005|volume=32|issue=1|pages=86\u201392|doi=10.1086/429602|citeseerx=10.1.1.488.3177}}</ref> A yellow [[Jelly beans|jelly bean]] with an atypical color name such as ''razzmatazz'' is more likely to be selected than one with a more typical name such as ''lemon yellow''. This could be due to greater interest in atypical names, as well as curiosity and willingness to \"figure out\" why that name was chosen. Purchasing intent patterns regarding custom sweatshirts from an online vendor also revealed a preference for atypical names. Participants were asked to imagine buying sweatshirts and were provided with a variety of color name options, some typical, some atypical. Color names that were atypical were selected more often than typical color names, again confirming a preference for atypical color names and for item descriptions using those names.<ref name=\"Miller et al.\" /> Moreover, those who chose sweatshirts bearing atypical color names were described as more content with their purchase than those who selected similar items bearing typical color names.\n\n=== Attracting attention ===\n[[File:Bangkok Chinatown Yaowarat street sign.jpg|thumb|upright|A store sign painted mainly red on a street in [[Bangkok]] to attract attention from passers-by]]\n\nColor is used as a means to attract consumer attention to a product that then influences buying behavior.<ref name=Kauppinen-Raisanen>{{cite journal| vauthors = Kauppinen-Raisanen H, Luomala HT |title=Exploring consumers' product-specific color meanings|journal=Qualitative Market Research|year=2010|volume=13|issue=3|pages=287\u2013308|doi=10.1108/13522751011053644}}</ref> Consumers use color to identify for known brands or search for new alternatives. Variety seekers look for non-typical colors when selecting new brands. Attractive color packaging receives more consumer attention than unattractive color packaging, which can then influence buying behavior. A study that looked at visual color cues focused on predicted purchasing behavior for known and unknown brands.<ref name=Kauppinen-Raisanen /> Participants were shown the same product in four different colors and brands. The results showed that people picked packages based on colors that attracted their voluntary and involuntary attention. Associations made with that color such as 'green fits menthol', also affected their decision. Based on these findings implications can be made on the best color choices for packages. New companies or new products could consider using dissimilar colors to attract attention to the brand, however, off brand companies could consider using similar colors to the leading brand to emphasize product similarity. If a company is changing the look of a product, but keeping the product the same, they consider keeping the same color scheme since people use color to identify and search for brands.<ref name=Kauppinen-Raisanen /> This can be seen in [[Crayola]] crayons, where the logo has changed many times since 1934, but the basic package colors, gold and green, have been kept throughout.\n\nAttention is captured subconsciously before people can consciously attend to something.<ref name=\"Kawasaki et al.\">{{cite journal | vauthors = Kawasaki M, Yamaguchi Y | title = Effects of subjective preference of colors on attention-related occipital theta oscillations | journal = NeuroImage | volume = 59 | issue = 1 | pages = 808\u201314 | date = January 2012 | pmid = 21820064 | doi = 10.1016/j.neuroimage.2011.07.042 }}</ref> Research looking at [[electroencephalography]] (EEGs) while people made decisions on color preference found brain activation when a favorite color is present before the participants consciously focused on it. When looking at various colors on a screen people focus on their favorite color, or the color that stands out more, before they purposefully turn their attention to it. This implies that products can capture someone's attention based on color, before the person willingly looks at the product.<ref name=\"Kawasaki et al.\" />\n\nIn interactive design and behavioral design, color is used to develop visual hierarchies where colors are placed into saliency hierarchies that match other hierarchies. Examples include matching a color hierarchy to a navigational structure hierarchy, or matching a behavioral science hierarchy to the most salient colors in a visual hierarchy, to increase the odds that important behavior change principles are noticed by a target audience and processed by them.<ref>Cugelman, B. Cugeman, R. et al. (2019) Color Psychology. AlterSpark. https://www.alterspark.com/color-psychology</ref>\n\n=== Store and display color ===\n\n[[File:Orange Armenia window display.JPG|right|thumb|Warm colored window display]]\n\nColor is not only used in products to attract attention, but also in window displays and stores.<ref name=\"Bellizzi et al.\">{{cite journal| vauthors = Bellizzi JA, Crowley AE, Hasty RW |title=The effects of color in store design|journal=Journal of Retailing|year=1983|volume=59|issue=1|pages=21\u201345}}</ref> When people are exposed to different colored walls and images of window displays and store interiors they tend to be drawn to some colors and not to others. Findings showed that people were physically drawn to warm colored displays; however, they rated cool colored displays as more favorable. This implies that warm colored store displays are more appropriate for spontaneous and unplanned purchases, whereas cool colored displays and store entrances may be a better fit for purchases where a lot of planning and customer deliberation occurs. This is especially relevant in shopping malls where patrons could easily walk into a store that attracts their attention without previous planning.<ref name=\"Bellizzi et al.\" />\n\nOther research has confirmed that store color, and not just the product, influences buying behavior.<ref name=\"Babin et al.\">{{cite journal|last=Babin|first=Barry J| last2 = Hardesty | first2 = David M | last3 = Suter | first3 = Tracy A | name-list-format = vanc |title=Color and shopping intentions|journal=Journal of Business Research|volume=56|issue=7|pages=541\u2013551|doi=10.1016/S0148-2963(01)00246-6|year=2003}}</ref> When people are exposed to different store color scenarios and then surveyed on intended buying behavior, store color, among various other factors, seems important for purchasing intentions. Particularly blue, a cool color, was rated as more favorable and produced higher purchasing intentions than orange, a warm color. However, all negative effects to orange were neutralized when orange store color was paired with soft lighting. This shows that store color and lighting actually interact.<ref name=\"Babin et al.\" />\n\nLighting color could have a strong effect on perceived experience in stores and other situation. For example, time seems to pass more slowly under red lights and time seems to pass quickly under blue light.<ref name=Singh /> Casinos take full advantage of this phenomenon by using color to get people to spend more time and hence more money in their casino.<ref name=Singh /> However, a presumed influence of coloured light (red vs. blue) on risk behaviour could not be demonstrated.<ref>Mao, T., Yang, J., Ru, T., Chen, Q., Shi, H., Zhou, J., & Zhou, G. (2018). Does red light induce people to be riskier? Exploring the colored light effect on the Balloon Analogue Risk Task (BART). Journal of Environmental Psychology, '''57''', 73-82. {{doi|10.1016/j.jenvp.2018.07.001}}</ref>\n\n== Individual differences ==\n[[File:Pink girls section of toy store.jpeg|thumb|upright|left|Pink girls section of toy store]]\n\n=== Gender ===\n\nChildren's toys are often categorized as either boys or girls toys solely based on color. In a study on color effects on perception, adult participants were shown blurred images of children's toys where the only decipherable feature visible was the toy's color.<ref name=\"Hull & Hull\" /> In general participants categorized the toys into girl and boy toys based on the visible color of the image. This can be seen in companies interested in marketing masculine toys, such as building sets, to boys. For example, [[Lego]] uses pink to specifically advertise some sets to girls rather than boys. The classification of 'girl' and 'boy' toys on the [[Disney Store]] website also uses color associations for each gender.<ref name=Auster>{{cite journal|last=Auster|first=Carol J.| last2 = Mansbach | first2 = Claire S. | name-list-format = vanc |title=The Gender Marketing of Toys: An Analysis of Color and Type of Toy on the Disney Store Website|journal=Sex Roles|year=2012|volume=67|issue=7\u20138|pages=375\u2013388|doi=10.1007/s11199-012-0177-8}}</ref> An analysis of the colors used showed that bold colored toys, such as red and black, were generally classified as 'boy only' toys and pastel colored toys, such as pink and purple, were classified as 'girl only' toys. Toys that were classified as both boy and girl toys took on 'boy only' toy colors. This again emphasizes the distinction in color use for children's toys.<ref name=Auster />\n\nGender differences in color associations can also be seen amongst adults.<ref name=\"Chen Part I\">{{cite journal|last=Ou|first=Li-Chen| last2 = Luo | first2 = M. Ronnier | last3 = Woodcock | first3 = Andree | last4 = Wright | first4 = Angela | name-list-format = vanc |title=A study of colour emotion and colour preference. Part I: Colour emotions for single colours|journal=Color Research & Application|year=2004|volume=29|issue=3|pages=232\u2013240|doi=10.1002/col.20010}}</ref> Differences were noted for male and female participants, where the two genders did not agree on which color pairs they enjoyed the most when presented with a variety of colors.<ref name=\"Hull & Hull\">{{cite journal| vauthors = Hull JH, Hull DB, Knopp C |title=The Impact of color on ratings of 'girl' and 'boy' toys|journal=North American Journal of Psychology|year=2011|volume=13|issue=3|pages=549\u2013562 |url=http://www.freepatentsonline.com/article/North-American-Journal-Psychology/276353296.html|id=276353296|access-date=2017-02-14|archive-url=https://web.archive.org/web/20161006014108/http://www.freepatentsonline.com/article/North-American-Journal-Psychology/276353296.html|archive-date=2016-10-06|url-status=dead}}</ref><ref name=\"Chen Part II\">{{cite journal|last=Ou|first=Li-Chen| last2 = Luo | first2 = M. Ronnier | last3 = Woodcock | first3 = Andree | last4 = Wright | first4 = Angela | name-list-format = vanc |title=A study of colour emotion and colour preference. Part II: Colour emotions for two-colour combinations|journal=Color Research & Application|volume=29|issue=4|pages=292\u2013298|doi=10.1002/col.20024 |year=2004}}</ref> Men and women also did not agree on which colors should be classified as masculine and feminine. This could imply that men and women generally prefer different colors when purchasing items. Men and women also misperceive what colors the opposite gender views as fitting for them.\n\n=== Age ===\n\nChildren's toys for younger age groups are often marketed based on color, however, as the age group increases color becomes less gender-stereotyped.<ref name=\"Hull & Hull\" /> In general many toys become gender neutral and hence adopt gender-neutral colors. In the United States it is common to associate baby girls with pink and baby boys with blue. This difference in young children is a learned difference rather than an inborn one.<ref name=\"LoBue et al.\">{{cite journal | vauthors = Lobue V, Deloache JS | title = Pretty in pink: The early development of gender-stereotyped colour preferences | journal = The British Journal of Developmental Psychology | volume = 29 | issue = Pt 3 | pages = 656\u201367 | date = September 2011 | pmid = 21848751 | doi = 10.1111/j.2044-835X.2011.02027.x | url = https://semanticscholar.org/paper/4a1fb5c48ff700289d231e61c5b55241646c26c2 }}</ref> Research has looked at young children's, ages 7 months to 5 years, preference for small objects in different colors. The results showed that by the age of 2\u20132.5 years socially constructed gendered colors affects children's color preference, where girls prefer pink and boys avoid pink, but show no preference for other colors.<ref name=\"LoBue et al.\" />\n\nSlightly older children who have developed a sense of favorite color often tend to pick items that are in that color.<ref name=\"Gollety (2011)\">{{cite journal| vauthors = Gollety M, Guichard N |title=The dilemma of flavor and color in the choice of packaging by children|journal=Young Consumers|year=2011|volume=12|issue=1|pages=82\u201390|doi=10.1108/17473611111114803}}</ref>\n\n=== Culture ===\n{{see also|Linguistic relativity}}\n\nMany cultural differences exist on perceived color personality, meaning, and preference. When deciding on brand and product logos, companies should take into account their target consumer, since cultural differences exist. A study looked at color preference in British and Chinese participants.<ref name=\"Chen Part I\" /> Each participant was presented with a total of 20 color swatches one at a time and had to rate the color on 10 different emotions. Results showed that British participants and Chinese participants differed on the like-dislike scale the most. Chinese participants tended to like colors that they self rated as clean, fresh, and modern, whereas British participants showed no such pattern. When evaluating purchasing intent, color preference affects buying behavior, where liked colors are more likely to be bought than disliked colors.<ref name=Kauppinen-Raisanen /> This implies that companies should consider choosing their target consumer first and then make product colors based on the target's color preferences.\n\nWollard, (2000)<ref>{{cite web|url =http://wf2la6.webfeat.org|title =Wollard, K. (2000). Orange you glad you're not blue? }}{{Dead link|date=November 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> seems to think that color can affect one's mood, but the effect also can depend on one's culture and what one's personal reflection may be. For example, someone from Japan may not associate red with anger, as people from the U.S. tend to do. Also, a person who likes the color brown may associate brown with happiness. However, Wollard does think that colors can make everyone feel the same, or close to the same, mood.\n\n== Color and sports performance ==\n\nIn particular, the color red has been found to influence sports performance. During the [[2004 Summer Olympics]] the competitors in [[boxing]], [[taekwondo]], [[freestyle wrestling]], and [[Greco-Roman wrestling]] were randomly given blue or red uniforms. A later study found that those wearing red won 55% of all the bouts which was a statistically significant increase over the expected 50%. The colors affected bouts where the competitors were closely matched in ability, where those wearing red won 60% of the bouts, but not bouts between more unevenly matched competitors. In England, since WWII, teams wearing red uniforms have averaged higher league positions and have had more league winners than teams using other colors. In cities with more than one team, the teams wearing red outperformed the teams wearing other colors. A study of the [[UEFA Euro 2004]] found similar results. Another study found that those taking [[Penalty kick (association football)|penalty kick]]s performed worst when the goalkeeper had a red uniform. More anecdotal is the historical dominance of the domestic honors by red-wearing teams such [[AFC Ajax]], [[FC Bayern Munich]], [[Liverpool F.C.]], and [[Manchester United F.C.]] Videos of taekwondo bouts were manipulated in one study so that the red and blue colors of the protective gears were reversed. Both the original and the manipulated videos were shown to referees. The competitors wearing red were given higher scores despite the videos otherwise being identical. A study on experienced players of [[first-person shooter]]s found that those assigned to wear red instead of blue won 55% of the matches.<ref name=AEP>Diana Widermann, Robert A. Barton, and Russel A. Hill. Evolutionary perspectives on sport and competition. In {{Cite book | vauthors = Roberts SC | veditors = Roberts SC | doi = 10.1093/acprof:oso/9780199586073.001.0001 | title = Applied Evolutionary Psychology | year = 2011 | publisher = Oxford University Press| isbn = 9780199586073 | pmid = | pmc = }}</ref>\n\nThere are several different explanations for this effect. Red is used in stop signs and traffic lights which may associate the color with halting. Red is also perceived as a strong and active color which may influence both the person wearing it and others. An [[evolutionary psychology]] explanation is that red may signal health as opposed to [[anemic]] paleness, or indicate anger due to [[flushing (physiology)|flushing]] instead of paleness due to fear. It has been argued that detecting flushing may have influenced the development of primate [[Trichromacy|trichromate]] vision. Primate studies have found that some species evaluate rivals and possible mates depending on red color characteristics. Facial redness is associated with testosterone levels in humans, and male skin tends to be redder than female skin.<ref name=AEP/>\n\n== Color and time perception ==\n\nRecent results<ref>Masahiro Shibasaki, Nobuo Masataka (2014) \"[http://www.nature.com/articles/srep05899 The color red distorts time perception for men, but not for women]\" ''Scientific Reports'' 4, Article number: 5899 <abbr>doi</abbr>:10.1038/srep05899\n</ref> showed that the perceived duration of a red screen was longer than was that of a blue screen. The results reflected sex differences; men, but not women, overestimated the duration of the red screen. Additionally, the reaction times to a red screen were faster than those to a blue screen. Participants who reacted quickly to a red screen overestimated its duration. In a demo with 150 people chosen at random, it was found that inside a pod bathed in blue color the average perceived duration of a minute was 11 seconds shorter than in a pod bathed in red color.<ref>Beau Lotto in \"[http://www.documentarymania.com/player.php?title=Do%20You%20See%20What%20I%20See Do You See What I See]\" (2011)</ref>\n\n== Game Immersion ==\nSince color is such an important element in how people interpret their environment, color psychology can enhance the feeling of immersion in people that play video games. By using color psychology to cause immersion in players, players can have less errors playing video games and felt more a part of the game they were playing in comparison to a game that did not have color psychology immersion.<ref name=\"Roohi 100298\"/>\n\n== See also ==\n* [[Color symbolism]]\n* [[Color vision]]\n* [[Kruithof curve]]\n* [[L\u00fcscher color test]]\n* [[Visual perception]]\n\n== References ==\n{{reflist|30em}}\n\n{{Color topics}}\n\n{{Psychology}}\n\n[[Category:Color in culture|Psychology]]\n[[Category:Branches of psychology]]\n", "text_old": "{{short description|Study of influence of color on human behaviour}}\n{{multiple issues|\n{{lead rewrite|date=February 2015}}\n{{lead too short|date=February 2015}}\n{{tone|date=February 2015}}\n}}\n[[File:Goethe Schiller Die Temperamentenrose.jpg|thumb|280px|The \"rose of temperaments\" (''Temperamenten-Rose'') compiled by [[Goethe]] and [[Schiller]] in 1798/9. The diagram matches twelve colors to human occupations or their character traits, grouped in the [[four temperaments]]:\n* choleric (red/orange/yellow): tyrants, heroes, adventurers\n* sanguine (yellow/green/cyan) hedonists, lovers, poets\n* phlegmatic (cyan/blue/violet): public speakers, historians\n* melancholic (violet/magenta/red): philosophers, pedants, rulers\n]]\n\n'''Color psychology''' is the study of hues as a determinant of human [[behavior]]. Color influences [[perceptions]] that are not obvious, such as the taste of food. Colors have qualities that can cause certain emotions in people.<ref name=\"Roohi 100298\">{{Cite journal|last=Roohi|first=Samad|last2=Forouzandeh|first2=Aynaz | name-list-format = vanc |date=May 2019|title=Regarding color psychology principles in adventure games to enhance the sense of immersion|journal=Entertainment Computing|volume=30|pages=100298|doi=10.1016/j.entcom.2019.100298|issn=1875-9521}}</ref> Colors can also enhance the effectiveness of [[placebo]]s.<ref name=\":0\" /> For example, red or orange pills are generally used as [[stimulant]]s.<ref name=\":0\" />  How color influences individuals may differ depending on age, gender, and culture. For instance, heterosexual men tend to report that red outfits enhance female attractiveness, while heterosexual females deny any outfit color impacting that of men.<ref name=\":1\">{{cite web|url=http://www.slate.com/articles/double_x/doublex/2013/03/new_book_drunk_tank_pink_argues_red_is_the_color_for_dating_profiles.html|title=I See Red|last=Alter|first=Adam| name-list-format = vanc |date=March 21, 2013|work=Slate}}</ref> Although color associations can vary contextually between cultures, color preference is to be relatively uniform across gender and race. <ref name=\":3\">{{Cite book|last=Birren|first=Faber| name-list-format = vanc |title=Colour Psychology & Colour Therapy|publisher=The Citadel Press|year=1961|isbn=0806506539|location=Secaucus, N. J|pages=198}}</ref>\n\nColor psychology is also widely used in [[marketing]] and [[brands|branding]]. Marketers see color as important as cole and can influence a consumers' emotions and perceptions about goods and services.  Logos for companies are important since the logos can attract more costumers. This happens when customers believe the company logo matches the personality of the goods and services such as the color pink being heavily used on Victoria's Secret branding.<ref>{{cite journal | vauthors = Elliot AJ, Maier MA | title = Color psychology: effects of perceiving color on psychological functioning in humans | journal = Annual Review of Psychology | volume = 65 | pages = 95\u2013120 | year = 2014 | pmid = 23808916 | doi = 10.1146/annurev-psych-010213-115035 | url = http://psych.annualreviews.org }}</ref> Colors are also important for window displays in stores. Research shows that colors such as red tended to attract spontaneous purchasers, despite cool colors such as blue being more favorable.<ref name=\"WW\" />\n\n== Influence of color on perception ==\n\nColor has a large impact on food. Color affects how people perceive the edibility and flavor of foods and drinks.<ref name=\":2\">{{Cite journal|last=Shankar|first=Maya U.|last2=Levitan|first2=Carmel A.|last3=Prescott|first3=John|last4=Spence|first4=Charles| name-list-format = vanc |date=2009-04-28|title=The Influence of Color and Label Information on Flavor Perception|journal=Chemosensory Perception|volume=2|issue=2|pages=53\u201358|doi=10.1007/s12078-009-9046-4|issn=1936-5802}}</ref> Not only the color of the food itself but also that of everything in the eater's field of vision can affect this. For example, in food stores, bread is normally sold in packaging decorated or tinted with golden or brown tones to promote the idea of home baked and oven freshness.<ref>{{cite book|last=Bleicher|first=Steven| name-list-format = vanc |title=Contemporary Colour: Theory & Use |year=2012|publisher=Delmar|location=New York|isbn=978-1-1335-7997-7|pages=48, 50|url=https://www.cengage.co.uk/books/9781133579977/}}</ref> People can mistake a cherry flavored drink for being lime or lemon flavored if that drink was a green color. Additionally, a flavor can be intensified by a color. People can rate a brown M&M as more chocolate flavored than a green M&M based on color.<ref name=\":2\" />\n\n== Placebo effect ==\n\nThe color of [[placebo]] pills is reported to be a factor in their effectiveness, with \"hot-colored\" pills working better as stimulants and \"cool-colored\" pills working better as depressants. This relationship is believed to be a consequence of the patient's expectations and not a direct effect of the color itself.<ref name=\":0\">{{cite journal | vauthors = de Craen AJ, Roos PJ, de Vries AL, Kleijnen J | title = Effect of colour of drugs: systematic review of perceived effect of drugs and of their effectiveness | journal = BMJ | volume = 313 | issue = 7072 | pages = 1624\u20136 | year = 1996 | pmid = 8991013 | pmc = 2359128 | doi = 10.1136/bmj.313.7072.1624 }}</ref> Consequently, these effects appear to be culture-dependent.<ref>{{cite journal | vauthors = Dolinska B | title = Empirical investigation into placebo effectiveness | year = 1999 | url = http://www.ijpm.org/content/pdf/139/placebo.pdf | journal = Irish Journal of Psychological Medicine | pages = 57\u201358 | volume = 16 | issue = 2 | format = w | access-date = 2009-04-29 | doi = 10.1017/s0790966700005176 | archive-url = https://web.archive.org/web/20110722003051/http://www.ijpm.org/content/pdf/139/placebo.pdf# | archive-date = 2011-07-22 | url-status = dead }}</ref>\n\n== Blue public lighting ==\n\nBlue light causes people to feel relaxed, which lead countries to add blue street lights in order to decrease suicide rates.<ref>{{Cite web|url=https://www.bbc.com/future/article/20190122-can-blue-lights-prevent-suicide-at-train-stations|title=Can blue lights prevent suicide at train stations?|last=Baraniuk|first=Chris| name-list-format = vanc |website=www.bbc.com|language=en|access-date=2020-03-11}}</ref> In 2000, the city of [[Glasgow]] installed blue street lighting in certain neighborhoods and subsequently reported the anecdotal finding of reduced crime in these areas.<ref>{{cite news|url=http://seattletimes.nwsource.com/html/nationworld/2008494010_bluelight11.html |work=The Seattle Times |title=Blue streetlights believed to prevent suicides, street crime |date=2008-12-11 |url-status=dead |archive-url=https://web.archive.org/web/20100913151600/http://seattletimes.nwsource.com/html/nationworld/2008494010_bluelight11.html |archive-date=September 13, 2010 }}</ref><ref>{{cite web|url=http://www.physorg.com/news148153021.html|title=Blue streetlights may prevent crime, suicide|date=December 10, 2008|first=Yomiuri|last=Shimbun| name-list-format = vanc |access-date=2010-02-18|archive-url=https://web.archive.org/web/20091009064638/http://www.physorg.com/news148153021.html#|archive-date=2009-10-09|url-status=dead}}</ref> A railroad company in Japan installed blue lighting on its stations in October 2009 in an effort to reduce the number of suicide attempts,<ref>[http://psychcentral.com/blog/archives/2008/12/13/can-blue-colored-light-prevent-suicide/ Can Blue-Colored Light Prevent Suicide?]</ref> although the effect of this technique has been questioned.<ref>[http://www.businessweek.com/globalbiz/blog/eyeonasia/archives/2009/11/will_blue_light.html Will Blue Lights Reduce Suicides in Japan?]</ref>\n\n== Color preference and associations between color and emotion ==\n{{Main|Color preferences}}\nHow people are affected by different color stimuli varies from person to person. Blue is the top choice for 35% of Americans, followed by green (16%), purple (10%) and red (9%).<ref>{{cite web|url=http://www.creativelatitude.com/articles/articles_lamacusa_color.html|title=Emotional Reactions to Color| first = Kathy | last = Lamancusa | name-list-format = vanc |work=Creative Latitude|access-date=2016-03-30|archive-url=https://web.archive.org/web/20160311005142/http://www.creativelatitude.com/articles/articles_lamacusa_color.html#|archive-date=2016-03-11|url-status=dead}}</ref> A preference for blue and green may be due to a preference for certain [[habitat]]s that were beneficial in the ancestral environment as explained in [[evolutionary aesthetics]].<ref name=\"Dutton\">[[Denis Dutton|Dutton, Denis]]. 2003. 'Aesthetics and Evolutionary Psychology' in \"The Oxford Handbook for Aesthetics\". Oxford University Press.</ref> The colours orange, yellow, and brown, are the three least popular colours, respectively.<ref name=\":4\" /> \n\nColor preference may also depend on ambient temperature. People who are cold prefer warm colors such as red or yellow while people who are hot prefer cool colors like blue and green.<ref name=\"WW\" /> Introverted individuals are also found to be more attracted to cool colours, while extroverts prefer warmer colours.<ref>{{Cite journal|last=Lichtl\u00e9|first=Marie-Christine| name-list-format = vanc |date=2007-01-01|title=The effect of an advertisement's colour on emotions evoked by attitude towards the ad|journal=International Journal of Advertising|volume=26|issue=1|pages=37\u201362|doi=10.1080/02650487.2007.11072995|issn=0265-0487}}</ref> \n\nGender has also shown to influence how colours are received, with some research has suggesting that women and men respectively prefer \"warm\" and \"cool\" colors.<ref name=\"WW\">{{cite journal | vauthors = Whitfield TW, Wiltshire TJ | title = Color psychology: a critical review | journal = Genetic, Social, and General Psychology Monographs | volume = 116 | issue = 4 | pages = 385\u2013411 | date = November 1990 | pmid = 2289687 }}</ref> Black, white, and gray are also shown to be received more positively by males than females. <ref name=\"Singh\" />  \n\nPsychologist [[Andrew J. Elliot]] tested to see if the color of a person's clothing could make them appear more sexually appealing. He found that, to heterosexual men, women dressed in the color red were significantly more likely to attract romantic attention than women dressed in any other color. The color did not affect heterosexual women's assessment of other women's attractiveness. Other studies have shown a preference for men dressed in red among heterosexual women.<ref name=\":1\" /> \n\nContrary to the unanimous adult preference for blue, in children the colour yellow is the most favoured colour, perhaps owing to its associations with happiness. <ref name=\"Aslam\" /> However, children's preferences for colors they find to be pleasant and comforting are mutable and can vary, while adult color preference is usually non-malleable.<ref name=\"WW\" />\n\nCultural background has been shown to have a strong influence on color associations & preference. Studies have shown that while people from the same region regardless of race will have the same color preferences, common associations connecting a color to a particular emotion may differ cross-culturally.<ref name=\"WW\" /> For instance, one study that examined color associations with emotion with participants from Germany, Mexico, Poland, Russia, and the United States found that in all nations the color red was associated with anger and was  perceived as strong and active.<ref name=\"AEP\" /> However, only Poles associated purple with both anger and jealousy while Germans associated jealousy with yellow, highlighting how the influence of different cultures can potentially change perceptions of color and its relationship to emotion.<ref name=\"shevell\" /> \n\n== Light, color, and surroundings ==\n\nLight and color can influence how people perceive the area around them. Different light sources affect how the colors of walls and other objects are seen. Specific hues of colors seen under natural sunlight may vary when seen under the light from an incandescent (tungsten) light-bulb: lighter colors may appear to be more orange or \"brownish\" and darker colors may appear even darker.<ref name=\"shevell\">{{cite journal | vauthors = Shevell SK, Kingdom FA | title = Color in complex scenes | journal = Annual Review of Psychology | volume = 59 | pages = 143\u201366 | year = 2008 | pmid = 18154500 | pmc =  | doi = 10.1146/annurev.psych.59.103006.093619 | url = https://semanticscholar.org/paper/088d3dd322719fdceacb99e852fcbfe5cded2340 }}</ref> Light and the color of an object can affect how one perceives its positioning. If light or shadow, or the color of the object, masks an object's true contour (outline of a figure) it can appear to be shaped differently from reality.<ref name=\"shevell\" /> Objects under a uniform light-source will promote better impression of three-dimensional shape.<ref name=\"shevell\" /> The color of an object may affect whether or not it seems to be in motion. In particular, the trajectories of objects under a light source whose intensity varies with space are more difficult to determine than identical objects under a uniform light source. This could possibly be interpreted as interference between motion and color perception, both of which are more difficult under variable lighting.<ref name=\"shevell\" />\n\n[[Carl Jung]] is most prominently associated with the pioneering stages of color psychology. Jung was most interested in colors' properties and meanings, as well as in art's potential as a tool for [[psychotherapy]]. His studies in and writings on color symbolism cover a broad range of topics, from [[mandala]]s to the works of Picasso to the near-universal sovereignty of the color gold, the lattermost of which, according to Charles A. Riley II, \"expresses ... the apex of spirituality, and intuition\".<ref>Riley, Charles A. II. \"Color Codes: Modern Theories of Color in Philosophy, Painting and Architecture, Literature, Music, and Psychology\". Hanover: University Press of New England, 1995, p. 307.</ref> In pursuing his studies of color usage and effects across cultures and time periods, as well as in examining his patients' self-created mandalas, Jung attempted to unlock and develop a language, or code, the ciphers of which would be colors. He looked to [[alchemy]] to further his understanding of the secret language of color, finding the key to his research in alchemical transmutation. His work has historically informed the modern field of color psychology.\n\n== General model ==\n\nThe general model of color psychology relies on six basic principles:\n\n# Color can carry a specific meaning.\n# Color meaning is either based in learned meaning or biologically innate meaning.\n# The perception of a color causes evaluation automatically by the person perceiving.\n# The evaluation process forces color-motivated behavior.\n# Color usually exerts its influence automatically.\n# Color meaning and effect has to do with context as well.<ref name=\"WW\" />\n\n== Uses in marketing ==\n\nSince color is an important factor in the visual appearance of products as well as in brand recognition, color psychology has become important to marketing. Recent work in marketing has shown that color can be used to communicate brand personality.<ref>{{cite journal | last1 = Labrecque | first1 = Lauren I. | last2 = Milne | first2 = George R. | name-list-format = vanc | year = 2012 | title = Exciting Red and Competent Blue: The Importance of Color in Marketing | journal = Journal of the Academy of Marketing Science | volume = 40 | issue = 5| pages = 711\u2013727 | doi=10.1007/s11747-010-0245-y}}</ref>\n\nMarketers must be aware of the application of color in different media (e.g. print vs. web), as well as the varying meanings and emotions that a particular audience can assign to color. Even though there are attempts to classify consumer response to different colors, everyone perceives color differently. The physiological and emotional effect of color in each person is influenced by several factors such as past experiences, culture, religion, natural environment, gender, race, and nationality. When making color decisions, it is important to determine the target audience in order to convey the right message. Color decisions can influence both direct messages and secondary brand values and attributes in any communication. Color should be carefully selected to align with the key message and emotions being conveyed in a piece.<ref>[http://www.lyquix.com/blog-and-news/connecting-with-color Connecting With Color]</ref>\n\nResearch on the effects of color on product preference and marketing shows that product color could affect consumer preference and hence purchasing culture. This is mostly due to associative learning. Most results show that it is not a specific color that attracts all audiences, but that certain colors are deemed appropriate for certain products.<ref>{{cite journal | doi = 10.1111/j.1745-459X.2011.00360.x | volume=26 | issue=6 | journal=Journal of Sensory Studies | pages=436\u2013444 | year=2011 | last1 = Fern\u00e1ndez-V\u00e1zquez | first1 = Roc\u00edo | name-list-format = vanc | title=Visual and Instrumental Evaluation of Orange Juice Color: A Consumers' Preference Study }}</ref>\n\n== Brand meaning ==\n\n[[File:RGB color wheel 72.svg|thumb|left|The Color Wheel]]\nColor is a very influential source of information when people are making a purchasing decision.<ref name=Singh>{{cite journal|last=Singh|first=Satyendra| name-list-format = vanc |title=Impact of color on marketing|journal=Management Decision|year=2006|volume=44|issue=6|pages=783\u2013789|doi=10.1108/00251740610673332}}</ref>{{citation needed|reason=There's no experimental information backing up that claim in the Singh article|date=July 2015}} Customers generally make an initial judgment on a product within 90 seconds of interaction with that product and about 62%-90% of that judgment is based on color.<ref name=Singh /> People often see the logo of a brand or company as a representation of that company. Without prior experience to a logo, we begin to associate a brand with certain characteristics based on the primary logo color.<ref name=\"Bottomley et al.\">{{cite journal| vauthors = Bottomley PA, Doyle JR |title=The interactive effects of colors and products on perceptions of brand logo appropriateness|journal=Marketing Theory|year=2006|volume=6|issue=1|pages=63\u201383|doi=10.1177/1470593106061263|url=https://semanticscholar.org/paper/553ace4aba7f25c15d990cf74ba82599f907c475}}</ref>\n\nColor mapping provides a means of identifying potential logo colors for new brands and ensuring brand differentiation within a visually cluttered marketplace.<ref>{{cite web| vauthors = O'Connor Z |title=Logo colour and differentiation: A new application of colour mapping|url=https://www.facebook.com/pages/Colour-Design-Research/34328846066208|work=Color Research & Application, 36 (1), p55-60|access-date=2016-03-30}}{{failed verification|date=November 2016}}</ref>\n\nA study on logo color asked participants to rate how appropriate the logo color was for fictional companies based on the products each company produced. Participants were presented with fictional products in eight different colors and had to rate the appropriateness of the color for each product. This study showed a pattern of logo color appropriateness based on product function. If the product was considered functional, fulfills a need or solves a problem, then a functional color was seen as most appropriate. If the product was seen as sensory-social, conveys attitudes, status, or social approval, then sensory-social colors were seen as more appropriate.<ref name=\"Bottomley et al.\" /> Companies should decide what types of products to produce and then choose a logo color that is connotative with their products' functions.\n\nCompany logos can portray meaning just through the use of color.<ref name=\"Labrecque et al.\">{{cite journal| vauthors = Labrecque LI, Milne GR |title=Exciting red and competent blue: the importance of color in marketing|journal=Journal of the Academy of Marketing Science|volume=40|issue=5|pages=711\u2013727|doi=10.1007/s11747-010-0245-y|year=2011}}</ref> Color affects people's perceptions of a new or unknown company. Some companies such as [[Victoria's Secret]] and [[H&R Block]] used color to change their corporate image and create a new brand personality for a specific target audience.<ref name=\"Labrecque et al.\" /> Research done on the relationship between logo color and five personality traits had participants rate a computer-made logo in different colors on scales relating to the [[dimensions of brand personality]]. Relationships were found between color and sincerity, excitement, competence, sophistication, and ruggedness. A follow up study tested the effects of perceived brand personality and purchasing intentions.<ref name=\"Labrecque et al.\" /> Participants were presented with a product and a summary of the preferred brand personality and had to rate the likelihood of purchasing a product based on packaging color. Purchasing intent was greater if the perceived personality matched the marketed product or service. In turn color affects perceived brand personality and brand personality affects purchasing intent.<ref name=\"Labrecque et al.\" />\n\nAlthough color can be useful in marketing, its value and extent of use depends on how it is used and the audience it is used on.<ref name=\"Warner et al.\">{{cite journal | vauthors = Warner L, Franzen R | title = Value of color in advertising | journal = The Journal of Applied Psychology | volume = 31 | issue = 3 | pages = 260\u201370 | date = June 1947 | pmid = 20241978 | doi = 10.1037/h0057772 }}</ref> The use of color will have different effects on different people, therefore experimental findings cannot be taken as universally true.\n\n=== Specific color meaning ===\n\nDifferent colors are perceived to mean different things. For example, tones of red lead to feelings of arousal while blue tones are often associated with feelings of relaxation. Both of these emotions are pleasant, so therefore, the colors themselves can procure positive feelings in advertisements. The chart below gives perceived meanings of different colors in the United States.\n\nFunctional (F): fulfills a need or solves a problem<ref name=\"Bottomley et al.\" />\n\nSensory-Social (S): conveys attitudes, status, or social approval<ref name=\"Bottomley et al.\" />\n\n{| class=\"wikitable\" style=\"font-size:90%; width:100%;\"\n\n|-\n\n! style=\"text-align:center; background-color:red;\"| <span style=\"color:white\">Red</span>\n\n! style=\"text-align:center; background-color:yellow;\"| <span style=\"color:black\">Yellow</span>\n\n! style=\"text-align:center; background-color:green;\"| <span style=\"color:white\">Green</span>\n\n! style=\"text-align:center; background-color:blue;\"| <span style=\"color:white\">Blue</span>\n\n! style=\"text-align:center; background-color:pink;\"| <span style=\"color:black\">Pink</span>\n\n! style=\"text-align:center; background-color:purple;\"| <span style=\"color:white\">Violet/Purple</span>\n\n! style=\"text-align:center; background-color:orange;\" |<span style=\"color:black\">Orange</span>\n! style=\"text-align:center; background-color:brown;\"| <span style=\"color:white\">Brown</span>\n\n! style=\"text-align:center; background-color:black;\"| <span style=\"color:white\">Black</span>\n\n! style=\"text-align:center; background-color:white;\"| <span style=\"color:black\">White</span>\n\n|-\n\n| Lust (S)<ref name=\"Aslam\" />\n\n| Competence (S)<ref name=\"Labrecque et al.\" />\n\n| Good Taste (F)<ref name=\"Aslam\" />\n\n| Masculine (S)<ref name=\"Aslam\" />\n\n| Sophistication (S)<ref name=\"Labrecque et al.\" />\n\n| Authority (S)<ref name=\"Aslam\" />\n\n| Warmth (S)<ref name=\":6\">{{cite journal | vauthors = Elliot AJ | title = Color and psychological functioning: a review of theoretical and empirical work | journal = Frontiers in Psychology | volume = 6 | pages = 368 | date = 2015-04-02 | pmid = 25883578 | pmc = 4383146 | doi = 10.3389/fpsyg.2015.00368 }}</ref>\n| Ruggedness (S)<ref name=\"Labrecque et al.\" />\n\n| Grief (S)<ref name=\"Aslam\" />\n\n| Happiness (S)<ref name=\"Aslam\" />\n\n|-\n\n| Power (S)<ref name=\"Piotrowski et al.\">{{cite journal| vauthors = Piotrowski C, Armstrong T |title=Color Red: Implications for applied psychology and marketing research|journal=Psychology and Education: An Interdisciplinary Journal|year=2012|volume=49|issue=1\u20132|pages=55\u201357|url=https://tricourilemele.ro/2018/01/30/color-red/}}</ref>\n\n|Happiness (S)<ref name=\"Aslam\" />\n\n| Envy (S)<ref name=Aslam>{{cite journal| vauthors = Aslam MM |title=Are You Selling the Right Colour? A Cross-cultural Review of Colour as a Marketing Cue|journal=Journal of Marketing Communications|year=2006|volume=12|issue=1|pages=15\u201330|doi=10.1080/13527260500247827|url=https://ro.uow.edu.au/commpapers/1043}}</ref>\n\n| Competence (S)<ref name=\"Labrecque et al.\" />\n\n| Sincerity (S)<ref name=\"Labrecque et al.\" />\n\n| Sophistication (S)<ref name=\"Labrecque et al.\" />\n|Excitement (S)<ref name=\":6\" />\n|\n| Sophistication (S)<ref name=\"Labrecque et al.\" />\n\n| Sincerity (S)<ref name=\"Labrecque et al.\" />\n|-\n\n| Excitement (S)<ref name=\"Labrecque et al.\" />\n\n|Inexpensive (F)<ref name=\":4\">{{Cite web|url=http://www.joehallock.com/edu/COM498/index.html|title=Colour Assignment - By Joe Hallock|website=www.joehallock.com|access-date=2020-03-26}}</ref>\n|Eco-Friendly (F) <ref name=\":5\">{{Cite book|last=Lupton | first = Ellen | name-list-format = vanc |title=Design is storytelling| year = 2017 |isbn=978-1-942303-19-0|location=New York, NY|oclc=982650081}}</ref>\n| High quality (F)<ref name=\"Aslam\" />\n\n| Feminine (S)<ref name=\"Aslam\" />\n\n| Power (S)<ref name=\"Aslam\" />\n|\n|\n| Expensive (F)<ref name=\"Aslam\" />\n\n| Purity (S)<ref name=\"Aslam\" />\n\n|-\n\n| Love (S)<ref name=\"Aslam\" />\n\n|Low Quality (F)<ref name=\":4\" />\n|Health (S) <ref name=\":5\" />\n| Corporate (F)<ref name=\"Aslam\" />\n\n|\n|\n|\n|\n| Fear (S)<ref name=\"Aslam\" />\n|\n|-\n|Speed (S) <ref name=\":3\" />\n|\n|\n|Reliability (F) <ref name=\":4\" />\n|\n|\n|\n|\n|Death (S)<ref>{{Cite journal|last=Meier|first=Brian|last2=Robinson|first2=Michael|last3=Clore|first3=Gerald | name-list-format = vanc |date=2004-03-01|title=Why good guys wear white|url=https://www.researchgate.net/publication/8906275|journal=Psychological Science|volume=15|issue=2|pages=82\u20137|doi=10.1111/j.0963-7214.2004.01502002.x|pmid=14738513}}</ref>\n|\n|-\n|Anger (S)<ref>{{Cite journal|last=Kauppinen-R\u00e4is\u00e4nen|first=Hannele|last2=Jauffret|first2=Marie-Nathalie | name-list-format = vanc |date=2018-01-08|title=Using colour semiotics to explore colour meanings|journal=Qualitative Market Research|language=en|volume=21|issue=1|pages=101\u2013117|doi=10.1108/QMR-03-2016-0033|issn=1352-2752}}</ref>\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|}\n\n=== Combining colors ===\n\n[[File:Target logo.svg|right|thumb|Target logo]]\n\nAlthough some companies use a single color to represent their brand, many other companies use a combination of colors in their logo, and can be perceived in different ways than those colors independently. When asked to rate color pair preference of preselected pairs, people generally prefer color pairs with similar hues when the two colors are both in the foreground; however, greater contrast between the figure and the background is preferred.<ref name=\"Schloss et al.\">{{cite journal | vauthors = Schloss KB, Palmer SE | title = Aesthetic response to color combinations: preference, harmony, and similarity | journal = Attention, Perception & Psychophysics | volume = 73 | issue = 2 | pages = 551\u201371 | date = February 2011 | pmid = 21264737 | pmc = 3037488 | doi = 10.3758/s13414-010-0027-0 }}</ref>\n\nIn contrast to a strong preference for similar color combinations, some people like to accent with a highly contrasting color.<ref name=\"Deng et al.\">{{cite journal| vauthors = Deng X, Hui SK, Huntchinson J |title=Consumer preferences for color combinations: An empirical analysis of similarity-based color|journal=Journal of Consumer Psychology|year=2010|volume=20|issue=4|pages=476\u2013484|doi=10.1016/j.jcps.2010.07.005}}</ref> In a study on color preference for [[Nike, Inc.]] sneakers, people generally combined colors near each other on the color wheel, such as blue and dark blue. However, a smaller segment preferred to have the Nike swoosh accentuated in a different, and contrasting, color. Most of the people also used a relatively small number of colors when designing their ideal athletic shoe. This finding has relevance for companies that produce multicolored merchandise, suggesting that to appeal to consumer preferences, companies should consider minimizing the number of colors visible and using similar hues in any one product.<ref name=\"Skorinko et al.\" />\n\n=== Color name ===\n\nAlthough different colors can be perceived in different ways, the names of those colors matters as well.<ref name=\"Skorinko et al.\" /><ref name=\"Babin et al.\" /> Many products and companies focus on producing a wide range of product colors to attract the largest population of consumers. For example, cosmetics brands produce a rainbow of colors for eye shadow and nail polish, to appeal to every type of person. Even companies such as [[Apple Inc.]] and [[Dell]] who make [[iPod]]s and laptops do so with a certain amount of color personalization available to attract buyers. Moreover, color name, not only the actual color, can attract or repel buyers as well. When asked to rate color swatches and products with either generic color names (such as brown) or \"fancy\" color names (such as ''mocha''), participants rated items with fancy names as significantly more likable than items with generic names.<ref name=\"Skorinko et al.\">{{cite journal| vauthors = Skorinko JL, Kemmer S, Hebl MR, Lane DM |title=15. A Rose by Any Other Name...: Color-Naming Influences on Decision Making|journal=Psychology & Marketing|year=2006|volume=23|issue=12|pages=975\u2013993|doi=10.1002/mar.20142|citeseerx=10.1.1.581.1374}}</ref> In fact, the same paint color swatch with two different names produced different rating levels, and the same effect was found when participants rated the pleasantness of towels given fancy or generic color names,<ref name=\"Skorinko et al.\" /> showing an overall pattern of preference for fancy color names over generic ones when describing exactly the same color.\n\nFurthermore, it would appear that in addition to fancy names being preferred for their aural appeal, they may actually contribute to the product they represent itself being liked more, and hence in this manner impact sales.<ref name=\"Miller et al.\">{{cite journal| vauthors = Miller EG, Kahn BE |title=Shades of Meaning: The Effect of Color and Flavor Names on Consumer Choice|journal=Journal of Consumer Research|year=2005|volume=32|issue=1|pages=86\u201392|doi=10.1086/429602|citeseerx=10.1.1.488.3177}}</ref> A yellow [[Jelly beans|jelly bean]] with an atypical color name such as ''razzmatazz'' is more likely to be selected than one with a more typical name such as ''lemon yellow''. This could be due to greater interest in atypical names, as well as curiosity and willingness to \"figure out\" why that name was chosen. Purchasing intent patterns regarding custom sweatshirts from an online vendor also revealed a preference for atypical names. Participants were asked to imagine buying sweatshirts and were provided with a variety of color name options, some typical, some atypical. Color names that were atypical were selected more often than typical color names, again confirming a preference for atypical color names and for item descriptions using those names.<ref name=\"Miller et al.\" /> Moreover, those who chose sweatshirts bearing atypical color names were described as more content with their purchase than those who selected similar items bearing typical color names.\n\n=== Attracting attention ===\n[[File:Bangkok Chinatown Yaowarat street sign.jpg|thumb|upright|A store sign painted mainly red on a street in [[Bangkok]] to attract attention from passers-by]]\n\nColor is used as a means to attract consumer attention to a product that then influences buying behavior.<ref name=Kauppinen-Raisanen>{{cite journal| vauthors = Kauppinen-Raisanen H, Luomala HT |title=Exploring consumers' product-specific color meanings|journal=Qualitative Market Research|year=2010|volume=13|issue=3|pages=287\u2013308|doi=10.1108/13522751011053644}}</ref> Consumers use color to identify for known brands or search for new alternatives. Variety seekers look for non-typical colors when selecting new brands. Attractive color packaging receives more consumer attention than unattractive color packaging, which can then influence buying behavior. A study that looked at visual color cues focused on predicted purchasing behavior for known and unknown brands.<ref name=Kauppinen-Raisanen /> Participants were shown the same product in four different colors and brands. The results showed that people picked packages based on colors that attracted their voluntary and involuntary attention. Associations made with that color such as 'green fits menthol', also affected their decision. Based on these findings implications can be made on the best color choices for packages. New companies or new products could consider using dissimilar colors to attract attention to the brand, however, off brand companies could consider using similar colors to the leading brand to emphasize product similarity. If a company is changing the look of a product, but keeping the product the same, they consider keeping the same color scheme since people use color to identify and search for brands.<ref name=Kauppinen-Raisanen /> This can be seen in [[Crayola]] crayons, where the logo has changed many times since 1934, but the basic package colors, gold and green, have been kept throughout.\n\nAttention is captured subconsciously before people can consciously attend to something.<ref name=\"Kawasaki et al.\">{{cite journal | vauthors = Kawasaki M, Yamaguchi Y | title = Effects of subjective preference of colors on attention-related occipital theta oscillations | journal = NeuroImage | volume = 59 | issue = 1 | pages = 808\u201314 | date = January 2012 | pmid = 21820064 | doi = 10.1016/j.neuroimage.2011.07.042 }}</ref> Research looking at [[electroencephalography]] (EEGs) while people made decisions on color preference found brain activation when a favorite color is present before the participants consciously focused on it. When looking at various colors on a screen people focus on their favorite color, or the color that stands out more, before they purposefully turn their attention to it. This implies that products can capture someone's attention based on color, before the person willingly looks at the product.<ref name=\"Kawasaki et al.\" />\n\nIn interactive design and behavioral design, color is used to develop visual hierarchies where colors are placed into saliency hierarchies that match other hierarchies. Examples include matching a color hierarchy to a navigational structure hierarchy, or matching a behavioral science hierarchy to the most salient colors in a visual hierarchy, to increase the odds that important behavior change principles are noticed by a target audience and processed by them.<ref>Cugelman, B. Cugeman, R. et al. (2019) Color Psychology. AlterSpark. https://www.alterspark.com/color-psychology</ref>\n\n=== Store and display color ===\n\n[[File:Orange Armenia window display.JPG|right|thumb|Warm colored window display]]\n\nColor is not only used in products to attract attention, but also in window displays and stores.<ref name=\"Bellizzi et al.\">{{cite journal| vauthors = Bellizzi JA, Crowley AE, Hasty RW |title=The effects of color in store design|journal=Journal of Retailing|year=1983|volume=59|issue=1|pages=21\u201345}}</ref> When people are exposed to different colored walls and images of window displays and store interiors they tend to be drawn to some colors and not to others. Findings showed that people were physically drawn to warm colored displays; however, they rated cool colored displays as more favorable. This implies that warm colored store displays are more appropriate for spontaneous and unplanned purchases, whereas cool colored displays and store entrances may be a better fit for purchases where a lot of planning and customer deliberation occurs. This is especially relevant in shopping malls where patrons could easily walk into a store that attracts their attention without previous planning.<ref name=\"Bellizzi et al.\" />\n\nOther research has confirmed that store color, and not just the product, influences buying behavior.<ref name=\"Babin et al.\">{{cite journal|last=Babin|first=Barry J| last2 = Hardesty | first2 = David M | last3 = Suter | first3 = Tracy A | name-list-format = vanc |title=Color and shopping intentions|journal=Journal of Business Research|volume=56|issue=7|pages=541\u2013551|doi=10.1016/S0148-2963(01)00246-6|year=2003}}</ref> When people are exposed to different store color scenarios and then surveyed on intended buying behavior, store color, among various other factors, seems important for purchasing intentions. Particularly blue, a cool color, was rated as more favorable and produced higher purchasing intentions than orange, a warm color. However, all negative effects to orange were neutralized when orange store color was paired with soft lighting. This shows that store color and lighting actually interact.<ref name=\"Babin et al.\" />\n\nLighting color could have a strong effect on perceived experience in stores and other situation. For example, time seems to pass more slowly under red lights and time seems to pass quickly under blue light.<ref name=Singh /> Casinos take full advantage of this phenomenon by using color to get people to spend more time and hence more money in their casino.<ref name=Singh /> However, a presumed influence of coloured light (red vs. blue) on risk behaviour could not be demonstrated.<ref>Mao, T., Yang, J., Ru, T., Chen, Q., Shi, H., Zhou, J., & Zhou, G. (2018). Does red light induce people to be riskier? Exploring the colored light effect on the Balloon Analogue Risk Task (BART). Journal of Environmental Psychology, '''57''', 73-82. {{doi|10.1016/j.jenvp.2018.07.001}}</ref>\n\n== Individual differences ==\n[[File:Pink girls section of toy store.jpeg|thumb|upright|left|Pink girls section of toy store]]\n\n=== Gender ===\n\nChildren's toys are often categorized as either boys or girls toys solely based on color. In a study on color effects on perception, adult participants were shown blurred images of children's toys where the only decipherable feature visible was the toy's color.<ref name=\"Hull & Hull\" /> In general participants categorized the toys into girl and boy toys based on the visible color of the image. This can be seen in companies interested in marketing masculine toys, such as building sets, to boys. For example, [[Lego]] uses pink to specifically advertise some sets to girls rather than boys. The classification of 'girl' and 'boy' toys on the [[Disney Store]] website also uses color associations for each gender.<ref name=Auster>{{cite journal|last=Auster|first=Carol J.| last2 = Mansbach | first2 = Claire S. | name-list-format = vanc |title=The Gender Marketing of Toys: An Analysis of Color and Type of Toy on the Disney Store Website|journal=Sex Roles|year=2012|volume=67|issue=7\u20138|pages=375\u2013388|doi=10.1007/s11199-012-0177-8}}</ref> An analysis of the colors used showed that bold colored toys, such as red and black, were generally classified as 'boy only' toys and pastel colored toys, such as pink and purple, were classified as 'girl only' toys. Toys that were classified as both boy and girl toys took on 'boy only' toy colors. This again emphasizes the distinction in color use for children's toys.<ref name=Auster />\n\nGender differences in color associations can also be seen amongst adults.<ref name=\"Chen Part I\">{{cite journal|last=Ou|first=Li-Chen| last2 = Luo | first2 = M. Ronnier | last3 = Woodcock | first3 = Andree | last4 = Wright | first4 = Angela | name-list-format = vanc |title=A study of colour emotion and colour preference. Part I: Colour emotions for single colours|journal=Color Research & Application|year=2004|volume=29|issue=3|pages=232\u2013240|doi=10.1002/col.20010}}</ref> Differences were noted for male and female participants, where the two genders did not agree on which color pairs they enjoyed the most when presented with a variety of colors.<ref name=\"Hull & Hull\">{{cite journal| vauthors = Hull JH, Hull DB, Knopp C |title=The Impact of color on ratings of 'girl' and 'boy' toys|journal=North American Journal of Psychology|year=2011|volume=13|issue=3|pages=549\u2013562 |url=http://www.freepatentsonline.com/article/North-American-Journal-Psychology/276353296.html|id=276353296|access-date=2017-02-14|archive-url=https://web.archive.org/web/20161006014108/http://www.freepatentsonline.com/article/North-American-Journal-Psychology/276353296.html|archive-date=2016-10-06|url-status=dead}}</ref><ref name=\"Chen Part II\">{{cite journal|last=Ou|first=Li-Chen| last2 = Luo | first2 = M. Ronnier | last3 = Woodcock | first3 = Andree | last4 = Wright | first4 = Angela | name-list-format = vanc |title=A study of colour emotion and colour preference. Part II: Colour emotions for two-colour combinations|journal=Color Research & Application|volume=29|issue=4|pages=292\u2013298|doi=10.1002/col.20024 |year=2004}}</ref> Men and women also did not agree on which colors should be classified as masculine and feminine. This could imply that men and women generally prefer different colors when purchasing items. Men and women also misperceive what colors the opposite gender views as fitting for them.\n\n=== Age ===\n\nChildren's toys for younger age groups are often marketed based on color, however, as the age group increases color becomes less gender-stereotyped.<ref name=\"Hull & Hull\" /> In general many toys become gender neutral and hence adopt gender-neutral colors. In the United States it is common to associate baby girls with pink and baby boys with blue. This difference in young children is a learned difference rather than an inborn one.<ref name=\"LoBue et al.\">{{cite journal | vauthors = Lobue V, Deloache JS | title = Pretty in pink: The early development of gender-stereotyped colour preferences | journal = The British Journal of Developmental Psychology | volume = 29 | issue = Pt 3 | pages = 656\u201367 | date = September 2011 | pmid = 21848751 | doi = 10.1111/j.2044-835X.2011.02027.x | url = https://semanticscholar.org/paper/4a1fb5c48ff700289d231e61c5b55241646c26c2 }}</ref> Research has looked at young children's, ages 7 months to 5 years, preference for small objects in different colors. The results showed that by the age of 2\u20132.5 years socially constructed gendered colors affects children's color preference, where girls prefer pink and boys avoid pink, but show no preference for other colors.<ref name=\"LoBue et al.\" />\n\nSlightly older children who have developed a sense of favorite color often tend to pick items that are in that color.<ref name=\"Gollety (2011)\">{{cite journal| vauthors = Gollety M, Guichard N |title=The dilemma of flavor and color in the choice of packaging by children|journal=Young Consumers|year=2011|volume=12|issue=1|pages=82\u201390|doi=10.1108/17473611111114803}}</ref>\n\n=== Culture ===\n{{see also|Linguistic relativity}}\n\nMany cultural differences exist on perceived color personality, meaning, and preference. When deciding on brand and product logos, companies should take into account their target consumer, since cultural differences exist. A study looked at color preference in British and Chinese participants.<ref name=\"Chen Part I\" /> Each participant was presented with a total of 20 color swatches one at a time and had to rate the color on 10 different emotions. Results showed that British participants and Chinese participants differed on the like-dislike scale the most. Chinese participants tended to like colors that they self rated as clean, fresh, and modern, whereas British participants showed no such pattern. When evaluating purchasing intent, color preference affects buying behavior, where liked colors are more likely to be bought than disliked colors.<ref name=Kauppinen-Raisanen /> This implies that companies should consider choosing their target consumer first and then make product colors based on the target's color preferences.\n\nWollard, (2000)<ref>{{cite web|url =http://wf2la6.webfeat.org|title =Wollard, K. (2000). Orange you glad you're not blue? }}{{Dead link|date=November 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> seems to think that color can affect one's mood, but the effect also can depend on one's culture and what one's personal reflection may be. For example, someone from Japan may not associate red with anger, as people from the U.S. tend to do. Also, a person who likes the color brown may associate brown with happiness. However, Wollard does think that colors can make everyone feel the same, or close to the same, mood.\n\n== Color and sports performance ==\n\nIn particular, the color red has been found to influence sports performance. During the [[2004 Summer Olympics]] the competitors in [[boxing]], [[taekwondo]], [[freestyle wrestling]], and [[Greco-Roman wrestling]] were randomly given blue or red uniforms. A later study found that those wearing red won 55% of all the bouts which was a statistically significant increase over the expected 50%. The colors affected bouts where the competitors were closely matched in ability, where those wearing red won 60% of the bouts, but not bouts between more unevenly matched competitors. In England, since WWII, teams wearing red uniforms have averaged higher league positions and have had more league winners than teams using other colors. In cities with more than one team, the teams wearing red outperformed the teams wearing other colors. A study of the [[UEFA Euro 2004]] found similar results. Another study found that those taking [[Penalty kick (association football)|penalty kick]]s performed worst when the goalkeeper had a red uniform. More anecdotal is the historical dominance of the domestic honors by red-wearing teams such [[AFC Ajax]], [[FC Bayern Munich]], [[Liverpool F.C.]], and [[Manchester United F.C.]] Videos of taekwondo bouts were manipulated in one study so that the red and blue colors of the protective gears were reversed. Both the original and the manipulated videos were shown to referees. The competitors wearing red were given higher scores despite the videos otherwise being identical. A study on experienced players of [[first-person shooter]]s found that those assigned to wear red instead of blue won 55% of the matches.<ref name=AEP>Diana Widermann, Robert A. Barton, and Russel A. Hill. Evolutionary perspectives on sport and competition. In {{Cite book | vauthors = Roberts SC | veditors = Roberts SC | doi = 10.1093/acprof:oso/9780199586073.001.0001 | title = Applied Evolutionary Psychology | year = 2011 | publisher = Oxford University Press| isbn = 9780199586073 | pmid = | pmc = }}</ref>\n\nThere are several different explanations for this effect. Red is used in stop signs and traffic lights which may associate the color with halting. Red is also perceived as a strong and active color which may influence both the person wearing it and others. An [[evolutionary psychology]] explanation is that red may signal health as opposed to [[anemic]] paleness, or indicate anger due to [[flushing (physiology)|flushing]] instead of paleness due to fear. It has been argued that detecting flushing may have influenced the development of primate [[Trichromacy|trichromate]] vision. Primate studies have found that some species evaluate rivals and possible mates depending on red color characteristics. Facial redness is associated with testosterone levels in humans, and male skin tends to be redder than female skin.<ref name=AEP/>\n\n== Color and time perception ==\n\nRecent results<ref>Masahiro Shibasaki, Nobuo Masataka (2014) \"[http://www.nature.com/articles/srep05899 The color red distorts time perception for men, but not for women]\" ''Scientific Reports'' 4, Article number: 5899 <abbr>doi</abbr>:10.1038/srep05899\n</ref> showed that the perceived duration of a red screen was longer than was that of a blue screen. The results reflected sex differences; men, but not women, overestimated the duration of the red screen. Additionally, the reaction times to a red screen were faster than those to a blue screen. Participants who reacted quickly to a red screen overestimated its duration. In a demo with 150 people chosen at random, it was found that inside a pod bathed in blue color the average perceived duration of a minute was 11 seconds shorter than in a pod bathed in red color.<ref>Beau Lotto in \"[http://www.documentarymania.com/player.php?title=Do%20You%20See%20What%20I%20See Do You See What I See]\" (2011)</ref>\n\n== Game Immersion ==\nSince color is such an important element in how people interpret their environment, color psychology can enhance the feeling of immersion in people that play video games. By using color psychology to cause immersion in players, players can have less errors playing video games and felt more a part of the game they were playing in comparison to a game that did not have color psychology immersion.<ref name=\"Roohi 100298\"/>\n\n== See also ==\n* [[Color symbolism]]\n* [[Color vision]]\n* [[Kruithof curve]]\n* [[L\u00fcscher color test]]\n* [[Visual perception]]\n\n== References ==\n{{reflist|30em}}\n\n{{Color topics}}\n\n{{Psychology}}\n\n[[Category:Color in culture|Psychology]]\n[[Category:Branches of psychology]]\n", "name_user": "Chriswagner87", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Color_psychology"}
{"title_page": "East Orange station", "text_new": "{{Use mdy dates|date=April 2020}}\n{{Infobox station\n | name=East Orange\n | style=NJ Transit\n | image= East Orange Station - April 2015.jpg\n | image_caption=The East Orange station in April 2015, facing toward Brick Church.\n | address= 65 City Hall Plaza, [[East Orange, New Jersey]]\n | coordinates=\n | line=\n | other={{bus icon|12px|NJT Bus}} '''[[New Jersey Transit Bus Operations|NJT Bus]]''': [[List of New Jersey Transit bus routes (1-99)|21, 71, 73, 79, and 94]]<br/>{{bus icon|12px|Intercity Bus}} '''[[Community Coach]]''': 77\n | platform=1 side platform and 1 island platform\n | levels=\n | tracks=3\n | parking=\n | bicycle=\n | passengers=455 (average weekday)<ref>{{cite web |url=http://media.nj.com/bergen_impact/other/1Q2013.pdf |title=QUARTERLY RIDERSHIP TRENDS ANALYSIS |publisher=New Jersey Transit |accessdate=January 4, 2013 |archiveurl=https://www.webcitation.org/6DEGzJnox?url=http://media.nj.com/bergen_impact/other/1Q2013.pdf |archivedate=December 27, 2012 |url-status=dead }}</ref><ref>{{Cite news|url=https://patch.com/new-jersey/hoboken/how-many-riders-use-nj-transit-s-hoboken-train-station|title=How Many Riders Use NJ Transit's Hoboken Train Station?|work=Hoboken Patch|access-date=July 18, 2018|language=en}}</ref>\n | pass_year=2017\n | pass_percent=\n | pass_system=\n | opened=November&nbsp;19, 1836{{sfn|Douglass|1912|p=339}}\n | closed=\n | rebuilt=April 21, 1921&ndash;December 18, 1922<ref>{{cite news |title=D., L. & W. Opens New Elevated Line |url=https://www.newspapers.com/clip/29183461/dlw_raise_december_18_1922/ |accessdate=March 5, 2019 |work=The Paterson Evening News |date=December 18, 1922 |page=1|via=Newspapers.com}} {{open access}}</ref>\n | electrified=\n | ADA=yes\n | code=\n | owned=[[New Jersey Transit]]\n | zone=4\n | nrhp={{Infobox NRHP\n  | name = East Orange Station\n  | embed = yes\n  | nrhp_type = \n  | image = East Orange Station house jeh.jpg\n  | caption = East Orange station depot\n  | location= \n  | coordinates = {{coord|40|45|40.8|N|74|12|39.5|W|region:US-NJ_type:railwaystation|display=inline,title}}\n| locmapin = USA New Jersey Essex County#New Jersey#USA\n  | map_caption = Location in Essex, County, New Jersey\n  | built = 1921\n  | architect = F.W. Nies\n  | architecture =  Tudor Revival, Jacobethan Revival          \n  | added = June 22, 1984\n <!-- | area = {{convert|2.5|acre}} -->\n  | mpsub = {{NRHP url|id=64000496|title=Operating Passenger Railroad Stations TR}}\n  | refnum = 84002638<ref name=\"nris\">{{NRISref|version=2010a}}</ref>\n  }}\n | former=\n | services= {{Adjacent stations|system1=NJ Transit\n|line1=Gladstone|left1=Brick Church|right1=Newark Broad Street\n|line2=Morristown|left2=Brick Church|right2=Newark Broad Street\n}}\n | other_services_header = Former services\n | other_services_collapsible = yes\n | other_services = {{Adjacent stations|\nsystem1=NJ Transit\n|line1=Gladstone|left1=Brick Church|right1=Grove Street|note-mid1=until April 7, 1991\n|line2=Morristown|left2=Brick Church|right2=Grove Street|note-mid2=until April 7, 1991\n|system3=Delaware, Lackawanna and Western Railroad\n|line3=main|left3=Brick Church|right3=Grove Street\n}}\n | mpassengers=\n}}\n\n'''East Orange''' is a [[New Jersey Transit]] station  on the Morris and Essex line in [[East Orange, New Jersey|East Orange]], [[Essex County, New Jersey|Essex County]], [[New Jersey]], United States. This elevated station was built in 1923 for the Lackawanna and now has trains from the [[Morristown Line]] and the [[Gladstone Branch]], including service to [[Hoboken Terminal]] and [[Kearny Connection|Midtown Direct]] service to [[Pennsylvania Station (New York City)|New York Penn Station]] in [[Midtown Manhattan]]. The station is next to the westbound lanes of Interstate 280 about five hundred yards west of the [[Garden State Parkway]]. The East Orange City Hall is north of the station.\n\nThe [[head house]] has been on the [[New Jersey Register of Historic Places|state]] and [[National Register of Historic Places|federal]] registers of historic places since 1984,<ref>[http://www.nationalregisterofhistoricplaces.com/NJ/Monmouth/state.html Monmouth County Listings], [[National Register of Historic Places]]. Accessed September 2, 2007.</ref> listed as part of the [[Operating Passenger Railroad Stations Thematic Resource (New Jersey)|Operating Passenger Railroad Stations Thematic Resource]].<ref>{{NRHP url|id=84002638|title=East Orange New Jersey Transit Railroad Station Survey}}</ref>\n\n==History==\nStation owner [[New Jersey Transit]] decided to perform work at East Orange station to improve accessibility for the handicapped and to repair eighty-year-old viaducts at the station.<ref name=\"NJofficial\">[http://www.njtransit.com/tm/tm_servlet.srv?hdnPageAction=Project031To M&E station improvement and viaduct rehabilitation] ''NJ Transit official site'' Retrieved August 6, 2007</ref> At a cost of $22.9&nbsp;million, repair work at East Orange, along with nearby stations [[Brick Church (NJT station)|Brick Church]] and [[South Orange (NJT station)|South Orange]], commenced in 2004.<ref name=\"progressive1\">[http://www.progressiverailroading.com/transitnews/article.asp?id=4270 NJ Transit approves $22.9 million in viaduct repairs] ''Progressive Railroading'' Retrieved August 6, 2007</ref>  East Orange received a mini-high level platform, the tracks surrounding the station were upgraded with concrete ties and the stairways leading to the platforms were replaced.<ref name=\"progressive2\">[http://www.progressiverailroading.com/transitnews/article.asp?id=4793 NJ Transit breaks ground on three-station rehab project] ''Progressive Railroading'' Retrieved August 7, 2007</ref>\n\n==Station layout==\nThe station has two low-level platforms serving all three tracks.\n{|cellspacing=0 cellpadding=3\n|style=\"border-top:solid 1px gray; border-bottom:solid 1px gray\" rowspan=5 width=75|'''P<br/>Platform level'''\n|style=\"border-top:solid 1px gray;border-right:solid 2px black;border-left:solid 2px black;border-bottom:solid 2px black;text-align:center;\" colspan=2|<small>[[Side platform]], doors will open on the right</small>\n|-\n|Track '''3'''\n|\u2190 {{rcb|system=NJ Transit|line=Morristown|inline=yes}} toward [[Dover station (NJ Transit)|Dover]] or [[Hackettstown station|Hackettstown]] <small>([[Brick Church station|Brick Church]])</small><br/>\u2190 {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Gladstone station (NJ Transit)|Gladstone]] <small>(Brick Church)</small>\n|-\n|style=\"border-top:solid 1px gray;\"|Track '''1'''\n|style=\"border-top:solid 1px gray;\"|\u2190 {{rcb|system=NJ Transit|line=Morristown|inline=yes}} toward [[Dover station (NJ Transit)|Dover]] or [[Hackettstown station|Hackettstown]] <small>(Brick Church)</small><br/>\u2190 {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Gladstone station (NJ Transit)|Gladstone]] <small>(Brick Church)</small><br/>{{0|\u2190}} {{rcb|system=NJ Transit|line=Morristown|inline=yes}} and {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Hoboken Terminal|Hoboken]] or [[Pennsylvania Station (New York City)|New York]] <small>([[Newark Broad Street station|Newark Broad Street]])</small> \u2192\n|-\n|style=\"border-top:solid 2px black;border-right:solid 2px black;border-left:solid 2px black;border-bottom:solid 2px black;text-align:center;\" colspan=2|<small>[[Island platform]], doors will open on the left or right</small>\n|-\n|style=\"border-bottom:solid 1px gray;\" width=100|Track '''2'''\n|style=\"border-bottom:solid 1px gray;\" width=650|{{0|\u2190}} {{rcb|system=NJ Transit|line=Morristown|inline=yes}} and {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Hoboken Terminal|Hoboken]] or [[Pennsylvania Station (New York City)|New York]] <small>(Newark Broad Street)</small> \u2192\n|-\n|style=\"border-bottom:solid 1px gray;\"|'''G'''\n|style=\"border-bottom:solid 1px gray;\"|Street level\n|style=\"border-bottom:solid 1px gray;\"|Station building, ticket machines, parking\n|}\n\n==See also==\n*[[List of New Jersey Transit stations]]\n\n==Bibliography==\n*{{cite book |first=A.M.|last=Douglass|title=The Railroad Trainman, Volume 29 |date=1912 |publisher=Brotherhood of Railroad Trainmen |location=[[Cleveland, Ohio]] |url=https://books.google.com/books?id=AszNAAAAMAAJ&newbks=1&newbks_redir=0&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false |accessdate=April 5, 2020|ref=harv}}\n\n==References==\n{{reflist}}\n\n==External links==\n{{Commonscat-inline|East Orange (NJT station)}}\n{{NJT links}}\n\n{{NJT stations navbox}}\n{{National Register of Historic Places in New Jersey}}\n\n{{DEFAULTSORT:East Orange (Njt Station)}}\n[[Category:East Orange, New Jersey]]\n[[Category:National Register of Historic Places in Essex County, New Jersey]]\n[[Category:NJ Transit Rail Operations stations]]\n[[Category:Railway stations in the United States opened in 1836]]\n[[Category:Railway stations on the National Register of Historic Places in New Jersey]]\n[[Category:Former Delaware, Lackawanna and Western Railroad stations]]\n[[Category:Railway stations in Essex County, New Jersey]]\n", "text_old": "{{Infobox station\n | name=East Orange\n | style=NJ Transit\n | image= East Orange Station - April 2015.jpg\n | image_caption=The East Orange station in April&nbsp;2015, facing toward Brick Church.\n | address= 65 City Hall Plaza, [[East Orange, New Jersey]]\n | coordinates=\n | line=\n | other={{bus icon|12px|NJT Bus}} '''[[New Jersey Transit Bus Operations|NJT Bus]]''': [[List of New Jersey Transit bus routes (1-99)|21, 71, 73, 79, and 94]]<br/>{{bus icon|12px|Intercity Bus}} '''[[Community Coach]]''': 77\n | platform=1 side platform and 1 island platform\n | levels=\n | tracks=3\n | parking=\n | bicycle=\n | passengers=455 (average weekday)<ref>{{cite web |url=http://media.nj.com/bergen_impact/other/1Q2013.pdf |title=QUARTERLY RIDERSHIP TRENDS ANALYSIS |publisher=New Jersey Transit |accessdate=January 4, 2013 |archiveurl=https://www.webcitation.org/6DEGzJnox?url=http://media.nj.com/bergen_impact/other/1Q2013.pdf |archivedate=December 27, 2012 |url-status=dead }}</ref><ref>{{Cite news|url=https://patch.com/new-jersey/hoboken/how-many-riders-use-nj-transit-s-hoboken-train-station|title=How Many Riders Use NJ Transit's Hoboken Train Station?|work=Hoboken Patch|access-date=2018-07-18|language=en}}</ref>\n | pass_year=2017\n | pass_percent=\n | pass_system=\n | opened=November&nbsp;19, 1836{{sfn|Douglass|1912|p=339}}\n | closed=\n | rebuilt=April 21, 1921&ndash;December 18, 1922<ref>{{cite news |title=D., L. & W. Opens New Elevated Line |url=https://www.newspapers.com/clip/29183461/dlw_raise_december_18_1922/ |accessdate=March 5, 2019 |work=The Paterson Evening News |date=December 18, 1922 |page=1|via=Newspapers.com}} {{open access}}</ref>\n | electrified=\n | ADA=yes\n | code=\n | owned=[[New Jersey Transit]]\n | zone=4\n | nrhp={{Infobox NRHP\n  | name = East Orange Station\n  | embed = yes\n  | nrhp_type = \n  | image = East Orange Station house jeh.jpg\n  | caption = East Orange station depot\n  | location= \n  | coordinates = {{coord|40|45|40.8|N|74|12|39.5|W|region:US-NJ_type:railwaystation|display=inline,title}}\n| locmapin = USA New Jersey Essex County#New Jersey#USA\n  | map_caption = Location in Essex, County, New Jersey\n  | built = 1921\n  | architect = F.W. Nies\n  | architecture =  Tudor Revival, Jacobethan Revival          \n  | added = June 22, 1984\n <!-- | area = {{convert|2.5|acre}} -->\n  | mpsub = {{NRHP url|id=64000496|title=Operating Passenger Railroad Stations TR}}\n  | refnum = 84002638<ref name=\"nris\">{{NRISref|version=2010a}}</ref>\n  }}\n | former=\n | services= {{Adjacent stations|system1=NJ Transit\n|line1=Gladstone|left1=Brick Church|right1=Newark Broad Street\n|line2=Morristown|left2=Brick Church|right2=Newark Broad Street\n}}\n | other_services_header = Former services\n | other_services_collapsible = yes\n | other_services = {{Adjacent stations|\nsystem1=NJ Transit\n|line1=Gladstone|left1=Brick Church|right1=Grove Street|note-mid1=until April 7, 1991\n|line2=Morristown|left2=Brick Church|right2=Grove Street|note-mid2=until April 7, 1991\n|system3=Delaware, Lackawanna and Western Railroad\n|line3=main|left3=Brick Church|right3=Grove Street\n}}\n | mpassengers=\n}}\n\n'''East Orange''' is a [[New Jersey Transit]] station  on the Morris and Essex line in [[East Orange, New Jersey|East Orange]], [[Essex County, New Jersey|Essex County]], [[New Jersey]], United States. This elevated station was built in 1923 for the Lackawanna and now has trains from the [[Morristown Line]] and the [[Gladstone Branch]], including service to [[Hoboken Terminal]] and [[Kearny Connection|Midtown Direct]] service to [[Pennsylvania Station (New York City)|New York Penn Station]] in [[Midtown Manhattan]]. The station is next to the westbound lanes of Interstate 280 about five hundred yards west of the [[Garden State Parkway]]. The East Orange City Hall is north of the station.\n\nThe [[head house]] has been on the [[New Jersey Register of Historic Places|state]] and [[National Register of Historic Places|federal]] registers of historic places since 1984,<ref>[http://www.nationalregisterofhistoricplaces.com/NJ/Monmouth/state.html Monmouth County Listings], [[National Register of Historic Places]]. Accessed September 2, 2007.</ref> listed as part of the [[Operating Passenger Railroad Stations Thematic Resource (New Jersey)|Operating Passenger Railroad Stations Thematic Resource]].<ref>{{NRHP url|id=84002638|title=East Orange New Jersey Transit Railroad Station Survey}}</ref>\n\n==History==\nStation owner [[New Jersey Transit]] decided to perform work at East Orange station to improve accessibility for the handicapped and to repair eighty-year-old viaducts at the station.<ref name=\"NJofficial\">[http://www.njtransit.com/tm/tm_servlet.srv?hdnPageAction=Project031To M&E station improvement and viaduct rehabilitation] ''NJ Transit official site'' Retrieved 2007-08-06</ref> At a cost of $22.9&nbsp;million, repair work at East Orange, along with nearby stations [[Brick Church (NJT station)|Brick Church]] and [[South Orange (NJT station)|South Orange]], commenced in 2004.<ref name=\"progressive1\">[http://www.progressiverailroading.com/transitnews/article.asp?id=4270 NJ Transit approves $22.9 million in viaduct repairs] ''Progressive Railroading'' Retrieved 2007-08-06</ref>  East Orange received a mini-high level platform, the tracks surrounding the station were upgraded with concrete ties and the stairways leading to the platforms were replaced.<ref name=\"progressive2\">[http://www.progressiverailroading.com/transitnews/article.asp?id=4793 NJ Transit breaks ground on three-station rehab project] ''Progressive Railroading'' Retrieved 2007-08-07</ref>\n\n==Station layout==\nThe station has two low-level platforms serving all three tracks.\n{|cellspacing=0 cellpadding=3\n|style=\"border-top:solid 1px gray; border-bottom:solid 1px gray\" rowspan=5 width=75|'''P<br/>Platform level'''\n|style=\"border-top:solid 1px gray;border-right:solid 2px black;border-left:solid 2px black;border-bottom:solid 2px black;text-align:center;\" colspan=2|<small>[[Side platform]], doors will open on the right</small>\n|-\n|Track '''3'''\n|\u2190 {{rcb|system=NJ Transit|line=Morristown|inline=yes}} toward [[Dover station (NJ Transit)|Dover]] or [[Hackettstown station|Hackettstown]] <small>([[Brick Church station|Brick Church]])</small><br/>\u2190 {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Gladstone station (NJ Transit)|Gladstone]] <small>(Brick Church)</small>\n|-\n|style=\"border-top:solid 1px gray;\"|Track '''1'''\n|style=\"border-top:solid 1px gray;\"|\u2190 {{rcb|system=NJ Transit|line=Morristown|inline=yes}} toward [[Dover station (NJ Transit)|Dover]] or [[Hackettstown station|Hackettstown]] <small>(Brick Church)</small><br/>\u2190 {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Gladstone station (NJ Transit)|Gladstone]] <small>(Brick Church)</small><br/>{{0|\u2190}} {{rcb|system=NJ Transit|line=Morristown|inline=yes}} and {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Hoboken Terminal|Hoboken]] or [[Pennsylvania Station (New York City)|New York]] <small>([[Newark Broad Street station|Newark Broad Street]])</small> \u2192\n|-\n|style=\"border-top:solid 2px black;border-right:solid 2px black;border-left:solid 2px black;border-bottom:solid 2px black;text-align:center;\" colspan=2|<small>[[Island platform]], doors will open on the left or right</small>\n|-\n|style=\"border-bottom:solid 1px gray;\" width=100|Track '''2'''\n|style=\"border-bottom:solid 1px gray;\" width=650|{{0|\u2190}} {{rcb|system=NJ Transit|line=Morristown|inline=yes}} and {{rcb|system=NJ Transit|line=Gladstone|inline=yes}} toward [[Hoboken Terminal|Hoboken]] or [[Pennsylvania Station (New York City)|New York]] <small>(Newark Broad Street)</small> \u2192\n|-\n|style=\"border-bottom:solid 1px gray;\"|'''G'''\n|style=\"border-bottom:solid 1px gray;\"|Street level\n|style=\"border-bottom:solid 1px gray;\"|Station building, ticket machines, parking\n|}\n\n==See also==\n*[[List of New Jersey Transit stations]]\n\n==Bibliography==\n*{{cite book |first=A.M.|last=Douglass|title=The Railroad Trainman, Volume 29 |date=1912 |publisher=Brotherhood of Railroad Trainmen |location=[[Cleveland, Ohio]] |url=https://books.google.com/books?id=AszNAAAAMAAJ&newbks=1&newbks_redir=0&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false |accessdate=April 5, 2020|ref=harv}}\n\n==References==\n{{reflist}}\n\n==External links==\n{{Commonscat-inline|East Orange (NJT station)}}\n{{NJT links}}\n\n{{NJT stations navbox}}\n{{National Register of Historic Places in New Jersey}}\n\n{{DEFAULTSORT:East Orange (Njt Station)}}\n[[Category:East Orange, New Jersey]]\n[[Category:National Register of Historic Places in Essex County, New Jersey]]\n[[Category:NJ Transit Rail Operations stations]]\n[[Category:Railway stations in the United States opened in 1836]]\n[[Category:Railway stations on the National Register of Historic Places in New Jersey]]\n[[Category:Former Delaware, Lackawanna and Western Railroad stations]]\n[[Category:Railway stations in Essex County, New Jersey]]\n", "name_user": "Kew Gardens 613", "label": "safe", "comment": "date formats perMOS:DATEFORMATbyscript", "url_page": "//en.wikipedia.org/wiki/East_Orange_station"}
{"title_page": "My Giant", "text_new": "{{multiple issues|\n{{lead too short|date=March 2015}}\n{{more citations needed|date=May 2015}}\n}}\n{{Use dmy dates|date=January 2020}}\n{{Infobox film\n| name           = My Giant\n| image          = My giant poster.jpg\n| image_size     = 225px\n| alt            = \n| caption        = theatrical release poster\n| director       = [[Michael Lehmann]]\n| producer       = [[Billy Crystal]]\n| writer         = [[David Seltzer]]\n| story          = {{Plainlist|\n* Billy Crystal\n* David Seltzer\n}}\n| starring       = {{Plainlist|\n* Billy Crystal\n* [[Kathleen Quinlan]]\n* [[Joanna Pacu\u0142a]]\n* [[Gheorghe Mure\u0219an]]\n}}\n| music          = [[Marc Shaiman]]\n| cinematography = [[Michael Coulter]]\n| editing        = [[Stephen Semel]]\n| studio         = [[Castle Rock Entertainment]]\n| distributor    = [[Columbia Pictures]]\n| released       = 10 April 1998 ''(US)''<br>12 February 1999 ''(UK)''\n| country        = United States\n| runtime        = 103 minutes\n| gross          = $8,072,007\n}}\n\n'''''My Giant''''' is a 1998 American [[comedy film]] directed by [[Michael Lehmann]]. The film stars [[Billy Crystal]], who also produced and co-wrote the story for the film, and Romanian [[National Basketball Association|NBA]] player [[Gheorghe Mure\u0219an]] in his only film appearance.<ref name=\"My Giant\">{{cite web|url=http://www.tcm.com/tcmdb/title/327296/My-Giant/full-credits.html|accessdate=19 April 2016|work=[[Turner Classic Movies]]|title=My Giant}}</ref> David Seltzer's script was inspired by Crystal's friendship with professional wrestler [[Andr\u00e9 the Giant]], whom he had met during the filming of ''[[The Princess Bride (film)|The Princess Bride]]''.\n\n==Plot==\nTalent agent and [[huckster]] Sammy Kamin travels to Romania on business after splitting up with his wife. After his young client fires him, Sammy crashes his car and is rescued, while unconscious, by an enormous Romanian man named Max who is close to 8 feet tall.\n\nSammy thinks the rescuer is [[God]], as he can only see Max's giant hands. When Sammy wakes up, he thinks he is in [[Heaven]]. But he is confused to find a statue of [[Jesus Christ|Jesus]] next to his bed, as he was raised Jewish. He then realizes Max has brought him to a monastery, where he was raised after being given up for [[adoption]] by his parents because of his height.\n\nOnce he wakes up and interacts with Max, he sees potential stardom in him. Sammy attempts to broker his introduction into the movies. In doing so, he exploits Max' desire to visit a long-lost paramour, Lilliana, in [[Gallup, New Mexico]]. First, Max obtains the role of a villain in a movie, but he is so drunk that he vomits on the protagonist (Sammy's former client). However, the scene is included in the movie.\n\nOne day, Sammy talks to [[Steven Seagal]] about including Max as a villain in one of his movies, convincing him that he needs a different kind of villain. At first Seagal rejects him because there was another actor who would take that role, but he changes his opinion after listening to an extract of a [[Shakespeare]]an play done by Max.\n\nSuddenly, after some medical exams, Max is diagnosed with [[Cardiovascular disease|heart disease]] which cannot be treated with a transplant because his heart is so big. Sammy decides to find Lilliana, and tries to convince her to meet Max again, but she rejects the invitation. Sammy then convinces his wife to take the role of Lilliana and after some words, Max asks her for a kiss.\n\nAfterwards, Sammy and his wife decide to get back together again, after realizing that they were truly in love and that their son needed them.\n\nSammy eventually decides to return Max home to Romania. Max refuses to go back, but finally he enters his old house, and meets his parents again. Sammy ends up watching Max's first filmed scene in a cinema with his family. Max dies shortly after, because of his heart, but he changed many people's lives forever.\n\n==Cast==\n* [[Billy Crystal]] as Sammy Kamin\n* [[Gheorghe Mure\u0219an]] as Maximus \"Max\" Sanferescu\n* [[Kathleen Quinlan]] as Serena Kamin\n* [[Joanna Pacu\u0142a]] as Lillianna Otarro\n* Zane Carney as Nick Kamin\n* [[Dan Castellaneta]] as Partlow\n* [[Steven Seagal]] as himself\n* [[Doris Roberts]] as Rose Kaminski\n* [[Jere Burns]] as Weller, the Movie Director \n* [[Raymond O'Connor]] as Eddie \n* [[Rider Strong]] as Justin Allen\n* [[Lorna Luft]] as Joanne\n\n==Reception==\n===Box office===\nThe film was not a [[box office]] success, grossing a little over $8 million domestically, far less than its $20 million budget.<ref>{{cite news|url= http://articles.latimes.com/1998/apr/13/entertainment/ca-38767 |title='City of Angels' Takes Wing in Heavenly Opening Weekend|newspaper=[[Los Angeles Times]] |date=13 April 1998 |accessdate=15 September 2013}}</ref><ref>{{cite news|url=http://articles.latimes.com/1998/apr/14/entertainment/ca-38964 |title=Top of the World for Warner Bros|newspaper=Los Angeles Times |date=14 April 1998 |accessdate=15 September 2013 |first=Robert W. |last=Welkos}}</ref>\n\n===Critical===\nThe film gained mostly negative reviews. [[Roger Ebert]] stated: \"The movie, which could have been a funny send-up of Hollywood talent requirements, gets distracted by subplots [that] after its promising start, ''My Giant'' isn't a comedy about an agent and a giant, so much as the heartwarming tale of a guy who learns to be a better family man.\"<ref>{{cite web |author= Roger Ebert | author-link= Roger Ebert |url=http://www.rogerebert.com/reviews/my-giant-1998 |title=My Giant - Review |publisher= [[Chicago Sun-Times]] |accessdate= 16 September 2013}}</ref>\n\n[[CNN]]'s Paul Tatara stated: \"Crystal is Crystal throughout, and I still like him for it. Muresan, on the other hand, is sweet but, shall we say, a limited performer. He also speaks as if he's storing potatoes in his cheeks for the oncoming Romanian winter. He's not any good, but, then again, [[Harrison Ford]] would be hard pressed to pretend that he's 7-foot-7. \"My Giant\" would probably play better to children. There's an itty-bitty bit of swearing. Beware of sugar comas.\"<ref>{{cite news|url=http://edition.cnn.com/SHOWBIZ/9804/16/review.my.giant/ |title=CNN - Review: 'My Giant' not good at tall - April 16, 1998 |publisher= CNN |date=16 April 1998 |accessdate=15 September 2013}}</ref>{{Dead link|date=October 2019}}\n\nThe review of the film's original [[VHS]] release from [[Entertainment Weekly]] was one of its few genuine praises, from critic David Everitt, describing the tape as \"a watchable rental. Crystal's wisenheimer Long Island charm wears well, and Muresan, the 7-foot-7 Washington Wizards center, is surprisingly endearing, especially when you can understand what he's saying. Watch for Steven Seagal's amusing cameo.\"<ref>{{cite web |date= 24 September 1998 |author= David Everitt |url= http://www.ew.com/ew/article/0,,63723,00.html |title= My Giant |website= [[Entertainment Weekly]] |accessdate= 10 October 2019 }}</ref>\n\nLisa Alspector of the ''[[Chicago Reader]]'' reviewed the film positively and stated \"''My Giant'' is exciting partly because it dares to get so close to [its] idea, even though it then pulls back.\"<ref>{{cite web |date= 9 April 1998 |last= Alspector |first=  Lisa |url=http://www.chicagoreader.com/chicago/too-big-to-ignore/Content?oid=896013 |title=Too Big to Ignore &#124; Movie Review |publisher= [[Chicago Reader]] |accessdate= 10 October 2019  }}</ref>\n\nOn [[Rotten Tomatoes]] the film has an approval rating of 19% based on reviews from 27 critics.<ref>{{Cite web |title= My Giant |url= https://www.rottentomatoes.com/m/my_giant/ | website= [[Rotten Tomatoes]]|accessdate=4 March 2018}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{IMDb title|0120765|My Giant}}\n* {{rotten-tomatoes|my_giant|My Giant}}\n* {{Mojo title|mygiant|My Giant}}\n\n{{Michael Lehmann}}\n{{Andr\u00e9 the Giant}}\n\n[[Category:1990s comedy films]]\n[[Category:1998 films]]\n[[Category:American comedy films]]\n[[Category:American films]]\n[[Category:Castle Rock Entertainment films]]\n[[Category:Columbia Pictures films]]\n[[Category:English-language films]]\n[[Category:Films directed by Michael Lehmann]]\n[[Category:Films scored by Marc Shaiman]]\n[[Category:Films shot in the Czech Republic]]\n[[Category:Romanian-language films]]\n[[Category:Films with screenplays by Billy Crystal]]\n", "text_old": "{{multiple issues|\n{{lead too short|date=March 2015}}\n{{more citations needed|date=May 2015}}\n}}\n{{Use dmy dates|date=January 2020}}\n{{Infobox film\n| name           = My Giant\n| image          = My giant poster.jpg\n| image_size     = 225px\n| alt            = \n| caption        = theatrical release poster\n| director       = [[Michael Lehmann]]\n| producer       = [[Billy Crystal]]\n| writer         = [[David Seltzer]]\n| story          = {{Plainlist|\n* Billy Crystal\n* David Seltzer\n}}\n| starring       = {{Plainlist|\n* Billy Crystal\n* [[Kathleen Quinlan]]\n* [[Joanna Pacu\u0142a]]\n* [[Gheorghe Mure\u0219an]]\n}}\n| music          = [[Marc Shaiman]]\n| cinematography = [[Michael Coulter]]\n| editing        = [[Stephen Semel]]\n| studio         = [[Castle Rock Entertainment]]\n| distributor    = [[Columbia Pictures]]\n| released       = 10 April 1998 ''(US)''<br>12 February 1999 ''(UK)''\n| country        = United States\n| runtime        = 103 minutes\n| gross          = $8,072,007\n}}\n\n'''''My Giant''''' is a 1998 American [[comedy film]] directed by [[Michael Lehmann]]. The film stars [[Billy Crystal]], who also produced and co-wrote the story for the film, and Romanian [[National Basketball Association|NBA]] player [[Gheorghe Mure\u0219an]] in his only film appearance.<ref name=\"My Giant\">{{cite web|url=http://www.tcm.com/tcmdb/title/327296/My-Giant/full-credits.html|accessdate=19 April 2016|work=[[Turner Classic Movies]]|title=My Giant}}</ref> David Seltzer's script was inspired by Crystal's friendship with professional wrestler [[Andr\u00e9 the Giant]], whom he had met during the filming of ''[[The Princess Bride (film)|The Princess Bride]]''.\n\n==Plot==\nTalent agent and [[huckster]] Sammy Kamin travels to Romania on business after splitting up with his wife. After his young client fires him, Sammy crashes his car and is rescued, while unconscious, by an enormous Romanian man named Max who is close to 8 feet tall.\n\nSammy thinks the rescuer is [[God]], as he can only see Max's giant hands. When Sammy wakes up, he thinks he is in [[Heaven]]. But he is confused to find a statue of [[Jesus Christ|Jesus]] next to his bed, as he was raised Jewish. He then realizes Max has brought him to a monastery, where he was raised after being given up for [[adoption]] by his parents because of his height.\n\nOnce he wakes up and interacts with Max, he sees potential stardom in him. Sammy attempts to broker his introduction into the movies. In doing so, he exploits Max' desire to visit a long-lost paramour, Lilliana, in [[Gallup, New Mexico]]. First, Max obtains the role of a villain in a movie, but he is so drunk that he vomits on the protagonist (Sammy's former client). However, the scene is included in the movie.\n\nOne day, Sammy talks to [[Steven Seagal]] about including Max as a villain in one of his movies, convincing him that he needs a different kind of villain. At first Seagal rejects him because there was another actor who would take that role, but he changes his opinion after listening to an extract of a [[Shakespeare]]an play done by Max.\n\nSuddenly, after some medical exams, Max is diagnosed with [[Cardiovascular disease|heart disease]] which cannot be treated with a transplant because his heart is so big. Sammy decides to find Lilliana, and tries to convince her to meet Max again, but she rejects the invitation. Sammy then convinces his wife to take the role of Lilliana and after some words, Max asks her for a kiss.\n\nAfterwards, Sammy and his wife decide to get back together again, after realizing that they were truly in love and that their son needed them.\n\nSammy eventually decides to return Max home to Romania. Max refuses to go back, but finally he enters his old house, and meets his parents again. Sammy ends up watching Max's first filmed scene in a cinema with his family. Max dies shortly after, because of his heart, but he changed many people's lives forever.\n\n==Cast==\n* [[Billy Crystal]] as Sammy Kamin\n* [[Gheorghe Mure\u0219an]] as Maximus \"Max\" Sanferescu\n* [[Kathleen Quinlan]] as Serena Kamin\n* [[Joanna Pacu\u0142a]] as Lillianna Otarro\n* Zane Carney as Nick Kamin\n* [[Dan Castellaneta]] as Partlow\n* [[Steven Seagal]] as himself\n* [[Doris Roberts]] as Rose Kaminski\n* [[Jere Burns]] as Weller, the Movie Director \n* [[Raymond O'Connor]] as Eddie \n* [[Rider Strong]] as Justin Allen\n\n==Reception==\n===Box office===\nThe film was not a [[box office]] success, grossing a little over $8 million domestically, far less than its $20 million budget.<ref>{{cite news|url= http://articles.latimes.com/1998/apr/13/entertainment/ca-38767 |title='City of Angels' Takes Wing in Heavenly Opening Weekend|newspaper=[[Los Angeles Times]] |date=13 April 1998 |accessdate=15 September 2013}}</ref><ref>{{cite news|url=http://articles.latimes.com/1998/apr/14/entertainment/ca-38964 |title=Top of the World for Warner Bros|newspaper=Los Angeles Times |date=14 April 1998 |accessdate=15 September 2013 |first=Robert W. |last=Welkos}}</ref>\n\n===Critical===\nThe film gained mostly negative reviews. [[Roger Ebert]] stated: \"The movie, which could have been a funny send-up of Hollywood talent requirements, gets distracted by subplots [that] after its promising start, ''My Giant'' isn't a comedy about an agent and a giant, so much as the heartwarming tale of a guy who learns to be a better family man.\"<ref>{{cite web |author= Roger Ebert | author-link= Roger Ebert |url=http://www.rogerebert.com/reviews/my-giant-1998 |title=My Giant - Review |publisher= [[Chicago Sun-Times]] |accessdate= 16 September 2013}}</ref>\n\n[[CNN]]'s Paul Tatara stated: \"Crystal is Crystal throughout, and I still like him for it. Muresan, on the other hand, is sweet but, shall we say, a limited performer. He also speaks as if he's storing potatoes in his cheeks for the oncoming Romanian winter. He's not any good, but, then again, [[Harrison Ford]] would be hard pressed to pretend that he's 7-foot-7. \"My Giant\" would probably play better to children. There's an itty-bitty bit of swearing. Beware of sugar comas.\"<ref>{{cite news|url=http://edition.cnn.com/SHOWBIZ/9804/16/review.my.giant/ |title=CNN - Review: 'My Giant' not good at tall - April 16, 1998 |publisher= CNN |date=16 April 1998 |accessdate=15 September 2013}}</ref>{{Dead link|date=October 2019}}\n\nThe review of the film's original [[VHS]] release from [[Entertainment Weekly]] was one of its few genuine praises, from critic David Everitt, describing the tape as \"a watchable rental. Crystal's wisenheimer Long Island charm wears well, and Muresan, the 7-foot-7 Washington Wizards center, is surprisingly endearing, especially when you can understand what he's saying. Watch for Steven Seagal's amusing cameo.\"<ref>{{cite web |date= 24 September 1998 |author= David Everitt |url= http://www.ew.com/ew/article/0,,63723,00.html |title= My Giant |website= [[Entertainment Weekly]] |accessdate= 10 October 2019 }}</ref>\n\nLisa Alspector of the ''[[Chicago Reader]]'' reviewed the film positively and stated \"''My Giant'' is exciting partly because it dares to get so close to [its] idea, even though it then pulls back.\"<ref>{{cite web |date= 9 April 1998 |last= Alspector |first=  Lisa |url=http://www.chicagoreader.com/chicago/too-big-to-ignore/Content?oid=896013 |title=Too Big to Ignore &#124; Movie Review |publisher= [[Chicago Reader]] |accessdate= 10 October 2019  }}</ref>\n\nOn [[Rotten Tomatoes]] the film has an approval rating of 19% based on reviews from 27 critics.<ref>{{Cite web |title= My Giant |url= https://www.rottentomatoes.com/m/my_giant/ | website= [[Rotten Tomatoes]]|accessdate=4 March 2018}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{IMDb title|0120765|My Giant}}\n* {{rotten-tomatoes|my_giant|My Giant}}\n* {{Mojo title|mygiant|My Giant}}\n\n{{Michael Lehmann}}\n{{Andr\u00e9 the Giant}}\n\n[[Category:1990s comedy films]]\n[[Category:1998 films]]\n[[Category:American comedy films]]\n[[Category:American films]]\n[[Category:Castle Rock Entertainment films]]\n[[Category:Columbia Pictures films]]\n[[Category:English-language films]]\n[[Category:Films directed by Michael Lehmann]]\n[[Category:Films scored by Marc Shaiman]]\n[[Category:Films shot in the Czech Republic]]\n[[Category:Romanian-language films]]\n[[Category:Films with screenplays by Billy Crystal]]\n", "name_user": "Moe1810", "label": "safe", "comment": "\u2192\u200eCast", "url_page": "//en.wikipedia.org/wiki/My_Giant"}
{"title_page": "All God's Children (2012 film)", "text_new": "{{short description|2012 film}}\n{{Use dmy dates|date=April 2020}}\n{{Infobox film\n| name           = All God's Children\n| image          = All God's Children 2012 poster.jpg\n| caption        = Film poster\n| director       = [[Adrian Popovici]]\n| producer       = \n| writer         = Adrian Popovici\n| starring       = [[Paolo Seganti]]\n| music          = \n| cinematography = Veaceslav Cebotari\n| editing        = \n| distributor    = \n| released       = {{Film date|df=yes|2012|10|19|Romania}}\n| runtime        = 90 minutes\n| country        = Moldova\n| language       = Romanian<br>Italian\n| budget         = \n}}\n\n'''''All God's Children''''' ({{lang-ro|'''Toti copiii domnului'''}}) is a 2012 Moldovan [[drama film]] directed by [[Adrian Popovici]]. The film was selected as the Moldovan entry for the [[Academy Award for Best Foreign Language Film|Best Foreign Language Film]] at the [[86th Academy Awards]],<ref name=\"Moldova\">{{cite web |url=https://variety.com/2013/film/news/oscar-has-some-surprises-in-final-foreign-language-list-1200692363/ |title=Oscar's Final Foreign-Language List Includes a Few Surprises |accessdate=8 October 2013 |work=Variety}}</ref> but was not nominated. It was the first time the country submitted a film in this category.<ref name=\"76Submissions\">{{cite web |url=http://www.oscars.org/press/pressreleases/2013/20131007a.html |title=76 Countries In Competition For 2013 Foreign Language Film Oscar |accessdate=7 October 2013 |work=The Academy of Motion Picture Arts and Sciences}}</ref>\n\nThe film narrates the story of Irina and Peter, a couple who lost their young child in an accident. The couple moves to Moldova and adopts Pavalas, a child that reminds them of their late son. In order to adopt the child, Irina and her friend travel to Italy where they get caught in a network of prostitution and crime that will test her resolve and love for the child.<ref>{{cite web|title=All God's Children|url=http://www.cinemagia.ro/filme/all-gods-children-578406/|website=Cinemagia|accessdate=28 August 2014}}</ref>\n\n==Cast==\n* [[Jhoni Alici]] as Bartender  \n* [[Lilia Bejan]] as The Maid  \n* [[Ion Beregoi]] as Gicu  \n* [[Vas Blackwood]] as Mark  \n* [[Emergian Cazac]] as Pavalas  \n* {{ill|Anatol Durbala|ro|Anatol Durbal\u0103}} as Feghea  \n* [[Arcel Ioseliani]] as Soldier #1  \n* [[Michael Ironside]] as Peter  \n* [[Alexei Machevnin]] as Soldier #2  \n* [[Rodica Oanta]] as Tatiana  \n* [[Leo Rudenco]] as Moustache Man  \n* [[Paolo Seganti]] as Bruno  \n* [[Mihaela Strambeanu]] as School Director  \n* [[Ina Surdu]] as Irina  \n* [[Alina Turcanu]] as Alina\n\n==See also==\n* [[List of submissions to the 86th Academy Awards for Best Foreign Language Film]]\n* [[List of Moldovan submissions for the Academy Award for Best Foreign Language Film]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{IMDb title|2507154|All God's Children}}\n\n{{DEFAULTSORT:All God's Children}}\n[[Category:2012 films]]\n[[Category:2012 drama films]]\n[[Category:Moldovan films]]\n[[Category:Italian-language films]]\n[[Category:Romanian-language films]]\n\n\n{{Moldova-stub}}\n{{2010s-drama-film-stub}}\n", "text_old": "{{short description|2012 film}}\n{{Use dmy dates|date=June 2015}}\n{{Infobox film\n| name           = All God's Children\n| image          = All God's Children 2012 poster.jpg\n| caption        = Film poster\n| director       = [[Adrian Popovici]]\n| producer       = \n| writer         = Adrian Popovici\n| starring       = [[Paolo Seganti]]\n| music          = \n| cinematography = Veaceslav Cebotari\n| editing        = \n| distributor    = \n| released       = {{Film date|df=yes|2012|10|19|Romania}}\n| runtime        = 90 minutes\n| country        = Moldova\n| language       = Romanian<br>Italian\n| budget         = \n}}\n\n'''''All God's Children''''' ({{lang-ro|'''Toti copiii domnului'''}}) is a 2012 Moldovan [[drama film]] directed by [[Adrian Popovici]]. The film was selected as the Moldovan entry for the [[Academy Award for Best Foreign Language Film|Best Foreign Language Film]] at the [[86th Academy Awards]],<ref name=\"Moldova\">{{cite web |url=https://variety.com/2013/film/news/oscar-has-some-surprises-in-final-foreign-language-list-1200692363/ |title=Oscar's Final Foreign-Language List Includes a Few Surprises |accessdate=8 October 2013 |work=Variety}}</ref> but was not nominated. It was the first time the country submitted a film in this category.<ref name=\"76Submissions\">{{cite web |url=http://www.oscars.org/press/pressreleases/2013/20131007a.html |title=76 Countries In Competition For 2013 Foreign Language Film Oscar |accessdate=7 October 2013 |work=The Academy of Motion Picture Arts and Sciences}}</ref>\n\nThe film narrates the story of Irina and Peter, a couple who lost their young child in an accident. The couple moves to Moldova and adopts Pavalas, a child that reminds them of their late son. In order to adopt the child, Irina and her friend travel to Italy where they get caught in a network of prostitution and crime that will test her resolve and love for the child.<ref>{{cite web|title=All God's Children|url=http://www.cinemagia.ro/filme/all-gods-children-578406/|website=Cinemagia|accessdate=28 August 2014}}</ref>\n\n==Cast==\n* [[Jhoni Alici]] as Bartender  \n* [[Lilia Bejan]] as The Maid  \n* [[Ion Beregoi]] as Gicu  \n* [[Vas Blackwood]] as Mark  \n* [[Emergian Cazac]] as Pavalas  \n* {{ill|Anatol Durbala|ro|Anatol Durbal\u0103}} as Feghea  \n* [[Arcel Ioseliani]] as Soldier #1  \n* [[Michael Ironside]] as Peter  \n* [[Alexei Machevnin]] as Soldier #2  \n* [[Rodica Oanta]] as Tatiana  \n* [[Leo Rudenco]] as Moustache Man  \n* [[Paolo Seganti]] as Bruno  \n* [[Mihaela Strambeanu]] as School Director  \n* [[Ina Surdu]] as Irina  \n* [[Alina Turcanu]] as Alina\n\n==See also==\n* [[List of submissions to the 86th Academy Awards for Best Foreign Language Film]]\n* [[List of Moldovan submissions for the Academy Award for Best Foreign Language Film]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{IMDb title|2507154|All God's Children}}\n\n{{DEFAULTSORT:All God's Children}}\n[[Category:2012 films]]\n[[Category:2012 drama films]]\n[[Category:Moldovan films]]\n[[Category:Italian-language films]]\n[[Category:Romanian-language films]]\n\n\n{{Moldova-stub}}\n{{2010s-drama-film-stub}}\n", "name_user": "Lugnuts", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/All_God%27s_Children_(2012_film)"}
