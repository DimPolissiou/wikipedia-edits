{"title_page": "Avtaar", "text_new": "{{short description|1983 film by Mohan Kumar}}\n{{Use dmy dates|date=December 2018}}\n{{Use Indian English|date=December 2018}}\n{{Infobox film\n| name = Avtaar\n| image = Avtaarfilm.jpg\n| caption = \n| director = [[Mohan Kumar]]\n| producer = [[Mohan Kumar]]\n| writer = \n| starring = [[Rajesh Khanna]]<br>[[Shabana Azmi]]<br>[[Sujit Kumar]]<br>[[AK Hangal]]<br>[[Gulshan Grover]]<br>[[Shashi Puri]]<br>[[Priti Sapru]]<br>[[Sachin (actor)|Sachin]]<br>[[Yunus Parvez]]\n| music = [[Laxmikant Pyarelal]]\n| lyrics = \n| cinematography = [[K. K. Mahajan]]\n| editing =\n| released = {{film date|df=y|1983|3|11}}\n| studio = \n| runtime = \n| country = [[India]]\n| language =\n}}\n\n'''''Avtaar''''' is a [[Bollywood films of 1983|1983]] drama film starring [[Rajesh Khanna]] and [[Shabana Azmi]]. It was directed by [[Mohan Kumar]], and the music was by  [[Laxmikant Pyarelal]]. ''Avtaar'' was a commercial hit, and was critically acclaimed.<ref>{{cite web|url=http://boxofficeindia.com/cpages.php?pageName=top_actors|title=BoxOffice India.com|publisher=BoxOffice India.com|archiveurl=https://web.archive.org/web/20131029191201/http://boxofficeindia.com/cpages.php?pageName=top_actors|archivedate=2013-10-29|url-status=dead|accessdate=2010-07-12}}</ref>  The film earned several [[Filmfare]] award nominations.<ref>[http://deep750.googlepages.com/FilmfareAwards.pdf 1st Filmfare Awards 1953<!-- Bot generated title -->]</ref> Rajesh Khanna achieved success with ''[[Amar Deep (1979 film)|Amardeep]]'' and ''[[Prem Bandhan]]'' onwards, but this was [[Rajesh Khanna]]'s biggest film in terms of box office collections since 1973. However, Rajesh Khanna lost the Best Actor award to [[Naseeruddin Shah]] for ''[[Masoom (1983 film)|Masoom]]''. In 1986, [[Mohan Kumar]] made ''[[Amrit (film)|Amrit]]'' with Rajesh Khanna in the lead as an old man, but with a different story line. Rajesh Khanna received an All-India Critics Association (AICA) Best Actor Award for his performance in this film in 1983.<ref name=\"hindustantimes\">{{cite web|url=http://www.hindustantimes.com/bollywood/happy-birthday-rajesh-khanna-his-top-15-films/story-xZ8WYc2v7LYVftj6D7lWHP.html|publisher=hindustantimes.com|title=Happy birthday Rajesh Khanna: His top 15 films &#124; bollywood &#124; Hindustan Times|accessdate=2017-02-24}}</ref>\n\nThe film was later remade into the [[Telugu language|Telugu]] film ''O Thandri Theerpu'' (1985), starring [[Murali Mohan]] and [[Jayasudha]]; in Kannada as ''Kaliyuga'', starring Rajesh and Aarthi; in Malayalam as ''[[Jeevitham (1984 film)|Jeevitham]]'', starring [[Madhu (actor)|Madhu]]; and in Tamil as ''[[Vaazhkai (1984 film)|Vaazhkai]]'', with [[Sivaji Ganesan]]. ''[[Baghban (2003 film)|Baghban]]'' in 2003, starring [[Amitabh Bachchan]] and [[Hema Malini]] was partially inspired by this film.\n\n==Plot==\n\nThe film starts with wife Radha Kishen ([[Shabana Azmi]]) garlanding her husband Avtaar Kishen's ([[Rajesh Khanna]]) bust while she mourns his death, then flashes back thirty years. Radha, is the only daughter of Seth Jugal Kishore ([[Madan Puri]]), and is in love with Avtaar, a poor boy. Her father disapproves, so the two elope and marry.\n\nAfter various hardships, the couple ultimately succeed; after three decades, Avtaar is owner of a small house and fortune. they have two sons, Chander ([[Gulshan Grover]]) and Ramesh ([[Shashi Puri]]). Chander is married to Renu (Rajni Sharma), while Ramesh is married to Sudha ([[Priti Sapru]]). Avtaar also has a servant named Sewak ([[Sachin (actor)|Sachin]]).\n\nChander marries the only daughter of Seth Laxmi Narayan ([[Pinchoo Kapoor]]) and becomes a [[Gharjamai]]. Meanwhile Ramesh registers the house under his wife's  name instead of Radha's. This enrages Avtaar and he leaves his home, accompanied by Radha and Sewak. With help of a moneylender Bawaji ([[Sujit Kumar]]), Avtaar starts his own garage.\n\nAvtaar faces an uphill task, since he has no money to buy equipment, is aged and his right hand was paralysed in an accident. Sewak helps his master by illegally donating blood to arrange funds, but when Avtaar assumes Sewak has resorted to robbery, Bawaji tells the truth. Moved, Radha and Avtaar come to regard him as a true son.\n\nThere's also story of Avtaar's friend Rashid Ahemed the best friend of avtaar also suffering from his cruel son Anwar ahemed and his cruel daughter-in-law - Zubeida anwar  his son was in Dubai and he always show's during the panwaala dukan - name - Ram dulare (Shivraj) where he shows that is daughter-in-law is very good but in reality very worst he works like a servant and suddenly becomes sick and his son expels him from the house and Avtaar sees that and opens a center on people who was suffering from their cruel relatives he named him \"Apna Ghar\".\n\nMeanwhile, both Ramesh and Chander are enjoying their lives. Avtaar's luck changes and the carburetor he is working on gives a successful result. Avtaar starts manufacturing the engine parts, and creates an industrial empire headed by himself, his wife and Sewak. \n\nAvtaar's success takes a toll on Laxmi Narayan's business and he holds Chander responsible. Ramesh commits bank fraud and is arrested. Shobha comes to Avtaar for help, but he rebukes her and sends her away. Radha becomes angry, but remains silent. Avtaar secretly gives Bawaji the bail money on the condition that he tells no one; Bawaji bails Ramesh.\n\nMeanwhile, holding Chandar responsible for the loss in business, Laxmi Narayan throws him out of the house. Ramesh, Chandar and Sudha go to Radha for assistance, but Avtaar disagrees. Next day, Avtaar goes to the office and does not return. Radha calls him and late at night, tries to convince him to help their children, but he refuses to listen. Emotionally, Radha accuses him of having become heartless. Bawaji meets Radha, who tells him the whole story. Bawaji confesses the truth to her as he cannot bear Avtaar being ill thought of. Meanwhile  Zubeida learns that Rashid is having lot of money by working in Apna Garh they come with their son - Saddu Ahmed Rashid's grandson. Zubeida and Anwar's show drama in front of Rashid so they  can get money from him but rashid ask them to leave.\n\nAfter On learning the truth, Radha realizes her mistake and tries to call Avtaar. Sewak informs her that Avtaar has had a heart attack, so the family goes to the hospital. Avtaar, have already written his will, he hands it to Radha and dies. He gave 2 lakhs to both of his sons only for realizing their mistake and gave the whole property to his servant Sewak. The story comes to the present, where Radha decorates Avtaar's bust because he developed the Apna Ghar center, and the film ends.\n\n==Cast==\n*[[Rajesh Khanna]] as Avtaar Kishen\n*[[Shabana Azmi]] as Radha kishen; Avatar's wife\n*[[A. K. Hangal]] as Rashid Ahmed; Avtaar's best friend\n*[[Madan Puri]] as Seth Jugal Kishore Radha's father\n*[[Gulshan Grover]] as Chander Kishen; Avtaar's cruel younger son; Renu's husband\n*[[Shashi Puri]] as Ramesh Kishen; Avtaar's cruel elder son; Sudha's husband\n*[[Sachin (actor)|Sachin]] as Sewak; Avtaar's good servant and like a son\n*[[Yunus Parvez]] as Ram Dulare; A paanwala and Avtaar's friend\n*[[Pinchoo Kapoor]] as Seth Laxmi Narayan; Renu's father\n*[[Sujit Kumar]] as Bawaji; Avtaar's master\n*[[Priti Sapru]] as Sudha Kishen; Ramesh's cruel wife\n*Rajni Sharma as Renu Chandar Kishen; Chandar's rich and cruel wife\n*Ranjan Grewal as Anwar R. Ahmed; Rashid's cruel son; Zubedais husband and Saddu's father\n*[[Madhu Malini]] as Zubeida Anwar Ahmed; Anwar's cruel wife and Saddu's mother\n*Unknow as Saddu; Zubeida and Anwar's son; Rashid's grandson.\n*Shivraj as Gopal; Avtaar's servant.\n\n==Soundtrack==\n{| border=\"10\" cellpadding=\"10\" cellspacing=\"0\" style=\"margin: 1em 1em 1em 0; background: #f9f9f9; border: 1px #aaa solid; border-collapse: collapse; font-size: 95%;\"\n|- bgcolor=\"#CCCCCC\" align=\"center\"\n! # !! Title !! Singer(s)\n|-\n| 1\n| \"Din Maheene Saal\"\n| [[Kishore Kumar]], [[Lata Mangeshkar]]\n|-\n| 2\n| \"Oopar Wale Tera Jawab Nahin\"\n| Kishore Kumar\n|-\n| 3\n| \"Chalo Bulawa Aaya Hai\"\n| [[Mahendra Kapoor]], [[Asha Bhosle]], [[Narendra Chanchal]]\n|-\n| 4\n| \"Yaaro Utho Chalo\"\n| Kishore Kumar, Mahendra Kapoor\n|-\n| 5\n| \"Zindagi Mauj Udane\"\n| Mahendra Kapoor, [[Suresh Wadkar]], [[Alka Yagnik]]\n|}\n\n==Filmfare Nominations==\n*Best Picture\n*Best Director-[[Mohan Kumar]]\n*Best Actor-[[Rajesh Khanna]]\n*Best Actress-[[Shabana Azmi]]\n*Best Story-[[Mohan Kumar]]\n*Best Dialogue Writer - Mushtaq Jalili\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title|0233264}}\n\n[[Category:1983 films]]\n[[Category:1980s Hindi-language films]]\n[[Category:Films scored by Laxmikant\u2013Pyarelal]]\n[[Category:Hindi films remade in other languages]]\n", "text_old": "{{short description|1983 film by Mohan Kumar}}\n{{Use dmy dates|date=December 2018}}\n{{Use Indian English|date=December 2018}}\n{{Infobox film\n| name = Avtaar\n| image = Avtaarfilm.jpg\n| caption = \n| director = [[Mohan Kumar]]\n| producer = [[Mohan Kumar]]\n| writer = \n| starring = [[Rajesh Khanna]]<br>[[Shabana Azmi]]<br>[[Sujit Kumar]]<br>[[AK Hangal]]<br>[[Gulshan Grover]]<br>[[Shashi Puri]]<br>[[Priti Sapru]]<br>[[Sachin (actor)|Sachin]]<br>[[Yunus Parvez]]\n| music = [[Laxmikant Pyarelal]]\n| lyrics = \n| cinematography = [[K. K. Mahajan]]\n| editing =\n| released = {{film date|df=y|1983|3|11}}\n| studio = \n| runtime = \n| country = [[India]]\n| language =\n}}\n\n'''''Avtaar''''' is a [[Bollywood films of 1983|1983]] drama film starring [[Rajesh Khanna]] and [[Shabana Azmi]]. It was directed by [[Mohan Kumar]], and the music was by  [[Laxmikant Pyarelal]]. ''Avtaar'' was a commercial hit, and was critically acclaimed.<ref>{{cite web|url=http://boxofficeindia.com/cpages.php?pageName=top_actors|title=BoxOffice India.com|publisher=BoxOffice India.com|archiveurl=https://web.archive.org/web/20131029191201/http://boxofficeindia.com/cpages.php?pageName=top_actors|archivedate=2013-10-29|url-status=dead|accessdate=2010-07-12}}</ref>  The film earned several [[Filmfare]] award nominations.<ref>[http://deep750.googlepages.com/FilmfareAwards.pdf 1st Filmfare Awards 1953<!-- Bot generated title -->]</ref> Rajesh Khanna achieved success with ''[[Amar Deep (1979 film)|Amardeep]]'' and ''[[Prem Bandhan]]'' onwards, but this was [[Rajesh Khanna]]'s biggest film in terms of box office collections since 1973. However, Rajesh Khanna lost the Best Actor award to [[Naseeruddin Shah]] for ''[[Masoom (1983 film)|Masoom]]''. In 1986, [[Mohan Kumar]] made ''[[Amrit (film)|Amrit]]'' with Rajesh Khanna in the lead as an old man, but with a different story line. Rajesh Khanna received an All-India Critics Association (AICA) Best Actor Award for his performance in this film in 1983.<ref name=\"hindustantimes\">{{cite web|url=http://www.hindustantimes.com/bollywood/happy-birthday-rajesh-khanna-his-top-15-films/story-xZ8WYc2v7LYVftj6D7lWHP.html|publisher=hindustantimes.com|title=Happy birthday Rajesh Khanna: His top 15 films &#124; bollywood &#124; Hindustan Times|accessdate=2017-02-24}}</ref>\n\nThe film was later remade into the [[Telugu language|Telugu]] film ''O Thandri Theerpu'' (1985), starring [[Murali Mohan]] and [[Jayasudha]]; in Kannada as ''Kaliyuga'', starring Rajesh and Aarthi; in Malayalam as ''[[Jeevitham (1984 film)|Jeevitham]]'', starring [[Madhu (actor)|Madhu]]; and in Tamil as ''[[Vaazhkai (1984 film)|Vaazhkai]]'', with [[Sivaji Ganesan]]. ''[[Baghban (2003 film)|Baghban]]'' in 2003, starring [[Amitabh Bachchan]] and [[Hema Malini]] was partially inspired by this film.\n\n==Plot==\n\nThe film starts with wife Radha Kishen ([[Shabana Azmi]]) garlanding her husband Avtaar Kishen's ([[Rajesh Khanna]]) bust while she mourns his death, then flashes back thirty years.  Radha, is the only daughter of Seth Jugal Kishore ([[Madan Puri]]), and is in love with Avtaar, a poor boy. Her father disapproves so the two elope and get married.\n\nAfter various hardships, the couple ultimately succeed; after three decades, Avtaar is owner of a small house and fortune. they have two sons, Chander ([[Gulshan Grover]]) and Ramesh ([[Shashi Puri]]). Chander is married to Renu (Rajni Sharma), while Ramesh is married to Sudha ([[Priti Sapru]]). Avtaar also has a servant named Sewak ([[Sachin (actor)|Sachin]]).\n\nChander gets married to the only daughter of Seth Laxmi Narayan ([[Pinchoo Kapoor]]) and becomes a [[Gharjamai]]. Meanwhile Ramesh registers the house under his wife's  name instead of Radha's. This enrages Avtaar and he leaves his home, accompanied by Radha and Sewak. With help of a moneylender Bawaji ([[Sujit Kumar]]), Avtaar starts his own garage.\n\nAvtaar faces an uphill task, since he has no money to buy equipment, is aged and his right hand was paralysed in an accident. Sewak helps his master by illegally donating blood to arrange funds, but when Avtaar assumes Sewak has resorted to robbery, Bawaji tells the truth. Moved, Radha and Avtaar come to regard him as a true son.\n\nThere's also story of Avtaar's friend Rashid Ahemed the best friend of avtaar also suffering from his cruel son Anwar ahemed and his cruel daughter in law - Zubeida anwar  his son was in Dubai and he always show's during the panwaala dukan - name - Ram dulare ( Shivraj ) where he shows that is daughter in law is very good but in reality very worst he works like a servant and suddenly becomes sick and his son throw him out of the house and avtaar's sees that and open a center on people who was suffering from there cruel relatives he named him - \" APNA GHAR \" .\n\nMeanwhile, both Ramesh and Chander are enjoying their lives. Avtaar's luck changes and the carburetor he is working on gives a successful result. Avtaar starts manufacturing the engine parts, and creates an industrial empire headed by himself, his wife and Sewak. \n\nAvtaar's success takes a toll on Laxmi Narayan's business and he holds Chander responsible. Ramesh commits bank fraud and is arrested. Shobha comes to Avtaar for help, but he rebukes her and sends her away. Radha becomes angry, but remains silent. Avtaar secretly gives Bawaji the bail money on the condition that he tells no one; Bawaji bails Ramesh.\n\nMeanwhile, holding Chandar responsible for the loss in business, Laxmi Narayan throws him out of the house. Ramesh, Chandar and Sudha go to Radha for assistance, but Avtaar disagrees. Next day, Avtaar goes to the office and does not return. Radha calls him and late at night, tries to convince him to help their children, but he refuses to listen. Emotionally, Radha accuses him of having become heartless. Bawaji meets Radha, who tells him the whole story. Bawaji confesses the truth to her as he cannot bear Avtaar being ill thought of. Meanwhile  Zubeida learns that Rashid is having lot of money by working in apna garh they come with their son - Saddu ahmed rashid's grandson. Zubeida and anwar's show drama in front of Rashid so they  can get money from him but rashid ask them to leave .\n\nAfter On learning the truth, Radha realizes her mistake and tries to call Avtaar. Sewak informs her that Avtaar has had a heart attack, so the family goes to the hospital. Avtaar, have already written his will, he hands it to Radha and dies. He gave 2 lacs to both of his sons only for realizing their mistake and gave the whole property to his servant sewak . The story comes to the present, where Radha decorates Avtaar's bust because he developed apna ghar center , and the film ends.\n\n==Cast==\n*[[Rajesh Khanna]] as Avtaar Kishen\n*[[Shabana Azmi]] as Radha kishen ; Avatar's wife.\n*[[A. K. Hangal]] as Rashid Ahmed ; Avtaar's best friend\n*[[Madan Puri]] as Seth Jugal Kishore Radha's Father\n*[[Gulshan Grover]] as Chander Kishen ; Avtaar's cruel younger son ; Renu's husband.\n*[[Shashi Puri]] as Ramesh Kishen ; Avtaar's cruel elder son ; Sudha's husband.\n*\n*[[Sachin (actor)|Sachin]] as Sewak ; Avtaar's good servant and like a son.\n*[[Yunus Parvez]] as Ram dulare ; A paanwala and Avtaar's friend.\n*[[Pinchoo Kapoor]] as Seth Laxmi Narayan ; renu's father.\n*[[Sujit Kumar]] as Bawaji ; Avtaar's Master.\n*[[Priti Sapru]] as Sudha Kishen ; Ramesh's cruel wife.\n*Rajni Sharma as Renu Chandar kishen ; Chandar's rich and cruel wife.\n*Ranjan Grewal as Anwar R. Ahmed ; Rashid's cruel son. ; Zubedais husband and saddu's father.\n*[[Madhu Malini]] as Zubeida Anwar Ahmed ; Anwar's cruel wife and saddu mother.\n*Unknow as saddu ; zubeida and anwar son ; rashid's grandson.\n*Shivraj as Gopal ; Avtaar's servant.\n\n==Soundtrack==\n{| border=\"10\" cellpadding=\"10\" cellspacing=\"0\" style=\"margin: 1em 1em 1em 0; background: #f9f9f9; border: 1px #aaa solid; border-collapse: collapse; font-size: 95%;\"\n|- bgcolor=\"#CCCCCC\" align=\"center\"\n! # !! Title !! Singer(s)\n|-\n| 1\n| \"Din Maheene Saal\"\n| [[Kishore Kumar]], [[Lata Mangeshkar]]\n|-\n| 2\n| \"Oopar Wale Tera Jawab Nahin\"\n| Kishore Kumar\n|-\n| 3\n| \"Chalo Bulawa Aaya Hai\"\n| [[Mahendra Kapoor]], [[Asha Bhosle]], [[Narendra Chanchal]]\n|-\n| 4\n| \"Yaaro Utho Chalo\"\n| Kishore Kumar, Mahendra Kapoor\n|-\n| 5\n| \"Zindagi Mauj Udane\"\n| Mahendra Kapoor, [[Suresh Wadkar]], [[Alka Yagnik]]\n|}\n\n==Filmfare Nominations==\n*Best Picture\n*Best Director-[[Mohan Kumar]]\n*Best Actor-[[Rajesh Khanna]]\n*Best Actress-[[Shabana Azmi]]\n*Best Story-[[Mohan Kumar]]\n*Best Dialogue Writer - Mushtaq Jalili\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{IMDb title|0233264}}\n\n[[Category:1983 films]]\n[[Category:1980s Hindi-language films]]\n[[Category:Films scored by Laxmikant\u2013Pyarelal]]\n[[Category:Hindi films remade in other languages]]\n", "name_user": "Jellysandwich0", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Avtaar"}
{"title_page": "Nate Dogg", "text_new": "{{Short description|American rapper and actor from California}}\n{{Use mdy dates|date=March 2015}}\n{{Infobox musical artist\n| name             = Nate Dogg\n| background       = solo_singer\n| image            = Nate dogg rapradar.jpg\n| caption          = Hale at Recording Academy Honors in Hollywood on June 8, 2006\n| birth_name       = Nathaniel Dwayne Hale\n| alias            = \n| birth_date       = {{birth date|1969|08|19}}\n| birth_place      = [[Clarksdale, Mississippi]], U.S.\n| origin           = [[Long Beach, California]], U.S.\n| death_date       = {{death date and age|2011|3|15|1969|08|19}}\n| death_place      = [[Long Beach, California]], U.S.\n| genre            = {{flatlist|\n* [[West Coast hip hop]]\n* [[gangsta rap]]\n* [[Contemporary R&B|R&B]]\n* [[G-funk]]\n* [[hip hop soul]]\n}}\n| occupation       = {{flatlist|\n* Rapper\n* singer\n* songwriter\n}}\n| years_active     = 1990\u20132008\n| label            = {{flatlist|\n* [[Death Row Records|Death Row]]\n* [[Elektra Records|Elektra]]\n* [[Doggystyle Records|Doggystyle]]\n* [[Atlantic Records|Atlantic]]\n}}\n| associated_acts  = {{flatlist|\n* [[213 (group)|213]]\n* [[Tupac Shakur|2Pac]]\n* [[50 Cent]]\n* [[Dr. Dre]]\n* [[Eminem]]\n* [[Fabolous]]\n* [[MC Ren]]\n* [[Obie Trice]]\n* [[Snoop Dogg]]\n* [[Shade Sheist]]\n* [[Tha Dogg Pound]]\n* [[Thug Life]]\n* [[Warren G]]\n* [[Westside Connection]]\n}}\n}}\n'''Nathaniel Dwayne Hale''' (August 19, 1969 \u2013 March 15, 2011), of the stage name '''Nate Dogg''', was an American R&B singer, especially famous for his guest singing of hooks on hit rap songs, especially in the [[West Coast Rap|West Coast rap]]'s [[G-funk]] era during [[Death Row Records]]' 1990s glory.<ref name=\":0\">Garth Cartwright, [https://www.theguardian.com/music/2011/mar/16/nate-dogg-obituary \"Nate Dogg obituary\"], [[The Guardian|''The Guardian'']] (UK), 16 Mar 2011.</ref> Yet first, he had formed a trio, [[213 (group)|213]], with rappers [[Snoop Dogg]] and [[Warren G]].<ref name=\":0\" />\n\nOnce Snoop joined Death Row Records as a solo artist, Nate Dogg's smooth, golden voice complemented some of his guest rapping on Dr. Dre's solo debut, [[The Chronic|''The Chronic'']], in 1992\u2014the new label's first album\u2014and then its second album, Snoop's own solo debut, ''[[Doggystyle]]'', in 1993.<ref name=\":0\" /> Yet in a 1994 single by Warren G, who had signed to Def Jam, Nate cowrote and carries nearly half of the hit, \"[[Regulate (song)|Regulate]],\" which scaled the music charts.<ref name=\":0\" />\n\nNate's solo work with Death Row, amid the label's snowballing travails, was delayed from 1996 until 1998: ''[[G-Funk Classics, Vol. 1 & 2]]''.<ref name=\":0\" /> Yet, with the G-funk sound rapidly aging, it made mediocre showing on the charts.<ref name=\":0\" /> Nate Dogg's followup, ''[[Music & Me (Nate Dogg album)|Music & Me]]'', with fresher material, arrived in 2001.<ref name=\":0\" /> And in 2004, the 213 trio at last released a studio album, [[The Hard Way (213 album)|''The Hard Way'']].<ref name=\":0\" /> Finally, in 2008, [[Nate Dogg (album)|Nate Dogg's enponymous album]], recorded in 2004, was released.<ref name=\":0\" />\n\nIn th meantime, Hale had several arrests involving numerous charges, but convictions were far fewer, including drug offenses, trespassing, battery, and driving under the influence. [[Hemiplegic]] since a 2007 stroke, he later sustained another stroke, and, by complications, at age 41, died in 2011.<ref name=\"Nate Dogg Died\">{{cite magazine|last=Perpetua|first=Matthew|date=March 15, 2011|title=Rapper and Singer Nate Dogg Dead at 41|url=https://www.rollingstone.com/music/news/rapper-and-singer-nate-dogg-dead-at-41-20110316|magazine=[[Rolling Stone]]|location=New York City|publisher=Wenner Media LLC|accessdate=May 7, 2011}}</ref> While central in the West Coast rise to prominence in the rap genre, Nate Dogg's numerous, memorable [[Hook (music)|hooks]]\u2014for artists also including 2Pac, Eminiem, [[50 Cent]], [[Ludacris]], [[Xzibit]], and more<ref>{{cite news|first=Garth|last=Cartwright|url=https://www.theguardian.com/music/2011/mar/16/nate-dogg-obituary|title=Nate Dogg obituary|newspaper=[[The Guardian]]|publisher=[[Guardian Media Group]]|location=London, England|date=March 16, 2011|accessdate=September 20, 2018}}</ref>\u2014helped propel the rap genre overall the fore of American music.<ref name=\":0\" />\n\n==Early life==\nAugust 19, 1969, Nathaniel Dwayne Hale was born in [[Clarksdale, Mississippi]].<ref>{{Cite news|first1=Gerrick D.|last1=Kennedy|first2=Valerie J.|last2=Nelson|url=http://articles.latimes.com/2011/mar/17/local/la-me-nate-dogg-20110317|title=Nate Dogg dies at 41; West Coast rapper created the blend of singing-rapping known as G-funk|newspaper=[[Los Angeles Times]]|publisher=[[Tronc]]|location=Los Angeles, California|date=March 17, 2011|access-date=June 21, 2018|language=en-US|issn=0458-3035}}</ref> At the Life Line Baptist Church\u2014where his father Daniel Lee Hale was pastor and his mother Ruth Holmes led the choir\u2014Nate began singing in childhood.\n\nAt age 14, following his parents' divorce, he moved to [[Long Beach, California]], where he continued singing at the New Hope Baptist Church. While friends with [[Warren G]] and [[RBX]], he was a cousin of [[Snoop Dogg]], [[Daz Dillinger]], [[Butch Cassidy (singer)|Butch Cassidy]], and Lil' \u00bd Dead.\n\n== Military career ==\nAt age 17, Hale dropped out of high school, left home, and 30 days later enlisted in the [[United States Marine Corps|Marines]].<ref name=\"marine\">{{cite web |url=http://www.usmc.net/famous_us_marines/ |title=Joining the Ranks of Famous Marines |publisher=USMC.net |accessdate=December 26, 2009 }}</ref> He was stationed at Camp Schwab, in [[Okinawa Prefecture|Okinawa, Japan]], in the Material Readiness Battalion of the 3rd Force Service Support Group, which supplied ammunition to most of the [[Pacific Ocean|Pacific]]. After three years as an ammunition specialist, he was [[military discharge|discharged]] in 1989. Hale would recall that joined the military since he had \"wanted to see if he was a man.\"<ref>{{cite web|first=Paul|last=Arnold|url=https://hiphopdx.com/editorials/id.1899/title.souljas-story-10-hip-hop-artists-who-served-their-country|title=Soulja's Story: 10 Hip Hop Artists Who Served Their Country|website=hiphopdx.com|location=Portland, Oregon|date=May 28, 2012|accessdate=May 3, 2018}}</ref>\n\n==Entertainment career==\n===213 trio===\nIn 1990,<ref name=\"About\"/> Nate Dogg, [[Snoop Dogg]],<ref>[{{Allmusic|class=artist|id=p200247/biography|pure_url=yes}} Nate Dogg Biography], [[AllMusic]]. Retrieved November 2, 2006.</ref> and [[Warren G]], formed a rap trio called [[213 (group)|213]].<ref name=\"About\">{{cite web|url=http://www.warreng.com/about|title=Warren G|accessdate=2019-09-28}}</ref> They recorded their first demo tape in the back of the famed V.I.P. record store in Long Beach. The demo was later heard by [[Dr. Dre]] at a bachelor party.<ref>{{cite web |last1=Ness |first1=Jimmy |title=Where Rhythm is Life & Life Is Rhythm: An Interview with Warren G |url=https://www.passionweiss.com/2015/08/25/warren-g-interview/ |website=Passion of the Weiss |accessdate=23 July 2018 |date=August 25, 2015}}</ref>\n\n===Solo career===\nNate Dogg debuted on Dr. Dre's first solo album, ''[[The Chronic]]'', in 1992. Nate's trademark singing, complementing the new gangsta rap sound G-funk, was well received by fans and critics alike, and he signed to Dre's label, [[Death Row Records]], in 1993. Nate Dogg also featured on [[Snoop Dogg]]'s debut solo album, ''[[Doggystyle]]'', in 1993, his singing prominent on the track \"Ain't No Fun (If the Homies Can't Have None).\"\n\nIn 1994, Nate Dogg cowrote his duo as Warren G's guest on the single \"[[Regulate (song)|Regulate]].\" Nate was also featured on [[2Pac]] releases, including his group's [[Thug Life: Volume 1|Thug Life's album]], also released in 1994. In July 1998, amid his departure from Death Row Records, the label released his double album, delayed about two years, ''G-Funk Classics Vol. 1 & 2''. In 2001, his [[Elektra Records]] followup, ''Music & Me'', peaked at #3 on the ''Billboard'' hip-hop chart.<ref>[{{Allmusic|class=artist|id=p200247/charts-awards|pure_url=yes}} Nate- Charts and Awards], [[AllMusic]]. Retrieved November 2, 2006</ref> He also had an [[Nate Dogg (album)|eponymous album]] that saw unauthorized release in 2003.{{Citation needed|date=March 2018}}\n\n===Collaborations===\nNate Dogg was often sought to sing on other artists' tracks, usually to sing the [[Refrain|hook]]. As a featured artist, he charted 16 times on the [[Billboard Hot 100|''Billboard'' Hot 100]], and in 2003 reached #1 via [[50 Cent]]'s \"[[21 Questions]].\"\n\nOtherwise, his successful collaborations are numerous, including [[Tupac Shakur|2Pac]]'s \"[[All Eyez on Me|All Bout U]],\" [[Dr. Dre]]'s \"[[The Next Episode]],\" [[Westside Connection]]'s \"[[Gangsta Nation]],\" [[Mos Def]]'s \"Oh No,\", [[Fabolous]]' \"[[Can't Deny It]],\" [[Ludacris]]'s \"[[Area Codes (song)|Area Codes]],\" [[Kurupt]]'s \"Behind the Walls,\" [[Mark Ronson]]'s \"Ooh Wee,\" [[Houston (singer)|Houston]]'s \"[[I Like That (Houston song)|I Like That]],\" [[Eminem]]'s \"[['Till I Collapse]],\" his \"Never Enough\", his \"[[Shake That]],\" and [[Mobb Deep]]'s \"[[Have a Party]].\"<ref name=\"Top Collaborations\">{{cite web|last=Perpetua|first=Matthew|title=Nate Dogg's Best Guest Appearances|url=https://www.rollingstone.com/music/photos/nate-doggs-best-guest-appearances-20110316/warren-g-featuring-nate-dogg-regulate-1994-0119022|work=Rolling Stone|accessdate=May 7, 2011}}</ref>\n\nFurther, in 2002, appearing on television, Nate Dogg was on a celebrity episode of ''[[The Weakest Link (American game show)|The Weakest Link]]'', where, finally eliminated by Xzibit and [[Young MC]], he was among the final three.<ref name=\"mtvweakestlink\">{{cite web|url=http://www.mtv.com/news/articles/1452885/rappers-face-off-on-weakest-link.jhtml|title=Xzibit, B-Real, DJ Quik Face Off On Hip-Hop 'Weakest Link'|last=Reid|first=Shaheem|date=March 13, 2002|website=[[MTV]]|publisher=[[Viacom (2005\u2013present)|Viacom]]|location=New York City|accessdate=February 17, 2011}}</ref>\n\n== Legal troubles ==\nNate Hale was charged for a 1991 robbery of a Check Changers shop and for a 1994 robbery of [[Taco Bell]] in [[San Pedro, Los Angeles|San Pedro]], but was acquitted.<ref name=\"cases\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE8FF7EB702DFB&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=L.B. Rapper Nate Dogg Out On Bail In Robbery Charge |work=Press-Telegram |date=December 14, 1994 |format=Fee required |accessdate=March 24, 2011 }}</ref><ref name=\"latimes\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE90E01DF7AB5B&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Nate Dogg Acquitted Of '91 Robbery Charges |work=Press-Telegram |date=July 22, 1996 |format=Fee required |accessdate=March 24, 2011 }}</ref><ref name=\"robbery\">{{cite web|url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE8FFE3724BC63&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM|title=L.B. Rapper Suspected of Robbery|date=December 19, 1994|work=Press-Telegram|format=Fee required|accessdate=March 15, 2018}}</ref> Yet in 1996, he was convicted of a drug offense in [[Los Angeles County, California|Los Angeles County]].<ref name=\"charges\">{{cite web |url=https://news.google.com/newspapers?id=fqFIAAAAIBAJ&sjid=cQsNAAAAIBAJ&pg=2583,12574 |title=Firearm charges dog another rapper |work=The Post and Courier |date=March 16, 2001 |accessdate=March 24, 2011 }}</ref>\n\nOn June 17, 2000, for allegedly assaulting his former girlfriend and setting her mother's car afire in [[Lakewood, California|Lakewood]], Hale was charged with kidnapping, domestic violence, terrorist threats, and arson.<ref name=\"arrest\">{{cite news |url=https://news.google.com/newspapers?id=TE0gAAAAIBAJ&sjid=a8kEAAAAIBAJ&pg=2870,2389837 |title=Nate Dogg arrested |agency=Associated Press   |date=June 16, 2000 |accessdate=March 24, 2011 }}</ref> Andre \"Dr. Dre\" Young posted $1 million bond.<ref name=\"bail\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE8B414DD615CD&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Rapper Nate Dogg Charged on 5 Counts |work=Press-Telegram |date=July 15, 2000 |format=Fee required |accessdate=March 24, 2011 }}</ref> The charges were dismissed while he pleaded [[Nolo contendere|no contest]] to illegal gun possession by a felon,<ref name=\"charges\" /> and received a $1,000 fine and three years [[probation]].<ref name=\"probation\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EE15D1E6658A22E&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Rapper Nate Dogg takes plea bargain |work=Press-Telegram |date=August 22, 2001 |accessdate=March 24, 2011 }}</ref>\n\nOn April 12, 2002, a tour bus carrying Hale, while outside of [[Kingman, Arizona]], was found with two pistols and four ounces of [[Cannabis (drug)|cannabis]], whereby he was booked and then released on $3,500 bond.<ref name=\"mohave\">{{cite web |first=Jim |last=Seckler |url=https://news.google.com/newspapers?id=LNpPAAAAIBAJ&sjid=D1MDAAAAIBAJ&pg=4186,1301415 |title=Rap singer released on bond after drug, gun arrest locally |work=[[Kingman Daily Miner]] |date=April 16, 2002 |accessdate=March 24, 2011 }}</ref> The next month, the weapon charges were dropped for his guilty plea on a drug charge, and he was sentenced to probation, community service, and drug counseling.<ref name=\"mtv\">{{cite web |first=Noah |last=Callahan-Bever |url=http://www.mtv.com/news/articles/1454560/nate-dogg-sentenced-drug-possession.jhtml |title=Nate Dogg Sentenced For Drug Possession |publisher=MTV News |date=May 28, 2002 |accessdate=March 15, 2018 }}</ref>\n\nIn July 2006, Hale was charged with misdemeanor aggravated trespassing, telephone harassment, battery assault, dissuading a witness from reporting a crime, and breaking a [[restraining order]]. In March 20, 2008, pleading guilty to trespassing and battery, he lost gun-ownership rights for 10 years, received three years probation, and was ordered to a domestic-violence intervention program.<ref name=\"hiphopdx\">{{cite web |url=http://www.hiphopdx.com/index/news/id.6636/title.nate-dogg-pleads-guilty-in-domestic-violence-charge |title=Nate Dogg Pleads Guilty In Domestic Violence Charge |work=HipHopDX |date=March 27, 2008 |accessdate=March 24, 2011 |archive-url=https://web.archive.org/web/20110614151316/http://www.hiphopdx.com/index/news/id.6636/title.nate-dogg-pleads-guilty-in-domestic-violence-charge |archive-date=June 14, 2011 |url-status=dead }}</ref>\n\nOn June 23, 2008, after allegedly threatening his estranged wife by emails and chasing her on [[Interstate 405 (California)|Interstate 405]], Hale was charged with two felony counts of criminal threats and one count of [[stalking]].<ref name=\"dropped\">{{cite news|url=http://www.deseretnews.com/article/705297775/Stalking-charge-dropped-against-Nate-Dogg.html|title=Stalking charge dropped against Nate Dogg|date=April 16, 2009|accessdate=March 24, 2011|agency=Associated Press}}</ref><ref name=\"dui\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=121FD15FC57C6B40&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Nate Dogg Pleads Not Guilty to Making Threats, Stalking Estranged Wife |work=Long Beach Press-Telegram |date=July 17, 2008 |format=Fee required |accessdate=March 24, 2011 }}</ref> He pleaded not guilty.<ref name=\"dui\" /> In April 2009, as the alleged victim had failed to contact prosecutors, the charges were dropped.<ref name=\"dropped\" /> Incidentally, he was also convicted of [[Drug\u2013impaired driving|driving under the influence of drugs]].<ref name=\"dui\" />\n\n==Health problems and death==\n[[File:Graffiti upami\u0119tniaj\u0105ce Nate Dogga na uj\u015bciu wody oligoce\u0144skiej przy skrzy\u017cowaniu al. Solidarno\u015bci i ul. \u017belaznej w Warszawie..jpg|thumb|Graffiti on Solidarno\u015bci Avenue in [[Warsaw]], April 2012]]\nOn December 19, 2007, reportedly Hale suffered a [[stroke]].<ref name=\"mtvstroke\">{{cite web|url=http://www.mtv.com/news/articles/1579019/hannah-montana-contest-winner-apologizes-fake-essay.jhtml|title=Hannah Montana Contest Winner Apologizes For Fake Essay; Plus 'American Idol,' C-Murder, R.E.M., Birdman & More, In For The Record|date=January 4, 2008|accessdate=February 17, 2011|publisher=MTV}}</ref> A week later, he released from the hospital and admitted to a [[Physical medicine and rehabilitation|rehabilitation]] facility. Despite his body's paralysis on its [[Hemiparesis|left side]], his doctors expected full recovery, while his voice would not be affected.\n\nOn September 12, 2008, Hale suffered another stroke.<ref name=\"MTV\">Jayson Rodriguez (January 18, 2008). [http://www.mtv.com/news/articles/1579891/20080118/nate_dogg.jhtml Nate Dogg Paralyzed After Stroke; Manager Slams Coverage Of 911 Call], MTV. Retrieved January 19, 2008.</ref> And on March 15, 2011, in [[Long Beach, California]], at age 41, via complications from multiple strokes,<ref name=\"Nate Dogg Died\" /> he died.<ref name=\"Nate Dogg Died\" /> Hale was interred at Forest Lawn Memorial Park, also in Long Beach.<ref>{{cite web|url=https://www.findagrave.com/memorial/66995284|title=Nathaniel Dwayne \"Nate Dogg\" Hale (1969 - 2011) - Find A Grave Memorial|publisher=}}</ref>\n\nIn 2015, Hale's son, Nathaniel Jr., released his own album titled ''Son of a G'', under the name Lil Nate Dogg.<ref>{{cite web|title=Nate Dogg\u2019s son Lil Nate Dogg \"Son Of A G\" album out now|url=http://www.digitaljournal.com/pr/3618629}}</ref> His other son, Naijiel, attended the [[University of Arizona]] to play football.<ref>{{cite web|title=Hip-Hop Singer Nate Dogg\u2019s Son, Naijiel Hale, Commits to Arizona for Football (Video)|url=https://nesn.com/2013/07/hip-hop-singer-nate-doggs-son-naijiel-hale-commits-to-arizona-for-football-video/|website=NESN.com|accessdate=August 30, 2017|date=July 5, 2013}}</ref>\n\n==Discography==\n{{Main|Nate Dogg discography}}\n\n===Solo albums===\n*''[[G-Funk Classics, Vol. 1 & 2]]'' (1998)\n*''[[Music & Me (Nate Dogg album)|Music & Me]]'' (2001)\n*''[[Nate Dogg (album)|Nate Dogg]]'' (2003)\n\n===Collaboration albums===\n*''[[The Hard Way (213 album)|The Hard Way]]'' <small>(with 213)</small> (2004)\n\n==Filmography==\n* ''[[Doggy Fizzle Televizzle]]'' (2002\u20132003)\n* ''[[Head of State (2003 film)|Head of State]]'' (2003)\n* ''[[The Boondocks (TV series)|The Boondocks]]'' (2008)\n\n==Awards and nominations==\nNate Dogg was nominated for four [[Grammy Awards]].\n\n{| class=\"wikitable\" style=\"text-align:\"\n! Category\n! Genre\n! Song\n! Year\n! Result\n|-\n| Best Rap/Sung Collaboration {{small|(with [[Eminem]])}}\n| Rap\n| \"[[Shake That]]\"\n| 2007\n| {{nom}}\n|-\n| Best Rap/Sung Collaboration {{small|(with [[Ludacris]])}}\n| Rap\n| \"Area Codes\"\n| 2002\n| {{nom}}\n|-\n| Best Rap Performance by a Duo or Group {{small|(uncredited with [[Dr. Dre]] and [[Snoop Dogg]])}}\n| Rap\n| \"[[The Next Episode]]\"\n| 2001\n| {{nom}}\n|-\n| Best Rap Performance by a Duo or Group {{small|(with [[Warren G]])}}\n| Rap\n| \"[[Regulate (song)|Regulate]]\"\n| 1995\n| {{nom}}\n|-\n| Best R&B Album {{small|(with [[Anderson .Paak]]) (as featuring artist)}}\n| R&B\n| \"[[Ventura (Anderson Paak album)|What Can We Do? (feat. Nate Dogg)]]\"\n| 2020\n| {{Won}}\n|}\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n{{Commons category}}\n{{Portal|Biography}}\n* {{MTV artist|nate-dogg}}\n* {{IMDb name|0230489}}\n* {{Find a Grave|66995284}}\n\n{{Nate Dogg}}\n{{213}}\n{{Tha Dogg Pound}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Dogg, Nate}}\n[[Category:1969 births]]\n[[Category:2011 deaths]]\n[[Category:20th-century American singers]]\n[[Category:21st-century American singers]]\n[[Category:African-American male rappers]]\n[[Category:African-American songwriters]]\n[[Category:American baritones]]\n[[Category:American hip hop singers]]\n[[Category:American people convicted of assault]]\n[[Category:American people convicted of drug offenses]]\n[[Category:Atlantic Records artists]]\n[[Category:Death Row Records artists]]\n[[Category:Deaths from cerebrovascular disease]]\n[[Category:G-funk artists]]\n[[Category:Gangsta rappers]]\n[[Category:Musicians from Clarksdale, Mississippi]]\n[[Category:Musicians from Long Beach, California]]\n[[Category:Rappers from Los Angeles]]\n[[Category:Rappers from Mississippi]]\n[[Category:Songwriters from Mississippi]]\n[[Category:Songwriters from California]]\n[[Category:United States Marines]]\n[[Category:West Coast hip hop musicians]]\n[[Category:Neurological disease deaths in the United States]]\n[[Category:213 (group) members]]\n[[Category:20th-century male singers]]\n[[Category:21st-century male singers]]\n", "text_old": "{{Short description|American rapper and actor from California}}\n{{Use mdy dates|date=March 2015}}\n{{Infobox musical artist\n| name             = Nate Dogg\n| background       = solo_singer\n| image            = Nate dogg rapradar.jpg\n| caption          = Hale at Recording Academy Honors in Hollywood on June 8, 2006\n| birth_name       = Nathaniel Dwayne Hale\n| alias            = \n| birth_date       = {{birth date|1969|08|19}}\n| birth_place      = [[Clarksdale, Mississippi]], U.S.\n| origin           = [[Long Beach, California]], U.S.\n| death_date       = {{death date and age|2011|3|15|1969|08|19}}\n| death_place      = [[Long Beach, California]], U.S.\n| genre            = {{flatlist|\n* [[West Coast hip hop]]\n* [[gangsta rap]]\n* [[Contemporary R&B|R&B]]\n* [[G-funk]]\n* [[hip hop soul]]\n}}\n| occupation       = {{flatlist|\n* Rapper\n* singer\n* songwriter\n}}\n| years_active     = 1990\u20132008\n| label            = {{flatlist|\n* [[Death Row Records|Death Row]]\n* [[Elektra Records|Elektra]]\n* [[Doggystyle Records|Doggystyle]]\n* [[Atlantic Records|Atlantic]]\n}}\n| associated_acts  = {{flatlist|\n* [[213 (group)|213]]\n* [[Tupac Shakur|2Pac]]\n* [[50 Cent]]\n* [[Dr. Dre]]\n* [[Eminem]]\n* [[Fabolous]]\n* [[MC Ren]]\n* [[Obie Trice]]\n* [[Snoop Dogg]]\n* [[Shade Sheist]]\n* [[Tha Dogg Pound]]\n* [[Thug Life]]\n* [[Warren G]]\n* [[Westside Connection]]\n}}\n}}\n'''Nathaniel Dwayne Hale''' (August 19, 1969 \u2013 March 15, 2011), of the stage name '''Nate Dogg''', was an American R&B singer, especially famous for his guest singing of hooks on hit rap songs, especially in the [[West Coast Rap|West Coast]]'s [[G-funk]] era during [[Death Row Records]]' 1990s glory.<ref name=\":0\">Garth Cartwright, [https://www.theguardian.com/music/2011/mar/16/nate-dogg-obituary \"Nate Dogg obituary\"], [[The Guardian|''The Guardian'']] (UK), 16 Mar 2011.</ref> Yet first, he had formed a trio, [[213 (group)|213]], with rappers [[Snoop Dogg]] and [[Warren G]].<ref name=\":0\" />\n\nOnce Snoop, as a solo artist, joined Death Row Records, Nate Dogg's smooth, golden voice complemented some of his raps on Dr. Dre's debit solo album [[The Chronic|''The Chronic'']] in 1992, the new label's first album, and then its second album, Snoop's solo debut ''[[Doggystyle]]'' in 1993.<ref name=\":0\" /> Yet Nate carries half of the single \"[[Regulate (song)|Regulate]],\" by Warren G, who had joined [[Def Jam Recordings|Def Jam]], a duet that Nate cowrote and that scaled the music charts.<ref name=\":0\" />\n\nNate's own solo work with Death Row, amid the label's growing travails, was delayed from 1996 until 1998, when it was offered as ''[[G-Funk Classics, Vol. 1 & 2]]''.<ref name=\":0\" /> Yet, with the G-funk sound rapidly aging, this release made mediocre showing on the charts.<ref name=\":0\" /> Nate Dogg's followup, ''[[Music & Me (Nate Dogg album)|Music & Me]]'', seeming more like his solo debut, arrived in 2001.<ref name=\":0\" /> And in 2004, the 213 trio at last released a studio album, [[The Hard Way (213 album)|''The Hard Way'']].<ref name=\":0\" /> Finally, in 2008, [[Nate Dogg (album)|Nate Dogg's enponymous album]], recorded in 2004, was released.<ref name=\":0\" />\n\nOtherwise, Hale had several arrests involving numerous charges, but convictions were far fewer, including drug offenses, trespassing, battery, and driving under the influence. In 2011, after multiple strokes\u2014the first, in 2007, having left him [[hemiplegic]]\u2014Hale died by complications.<ref name=\"Nate Dogg Died\">{{cite magazine|last=Perpetua|first=Matthew|date=March 15, 2011|title=Rapper and Singer Nate Dogg Dead at 41|url=https://www.rollingstone.com/music/news/rapper-and-singer-nate-dogg-dead-at-41-20110316|magazine=[[Rolling Stone]]|location=New York City|publisher=Wenner Media LLC|accessdate=May 7, 2011}}</ref> While central in West Coast hip hop's rise to prominence, Nate Dogg's numerous, memorable [[Hook (music)|hooks]]\u2014for artists also including 2Pac, Eminiem, [[50 Cent]], [[Ludacris]], [[Xzibit]], and more<ref>{{cite news|first=Garth|last=Cartwright|url=https://www.theguardian.com/music/2011/mar/16/nate-dogg-obituary|title=Nate Dogg obituary|newspaper=[[The Guardian]]|publisher=[[Guardian Media Group]]|location=London, England|date=March 16, 2011|accessdate=September 20, 2018}}</ref>\u2014helped propel the rap genre overall the fore of American music.<ref name=\":0\" />\n\n==Early life==\nAugust 19, 1969, Nathaniel Dwayne Hale was born in [[Clarksdale, Mississippi]].<ref>{{Cite news|first1=Gerrick D.|last1=Kennedy|first2=Valerie J.|last2=Nelson|url=http://articles.latimes.com/2011/mar/17/local/la-me-nate-dogg-20110317|title=Nate Dogg dies at 41; West Coast rapper created the blend of singing-rapping known as G-funk|newspaper=[[Los Angeles Times]]|publisher=[[Tronc]]|location=Los Angeles, California|date=March 17, 2011|access-date=June 21, 2018|language=en-US|issn=0458-3035}}</ref> At the Life Line Baptist Church\u2014where his father Daniel Lee Hale was pastor and his mother Ruth Holmes led the choir\u2014Nate began singing in childhood.\n\nAt age 14, following his parents' divorce, he moved to [[Long Beach, California]], where he continued singing at the New Hope Baptist Church. While friends with [[Warren G]] and [[RBX]], he was a cousin of [[Snoop Dogg]], [[Daz Dillinger]], [[Butch Cassidy (singer)|Butch Cassidy]], and Lil' \u00bd Dead.\n\n== Military career ==\nAt age 17, Hale dropped out of high school, left home, and 30 days later enlisted in the [[United States Marine Corps|Marines]].<ref name=\"marine\">{{cite web |url=http://www.usmc.net/famous_us_marines/ |title=Joining the Ranks of Famous Marines |publisher=USMC.net |accessdate=December 26, 2009 }}</ref> He was stationed at Camp Schwab, in [[Okinawa Prefecture|Okinawa, Japan]], in the Material Readiness Battalion of the 3rd Force Service Support Group, which supplied ammunition to most of the [[Pacific Ocean|Pacific]]. After three years as an ammunition specialist, he was [[military discharge|discharged]] in 1989. Hale would recall that joined the military since he had \"wanted to see if he was a man.\"<ref>{{cite web|first=Paul|last=Arnold|url=https://hiphopdx.com/editorials/id.1899/title.souljas-story-10-hip-hop-artists-who-served-their-country|title=Soulja's Story: 10 Hip Hop Artists Who Served Their Country|website=hiphopdx.com|location=Portland, Oregon|date=May 28, 2012|accessdate=May 3, 2018}}</ref>\n\n==Entertainment career==\n===213 trio===\nIn 1990,<ref name=\"About\"/> Nate Dogg, [[Snoop Dogg]],<ref>[{{Allmusic|class=artist|id=p200247/biography|pure_url=yes}} Nate Dogg Biography], [[AllMusic]]. Retrieved November 2, 2006.</ref> and [[Warren G]], formed a rap trio called [[213 (group)|213]].<ref name=\"About\">{{cite web|url=http://www.warreng.com/about|title=Warren G|accessdate=2019-09-28}}</ref> They recorded their first demo tape in the back of the famed V.I.P. record store in Long Beach. The demo was later heard by [[Dr. Dre]] at a bachelor party.<ref>{{cite web |last1=Ness |first1=Jimmy |title=Where Rhythm is Life & Life Is Rhythm: An Interview with Warren G |url=https://www.passionweiss.com/2015/08/25/warren-g-interview/ |website=Passion of the Weiss |accessdate=23 July 2018 |date=August 25, 2015}}</ref>\n\n===Solo career===\nNate Dogg debuted on Dr. Dre's first solo album, ''[[The Chronic]]'', in 1992. Nate's trademark singing, complementing the new gangsta rap sound G-funk, was well received by fans and critics alike, and he signed to Dre's label, [[Death Row Records]], in 1993. Nate Dogg also featured on [[Snoop Dogg]]'s debut solo album, ''[[Doggystyle]]'', in 1993, his singing prominent on the track \"Ain't No Fun (If the Homies Can't Have None).\"\n\nIn 1994, Nate Dogg cowrote his duo as Warren G's guest on the single \"[[Regulate (song)|Regulate]].\" Nate was also featured on [[2Pac]] releases, including his group's [[Thug Life: Volume 1|Thug Life's album]], also released in 1994. In July 1998, amid his departure from Death Row Records, the label released his double album, delayed about two years, ''G-Funk Classics Vol. 1 & 2''. In 2001, his [[Elektra Records]] followup, ''Music & Me'', peaked at #3 on the ''Billboard'' hip-hop chart.<ref>[{{Allmusic|class=artist|id=p200247/charts-awards|pure_url=yes}} Nate- Charts and Awards], [[AllMusic]]. Retrieved November 2, 2006</ref> He also had an [[Nate Dogg (album)|eponymous album]] that saw unauthorized release in 2003.{{Citation needed|date=March 2018}}\n\n===Collaborations===\nNate Dogg was often sought to sing on other artists' tracks, usually to sing the [[Refrain|hook]]. As a featured artist, he charted 16 times on the [[Billboard Hot 100|''Billboard'' Hot 100]], and in 2003 reached #1 via [[50 Cent]]'s \"[[21 Questions]].\"\n\nOtherwise, his successful collaborations are numerous, including [[Tupac Shakur|2Pac]]'s \"[[All Eyez on Me|All Bout U]],\" [[Dr. Dre]]'s \"[[The Next Episode]],\" [[Westside Connection]]'s \"[[Gangsta Nation]],\" [[Mos Def]]'s \"Oh No,\", [[Fabolous]]' \"[[Can't Deny It]],\" [[Ludacris]]'s \"[[Area Codes (song)|Area Codes]],\" [[Kurupt]]'s \"Behind the Walls,\" [[Mark Ronson]]'s \"Ooh Wee,\" [[Houston (singer)|Houston]]'s \"[[I Like That (Houston song)|I Like That]],\" [[Eminem]]'s \"[['Till I Collapse]],\" his \"Never Enough\", his \"[[Shake That]],\" and [[Mobb Deep]]'s \"[[Have a Party]].\"<ref name=\"Top Collaborations\">{{cite web|last=Perpetua|first=Matthew|title=Nate Dogg's Best Guest Appearances|url=https://www.rollingstone.com/music/photos/nate-doggs-best-guest-appearances-20110316/warren-g-featuring-nate-dogg-regulate-1994-0119022|work=Rolling Stone|accessdate=May 7, 2011}}</ref>\n\nFurther, in 2002, appearing on television, Nate Dogg was on a celebrity episode of ''[[The Weakest Link (American game show)|The Weakest Link]]'', where, finally eliminated by Xzibit and [[Young MC]], he was among the final three.<ref name=\"mtvweakestlink\">{{cite web|url=http://www.mtv.com/news/articles/1452885/rappers-face-off-on-weakest-link.jhtml|title=Xzibit, B-Real, DJ Quik Face Off On Hip-Hop 'Weakest Link'|last=Reid|first=Shaheem|date=March 13, 2002|website=[[MTV]]|publisher=[[Viacom (2005\u2013present)|Viacom]]|location=New York City|accessdate=February 17, 2011}}</ref>\n\n== Legal troubles ==\nNate Hale was charged for a 1991 robbery of a Check Changers shop and for a 1994 robbery of [[Taco Bell]] in [[San Pedro, Los Angeles|San Pedro]], but was acquitted.<ref name=\"cases\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE8FF7EB702DFB&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=L.B. Rapper Nate Dogg Out On Bail In Robbery Charge |work=Press-Telegram |date=December 14, 1994 |format=Fee required |accessdate=March 24, 2011 }}</ref><ref name=\"latimes\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE90E01DF7AB5B&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Nate Dogg Acquitted Of '91 Robbery Charges |work=Press-Telegram |date=July 22, 1996 |format=Fee required |accessdate=March 24, 2011 }}</ref><ref name=\"robbery\">{{cite web|url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE8FFE3724BC63&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM|title=L.B. Rapper Suspected of Robbery|date=December 19, 1994|work=Press-Telegram|format=Fee required|accessdate=March 15, 2018}}</ref> In 1996, though, he was convicted of a drug offense in [[Los Angeles County, California|Los Angeles County]].<ref name=\"charges\">{{cite web |url=https://news.google.com/newspapers?id=fqFIAAAAIBAJ&sjid=cQsNAAAAIBAJ&pg=2583,12574 |title=Firearm charges dog another rapper |work=The Post and Courier |date=March 16, 2001 |accessdate=March 24, 2011 }}</ref>\n\nOn June 17, 2000, for allegedly assaulting his former girlfriend and setting her mother's car afire in [[Lakewood, California|Lakewood]], Hale was charged with kidnapping, domestic violence, terrorist threats, and arson.<ref name=\"arrest\">{{cite news |url=https://news.google.com/newspapers?id=TE0gAAAAIBAJ&sjid=a8kEAAAAIBAJ&pg=2870,2389837 |title=Nate Dogg arrested |agency=Associated Press   |date=June 16, 2000 |accessdate=March 24, 2011 }}</ref> Andre \"Dr. Dre\" Young posted his $1 million bond.<ref name=\"bail\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EAE8B414DD615CD&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Rapper Nate Dogg Charged on 5 Counts |work=Press-Telegram |date=July 15, 2000 |format=Fee required |accessdate=March 24, 2011 }}</ref> These charges were dismissed while he pleaded [[Nolo contendere|no contest]] to illegal gun possession by a felon,<ref name=\"charges\" /> and received a $1,000 fine and three years [[probation]].<ref name=\"probation\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EE15D1E6658A22E&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Rapper Nate Dogg takes plea bargain |work=Press-Telegram |date=August 22, 2001 |accessdate=March 24, 2011 }}</ref>\n\nOn April 12, 2002, a tour bus carrying Hale, while outside of [[Kingman, Arizona]], was found with two pistols and four ounces of [[Cannabis (drug)|cannabis]], whereby he was booked and then released on $3,500 bond.<ref name=\"mohave\">{{cite web |first=Jim |last=Seckler |url=https://news.google.com/newspapers?id=LNpPAAAAIBAJ&sjid=D1MDAAAAIBAJ&pg=4186,1301415 |title=Rap singer released on bond after drug, gun arrest locally |work=[[Kingman Daily Miner]] |date=April 16, 2002 |accessdate=March 24, 2011 }}</ref> The weapon charges were dropped for his guilty plea on a drug charge, and was sentenced to probation, community service, and drug counseling in May 2002.<ref name=\"mtv\">{{cite web |first=Noah |last=Callahan-Bever |url=http://www.mtv.com/news/articles/1454560/nate-dogg-sentenced-drug-possession.jhtml |title=Nate Dogg Sentenced For Drug Possession |publisher=MTV News |date=May 28, 2002 |accessdate=March 15, 2018 }}</ref>\n\nIn July 2006, Hale was charged with misdemeanor aggravated trespassing, telephone harassment, battery assault, dissuading a witness from reporting a crime, and violating a [[restraining order]]. In March 20, 2008, pleading guilty to trespassing and battery, he lost gun-ownership rights for 10 years, received three years of probation, and was ordered to complete an intervention program for domestic violence.<ref name=\"hiphopdx\">{{cite web |url=http://www.hiphopdx.com/index/news/id.6636/title.nate-dogg-pleads-guilty-in-domestic-violence-charge |title=Nate Dogg Pleads Guilty In Domestic Violence Charge |work=HipHopDX |date=March 27, 2008 |accessdate=March 24, 2011 |archive-url=https://web.archive.org/web/20110614151316/http://www.hiphopdx.com/index/news/id.6636/title.nate-dogg-pleads-guilty-in-domestic-violence-charge |archive-date=June 14, 2011 |url-status=dead }}</ref>\n\nOn June 23, 2008, after allegedly sending [[death threat|threatening]] emails to his estranged wife and chasing her on [[Interstate 405 (California)|Interstate 405]], Hale was charged with two felony counts of making criminal threats and one count of [[stalking]].<ref name=\"dropped\">{{cite news|url=http://www.deseretnews.com/article/705297775/Stalking-charge-dropped-against-Nate-Dogg.html|title=Stalking charge dropped against Nate Dogg|date=April 16, 2009|accessdate=March 24, 2011|agency=Associated Press}}</ref><ref name=\"dui\">{{cite web |url=http://nl.newsbank.com/nl-search/we/Archives?p_product=LB&p_theme=lb&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=121FD15FC57C6B40&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date:D&s_trackval=GooglePM |title=Nate Dogg Pleads Not Guilty to Making Threats, Stalking Estranged Wife |work=Long Beach Press-Telegram |date=July 17, 2008 |format=Fee required |accessdate=March 24, 2011 }}</ref> He pleaded not guilty.<ref name=\"dui\" /> In April 2009, as the alleged victim had failed to contact prosecutors, the charges were dropped.<ref name=\"dropped\" /> Incidentally, otherwise, he was convicted of [[Drug\u2013impaired driving|driving under the influence of drugs]].<ref name=\"dui\" />\n\n==Health problems and death==\n[[File:Graffiti upami\u0119tniaj\u0105ce Nate Dogga na uj\u015bciu wody oligoce\u0144skiej przy skrzy\u017cowaniu al. Solidarno\u015bci i ul. \u017belaznej w Warszawie..jpg|thumb|Graffiti on Solidarno\u015bci Avenue in [[Warsaw]], April 2012]]\nOn December 19, 2007, reportedly Hale suffered a [[stroke]].<ref name=\"mtvstroke\">{{cite web|url=http://www.mtv.com/news/articles/1579019/hannah-montana-contest-winner-apologizes-fake-essay.jhtml|title=Hannah Montana Contest Winner Apologizes For Fake Essay; Plus 'American Idol,' C-Murder, R.E.M., Birdman & More, In For The Record|date=January 4, 2008|accessdate=February 17, 2011|publisher=MTV}}</ref> A week later, he released from the hospital and admitted to a [[Physical medicine and rehabilitation|rehabilitation]] facility. Despite his body's paralysis on its [[Hemiparesis|left side]], his doctors expected full recovery, while his voice would not be affected.\n\nOn September 12, 2008, Hale suffered another stroke.<ref name=\"MTV\">Jayson Rodriguez (January 18, 2008). [http://www.mtv.com/news/articles/1579891/20080118/nate_dogg.jhtml Nate Dogg Paralyzed After Stroke; Manager Slams Coverage Of 911 Call], MTV. Retrieved January 19, 2008.</ref> And on March 15, 2011, in [[Long Beach, California]], at age 41, via complications from multiple strokes,<ref name=\"Nate Dogg Died\" /> he died.<ref name=\"Nate Dogg Died\" /> Hale was interred at Forest Lawn Memorial Park, also in Long Beach.<ref>{{cite web|url=https://www.findagrave.com/memorial/66995284|title=Nathaniel Dwayne \"Nate Dogg\" Hale (1969 - 2011) - Find A Grave Memorial|publisher=}}</ref>\n\nIn 2015, Hale's son, Nathaniel Jr., released his own album titled ''Son of a G'', under the name Lil Nate Dogg.<ref>{{cite web|title=Nate Dogg\u2019s son Lil Nate Dogg \"Son Of A G\" album out now|url=http://www.digitaljournal.com/pr/3618629}}</ref> His other son, Naijiel, attended the [[University of Arizona]] to play football.<ref>{{cite web|title=Hip-Hop Singer Nate Dogg\u2019s Son, Naijiel Hale, Commits to Arizona for Football (Video)|url=https://nesn.com/2013/07/hip-hop-singer-nate-doggs-son-naijiel-hale-commits-to-arizona-for-football-video/|website=NESN.com|accessdate=August 30, 2017|date=July 5, 2013}}</ref>\n\n==Discography==\n{{Main|Nate Dogg discography}}\n\n===Solo albums===\n*''[[G-Funk Classics, Vol. 1 & 2]]'' (1998)\n*''[[Music & Me (Nate Dogg album)|Music & Me]]'' (2001)\n*''[[Nate Dogg (album)|Nate Dogg]]'' (2003)\n\n===Collaboration albums===\n*''[[The Hard Way (213 album)|The Hard Way]]'' <small>(with 213)</small> (2004)\n\n==Filmography==\n* ''[[Doggy Fizzle Televizzle]]'' (2002\u20132003)\n* ''[[Head of State (2003 film)|Head of State]]'' (2003)\n* ''[[The Boondocks (TV series)|The Boondocks]]'' (2008)\n\n==Awards and nominations==\nNate Dogg was nominated for four [[Grammy Awards]].\n\n{| class=\"wikitable\" style=\"text-align:\"\n! Category\n! Genre\n! Song\n! Year\n! Result\n|-\n| Best Rap/Sung Collaboration {{small|(with [[Eminem]])}}\n| Rap\n| \"[[Shake That]]\"\n| 2007\n| {{nom}}\n|-\n| Best Rap/Sung Collaboration {{small|(with [[Ludacris]])}}\n| Rap\n| \"Area Codes\"\n| 2002\n| {{nom}}\n|-\n| Best Rap Performance by a Duo or Group {{small|(uncredited with [[Dr. Dre]] and [[Snoop Dogg]])}}\n| Rap\n| \"[[The Next Episode]]\"\n| 2001\n| {{nom}}\n|-\n| Best Rap Performance by a Duo or Group {{small|(with [[Warren G]])}}\n| Rap\n| \"[[Regulate (song)|Regulate]]\"\n| 1995\n| {{nom}}\n|-\n| Best R&B Album {{small|(with [[Anderson .Paak]]) (as featuring artist)}}\n| R&B\n| \"[[Ventura (Anderson Paak album)|What Can We Do? (feat. Nate Dogg)]]\"\n| 2020\n| {{Won}}\n|}\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n{{Commons category}}\n{{Portal|Biography}}\n* {{MTV artist|nate-dogg}}\n* {{IMDb name|0230489}}\n* {{Find a Grave|66995284}}\n\n{{Nate Dogg}}\n{{213}}\n{{Tha Dogg Pound}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Dogg, Nate}}\n[[Category:1969 births]]\n[[Category:2011 deaths]]\n[[Category:20th-century American singers]]\n[[Category:21st-century American singers]]\n[[Category:African-American male rappers]]\n[[Category:African-American songwriters]]\n[[Category:American baritones]]\n[[Category:American hip hop singers]]\n[[Category:American people convicted of assault]]\n[[Category:American people convicted of drug offenses]]\n[[Category:Atlantic Records artists]]\n[[Category:Death Row Records artists]]\n[[Category:Deaths from cerebrovascular disease]]\n[[Category:G-funk artists]]\n[[Category:Gangsta rappers]]\n[[Category:Musicians from Clarksdale, Mississippi]]\n[[Category:Musicians from Long Beach, California]]\n[[Category:Rappers from Los Angeles]]\n[[Category:Rappers from Mississippi]]\n[[Category:Songwriters from Mississippi]]\n[[Category:Songwriters from California]]\n[[Category:United States Marines]]\n[[Category:West Coast hip hop musicians]]\n[[Category:Neurological disease deaths in the United States]]\n[[Category:213 (group) members]]\n[[Category:20th-century male singers]]\n[[Category:21st-century male singers]]\n", "name_user": "Occurring", "label": "safe", "comment": "trimming some", "url_page": "//en.wikipedia.org/wiki/Nate_Dogg"}
{"title_page": "Australian Professional Championship", "text_new": "{{more citations needed|date=January 2018}}\n{{Infobox Snooker tournament\n|tournament_name = Australian Professional Championship\n|image = \n|venue = \n|location = \n|country = [[Australia]]\n|establishment = 1963\n|organisation = \n|format = Non-[[Snooker world rankings|ranking]] event\n|prizefund = \n|final year= 1988\n|Current Champion =\n|Final Champion = {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]] \n}}\n\nThe '''Australian Professional Championship''' was a professional [[snooker]] tournament which was open only for Australian or Australian-based players.\n\n==History==\nFrom 1963 to 1975 the Australian Professional Championship was held on a challenge basis and dominated by [[Eddie Charlton]] who won ten times in that period. There was no event in 1975 and it became a knockout tournament in 1977 and again in 1978. It was then not held until 1984 when the [[World Professional Billiards and Snooker Association|WPBSA]] offered a subsidy of \u00a31,000 per man to any country holding a national professional championship. This subsidy ended in [[Snooker season 1988/1989|1988/1989]] after which date most national championships were discontinued.\n\nEddie Charlton won the tournament on a record 20 occasions.\n\n==Winners==\n<ref>{{cite web|title=Australian Professional Championship|url=https://web.archive.org/web/20120107161810/http://www.cajt.pwp.blueyonder.co.uk/natpro.html|publisher=Chris Turner's Snooker Archive|accessdate=5 December 2017}}</ref>\n{| class=\"wikitable\" style=\"margin: auto\"\n|-\n! style=\"text-align: center; background-color: #00af00\" | Year\n! style=\"text-align: left; background-color: #00af00\" | Winner\n! style=\"text-align: left; background-color: #00af00\" | Runner-up\n! style=\"text-align: left; background-color: #00af00\" | Final score\n! style=\"text-align: left; background-color: #00af00\" | Venue\n! style=\"text-align: left; background-color: #00af00\" | Season\n|-\n| 1963\n| {{flagicon|AUS}} [[Warren Simpson]]\n| {{flagicon|AUS}} Norman Squire\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Bankstown]]\n| 1963/64\n|-\n| 1964\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Newcastle, New South Wales|Newcastle]]\n| 1964/65\n|-\n| 1965\n| {{flagicon|AUS}} Norman Squire\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Sutherland, New South Wales|Sutherland]]\n| 1965/66\n|-\n| 1966\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|7\u20134\n| [[Freshwater, New South Wales|Harbord]]\n| 1966/67\n|-\n| 1967\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|7\u20131\n| [[Sydney]]\n| 1967/68\n|-\n| 1968\n| {{flagicon|AUS}} [[Warren Simpson]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|11\u201310\n| [[Sydney]]\n| [[Snooker season 1968/1969|1968/69]]\n|-\n| 1969\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|11\u20136\n| [[Sydney]]\n| [[1969\u201370 snooker season|1969/70]]\n|-\n| 1970\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} Norman Squire\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Sydney]]\n| [[Snooker season 1970/1971|1970/71]]\n|-\n| 1971\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|11\u20137\n| [[Sydney]]\n| [[Snooker season 1971/1972|1971/72]]\n|-\n| 1972\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|WAL}} [[Gary Owen (snooker player)|Gary Owen]]\n| align=\"center\"|19\u201310\n| [[Sydney]]\n| [[Snooker season 1972/1973|1972/73]]\n|-\n| 1973\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|WAL}} [[Gary Owen (snooker player)|Gary Owen]]\n| align=\"center\"|31\u201310\n| [[Wagga Wagga]]\n| [[Snooker season 1973/1974|1973/74]]\n|-\n| 1974\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|44\u201317\n| [[Wagga Wagga]]\n| [[Snooker season 1974/1975|1974/75]]\n|-\n| [[1975 Australian Professional Championship|1975]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} Dennis Wheelwright\n| align=\"center\"|31\u201310\n| [[Wagga Wagga]]\n| [[Snooker season 1975/1976|1975/76]]\n|-\n| [[1976 Australian Professional Championship|1976]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Paddy Morgan]]\n| align=\"center\"|Walkover\n| [[Melbourne]]\n| [[Snooker season 1976/1977|1976/77]]\n|-\n| [[1977 Australian Professional Championship|1977]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Paddy Morgan]]\n| align=\"center\"|25\u201321\n| [[Melbourne]]\n| [[Snooker season 1977/1978|1977/78]]\n|-\n| [[1978 Australian Professional Championship|1978]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Ian Anderson (snooker player)|Ian Anderson]]\n| align=\"center\"|29\u201313\n| [[Grafton, New South Wales|Grafton]]\n| [[Snooker season 1978/1979|1978/79]]\n|-\n| [[1984 Australian Professional Championship|1984]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren King (snooker player)|Warren King]]\n| align=\"center\"|10\u20133\n| [[Dubbo]]\n| [[Snooker season 1984/1985|1984/85]]\n|-\n| [[1985 Australian Professional Championship|1985]]\n| {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|10\u20137\n| [[Sydney]]\n| [[Snooker season 1985/1986|1985/86]]\n|-\n| [[1986 Australian Professional Championship|1986]]\n| {{flagicon|AUS}} [[Warren King (snooker player)|Warren King]]\n| {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]]\n| align=\"center\"|10\u20133\n| [[Wollongong]]\n| [[Snooker season 1986/1987|1986/87]]\n|-\n| [[1987 Australian Professional Championship|1987]]\n| {{flagicon|AUS}} [[Warren King (snooker player)|Warren King]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|10\u20137\n| [[Sydney]]\n| [[Snooker season 1987/1988|1987/88]]\n|-\n| [[1988 Australian Professional Championship|1988]]\n| {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]]\n| {{flagicon|AUS}} [[Robby Foldvari]]\n| align=\"center\"|9\u20137\n| [[Sydney]]\n| [[Snooker season 1988/1989|1988/89]]\n|}\n\n==References==\n{{Reflist}}\n\n{{Snooker tournaments}}\n{{Australian National Championships}}\n\n[[Category:Snooker non-ranking competitions]]\n[[Category:Defunct snooker competitions]]\n[[Category:Snooker competitions in Australia]]\n\n\n{{snooker-stub}}\n", "text_old": "{{refimprove|date=January 2018}}\n{{Infobox Snooker tournament\n|tournament_name = Australian Professional Championship\n|image = \n|venue = \n|location = \n|country = [[Australia]]\n|establishment = 1963\n|organisation = \n|format = Non-[[Snooker world rankings|ranking]] event\n|prizefund = \n|final year= 1988\n|Current Champion =\n|Final Champion = {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]] \n}}\n\nThe '''Australian Professional Championship''' was a professional [[snooker]] tournament which was open only for Australian or Australian-based players. \n\n==History==\nFrom 1963 to 1975 the Australian Professional Championship was held on a challenge basis and dominated by [[Eddie Charlton]] who won ten times in that period. There was no event in 1975 and it became a knockout tournament in 1977 and again in 1978. It was then not held until 1984 when the [[World Professional Billiards and Snooker Association|WPBSA]] offered a subsidy of \u00a31,000 per man to any country holding a national professional championship. This subsidy ended in [[Snooker season 1988/1989|1988/1989]] after which date most national championships were discontinued.\n\nEddie Charlton won the tournament on a record 20 occasions.\n\n==Winners==\n<ref>{{cite web|title=Australian Professional Championship|url=https://web.archive.org/web/20120107161810/http://www.cajt.pwp.blueyonder.co.uk/natpro.html|publisher=Chris Turner's Snooker Archive|accessdate=5 December 2017}}</ref>\n{| class=\"wikitable\" style=\"margin: auto\"\n|-\n! style=\"text-align: center; background-color: #00af00\" | Year\n! style=\"text-align: left; background-color: #00af00\" | Winner\n! style=\"text-align: left; background-color: #00af00\" | Runner-up\n! style=\"text-align: left; background-color: #00af00\" | Final score\n! style=\"text-align: left; background-color: #00af00\" | Venue\n! style=\"text-align: left; background-color: #00af00\" | Season\n|-\n| 1963\n| {{flagicon|AUS}} [[Warren Simpson]]\n| {{flagicon|AUS}} Norman Squire\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Bankstown]]\n| 1963/64\n|-\n| 1964\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Newcastle, New South Wales|Newcastle]]\n| 1964/65\n|-\n| 1965\n| {{flagicon|AUS}} Norman Squire\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Sutherland, New South Wales|Sutherland]]\n| 1965/66\n|-\n| 1966\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|7\u20134\n| [[Freshwater, New South Wales|Harbord]]\n| 1966/67\n|-\n| 1967\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|7\u20131\n| [[Sydney]]\n| 1967/68\n|-\n| 1968\n| {{flagicon|AUS}} [[Warren Simpson]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|11\u201310\n| [[Sydney]]\n| [[Snooker season 1968/1969|1968/69]]\n|-\n| 1969\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|11\u20136\n| [[Sydney]]\n| [[Snooker season 1969/1970|1969/70]]\n|-\n| 1970\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} Norman Squire\n| align=\"center\"|{{nowrap|Round-robin}}\n| [[Sydney]]\n| [[Snooker season 1970/1971|1970/71]]\n|-\n| 1971\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|11\u20137\n| [[Sydney]]\n| [[Snooker season 1971/1972|1971/72]]\n|-\n| 1972\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|WAL}} [[Gary Owen (snooker player)|Gary Owen]]\n| align=\"center\"|19\u201310\n| [[Sydney]]\n| [[Snooker season 1972/1973|1972/73]]\n|-\n| 1973\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|WAL}} [[Gary Owen (snooker player)|Gary Owen]]\n| align=\"center\"|31\u201310\n| [[Wagga Wagga]]\n| [[Snooker season 1973/1974|1973/74]]\n|-\n| 1974\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren Simpson]]\n| align=\"center\"|44\u201317\n| [[Wagga Wagga]]\n| [[Snooker season 1974/1975|1974/75]]\n|-\n| [[1975 Australian Professional Championship|1975]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} Dennis Wheelwright\n| align=\"center\"|31\u201310\n| [[Wagga Wagga]]\n| [[Snooker season 1975/1976|1975/76]]\n|-\n| [[1976 Australian Professional Championship|1976]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Paddy Morgan]]\n| align=\"center\"|Walkover\n| [[Melbourne]]\n| [[Snooker season 1976/1977|1976/77]]\n|-\n| [[1977 Australian Professional Championship|1977]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Paddy Morgan]]\n| align=\"center\"|25\u201321\n| [[Melbourne]]\n| [[Snooker season 1977/1978|1977/78]]\n|-\n| [[1978 Australian Professional Championship|1978]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Ian Anderson (snooker player)|Ian Anderson]]\n| align=\"center\"|29\u201313\n| [[Grafton, New South Wales|Grafton]]\n| [[Snooker season 1978/1979|1978/79]]\n|-\n| [[1984 Australian Professional Championship|1984]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| {{flagicon|AUS}} [[Warren King (snooker player)|Warren King]]\n| align=\"center\"|10\u20133\n| [[Dubbo]]\n| [[Snooker season 1984/1985|1984/85]]\n|-\n| [[1985 Australian Professional Championship|1985]]\n| {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|10\u20137\n| [[Sydney]]\n| [[Snooker season 1985/1986|1985/86]]\n|-\n| [[1986 Australian Professional Championship|1986]]\n| {{flagicon|AUS}} [[Warren King (snooker player)|Warren King]]\n| {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]]\n| align=\"center\"|10\u20133\n| [[Wollongong]]\n| [[Snooker season 1986/1987|1986/87]]\n|-\n| [[1987 Australian Professional Championship|1987]]\n| {{flagicon|AUS}} [[Warren King (snooker player)|Warren King]]\n| {{flagicon|AUS}} [[Eddie Charlton]]\n| align=\"center\"|10\u20137\n| [[Sydney]]\n| [[Snooker season 1987/1988|1987/88]]\n|-\n| [[1988 Australian Professional Championship|1988]]\n| {{flagicon|AUS}} [[John Campbell (snooker player)|John Campbell]]\n| {{flagicon|AUS}} [[Robby Foldvari]]\n| align=\"center\"|9\u20137\n| [[Sydney]]\n| [[Snooker season 1988/1989|1988/89]]\n|}\n\n==References==\n{{Reflist}}\n\n{{Snooker tournaments}}\n{{Australian National Championships}}\n\n[[Category:Snooker non-ranking competitions]]\n[[Category:Defunct snooker competitions]]\n[[Category:Snooker competitions in Australia]]\n\n{{snooker-stub}}\n", "name_user": "Lee Vilenski", "label": "safe", "comment": "clean up, replaced: [[Snooker season 1969/1970 \u2192 [[1969\u201370 snooker season", "url_page": "//en.wikipedia.org/wiki/Australian_Professional_Championship"}
{"title_page": "South Asian cinema", "text_new": "{{South Asian cinema}}\n'''South Asian cinema''' refers to the [[film|cinema]] of [[Bangladesh]], [[Bhutan]], [[India]], [[Maldives]], [[Nepal]], [[Pakistan]] and [[Sri Lanka]].<ref>{{cite book|last1=Dickey |first1=Sara  |last2= Dudrah |first2=Rajinder Kumar |title=South Asian Cinemas: Widening the Lens |date=2012}}</ref><ref name=\"Asiancinema\">{{cite book|last1=Teo |first1=Stephen |title=The Asian Cinema Experience: Styles, Spaces, Theory |date=2013}}</ref><ref>{{cite book|last1=Chaudhuri |first1=Shohini |title=Contemporary World Cinema: Europe, the Middle East, East Asia and South Asia |date=2005}}</ref> The terms '''Asian cinema''', '''Eastern cinema''' and '''Oriental cinema''' in common usage often encompass [[South Asia]] as well as [[East Asia]] and [[South East Asia]].<ref name=\"Asiancinema\"/>\n\n[[File:Eastern World.PNG|thumb|right|200px|The [[Far East]] as a cultural block includes [[East Asia]] (green), [[Southeast Asia]] (blue) and [[South Asia]] (orange).]]\n\n==Styles and genres==\nThe scope of South Asian cinema is huge and takes in a wide array of different [[film styles]], linguistic regions and [[Film genre|genres]]. South Asian cinema is particularly famous in the West for:\n\n* [[Melodrama|Melodramatic film]]s\n** [[Action films]]\n** [[Curry Western]]s\n** [[Escapism]]\n** [[Musical film|Musicals]]\n** [[Romance film]]s\n** [[Masala (film genre)|Masala film]]s\n* [[Parallel cinema]]\n** [[Drama (film and television)|Drama film]]s\n** [[Thriller (genre)|Thriller film]]s\n** [[Art film]]s\n** [[Organised crime in India|Indian neo-noir]]\n** [[Neorealism (art)|Neorealism]]\n* [[Heroic bloodshed]]\n* [[Historical drama film|Historical drama]]\n\n==Regional industries==\n===Bangladeshi cinema===\n{{main|Cinema of Bangladesh}}\n* '''[[Cinema of Bangladesh|Bangladeshi film industry]]''', is the [[Bengali language|Bengali]] language film industry based in [[Dhaka]], [[Bangladesh]]. The industry often generally referred to as [[Dhallywood]], has been a significant film industry since the early 1970s. The 1960s, 1970s, 1980s and the first half of the 1990s were the golden years for Bangladeshi films as the industry produced many successful films. The industry has recently begun receiving international acclaim and many Bangladeshi films are getting released internationally.\n* '''[[Dhallywood]]''', is a portmanteau of the words [[Dhaka]] and [[Hollywood]].\n\n===Indian cinema===\n{{main|Cinema of India}}\nIndia contains many [[national languages of India|state languages]] which have film industries centered on them.  Although [[Hindi]] is the official language of government business of northern regions of India, its often-used dialect [[Hindustani language|Hindustani]] is the most widespread language but covering only 40% of the total population, and English is widely understood irrespective of region, the state languages are preserved for official use by different states in India, and many have as many speakers as an average [[European cinema|European]] nation. Regional industries have also tended to produce a higher percentage of serious art film and political film. Bangladeshi cinema is filmed in [[Bengali language|Bengali]] and Sri Lankan cinema is filmed in [[Sinhalese language|Sinhala]] and [[Tamil language|Tamil]]. Last but not least is Indonesian cinema. In the beginning the Indonesian cinema grew after World War I, rooted from the Folk Theater Drama called Dardanela. Under Usmar Ismail, Indonesian cinema became the new entertainment in 1950 to 1980. Hundred of film stars were born, such as: Citra Dewi (1960), Tanty Yosepha (1970). Yenny Rachman and Christine Hakim (1980) and Dian Sastro (late 1990s). [[Teguh Karya]] was one of the leading Film Director in Indonesia after the era of Usmar Ismail. Now, by the popularity of television, film is replaced with electronic cinema which is popular as sinetron. This industry has made the Indian born producer, Raam Punjabi, a tycon of sinetron in Indonesia.\n* '''[[Hindi Cinema]]''', popularly known as ''[[Bollywood]]'', is based in [[Mumbai]]. This film industry is the most prolific and popular in South Asia.\n* '''[[Cinema of Andhra Pradesh|Telugu film industry]]''', popularly known as ''Tollywood'', which comes from the mixture of [[Hollywood]] and [[Telugu language|Telugu]], based in [[Hyderabad, Telangana]]. It is one of the three popular branches of Indian Cinema. It was formerly located in [[Chennai]], [[Tamil Nadu]].\n* '''[[Tamil Cinema]]''', the [[Tamil language|Tamil]] film industry based in the [[Kodambakkam]] area of [[Chennai]] (formerly Madras). It is one of the three popular branches of Indian Cinema. [[Tamil films]] are also produced in [[Sri Lankan Tamil Cinema|Sri Lanka]], [[Malaysia]], [[Singapore]] and [[Canada]].\n* '''[[Chhollywood]]''', the [[Chhattisgarhi language]] based film industry based in the state of [[Chhattisgarh]].\n* '''[[Dogri cinema]]''', Dogri Language cinema of [[Jammu]] region.\n* '''[[Malayalam Cinema]]''', [[Malayalam language|Malayalam]] film industry, sometimes known as ''[[Cinema of Kerala|Mollywood]]'', based in [[Kochi]] and [[Trivandrum]] in [[Kerala]]. Several of its directors such as [[Shaji N. Karun]] have also received international acclaim.\n* '''[[Cinema of Karnataka|Kannada film industry]]''', based in [[Bangalore]], [[Karnataka]].\n* '''[[Cinema of West Bengal|Bengali film industry]]''', long centered in the [[Tollygunge]] area of [[Kolkata]] (formerly Calcutta). This film industry is known for producing many internationally acclaimed films by directors such as [[Satyajit Ray]], [[Budhhadeb Dasgupta]], [[Mrinal Sen]] and [[Ritwik Ghatak]].\n* '''[[Gujarati cinema]]''', based in [[Gujarat]].\n*'''[[Haryanvi Cinema|Haryanvi cinema]]''', Haryanvi language cinema, based in [[Haryana]]\n*'''[[Kashmiri cinema]]''', Kashmiri Language cinema of [[Kashmir valley]].\n*'''[[Cinema of Rajasthan]]''', Based in Rajasthan\n* '''[[Cinema of Odisha]]''', the [[Odia language]] film industry based in [[Bhubaneshwar]] and [[Cuttack]].\n* '''[[Marathi cinema|Marathi film industry]]''', based in [[Mumbai]] and [[Pune]].\n* '''[[Punjabi cinema|Punjabi film industry]]''', based in [[Punjab, India|Punjab]], India.\n* '''[[Cinema of Assam|Assamese film industry]]''', based in [[Assam]]. This is the only major film industry in [[North-East India]].\n*'''[[Tulu cinema]]''' Tulu language speaking regions of Karnataka viz. [[Tulu Nadu]] based in [[Mangalore]].\n*'''[[Santali cinema]]''' Santali speaking regions on Jharkhand, West Bengal, Odisha & Assam.\n\n===Nepali cinema===\n{{main|Cinema of Nepal}}\n* '''[[Cinema of Nepal|Nepali film industry]]''', the [[Cinema of Nepal|Nepali film industry]] based in [[Kathmandu]], has recently begun receiving international acclaim with films such as [[The Black Hen]] (2015), [[kagbeni (film)|Kagbeni]] (2006), [[Dying Candle (film)|''Dying Candle'']] (2016) and others.\n* '''[[Tharu Cinema]]''' based in [[Terai]],'''[[Tharuhat]]''' is the home of the [[Tharu languages]] [[Film industry|cinema]].\n\n===Pakistani cinema===\n{{main|Cinema of Pakistan}}\n* '''[[Balochi cinema]]''', based in [[Quetta]], [[Balochistan]] is the home of Balochi language film productions.\n* '''[[Lollywood]]''', based in [[Lahore]], [[Punjab, Pakistan|Punjab]] is the home of [[Punjabi cinema (Pakistan)|Punjabi cinema]].\n* '''[[Kariwood]]''', based in [[Karachi]], [[Sindh]], [[Pakistan]] is the home of the Urdu cinema.\n* '''[[Pashto cinema]]''', based in [[Peshawar]], [[Khyber Pakhtunkhwa]] is the home of Pashto language film productions.\n* '''[[Sindhi cinema]]''', based in [[Karachi]], [[Sindh]], [[Pakistan]] is the home of the Sindhi language film productions.\n\n===Others===\n* [[Cinema of Sri Lanka|Sri Lankan cinema]]\n* [[Sri Lankan Tamil cinema]]\n* [[Cinema of Bhutan|Bhutanese cinema]]\n\n==Some figures of South Asian cinema==\n===Directors===\n<!-- Deleted image removed: [[File:SatyajitProduction.jpg|right|thumb|300px|Satyajit Ray at work on the set of ''Two''.]] -->\n<!-- ONLY ADD A PERSON TO THIS LIST IF THEY ALREADY HAVE AN ARTICLE IN THE ENGLISH WIKIPEDIA -->\n* [[A. R. Murugadoss]] \u2013 Tamil director (''[[Ghajini (2005 film)|Ghajini]]'', ''[[Ghajini (2008 film)|Ghajini]]'', ''[[Thuppakki]]'', ''[[Kaththi]]'', ''[[Sarkar (2018 film)|Sarkar]]'', ''[[Darbar (2020 film)|Darbar]]'')\n* [[Abu Shahed Emon]]\n* [[Adoor Gopalakrishnan]] \u2013 Malayalam director (''[[Elippathayam]]'', ''[[Swayamvaram (1972 film)|Swayamvaram]]'').\n* [[Alamgir Kabir (film maker)|Alamgir Kabir]]\n* [[Amitabh Reza Chowdhury]] \n* [[Anurag Kashyap]] - (''[[Gangs of Wasseypur]]'', ''[[Black Friday (2007 film)|Black Friday]]'')\n* [[Aparna Sen]] \u2013 Bengali Indian actress and director (''[[36 Chowringhee Lane]]'', ''[[Mr. and Mrs. Iyer]]'').\n* [[Ashutosh Gowariker]] \u2013 Contemporary Hindi actor, director and producer (''[[Lagaan]]'').\n* [[Balu Mahendra]] \u2013 [[Sri Lanka]]-born Tamil and [[Malayalam]] director (''Sandhya Raagam'', ''Veedu'').\n* [[Basu Chatterjee]] \u2013 (''[[Chitchor]]'').\n* [[Bharathiraja]] \u2013 Tamil director who captured village life (''[[Muthal Mariyathai]]'', ''[[Vedham Pudhithu]]'').\n* [[Bimal Roy]] \u2013 Hindi film director (''[[Devdas (1955 film)|Devdas]]'', ''[[Do Bigha Zameen]]'').\n* [[Budhhadeb Dasgupta]] \u2013 ''[[Uttara (film)|Uttara]]'', internationally acclaimed filmmaker known for surrealism and magical realism. \n* [[Chashi Nazrul Islam]]\n* [[Deepa Mehta]] \u2013 Indian-born Canadian director best known for her \"elements trilogy\". ''[[Fire (1996 film)|Fire]]'', ''[[Earth (1998 film)|Earth]]'', ''[[Water (2005 film)|Water]]'').\n* [[Ehtesham]]\n* [[Fateh Lohani]]\n* [[Girish Karnad]] \u2013 (''[[Anand Bhairavi]]'').\n* [[Govind Nihalani]] \u2013 Cinematographer and director.\n* [[Gurinder Chadha]] \u2013 British director (''[[Bend It Like Beckham]]'', ''[[Bride and Prejudice]]'').\n* [[Guru Dutt]] \u2013 Hindi actor, director and producer of the 1950s and '60s (''[[Mr. & Mrs. '55]]'', ''[[Kaagaz Ke Phool]]'', ''[[Pyaasa]]'').\n* [[Hrishikesh Mukherjee]] \u2013 Hindi film director known for (''[[Anand (1971 film)|Anand]]'', ''[[Abhimaan (1973 film)|Abhimaan]]'').\n* [[Humayun Ahmed]] \u2013 One of the most successful writers and directors of [[Bangladesh]].\n* [[K. Asif]] \u2013 [[Mughal-e-Azam]]\n* [[K. Balachander]] \u2013 Tamil director.\n* [[K. S. Ravikumar]] \u2013 Tamil commercial film director (''[[Muthu (1995 film)|Muthu]]'', ''[[Padayappa]]'', ''[[Dasavathaaram]]'')\n* [[K. Viswanath]] \u2013 Telugu director known for films like ''[[Sankarabharanam (1980 film)|Sankarabharanam]]'', ''[[Swathi Muthyam]]'', ''[[Swayam Krushi]]''.\n* [[Kamal Amrohi]] \u2013 [[Mahal (1949 film)|Mahal]] [[Pakeeza]] [[Raziya Sultan (film)|Razia Sultan]]\n* [[Kamar Ahmed Saimon]]\n* [[Ketan Mehta]] \u2013 (''[[Bhavni Bhavai]]'', ''[[Maya Memsaab]]'').\n* [[Khan Ataur Rahman]]\n* [[Lester James Peries]] - (b 1919) Sri Lankan film director\n* [[Madhur Bhandarkar]] \u2013 Director and screenwriter (''[[Page 3 (film)|Page 3]]'', ''[[Chandni Bar]]'').\n* [[Mahboob]]\n* [[Mani Ratnam]] \u2013 Generally works in Tamil films but has worked in Hindi, Malayalam, Telugu and Kannada industries. (''[[Kannathil Muthamittal]]'', ''[[Guru (2007 film)|Guru]]'').\n* [[Mani Shankar]] \u2013 Director of Bollywood action thrillers (''[[16 December (film)|16 December]]'', ''[[Tango Charlie]]'')\n* [[Manmohan Desai]] \u2013 (''[[Parvarish (1977 film)|Parvarish]]'', ''[[Amar Akbar Anthony]]'').\n* [[Mira Nair]] \u2013 (''[[Monsoon Wedding]]'', ''[[Salaam Bombay!]]'').\n* [[Morshedul Islam]]\n* [[Mostofa Sarwar Farooki]]\n* [[Mrinal Sen]] \u2013 Bengali film director, has won awards at major film festivals (''Baishey Shravan'', ''Bhuvan Shome'').\n* [[Nagathihalli Chandrashekhar]] \u2013 (''[[America! America!!]]'', ''[[Amruthadhaare]]'').\n* [[Narayan Ghosh Mita]]\n* [[Nasir Hussain]] \u2013 (''[[Qayamat Se Qayamat Tak]]'')\n* [[Nischal Basnet]] \u2013 (''[[Loot (2012 film)|Loot]]'', ''[[Loot 2]]'')\n* [[Partho Sen-Gupta]] \u2013 Avant-garde independent director (''[[Hava Aney Dey]]'').\n* [[Prakash Jha]] \u2013 Contemporary Hindi director (''[[Gangaajal]]'', ''[[Apaharan]]'').\n* [[Prakash Mehra]] \u2013 (''[[Zanjeer (1973 film)|Zanjeer]]'', ''[[Hera Pheri (1976 film)|Hera Pheri]]'').\n* [[Prashanta Nanda]] \u2013 Oriya film director who won most of the National Awards for his contribution for [[Oriya Film Industry]].\n* [[Puttanna Kanagal]] \u2013 (''[[Belli moda]]'').\n* [[Rajkumar Santoshi]] \u2013 (''[[Ghayal (1990 film)|Ghayal]]'', ''[[Andaz Apna Apna]]'').\n* [[Rakesh Roshan]] \u2013 (''[[Karan Arjun]]'', ''[[Krrish]]'').\n* [[Rakeysh Omprakash Mehra]] \u2013 Director and screenwriter (''[[Aks (2001 film)|Aks]]'', ''[[Rang De Basanti]]'').\n* [[Ram Gopal Varma]] \u2013 Tollywood and Bollywood director known for his gritty films. (''[[Shiva (1989 film)|Shiva]]'', ''[[Rangeela (1995 film)|Rangeela]]'').\n* [[Ramesh Sippy]] \u2013 (''[[Sholay]]'', ''[[Andaz (1971 film)|Andaz]]'')\n* [[Ritwik Ghatak]] \u2013 Bengali film director, (''[[Nagarik]]'', ''[[Meghe Dhaka Tara]]'').\n* [[S. S. Rajamouli]] \u2013 Commercial Telugu director.\n* [[S. Shankar]] \u2013  Tamil director and producer (''[[Gentleman (1993 film)|Gentleman]]'', ''[[Indian (1996 film)|Indian]]'', ''[[Mudhalvan]]'', ''[[Anniyan]]'', ''[[Sivaji: The Boss]]'', ''[[Enthiran]]'', ''[[I (film)|I]]'', ''[[2.0 (film)|2.0]]'')\n* [[Saawan Kumar]]\n* [[Sanjay Gupta (Director)|Sanjay Gupta]] \u2013 (''[[Zinda (film)|Zinda]]'')\n* [[Sanjay Leela Bhansali]] \u2013 (''[[Devdas (2002 Hindi film)|Devdas]]'', ''[[Black (2005 film)|Black]]'')\n* [[Santosh Sivan]] \u2013 Award-winning cinematographer and director (''[[The Terrorist (1997 film)|The Terrorist]]'', ''[[Asoka (2001 film)|Asoka]]'').\n* [[Satyajit Ray]] \u2013 Bengali film director, widely regarded as one of the greatest [[auteur]]s of 20th century cinema (''[[Apu trilogy]]'').\n* [[Shekhar Kapur]] \u2013 [[British Raj|British India]]-born director and producer (''[[Elizabeth (film)|Elizabeth]]'', ''[[Bandit Queen]]'').\n* [[Shyam Benegal]] \u2013 Important part of the New India Cinema movement (''[[Ankur (film)|Ankur]]'', ''[[Bhumika (film)|Bhumika]]'').\n* [[Sonali Gulati]] \u2013 contemporary independent filmmaker, activist, and feminist who has made award-winning documentary and experimental films.\n* [[Subhash Dutta]]\n* [[Sudhir Mishra]] \u2013 Contemporary director and screenwriter (''[[Hazaaron Khwaishein Aisi]]'', ''[[Chameli (film)|Chameli]]'').\n* [[Tanvir Mokammel]]\n* [[Tareque Masud]]\n* [[Tulsi Ghimire]] \u2013 Nepali movie director (Known for ''[[Kusume Rumal]]'', ''[[Lahure (film)|Lahure]]'', ''[[Darpan Chaya]]'')\n* [[Upendra (film)|Upendra]] \u2013 (''[[A (1998 Indian film)|A]]'', ''Om'').\n* [[V. Shantaram]] \u2013 Hindi director and actor (''[[Do Aankhen Barah Haath]]'').\n* [[Vidhu Vinod Chopra]] \u2013 (''[[An Encounter with Faces]]'', ''[[1942: A Love Story]]'').\n* [[Vijay Anand (Hindi film maker)|Vijay Anand]] \u2013 Bollywood actor, director and producer mainly during the 1960s and '70s. (''Johnny Mera Naam'', ''Jewel Thief'')\n* [[Vikram Bhatt]] \u2013 (''[[Inteha (2003 film)|Inteha]]'', ''[[Deewane Huye Pagal]]'').\n* [[Yash Chopra]] \u2013 Veteran producer and director (''[[Waqt (1965 film)|Waqt]]'', ''[[Deewaar]]'').\n* [[Yograj Bhat]] \u2013 (''[[Mungaru Male]]'').\n* [[Zahir Raihan]]\n* [[Tulsi Ghimire]] \u2013 Nepali movie director (Known for ''[[Kusume Rumal]]'', ''[[Lahure (film)|Lahure]]'', ''[[Darpan Chaya]]'')\n\n===Actors===\n<!-- ONLY ADD A PERSON TO THIS LIST IF THEY ALREADY HAVE AN ARTICLE IN THE ENGLISH WIKIPEDIA -->\n* [[Aamir Khan]]\n* [[Aaryan Sigdel]]\n* [[Abdur Razzak (actor)|Abdur Razzak]] \n* [[Abhishek Bachchan]]\n* [[Ajay Devgan]]\n* [[Ajith Kumar]]\n* [[Akkineni Nagarjuna]]\n* [[Akkineni Nageswara Rao]]\n* [[Akshay Kumar]]\n* [[Alamgir (actor)|Alamgir]] \n* [[Allu Arjun]]\n* [[Ambareesh]]\n* [[Amitabh Bachchan]]\n* [[Amrish Puri]]\n* [[Anant Nag]]\n* [[Ananta Jalil]] \n* [[Anil Chatterjee]]\n* [[Anil Kapoor]]\n* [[Anmol K.C.]]\n* [[Anubhav Mohanty]]\n* [[Anwar Hossain (actor)|Anwar Hossain]]\n* [[Arifin Shuvoo]]\n* [[Arjun Sarja]]\n* [[Arpan Thapa]]\n* [[Arshad Warsi]]\n* [[Ashok Kumar]]\n* [[ATM Shamsuzzaman]] \n* [[Balakrishna]]\n* [[Balraj Sahni]]\n* [[Bappy Chowdhury]]\n* [[Bhuwan K.C.]]\n* [[Bipin Karki]]\n* [[Biraj Bhatta]]\n* [[Bulbul Ahmed]]\n* [[Chhabi Biswas]]\n* [[Chiranjeet]]\n* [[Chiranjeevi]]\n* [[Chunky Pandey]]\n* [[Daggubati Venkatesh]]\n* [[Dev (Bengali actor)|Deepak Adhikari Dev]]\n* [[Dev Anand]]\n* [[Dhanush]]\n* [[Dharmendra]]\n* [[Dilip Kumar]]\n* [[Dipjol]] \n* [[Dulquer Salmaan]]\n* [[Faisal Rehman]]\n* [[Ferdous Ahmed]]\n* [[Feroz Khan (actor, born 1939)|Feroz Khan]]\n* [[Hamza Ali Abbasi]]\n* [[Hrithik Roshan]]\n* [[Humayun Faridi]] \n* [[Humayun Saeed]]\n* [[Ilias Kanchan]] \n* [[Jackie Shroff]]\n* [[Jagathi Sreekumar]]\n* [[Jayam Ravi]]\n* [[Jayan]]\n* [[Jeet (actor)]]\n* [[Jeetendra]]\n* [[Kamal Hassan]]\n* [[Kazi Maruf]]\n* [[Kishore Kumar]]\n* [[M.G. Ramachandran]]\n* [[Mahesh Babu]]\n* [[Mammooty]]\n* [[Mamnun Hasan Emon]]\n* [[Manna (actor)|Manna]] \n* [[Manoj Bajpai]]\n* [[Manoj Kumar]]\n* [[Mehmood Ali|Mehmood]]\n* [[Mithun Chakraborty]]\n* [[Moammar Rana]]\n* [[Mohammad Ali (actor)|Mohammad Ali]]\n* [[Mohanlal]]\n* [[Mohib Mirza]]\n* [[Mukesh (Malayalam actor)|Mukesh]]\n* [[N. T. Rama Rao Jr.]]\n* [[N. T. Rama Rao]]\n* [[Nadeem Baig (actor)|Nadeem]]\n* [[Nana Patekar]]\n* [[Nani (actor)|Nani]]\n* [[Naseeruddin Shah]]\n* [[Nikhil Upreti]]\n* [[Nivin Pauly]]\n* [[Om Puri]]\n* [[Omar Sani]]\n* [[Pahari Sanyal]]\n* [[Pawan Kalyan]]\n* [[Prabhas]]\n* [[Prabhu Deva]]\n* [[Pran (actor)|Pran]]\n* [[Prithviraj Sukumaran]]\n* [[Prosenjit Chatterjee]] \n* [[Puneeth Rajkumar]]\n* [[R. Madhavan|Madavan]]\n* [[Raaj Kumar]]\n* [[Rabi Ghosh]]\n* [[Rahsaan Islam]] \n* [[Raisul Islam Asad]] \n* [[Raj Kapoor]]\n* [[Rajesh Hamal]]\n* [[Rajesh Khanna]]\n* [[Rajinikanth]]\n* [[Rajkumar (actor)|Rajkumar]]\n* [[Ram Charan]]\n* [[Ramesh Aravind]]\n* [[Ranjit Mallick]]\n* [[Riaz]] \n* [[Rishi Kapoor]]\n* [[Ritesh Deshmukh]]\n* [[Saif Ali Khan]]\n* [[Salman Khan]]\n* [[Salman Shah (actor)|Salman Shah]] \n* [[Sanjay Dutt]]\n* [[Sanjeev Kumar]]\n* [[Saugat Malla]]\n* [[Shaan Shahid|Shaan]]\n* [[Shah Rukh Khan|Shahrukh Khan]]\n* [[Shakib Khan]]\n* [[Shammi Kapoor]]\n* [[Shamoon Abbasi]]\n* [[Shankar Nag]]\n* [[Siddhanta Mahapatra]]\n* [[Siddharth (actor)]]\n* [[Sivaji Ganeshan]]\n* [[Sohel Rana (actor)]] \n* [[Soumitra Chatterjee]] \n* [[Subhash Dutta]]\n* [[Sunil Dutt]]\n* [[Sunny Deol]]\n* [[Suresh Gopi]]\n* [[Surya Sivakumar]]\n* [[Symon Sadik]]\n* [[Tapas Paul]]\n* [[Tapen Chatterjee]]\n* [[Thilakan]]\n* [[Tulsi Chakraborty]]\n* [[Upendra (actor)|Upendra]]\n* [[Uttam Kumar]]\n* [[Uttam Mohanty]]\n* [[Vidyut Jamwal]]\n* [[Vijay (actor)|Vijay]]\n* [[Vijay Sethupathi]]\n* [[Vikram (actor)|Vikram]]\n* [[Vinod Khanna]]\n* [[Vinod Mehra]]\n* [[Vishnuvardhan (actor)|Vishnuvardhan]]\n* [[Vivek Oberoi]]\n* [[Waheed Murad]]\n* [[Wasim (actor)|Wasim]]\n* [[Zafar Iqbal (actor)|Zafar Iqbal]]\n\n===Actresses===\n<!-- ONLY ADD A PERSON TO THIS LIST IF THEY ALREADY HAVE AN ARTICLE IN THE ENGLISH WIKIPEDIA -->\n* [[Achol]]\n* [[Aishwarya Rai]] - [[Miss World 1994]]\n* [[Aishwarya Rajesh]]\n* [[Akshara Haasan]]\n* [[Alisha Pradhan]]\n* [[Amala Paul]]\n* [[Amrita Acharia]] - ([[Game Of Thrones]]) [[Nepali people|Nepali]] [[Actress]]\n* [[Amrita Rao]]\n* [[Andrea Jeremiah]]\n* [[Anjali (film actress)]]\n* [[Anju Ghosh]]\n* [[Anu Emmanuel]]\n* [[Anushka Shetty]]\n* [[Aparajita Mohanty]]\n* [[Apu Biswas]]\n* [[Archita Sahu]]\n* [[Asin Thottumkal]]\n* [[Ayesha Takia]]\n* [[B. Saroja Devi]]\n* [[Barsha Priyadarshini]]\n* [[Bidya Sinha Saha Mim]]\n* [[Bipasha Basu]]\n* [[Bobby (Actress)|Bobby]]\n* [[Bobita]] ( [[Dhallywood]] Actress )\n* [[Catherine Tresa]]\n* [[Celina Jaitly]] - [[Femina Miss India|Miss India 2001]]\n* [[Debashree Roy]]\n* [[Deepika Padukone]] \n* [[Devika Rani]]\n* [[Dia Mirza]] - Miss Asia Pacific 2000\n* [[Diana Hayden]] - [[Miss World 1997]] \n* [[Dimple Kapadia]]\n* [[Divya Spandana]] (Ramya)\n* [[Esha Gupta]]\n* [[Hansika Motwani]]\n* [[Hema Malini]]\n* [[Ileana D'Cruz]]\n* [[Jaya Bachchan]]\n* [[Jayanthi (actress)|Jayanthi]]\n* [[Juhi Chawala]] - [[Femina Miss India|Miss India 1984]]\n* [[Kajal Aggarwal]]\n* [[Kajol]]\n* [[Kangana Ranaut]]\n* [[Kareena Kapoor]]\n* [[Karishma Kapoor]]\n* [[Karishma Manandhar]]\n* [[Keerthy Suresh]]\n* [[Keki Adhikari]]\n* [[Koel Mallick]]\n* [[Lakshmi Menon (actress)|Lakshmi Menon]]\n* [[Lakshmi Rai]]\n* [[Lara Dutta]] - [[Miss Universe 2000]], Miss Intercontinental 1997\n* [[Madhubala]]\n* [[Madhuri Dixit]]\n* [[Mahiya Mahi]] \n* [[Manisha Koirala]]\n* [[Meena Kumari]]\n* [[Moushumi]]\n* [[Nandita Das]]\n* [[Nargis]]\n* [[Nayanthara]]\n* [[Neha Dhupia]] - [[Femina Miss India|Miss India 2002]]\n* [[Nicole Faria]] - [[Miss Earth 2010]]\n* [[Nikki Galrani]]\n* [[Nisha Adhikari]]\n* [[Nithya Menon]]\n* [[Noor Jehan]]\n* [[Nutan]]\n* [[Padmini (actress)|Padmini]]\n* [[Parvatii Nair]]\n* [[Popy]]\n* [[Preity Zinta]]\n* [[Prema (actress, born 1977)|Prema]]\n* [[Priyamani]]\n* [[Priyanka Chopra]] - [[Miss World 2000]]\n* [[Priyanka Karki]] - [[Miss Teen]]\n* [[Purnima (Bangladeshi actress)|Purnima]]\n* [[Rachana Banerjee]]\n* [[Rakul Preet Singh]]\n* [[Rani Mukerji]]\n* [[Regina Cassandra]]\n* [[Rekha Thapa]]\n* [[Rekha]]\n* [[Richa Gangopadhyay]]\n* [[Ritika Singh]]\n* [[Ritu Varma]]\n* [[Rituparna Sengupta]]\n* [[Samantha Ruth Prabhu]]\n* [[Sanjjanaa]]\n* [[Sayyeshaa]]\n* [[Shabana Azmi]]\n* [[Shabana]] ([[Dhallywood]] actress)\n* [[Shabnur]]\n* [[Sharmila Tagore]]\n* [[Shriya Saran]]\n* [[Shruti Haasan]]\n* [[Smita Patil]]\n* [[Soha Ali Khan]]\n* [[Sonali Bendre]]\n* [[Soundarya]]\n* [[Sri Divya]]\n* [[Sridevi]]\n* [[Suchitra Sen]]\n* [[Sushmita Sen]] - [[Miss Universe 1994]]\n* [[Swastima Khadka]]\n* [[Tabu (actress)|Tabu]]\n* [[Tamannaah]]\n* [[Trisha Krishnan]]\n* [[Urmila Matondkar]]\n* [[Vidya Balan]]\n* [[Vyjayanthimala]]\n* [[Yukta Mookhey]] - [[Miss World 1999]]\n* [[Zeenat Aman]] - Miss Asia Pacific 1970\n\n==See also==\n* [[Cinema of the world]]\n* [[Alpavirama South Asian Short Film Festival (Alpavirama)]]\n* [[Asian cinema]]\n* [[Dhallywood]] (also known as [[Bangladeshi film industry]])\n* [[Cinema of Bangladesh]]\n* [[Cinema of India]]\n* [[Cinema of Pakistan]]\n* [[Hindi cinema]]\n* [[Telugu cinema]]\n* [[Cinema of West Bengal]]\n* [[Malayalam cinema]]\n* [[Tamil cinema]]\n* [[Kannada cinema]]\n* [[World cinema]]\n* [[Sambalpuri Cinema]]\n*[[List of Hollywood-inspired nicknames]]\n\n==Further reading==\n* ''Contemporary Asian Cinema'', Anne Tereska Ciecko, editor. Berg, 2006. {{ISBN|1-84520-237-6}}\n\n{{Worldcinema}}\n\n==References==\n{{Reflist}}\n\n[[Category:Asian cinema]]\n", "text_old": "{{South Asian cinema}}\n'''South Asian cinema''' refers to the [[film|cinema]] of [[Bangladesh]], [[Bhutan]], [[India]], [[Maldives]], [[Nepal]], [[Pakistan]] and [[Sri Lanka]].<ref>{{cite book|last1=Dickey |first1=Sara  |last2= Dudrah |first2=Rajinder Kumar |title=South Asian Cinemas: Widening the Lens |date=2012}}</ref><ref name=\"Asiancinema\">{{cite book|last1=Teo |first1=Stephen |title=The Asian Cinema Experience: Styles, Spaces, Theory |date=2013}}</ref><ref>{{cite book|last1=Chaudhuri |first1=Shohini |title=Contemporary World Cinema: Europe, the Middle East, East Asia and South Asia |date=2005}}</ref> The terms '''Asian cinema''', '''Eastern cinema''' and '''Oriental cinema''' in common usage often encompass [[South Asia]] as well as [[East Asia]] and [[South East Asia]].<ref name=\"Asiancinema\"/>\n\n[[File:Eastern World.PNG|thumb|right|200px|The [[Far East]] as a cultural block includes [[East Asia]] (green), [[Southeast Asia]] (blue) and [[South Asia]] (orange).]]\n\n==Styles and genres==\nThe scope of South Asian cinema is huge and takes in a wide array of different [[film styles]], linguistic regions and [[Film genre|genres]]. South Asian cinema is particularly famous in the West for:\n\n* [[Melodrama|Melodramatic film]]s\n** [[Action films]]\n** [[Curry Western]]s\n** [[Escapism]]\n** [[Musical film|Musicals]]\n** [[Romance film]]s\n** [[Masala (film genre)|Masala film]]s\n* [[Parallel cinema]]\n** [[Drama (film and television)|Drama film]]s\n** [[Thriller (genre)|Thriller film]]s\n** [[Art film]]s\n** [[Organised crime in India|Indian neo-noir]]\n** [[Neorealism (art)|Neorealism]]\n* [[Heroic bloodshed]]\n* [[Historical drama film|Historical drama]]\n\n==Regional industries==\n===Bangladeshi cinema===\n{{main|Cinema of Bangladesh}}\n* '''[[Cinema of Bangladesh|Bangladeshi film industry]]''', is the [[Bengali language|Bengali]] language film industry based in [[Dhaka]], [[Bangladesh]]. The industry often generally referred to as [[Dhallywood]], has been a significant film industry since the early 1970s. The 1960s, 1970s, 1980s and the first half of the 1990s were the golden years for Bangladeshi films as the industry produced many successful films. The industry has recently begun receiving international acclaim and many Bangladeshi films are getting released internationally.\n* '''[[Dhallywood]]''', is a portmanteau of the words [[Dhaka]] and [[Hollywood]].\n\n===Indian cinema===\n{{main|Cinema of India}}\nIndia contains many [[national languages of India|state languages]] which have film industries centered on them.  Although [[Hindi]] is the official language of government business of northern regions of India, its often-used dialect [[Hindustani language|Hindustani]] is the most widespread language but covering only 40% of the total population, and English is widely understood irrespective of region, the state languages are preserved for official use by different states in India, and many have as many speakers as an average [[European cinema|European]] nation. Regional industries have also tended to produce a higher percentage of serious art film and political film. Bangladeshi cinema is filmed in [[Bengali language|Bengali]] and Sri Lankan cinema is filmed in [[Sinhalese language|Sinhala]] and [[Tamil language|Tamil]]. Last but not least is Indonesian cinema. In the beginning the Indonesian cinema grew after World War I, rooted from the Folk Theater Drama called Dardanela. Under Usmar Ismail, Indonesian cinema became the new entertainment in 1950 to 1980. Hundred of film stars were born, such as: Citra Dewi (1960), Tanty Yosepha (1970). Yenny Rachman and Christine Hakim (1980) and Dian Sastro (late 1990s). [[Teguh Karya]] was one of the leading Film Director in Indonesia after the era of Usmar Ismail. Now, by the popularity of television, film is replaced with electronic cinema which is popular as sinetron. This industry has made the Indian born producer, Raam Punjabi, a tycon of sinetron in Indonesia.\n* '''[[Hindi Cinema]]''', popularly known as ''[[Bollywood]]'', is based in [[Mumbai]]. This film industry is the most prolific and popular in South Asia.\n* '''[[Cinema of Andhra Pradesh|Telugu film industry]]''', popularly known as ''Tollywood'', which comes from the mixture of [[Hollywood]] and [[Telugu language|Telugu]], based in [[Hyderabad, Telangana]]. It is one of the three popular branches of Indian Cinema. It was formerly located in [[Chennai]], [[Tamil Nadu]].\n* '''[[Tamil Cinema]]''', the [[Tamil language|Tamil]] film industry based in the [[Kodambakkam]] area of [[Chennai]] (formerly Madras). It is one of the three popular branches of Indian Cinema. [[Tamil films]] are also produced in [[Sri Lankan Tamil Cinema|Sri Lanka]], [[Malaysia]], [[Singapore]] and [[Canada]].\n* '''[[Chhollywood]]''', the [[Chhattisgarhi language]] based film industry based in the state of [[Chhattisgarh]].\n* '''[[Dogri cinema]]''', Dogri Language cinema of [[Jammu]] region.\n* '''[[Malayalam Cinema]]''', [[Malayalam language|Malayalam]] film industry, sometimes known as ''[[Cinema of Kerala|Mollywood]]'', based in [[Kochi]] and [[Trivandrum]] in [[Kerala]]. Several of its directors such as [[Shaji N. Karun]] have also received international acclaim.\n* '''[[Cinema of Karnataka|Kannada film industry]]''', based in [[Bangalore]], [[Karnataka]].\n* '''[[Cinema of West Bengal|Bengali film industry]]''', long centered in the [[Tollygunge]] area of [[Kolkata]] (formerly Calcutta). This film industry is known for producing many internationally acclaimed films by directors such as [[Satyajit Ray]], [[Budhhadeb Dasgupta]], [[Mrinal Sen]] and [[Ritwik Ghatak]].\n* '''[[Gujarati cinema]]''', based in [[Gujarat]].\n*'''[[Haryanvi Cinema|Haryanvi cinema]]''', Haryanvi language cinema, based in [[Haryana]]\n*'''[[Kashmiri cinema]]''', Kashmiri Language cinema of [[Kashmir valley]].\n*'''[[Cinema of Rajasthan]]''', Based in Rajasthan\n* '''[[Cinema of Odisha]]''', the [[Odia language]] film industry based in [[Bhubaneshwar]] and [[Cuttack]].\n* '''[[Marathi cinema|Marathi film industry]]''', based in [[Mumbai]] and [[Pune]].\n* '''[[Punjabi cinema|Punjabi film industry]]''', based in [[Punjab, India|Punjab]], India.\n* '''[[Cinema of Assam|Assamese film industry]]''', based in [[Assam]]. This is the only major film industry in [[North-East India]].\n*'''[[Tulu cinema]]''' Tulu language speaking regions of Karnataka viz. [[Tulu Nadu]] based in [[Mangalore]].\n*'''[[Santali cinema]]''' Santali speaking regions on Jharkhand, West Bengal, Odisha & Assam.\n\n===Nepali cinema===\n{{main|Cinema of Nepal}}\n* '''[[Cinema of Nepal|Nepali film industry]]''', the [[Cinema of Nepal|Nepali film industry]] based in [[Kathmandu]], has recently begun receiving international acclaim with films such as [[The Black Hen]] (2015), [[kagbeni (film)|Kagbeni]] (2006), [[Dying Candle (film)|''Dying Candle'']] (2016) and others.\n* '''[[Tharu Cinema]]''' based in [[Terai]],'''[[Tharuhat]]''' is the home of the [[Tharu languages]] [[Film industry|cinema]].\n\n===Pakistani cinema===\n{{main|Cinema of Pakistan}}\n* '''[[Balochi cinema]]''', based in [[Quetta]], [[Balochistan]] is the home of Balochi language film productions.\n* '''[[Lollywood]]''', based in [[Lahore]], [[Punjab, Pakistan|Punjab]] is the home of [[Punjabi cinema (Pakistan)|Punjabi cinema]].\n* '''[[Kariwood]]''', based in [[Karachi]], [[Sindh]], [[Pakistan]] is the home of the Urdu cinema.\n* '''[[Pashto cinema]]''', based in [[Peshawar]], [[Khyber Pakhtunkhwa]] is the home of Pashto language film productions.\n* '''[[Sindhi cinema]]''', based in [[Karachi]], [[Sindh]], [[Pakistan]] is the home of the Sindhi language film productions.\n\n===Others===\n* [[Cinema of Sri Lanka|Sri Lankan cinema]]\n* [[Sri Lankan Tamil cinema]]\n* [[Cinema of Bhutan|Bhutanese cinema]]\n\n==Some figures of South Asian cinema==\n===Directors===\n<!-- Deleted image removed: [[File:SatyajitProduction.jpg|right|thumb|300px|Satyajit Ray at work on the set of ''Two''.]] -->\n<!-- ONLY ADD A PERSON TO THIS LIST IF THEY ALREADY HAVE AN ARTICLE IN THE ENGLISH WIKIPEDIA -->\n* [[Lester James Peries]] - (b 1919) Sri Lankan film director\n* [[Satyajit Ray]] \u2013 Bengali film director, widely regarded as one of the greatest [[auteur]]s of 20th century cinema (''[[Apu trilogy]]'').\n* [[Ritwik Ghatak]] \u2013 Bengali film director, (''[[Nagarik]]'', ''[[Meghe Dhaka Tara]]'').\n* [[Mrinal Sen]] \u2013 Bengali film director, has won awards at major film festivals (''Baishey Shravan'', ''Bhuvan Shome'').\n* [[K. Balachander]] \u2013 Tamil director.\n* [[K. Viswanath]] \u2013 Telugu director known for films like ''[[Sankarabharanam (1980 film)|Sankarabharanam]]'', ''[[Swathi Muthyam]]'', ''[[Swayam Krushi]]''.\n* [[Bimal Roy]] \u2013 Hindi film director (''[[Devdas (1955 film)|Devdas]]'', ''[[Do Bigha Zameen]]'').\n* [[Mani Ratnam]] \u2013 Generally works in Tamil films but has worked in Hindi, Malayalam, Telugu and Kannada industries. (''[[Kannathil Muthamittal]]'', ''[[Guru (2007 film)|Guru]]'').\n* [[Guru Dutt]] \u2013 Hindi actor, director and producer of the 1950s and '60s (''[[Mr. & Mrs. '55]]'', ''[[Kaagaz Ke Phool]]'', ''[[Pyaasa]]'').\n* [[V. Shantaram]] \u2013 Hindi director and actor (''[[Do Aankhen Barah Haath]]'').\n* [[Ashutosh Gowariker]] \u2013 Contemporary Hindi actor, director and producer (''[[Lagaan]]'').\n* [[Budhhadeb Dasgupta]] \u2013 ''[[Uttara (film)|Uttara]]'', internationally acclaimed filmmaker known for surrealism and magical realism. \n* [[Vijay Anand (Hindi film maker)|Vijay Anand]] \u2013 Bollywood actor, director and producer mainly during the 1960s and '70s. (''Johnny Mera Naam'', ''Jewel Thief'')\n* [[Hrishikesh Mukherjee]] \u2013 Hindi film director known for (''[[Anand (1971 film)|Anand]]'', ''[[Abhimaan (1973 film)|Abhimaan]]'').\n* [[K. Asif]] \u2013 [[Mughal-e-Azam]]\n* [[Mahboob]]\n* [[Fateh Lohani]]\n* [[Khan Ataur Rahman]]\n* [[Zahir Raihan]]\n* [[Subhash Dutta]]\n* [[Ehtesham]]\n* [[Chashi Nazrul Islam]]\n* [[Alamgir Kabir (film maker)|Alamgir Kabir]]\n* [[Narayan Ghosh Mita]]\n* [[Tareque Masud]]\n* [[Tanvir Mokammel]]\n* [[Morshedul Islam]]\n* [[Humayun Ahmed]] \u2013 One of the most successful writers and directors of [[Bangladesh]].\n* [[Mostofa Sarwar Farooki]]\n* [[Kamar Ahmed Saimon]]\n* [[Amitabh Reza Chowdhury]] \n* [[Abu Shahed Emon]]\n* [[Puttanna Kanagal]] \u2013 (''[[Belli moda]]'').\n* [[Kamal Amrohi]] \u2013 [[Mahal (1949 film)|Mahal]] [[Pakeeza]] [[Raziya Sultan (film)|Razia Sultan]]\n* [[Sonali Gulati]] \u2013 contemporary independent filmmaker, activist, and feminist who has made award-winning documentary and experimental films.\n* [[Nasir Hussain]] \u2013 (''[[Qayamat Se Qayamat Tak]]'')\n* [[Sudhir Mishra]] \u2013 Contemporary director and screenwriter (''[[Hazaaron Khwaishein Aisi]]'', ''[[Chameli (film)|Chameli]]'').\n* [[Bharathiraja]] \u2013 Tamil director who captured village life (''[[Muthal Mariyathai]]'', ''[[Vedham Pudhithu]]'').\n* [[Saawan Kumar]]\n* [[Balu Mahendra]] \u2013 [[Sri Lanka]]-born Tamil and [[Malayalam]] director (''Sandhya Raagam'', ''Veedu'').\n* [[Prakash Jha]] \u2013 Contemporary Hindi director (''[[Gangaajal]]'', ''[[Apaharan]]'').\n* [[Adoor Gopalakrishnan]] \u2013 Malayalam director (''[[Elippathayam]]'', ''[[Swayamvaram (1972 film)|Swayamvaram]]'').\n* [[Mani Shankar]] \u2013 Director of Bollywood action thrillers (''[[16 December (film)|16 December]]'', ''[[Tango Charlie]]'')\n* [[Rakeysh Omprakash Mehra]] \u2013 Director and screenwriter (''[[Aks (2001 film)|Aks]]'', ''[[Rang De Basanti]]'').\n* [[Sanjay Leela Bhansali]] \u2013 (''[[Devdas (2002 Hindi film)|Devdas]]'', ''[[Black (2005 film)|Black]]'')\n* [[S. Shankar]] \u2013  Tamil director and producer (''[[Gentleman (1993 film)|Gentleman]]'', ''[[Indian (1996 film)|Indian]]'', ''[[Mudhalvan]]'', ''[[Anniyan]]'', ''[[Sivaji: The Boss]]'', ''[[Enthiran]]'', ''[[I (film)|I]]'', ''[[2.0 (film)|2.0]]'')\n* [[A. R. Murugadoss]] \u2013 Tamil director (''[[Ghajini (2005 film)|Ghajini]]'', ''[[Ghajini (2008 film)|Ghajini]]'', ''[[Thuppakki]]'', ''[[Kaththi]]'', ''[[Sarkar (2018 film)|Sarkar]]'', ''[[Darbar (2020 film)|Darbar]]'')\n* [[Ram Gopal Varma]] \u2013 Tollywood and Bollywood director known for his gritty films. (''[[Shiva (1989 film)|Shiva]]'', ''[[Rangeela (1995 film)|Rangeela]]'').\n* [[Shekhar Kapur]] \u2013 [[British Raj|British India]]-born director and producer (''[[Elizabeth (film)|Elizabeth]]'', ''[[Bandit Queen]]'').\n* [[Shyam Benegal]] \u2013 Important part of the New India Cinema movement (''[[Ankur (film)|Ankur]]'', ''[[Bhumika (film)|Bhumika]]'').\n* [[S. S. Rajamouli]] \u2013 Commercial Telugu director.\n* [[Upendra (film)|Upendra]] \u2013 (''[[A (1998 Indian film)|A]]'', ''Om'').\n* [[K. S. Ravikumar]] \u2013 Tamil commercial film director (''[[Muthu (1995 film)|Muthu]]'', ''[[Padayappa]]'', ''[[Dasavathaaram]]'')\n* [[Madhur Bhandarkar]] \u2013 Director and screenwriter (''[[Page 3 (film)|Page 3]]'', ''[[Chandni Bar]]'').\n* [[Deepa Mehta]] \u2013 Indian-born Canadian director best known for her \"elements trilogy\". ''[[Fire (1996 film)|Fire]]'', ''[[Earth (1998 film)|Earth]]'', ''[[Water (2005 film)|Water]]'').\n* [[Aparna Sen]] \u2013 Bengali Indian actress and director (''[[36 Chowringhee Lane]]'', ''[[Mr. and Mrs. Iyer]]'').\n* [[Ketan Mehta]] \u2013 (''[[Bhavni Bhavai]]'', ''[[Maya Memsaab]]'').\n* [[Rakesh Roshan]] \u2013 (''[[Karan Arjun]]'', ''[[Krrish]]'').\n* [[Mira Nair]] \u2013 (''[[Monsoon Wedding]]'', ''[[Salaam Bombay!]]'').\n* [[Girish Karnad]] \u2013 (''[[Anand Bhairavi]]'').\n* [[Govind Nihalani]] \u2013 Cinematographer and director.\n* [[Santosh Sivan]] \u2013 Award-winning cinematographer and director (''[[The Terrorist (1997 film)|The Terrorist]]'', ''[[Asoka (2001 film)|Asoka]]'').\n*[[Tulsi Ghimire]] \u2013 Nepali movie director (Known for ''[[Kusume Rumal]]'', ''[[Lahure (film)|Lahure]]'', ''[[Darpan Chaya]]'')\n* [[Gurinder Chadha]] \u2013 British director (''[[Bend It Like Beckham]]'', ''[[Bride and Prejudice]]'').\n* [[Prakash Mehra]] \u2013 (''[[Zanjeer (1973 film)|Zanjeer]]'', ''[[Hera Pheri (1976 film)|Hera Pheri]]'').\n* [[Manmohan Desai]] \u2013 (''[[Parvarish (1977 film)|Parvarish]]'', ''[[Amar Akbar Anthony]]'').\n* [[Basu Chatterjee]] \u2013 (''[[Chitchor]]'').\n* [[Rajkumar Santoshi]] \u2013 (''[[Ghayal (1990 film)|Ghayal]]'', ''[[Andaz Apna Apna]]'').\n* [[Partho Sen-Gupta]] \u2013 Avant-garde independent director (''[[Hava Aney Dey]]'').\n* [[Vikram Bhatt]] \u2013 (''[[Inteha (2003 film)|Inteha]]'', ''[[Deewane Huye Pagal]]'').\n* [[Anurag Kashyap]] - (''[[Gangs of Wasseypur]]'', ''[[Black Friday (2007 film)|Black Friday]]'')\n* [[Sanjay Gupta (Director)|Sanjay Gupta]] \u2013 (''[[Zinda (film)|Zinda]]'')\n* [[Yash Chopra]] \u2013 Veteran producer and director (''[[Waqt (1965 film)|Waqt]]'', ''[[Deewaar]]'').\n* [[Ramesh Sippy]] \u2013 (''[[Sholay]]'', ''[[Andaz (1971 film)|Andaz]]'')\n* [[Vidhu Vinod Chopra]] \u2013 (''[[An Encounter with Faces]]'', ''[[1942: A Love Story]]'').\n* [[Nischal Basnet]] \u2013 (''[[Loot (2012 film)|Loot]]'', ''[[Loot 2]]'')\n* [[Nagathihalli Chandrashekhar]] \u2013 (''[[America! America!!]]'', ''[[Amruthadhaare]]'').\n* [[Yograj Bhat]] \u2013 (''[[Mungaru Male]]'').\n* [[Prashanta Nanda]] \u2013 Oriya film director who won most of the National Awards for his contribution for [[Oriya Film Industry]].\n\n===Actors===\n<!-- ONLY ADD A PERSON TO THIS LIST IF THEY ALREADY HAVE AN ARTICLE IN THE ENGLISH WIKIPEDIA -->\n* [[Aamir Khan]]\n* [[Aaryan Sigdel]]\n* [[Abdur Razzak (actor)|Abdur Razzak]] \n* [[Abhishek Bachchan]]\n* [[Ajay Devgan]]\n* [[Ajith Kumar]]\n* [[Akkineni Nagarjuna]]\n* [[Akkineni Nageswara Rao]]\n* [[Akshay Kumar]]\n* [[Alamgir (actor)|Alamgir]] \n* [[Allu Arjun]]\n* [[Ambareesh]]\n* [[Amitabh Bachchan]]\n* [[Amrish Puri]]\n* [[Anant Nag]]\n* [[Ananta Jalil]] \n* [[Anil Chatterjee]]\n* [[Anil Kapoor]]\n* [[Anmol K.C.]]\n* [[Anubhav Mohanty]]\n* [[Anwar Hossain (actor)|Anwar Hossain]]\n* [[Arifin Shuvoo]]\n* [[Arjun Sarja]]\n* [[Arpan Thapa]]\n* [[Arshad Warsi]]\n* [[Ashok Kumar]]\n* [[ATM Shamsuzzaman]] \n* [[Balakrishna]]\n* [[Balraj Sahni]]\n* [[Bappy Chowdhury]]\n* [[Bhuwan K.C.]]\n* [[Bipin Karki]]\n* [[Biraj Bhatta]]\n* [[Bulbul Ahmed]]\n* [[Chhabi Biswas]]\n* [[Chiranjeet]]\n* [[Chiranjeevi]]\n* [[Chunky Pandey]]\n* [[Daggubati Venkatesh]]\n* [[Dev (Bengali actor)|Deepak Adhikari Dev]]\n* [[Dev Anand]]\n* [[Dhanush]]\n* [[Dharmendra]]\n* [[Dilip Kumar]]\n* [[Dipjol]] \n* [[Dulquer Salmaan]]\n* [[Faisal Rehman]]\n* [[Ferdous Ahmed]]\n* [[Feroz Khan (actor, born 1939)|Feroz Khan]]\n* [[Hamza Ali Abbasi]]\n* [[Hrithik Roshan]]\n* [[Humayun Faridi]] \n* [[Humayun Saeed]]\n* [[Ilias Kanchan]] \n* [[Jackie Shroff]]\n* [[Jagathi Sreekumar]]\n* [[Jayam Ravi]]\n* [[Jayan]]\n* [[Jeet (actor)]]\n* [[Jeetendra]]\n* [[Kamal Hassan]]\n* [[Kazi Maruf]]\n* [[Kishore Kumar]]\n* [[M.G. Ramachandran]]\n* [[Mahesh Babu]]\n* [[Mammooty]]\n* [[Mamnun Hasan Emon]]\n* [[Manna (actor)|Manna]] \n* [[Manoj Bajpai]]\n* [[Manoj Kumar]]\n* [[Mehmood Ali|Mehmood]]\n* [[Mithun Chakraborty]]\n* [[Moammar Rana]]\n* [[Mohammad Ali (actor)|Mohammad Ali]]\n* [[Mohanlal]]\n* [[Mohib Mirza]]\n* [[Mukesh (Malayalam actor)|Mukesh]]\n* [[N. T. Rama Rao Jr.]]\n* [[N. T. Rama Rao]]\n* [[Nadeem Baig (actor)|Nadeem]]\n* [[Nana Patekar]]\n* [[Nani (actor)|Nani]]\n* [[Naseeruddin Shah]]\n* [[Nikhil Upreti]]\n* [[Nivin Pauly]]\n* [[Om Puri]]\n* [[Omar Sani]]\n* [[Pahari Sanyal]]\n* [[Pawan Kalyan]]\n* [[Prabhas]]\n* [[Prabhu Deva]]\n* [[Pran (actor)|Pran]]\n* [[Prithviraj Sukumaran]]\n* [[Prosenjit Chatterjee]] \n* [[Puneeth Rajkumar]]\n* [[R. Madhavan|Madavan]]\n* [[Raaj Kumar]]\n* [[Rabi Ghosh]]\n* [[Rahsaan Islam]] \n* [[Raisul Islam Asad]] \n* [[Raj Kapoor]]\n* [[Rajesh Hamal]]\n* [[Rajesh Khanna]]\n* [[Rajinikanth]]\n* [[Rajkumar (actor)|Rajkumar]]\n* [[Ram Charan]]\n* [[Ramesh Aravind]]\n* [[Ranjit Mallick]]\n* [[Riaz]] \n* [[Rishi Kapoor]]\n* [[Ritesh Deshmukh]]\n* [[Saif Ali Khan]]\n* [[Salman Khan]]\n* [[Salman Shah (actor)|Salman Shah]] \n* [[Sanjay Dutt]]\n* [[Sanjeev Kumar]]\n* [[Saugat Malla]]\n* [[Shaan Shahid|Shaan]]\n* [[Shah Rukh Khan|Shahrukh Khan]]\n* [[Shakib Khan]]\n* [[Shammi Kapoor]]\n* [[Shamoon Abbasi]]\n* [[Shankar Nag]]\n* [[Siddhanta Mahapatra]]\n* [[Siddharth (actor)]]\n* [[Sivaji Ganeshan]]\n* [[Sohel Rana (actor)]] \n* [[Soumitra Chatterjee]] \n* [[Subhash Dutta]]\n* [[Sunil Dutt]]\n* [[Sunny Deol]]\n* [[Suresh Gopi]]\n* [[Surya Sivakumar]]\n* [[Symon Sadik]]\n* [[Tapas Paul]]\n* [[Tapen Chatterjee]]\n* [[Thilakan]]\n* [[Tulsi Chakraborty]]\n* [[Upendra (actor)|Upendra]]\n* [[Uttam Kumar]]\n* [[Uttam Mohanty]]\n* [[Vidyut Jamwal]]\n* [[Vijay (actor)|Vijay]]\n* [[Vijay Sethupathi]]\n* [[Vikram (actor)|Vikram]]\n* [[Vinod Khanna]]\n* [[Vinod Mehra]]\n* [[Vishnuvardhan (actor)|Vishnuvardhan]]\n* [[Vivek Oberoi]]\n* [[Waheed Murad]]\n* [[Wasim (actor)|Wasim]]\n* [[Zafar Iqbal (actor)|Zafar Iqbal]]\n\n===Actresses===\n<!-- ONLY ADD A PERSON TO THIS LIST IF THEY ALREADY HAVE AN ARTICLE IN THE ENGLISH WIKIPEDIA -->\n* [[Achol]]\n* [[Aishwarya Rai]] - [[Miss World 1994]]\n* [[Aishwarya Rajesh]]\n* [[Akshara Haasan]]\n* [[Alisha Pradhan]]\n* [[Amala Paul]]\n* [[Amrita Acharia]] - ([[Game Of Thrones]]) [[Nepali people|Nepali]] [[Actress]]\n* [[Amrita Rao]]\n* [[Andrea Jeremiah]]\n* [[Anjali (film actress)]]\n* [[Anju Ghosh]]\n* [[Anu Emmanuel]]\n* [[Anushka Shetty]]\n* [[Aparajita Mohanty]]\n* [[Apu Biswas]]\n* [[Archita Sahu]]\n* [[Asin Thottumkal]]\n* [[Ayesha Takia]]\n* [[B. Saroja Devi]]\n* [[Barsha Priyadarshini]]\n* [[Bidya Sinha Saha Mim]]\n* [[Bipasha Basu]]\n* [[Bobby (Actress)|Bobby]]\n* [[Bobita]] ( [[Dhallywood]] Actress )\n* [[Catherine Tresa]]\n* [[Celina Jaitly]] - [[Femina Miss India|Miss India 2001]]\n* [[Debashree Roy]]\n* [[Deepika Padukone]] \n* [[Devika Rani]]\n* [[Dia Mirza]] - Miss Asia Pacific 2000\n* [[Diana Hayden]] - [[Miss World 1997]] \n* [[Dimple Kapadia]]\n* [[Divya Spandana]] (Ramya)\n* [[Esha Gupta]]\n* [[Hansika Motwani]]\n* [[Hema Malini]]\n* [[Ileana D'Cruz]]\n* [[Jaya Bachchan]]\n* [[Jayanthi (actress)|Jayanthi]]\n* [[Juhi Chawala]] - [[Femina Miss India|Miss India 1984]]\n* [[Kajal Aggarwal]]\n* [[Kajol]]\n* [[Kangana Ranaut]]\n* [[Kareena Kapoor]]\n* [[Karishma Kapoor]]\n* [[Karishma Manandhar]]\n* [[Keerthy Suresh]]\n* [[Keki Adhikari]]\n* [[Koel Mallick]]\n* [[Lakshmi Menon (actress)|Lakshmi Menon]]\n* [[Lakshmi Rai]]\n* [[Lara Dutta]] - [[Miss Universe 2000]], Miss Intercontinental 1997\n* [[Madhubala]]\n* [[Madhuri Dixit]]\n* [[Mahiya Mahi]] \n* [[Manisha Koirala]]\n* [[Meena Kumari]]\n* [[Moushumi]]\n* [[Nandita Das]]\n* [[Nargis]]\n* [[Nayanthara]]\n* [[Neha Dhupia]] - [[Femina Miss India|Miss India 2002]]\n* [[Nicole Faria]] - [[Miss Earth 2010]]\n* [[Nikki Galrani]]\n* [[Nisha Adhikari]]\n* [[Nithya Menon]]\n* [[Noor Jehan]]\n* [[Nutan]]\n* [[Padmini (actress)|Padmini]]\n* [[Parvatii Nair]]\n* [[Popy]]\n* [[Preity Zinta]]\n* [[Prema (actress, born 1977)|Prema]]\n* [[Priyamani]]\n* [[Priyanka Chopra]] - [[Miss World 2000]]\n* [[Priyanka Karki]] - [[Miss Teen]]\n* [[Purnima (Bangladeshi actress)|Purnima]]\n* [[Rachana Banerjee]]\n* [[Rakul Preet Singh]]\n* [[Rani Mukerji]]\n* [[Regina Cassandra]]\n* [[Rekha Thapa]]\n* [[Rekha]]\n* [[Richa Gangopadhyay]]\n* [[Ritika Singh]]\n* [[Ritu Varma]]\n* [[Rituparna Sengupta]]\n* [[Samantha Ruth Prabhu]]\n* [[Sanjjanaa]]\n* [[Sayyeshaa]]\n* [[Shabana Azmi]]\n* [[Shabana]] ([[Dhallywood]] actress)\n* [[Shabnur]]\n* [[Sharmila Tagore]]\n* [[Shriya Saran]]\n* [[Shruti Haasan]]\n* [[Smita Patil]]\n* [[Soha Ali Khan]]\n* [[Sonali Bendre]]\n* [[Soundarya]]\n* [[Sri Divya]]\n* [[Sridevi]]\n* [[Suchitra Sen]]\n* [[Sushmita Sen]] - [[Miss Universe 1994]]\n* [[Swastima Khadka]]\n* [[Tabu (actress)|Tabu]]\n* [[Tamannaah]]\n* [[Trisha Krishnan]]\n* [[Urmila Matondkar]]\n* [[Vidya Balan]]\n* [[Vyjayanthimala]]\n* [[Yukta Mookhey]] - [[Miss World 1999]]\n* [[Zeenat Aman]] - Miss Asia Pacific 1970\n\n==See also==\n* [[Cinema of the world]]\n* [[Alpavirama South Asian Short Film Festival (Alpavirama)]]\n* [[Asian cinema]]\n* [[Dhallywood]] (also known as [[Bangladeshi film industry]])\n* [[Cinema of Bangladesh]]\n* [[Cinema of India]]\n* [[Cinema of Pakistan]]\n* [[Hindi cinema]]\n* [[Telugu cinema]]\n* [[Cinema of West Bengal]]\n* [[Malayalam cinema]]\n* [[Tamil cinema]]\n* [[Kannada cinema]]\n* [[World cinema]]\n* [[Sambalpuri Cinema]]\n*[[List of Hollywood-inspired nicknames]]\n\n==Further reading==\n* ''Contemporary Asian Cinema'', Anne Tereska Ciecko, editor. Berg, 2006. {{ISBN|1-84520-237-6}}\n\n{{Worldcinema}}\n\n==References==\n{{Reflist}}\n\n[[Category:Asian cinema]]\n", "name_user": "Arjayay", "label": "safe", "comment": "\u2192\u200eDirectors:A > Z", "url_page": "//en.wikipedia.org/wiki/South_Asian_cinema"}
{"title_page": "Kattabomman (film)", "text_new": "{{For|the 1959 film|Veerapandiya Kattabomman (film)}}\n{{Use dmy dates|date=October 2015}}\n{{Use Indian English|date=October 2015}}\n{{Infobox film\n| name           = Kattabomman\n| image          = \n| image_size     =\n| caption        = \n| director       = [[Manivasagam]]\n| producer       = Rajeswari Manivasagam\n| writer         = Manivasagam\n| starring       = {{ubl|[[R. Sarathkumar|Sarath Kumar]]|[[Vineetha]]|[[Nagesh]]|[[Srividya]]|[[Goundamani]]|[[Senthil]]|[[Vijayakumar (actor)|Vijayakumar]]|Sakthivel|[[Kavitha (actress)|Kavitha]]|[[Uday Prakash (actor)|Uday Prakash]]|[[Poonam Dasgupta]]}}\n| music          = [[Deva (music director)|Deva]]\n| cinematography = K. B. Dhayalan\n| editing        = L. Kesavan\n| distributor    = Raja Pushpa Pictures\n| studio         = Raja Pushpa Pictures\n| released       = {{Film date|df=y|1993|11|13}}\n| runtime        = 140 minutes\n| country        = India\n| language       = Tamil\n}}\n\n'''''Kattabomman''''' is a [[Tamil films of 1993|1993]] [[Tamil language|Tamil]] [[comedy-drama film]] directed by [[Manivasagam]]. The film features [[R. Sarathkumar|Sarath Kumar]] and [[Vineetha]] in lead roles. The film, produced by Rajeswari Manivasagam, had musical score by [[Deva (music director)|Deva]] and was released on 13 November 1993 as a Deepavali release .<ref>{{cite web|url=http://www.cinesouth.com/cgi-bin/filmography/newfilmdb.cgi?name=katta%20bomman|title=Filmography of katta bomman|accessdate=2013-01-19| publisher=cinesouth.com}}</ref><ref>{{cite web|url=http://en.600024.com/movie/kattabomman/|title=Kattabomman (1993)|accessdate=2013-01-19|publisher=en.600024.com|archive-url=https://web.archive.org/web/20110606151245/http://en.600024.com/movie/kattabomman/|archive-date=6 June 2011|url-status=dead}}</ref>\n\n==Plot==\n\nThe village chairman Kalingarayan (Sakthivel) and his son Rajappa ([[Uday Prakash (actor)|Uday Prakash]]) spread terror among the villagers. Whereas Kattabomman ([[R. Sarathkumar|Sarath Kumar]]) is an angry man who cannot tolerate injustice. He was brought up by his grandfather ([[Nagesh]]) and his widowed mother. His family and Kalingarayan's family are in a feud for several years. Kattabomman falls in love with Kalingarayan's daughter Priya ([[Vineetha]]) and he marries her despite their families' wishes. In angry, his grandfather tells their past. What transpires later forms the crux of the story.\n\n==Cast==\n{{colbegin}}\n*[[R. Sarathkumar|Sarath Kumar]] as Kattabomman\n*[[Vineetha]] as Priya\n*[[Nagesh]] as Kattabomman's grandfather\n*[[Srividya]] as Devada, Kattabomman's sister-in-law\n*[[Goundamani]] as Subramani\n*[[Senthil]] as Pazhani\n*[[Vijayakumar (actor)|Vijayakumar]] as Ponnurangam, Kattabomman's father\n*Sakthivel as Kalingarayan\n*[[Kavitha (actress)|Kavitha]] as Saroja, Kalingarayan's wife\n*[[Uday Prakash (actor)|Uday Prakash]] as Rajappa\n*[[Poonam Dasgupta]] as Rani\n*[[S. N. Lakshmi]]\n*Varalakshmi as Kattabomman's mother\n*Vahini\n*Prasanna Kumar as Iyer Mama\n*Vasuki as Azhamu Mami\n*Krishnamoorthy\n*[[Karuppu Subbiah]]\n*Tiruppur Ramasamy as Ramasamy \n*Jayamani\n{{colend}}\n\n==Soundtrack==\n\n{{Infobox album\n| name       = Kattabomman\n| type       = [[soundtrack]]\n| artist     = [[Deva (music director)|Deva]]\n| cover      =\n| alt        =\n| released   = 1993\n| recorded   = 1993\n| venue      =\n| studio     =\n| genre      = [[Film soundtrack|Feature film soundtrack]]\n| length     = 22:19\n| label      =\n| producer   = [[Deva (music director)|Deva]]\n| prev_title =\n| prev_year  =\n| next_title =\n| next_year  =\n}}\n\nThe film score and the soundtrack were composed by [[film composer]] [[Deva (music director)|Deva]]. The soundtrack, released in 1993, features 5 tracks with lyrics written by Kalidasan.<ref>{{cite web|url=http://www.hummaa.com/music/album/kattabomman/26659|title=Kattabomman : Tamil Movie|accessdate=2013-01-19|publisher=hummaa.com}}</ref>\n\n{| border=\"2\" cellpadding=\"4\" cellspacing=\"0\" style=\"margin: 1em 1em 1em 0; background: #f9f9f9; border: 1px #aaa solid; border-collapse: collapse; font-size: 95%;\"\n|- bgcolor=\"#CCCCCC\" align=\"center\"\n! Track !! Song !! Singer(s) !! Duration\n|- \n|1 || 'Enga Then Pandi' || [[Malaysia Vasudevan]], [[Swarnalatha]]  || 4:27\n|-\n|2 || 'Koondaivittu' || [[K. J. Yesudas]], [[P. Susheela]] || 4:42\n|-\n|3 || 'Palaivanathil' || [[S. P. Balasubrahmanyam]], [[K. S. Chithra]] || 5:05\n|-\n|4 || 'Priya Priya' || S. P. Balasubrahmanyam, K. S. Chithra || 5:13\n|-\n|5 || 'Thulasi Chediyoram' || [[S. Janaki]] || 4:31\n|}\n\n==Reception==\n''The Indian Express'' wrote \"Kattabomman is a fiasco and the only way to enjoy the film is to close one's eyes and ears\".<ref>https://news.google.com/newspapers?nid=P9oYG7HA76QC&dat=19931112&printsec=frontpage&hl=en</ref>\n\n==References==\n{{reflist}}\n\n[[Category:1993 films]]\n[[Category:Indian films]]\n[[Category:Tamil-language films]]\n[[Category:Films scored by Deva (composer)]]\n[[Category:1990s Tamil-language films]]\n", "text_old": "{{For|the 1959 film|Veerapandiya Kattabomman (film)}}\n{{Use dmy dates|date=October 2015}}\n{{Use Indian English|date=October 2015}}\n{{Infobox film\n| name           = Kattabomman\n| image          = \n| image_size     =\n| caption        = \n| director       = [[Manivasagam]]\n| producer       = Rajeswari Manivasagam\n| writer         = Manivasagam\n| starring       = {{ubl|[[R. Sarathkumar|Sarath Kumar]]|[[Vineetha]]|[[Nagesh]]|[[Srividya]]|[[Goundamani]]|[[Senthil]]|[[Vijayakumar (actor)|Vijayakumar]]|Sakthivel|[[Kavitha (actress)|Kavitha]]|[[Uday Prakash (actor)|Uday Prakash]]|[[Poonam Dasgupta]]}}\n| music          = [[Deva (music director)|Deva]]\n| cinematography = K. B. Dhayalan\n| editing        = L. Kesavan\n| distributor    = Raja Pushpa Pictures\n| studio         = Raja Pushpa Pictures\n| released       = {{Film date|df=y|1993|11|13}}\n| runtime        = 140 minutes\n| country        = India\n| language       = Tamil\n}}\n\n'''''Kattabomman''''' is a [[Tamil films of 1993|1993]] [[Tamil language|Tamil]] [[comedy-drama film]] directed by [[Manivasagam]]. The film features [[R. Sarathkumar|Sarath Kumar]] and [[Vineetha]] in lead roles. The film, produced by Rajeswari Manivasagam, had musical score by [[Deva (music director)|Deva]] and was released on 13 November 1993 as a Deepavali release .<ref>{{cite web|url=http://www.cinesouth.com/cgi-bin/filmography/newfilmdb.cgi?name=katta%20bomman|title=Filmography of katta bomman|accessdate=2013-01-19| publisher=cinesouth.com}}</ref><ref>{{cite web|url=http://en.600024.com/movie/kattabomman/|title=Kattabomman (1993)|accessdate=2013-01-19|publisher=en.600024.com|archive-url=https://web.archive.org/web/20110606151245/http://en.600024.com/movie/kattabomman/|archive-date=6 June 2011|url-status=dead}}</ref>\n\n==Plot==\n\nThe village chairman Kalingarayan (Sakthivel) and his son Rajappa ([[Uday Prakash (actor)|Uday Prakash]]) spread terror among the villagers. Whereas Kattabomman ([[R. Sarathkumar|Sarath Kumar]]) is an angry man who cannot tolerate injustice. He was brought up by his grandfather ([[Nagesh]]) and his widowed mother. His family and Kalingarayan's family are in a feud for several years. Kattabomman falls in love with Kalingarayan's daughter Priya ([[Vineetha]]) and he gets married to her without their family's wishes. In angry, his grandfather tells their past. What transpires later forms the crux of the story.\n\n==Cast==\n{{colbegin}}\n*[[R. Sarathkumar|Sarath Kumar]] as Kattabomman\n*[[Vineetha]] as Priya\n*[[Nagesh]] as Kattabomman's grandfather\n*[[Srividya]] as Devada, Kattabomman's sister-in-law\n*[[Goundamani]] as Subramani\n*[[Senthil]] as Pazhani\n*[[Vijayakumar (actor)|Vijayakumar]] as Ponnurangam, Kattabomman's father\n*Sakthivel as Kalingarayan\n*[[Kavitha (actress)|Kavitha]] as Saroja, Kalingarayan's wife\n*[[Uday Prakash (actor)|Uday Prakash]] as Rajappa\n*[[Poonam Dasgupta]] as Rani\n*[[S. N. Lakshmi]]\n*Varalakshmi as Kattabomman's mother\n*Vahini\n*Prasanna Kumar as Iyer Mama\n*Vasuki as Azhamu Mami\n*Krishnamoorthy\n*[[Karuppu Subbiah]]\n*Tiruppur Ramasamy as Ramasamy \n*Jayamani\n{{colend}}\n\n==Soundtrack==\n\n{{Infobox album\n| name       = Kattabomman\n| type       = [[soundtrack]]\n| artist     = [[Deva (music director)|Deva]]\n| cover      =\n| alt        =\n| released   = 1993\n| recorded   = 1993\n| venue      =\n| studio     =\n| genre      = [[Film soundtrack|Feature film soundtrack]]\n| length     = 22:19\n| label      =\n| producer   = [[Deva (music director)|Deva]]\n| prev_title =\n| prev_year  =\n| next_title =\n| next_year  =\n}}\n\nThe film score and the soundtrack were composed by [[film composer]] [[Deva (music director)|Deva]]. The soundtrack, released in 1993, features 5 tracks with lyrics written by Kalidasan.<ref>{{cite web|url=http://www.hummaa.com/music/album/kattabomman/26659|title=Kattabomman : Tamil Movie|accessdate=2013-01-19|publisher=hummaa.com}}</ref>\n\n{| border=\"2\" cellpadding=\"4\" cellspacing=\"0\" style=\"margin: 1em 1em 1em 0; background: #f9f9f9; border: 1px #aaa solid; border-collapse: collapse; font-size: 95%;\"\n|- bgcolor=\"#CCCCCC\" align=\"center\"\n! Track !! Song !! Singer(s) !! Duration\n|- \n|1 || 'Enga Then Pandi' || [[Malaysia Vasudevan]], [[Swarnalatha]]  || 4:27\n|-\n|2 || 'Koondaivittu' || [[K. J. Yesudas]], [[P. Susheela]] || 4:42\n|-\n|3 || 'Palaivanathil' || [[S. P. Balasubrahmanyam]], [[K. S. Chithra]] || 5:05\n|-\n|4 || 'Priya Priya' || S. P. Balasubrahmanyam, K. S. Chithra || 5:13\n|-\n|5 || 'Thulasi Chediyoram' || [[S. Janaki]] || 4:31\n|}\n\n==Reception==\n''The Indian Express'' wrote \"Kattabomman is a fiasco and the only way to enjoy the film is to close one's eyes and ears\".<ref>https://news.google.com/newspapers?nid=P9oYG7HA76QC&dat=19931112&printsec=frontpage&hl=en</ref>\n\n==References==\n{{reflist}}\n\n[[Category:1993 films]]\n[[Category:Indian films]]\n[[Category:Tamil-language films]]\n[[Category:Films scored by Deva (composer)]]\n[[Category:1990s Tamil-language films]]\n", "name_user": "Jellysandwich0", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Kattabomman_(film)"}
{"title_page": "Automatic summarization", "text_new": "{{Multiple issues|\n{{more footnotes|date=March 2015}}\n{{tone|date=March 2015}}\n}}\n\n'''Automatic summarization''' is the process of shortening a set of data computationally, to create a subset (a [[Abstract (summary)|summary]]) that represents the most important or relevant information within the original content. \n\nIn addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document<ref name=\"Torres2014\">{{cite book|author1=Torres-Moreno, Juan-Manuel|title=Automatic Text Summarization|url=https://www.wiley.com/en-gb/Automatic+Text+Summarization-p-9781848216686|date=1 October 2014|publisher=Wiley|isbn=978-1-848-21668-6|pages=320\u2013}}</ref>; image summarization finds the most representative images within an image collection {{citation needed|date=February 2019}}; video summarization extracts the most important frames from the video content.<ref name=\"PalPetrosino2012\">{{cite book|author1=Sankar K. Pal|author2=Alfredo Petrosino|author3=Lucia Maddalena|title=Handbook on Soft Computing for Video Surveillance|url=https://books.google.com/?id=O0fNBQAAQBAJ&pg=PA81&dq=video+surveillance+summarization#v=onepage&q=summarization&f=false|date=25 January 2012|publisher=CRC Press|isbn=978-1-4398-5685-7|pages=81\u2013}}</ref>\n\n==Approaches==\n\nThere are two general approaches to automatic summarization: [[Information extraction|extraction]] and [[abstract (summary)|abstraction]]. \n\n===Extraction-based summarization===\n\nHere, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.<ref> Richard Sutz, Peter Weverka. How to skim text. https://www.dummies.com/education/language-arts/speed-reading/how-to-skim-text/ Accessed Dec 2019. </ref>\n\n===Abstraction-based summarization===\n\nThis has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by [[automated paraphrasing|paraphrasing]] sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both [[natural language processing]] and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.\n\n===Aided summarization===\n\nApproaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\n\n==Applications and systems for summarization==\n<!--needs to make more clear about how to categorize the type of summaries-->\n\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is ''generic summarization'', which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  ''query relevant summarization'', sometimes called ''query-based summarization'', which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a [[cluster analysis|cluster]] of articles on the same topic). This problem is called [[multi-document summarization]]. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.<ref>Jorge E. Camargo and Fabio A. Gonz\u00e1lez. A Multi-class Kernel Alignment Method for Image Collection Summarization. In Proceedings of the 14th Iberoamerican Conference on Pattern Recognition: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications (CIARP '09), Eduardo Bayro-Corrochano and Jan-Olof Eklundh (Eds.). Springer-Verlag, Berlin, Heidelberg, 545-552.  {{DOI|10.1007/978-3-642-10268-4_64}}</ref> A summary in this context is useful to show the most representative images of results in an [[image collection exploration]] system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n\nAt a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the ''core-set''. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, [[Submodular set function]], [[Determinantal point process]], maximal marginal relevance (MMR) etc.\n\n===Keyphrase extraction===\nThe task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.<ref>{{Cite book |doi = 10.1007/978-3-319-66939-7_19|chapter = SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation|title = Advances in Computational Intelligence Systems|volume = 650|pages = 222\u2013235|series = Advances in Intelligent Systems and Computing|year = 2018|last1 = Alrehamy|first1 = Hassan H|last2 = Walker|first2 = Coral|isbn = 978-3-319-66938-0}}</ref> In the case of [[research article]]s, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\nConsider the example text from a news article:\n\n:\"The Army Corps of Engineers, rushing to meet President Bush's promise to protect New Orleans by the start of the 2006 hurricane season, installed defective flood-control pumps last year despite warnings from its own expert that the equipment would fail during a storm, according to documents obtained by The Associated Press\".\n\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep [[natural language understanding|understanding of the text]], which makes it difficult for a computer system.\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve [[information retrieval]] (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a [[full-text search]]), and be employed in generating index entries for a large text corpus.\n\nDepending on the different literature and the definition of key terms, words or phrases, [[keyword extraction]] is a highly related theme.\n\n====Supervised learning approaches====\nBeginning with the work of Turney,<ref>{{Cite journal |arxiv = cs/0212020|last1 = Turney|first1 = Peter D|title = Learning Algorithms for Keyphrase Extraction|journal = Information Retrieval, )|volume = 2|issue = 4|pages = 303\u2013336|year = 2002|doi = 10.1023/A:1009976227802}}</ref> many researchers have approached keyphrase extraction as a [[supervised machine learning]] problem.\nGiven a document, we construct an example for each [[unigram]], [[bigram]], and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a [[binary classification]] for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\nKeyphrase extractors are generally evaluated using precision and recall. Precision measures how\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\nharmonic mean of the two (''F''&nbsp;=&nbsp;2''PR''/(''P''&nbsp;+&nbsp;''R'') ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\n\nDesigning a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\n\nWe also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney\u2019s seminal paper.\n\nIn the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\n\nOnce examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, [[Naive Bayes]], and rule induction. In the case of Turney's GenEx algorithm, a [[genetic algorithm]] is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\n\n====Unsupervised approach: TextRank====\nAnother keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of [[training set|training data]]. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm<ref>Rada Mihalcea and Paul Tarau, 2004: ''TextRank: Bringing Order into Texts'', Department of Computer Science University of North Texas {{cite web|url=http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf |title=Archived copy |accessdate=2012-07-20 |url-status=dead |archiveurl=https://web.archive.org/web/20120617170501/http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf |archivedate=2012-06-17 }}</ref> exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that [[PageRank]] selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from [[social network]]s. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\n\nTextRank is a general purpose [[Graph (abstract data type)|graph]]-based ranking algorithm for [[natural language processing|NLP]]. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or [[lexical (semiotics)|lexical]] [[semantic similarity|similarity]] between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to [[eigenvalue]] 1 (i.e., the [[stationary distribution]] of the [[random walk]] on the graph).\n\nThe vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\n\nEdges are created based on word [[co-occurrence]] in this application of TextRank. Two vertices are connected by an edge if the [[unigram]]s appear within a window of size N in the original text. N is typically around 2\u201310. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text [[Cohesion (linguistics)|cohesion]]\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\n\nSince this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\n\nIt is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\n\nIn short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\n\n===Document summarization===\nLike keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units\u2014whole sentences instead of words and phrases.\n\nBefore getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated. The most common way is using the so-called [[ROUGE (metric)|ROUGE]] (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.\n\nIf there are multiple references, the ROUGE-1 scores are averaged. Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner. High-order n-gram ROUGE measures try to judge fluency to some degree.\nNote that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.\n\nA promising line in document summarization is adaptive document/text summarization.<ref>{{Cite journal | doi=10.3103/S0005105510030027|title = Automatic genre recognition and adaptive text summarization| journal=Automatic Documentation and Mathematical Linguistics| volume=44| issue=3| pages=111\u2013120|year = 2010|last1 = Yatsko|first1 = V. A.| last2=Starikov| first2=M. S.| last3=Butakov| first3=A. V.}}</ref> The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre. First summarizes that perform adaptive summarization have been created.<ref>[http://yatsko.zohosites.com/universal-summarizer-unis.html UNIS (Universal Summarizer)]</ref>\n\n====Supervised learning approaches====\nSupervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.\n\n====Maximum entropy-based summarization====\nDuring the DUC 2001 and 2002 evaluation workshops, [[Netherlands Organisation for Applied Scientific Research|TNO]] developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a [[naive Bayes]] classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a [[maximum entropy classifier|maximum entropy]] (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\n\n====TextRank and LexRank====\nThe unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"[[centroid]]\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\n\nA more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank<ref>G\u00fcne\u015f Erkan and Dragomir R. Radev: ''LexRank: Graph-based Lexical Centrality as Salience in Text Summarization [https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html]''</ref> is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n\nIn both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\n\nThe edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses [[cosine similarity]] of [[TF-IDF]] vectors, TextRank uses a very similar measure based on the number of words two sentences have in common ([[Quantile normalization|normalized]] by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous [[similarity score]]s as weights.\n\nIn both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\n\nIt is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ([[MEAD]]) that combines the LexRank score (stationary probability) with other features like sentence position and length using a [[linear combination]] with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\n\nAnother important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization. The task remains the same in both cases\u2014only the number of sentences to choose from has grown. However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary. The method used is called Cross-Sentence Information Subsumption (CSIS).\n\nThese methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. The importance of this sentence also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\n\n====Multi-document summarization====\n{{Main|Multi-document summarization}}\n'''Multi-document summarization''' is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]]. Multi-document summarization may also be done in response to a question.<ref>\"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]\", International Journal of Intelligent Information Database Systems, 5(2), 119-142, 2011.</ref>\n\nMulti-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\nAutomatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. {{dubious|date=June 2018}}\n\n=====Incorporating diversity=====\nMulti-document extractive summarization faces a problem of potential redundancy. Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),<ref>Carbonell, Jaime, and Jade Goldstein. \"[https://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/jgc/publication/MMR_DiversityBased_Reranking_SIGIR_1998.pdf The use of MMR, diversity-based reranking for reordering documents and producing summaries].\" Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1998.</ref> in trying to eliminate redundancy in information retrieval results. There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on [[absorbing Markov chain]] random walks. (An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.) The algorithm is called GRASSHOPPER.<ref>Zhu, Xiaojin, et al. \"[http://www.aclweb.org/anthology/N07-1013 Improving Diversity in Ranking using Absorbing Random Walks].\" HLT-NAACL. 2007.</ref> In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\n\nThe state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.<ref>Hui Lin, Jeff Bilmes. \"[https://arxiv.org/pdf/1210.4871 Learning mixtures of submodular shells with application to document summarization]</ref> Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.<ref>Alex Kulesza and Ben Taskar, [http://www.nowpublishers.com/article/DownloadSummary/MAL-044 Determinantal point processes for machine learning]. Foundations and Trends in Machine Learning, December 2012.</ref>\n\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed. This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\n\n===Submodular functions as generic tools for summarization===\nThe idea of a [[submodular set function]] has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of ''coverage'', ''information'', ''representation'' and ''diversity''. Moreover, several important [[combinatorial optimization]] problems occur as special instances of submodular optimization. For example, the [[set cover problem]] is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which ''cover'' a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the [[facility location problem]] is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a [[determinantal point process]] to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\n\nWhile submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple [[greedy algorithm]] admits a constant factor guarantee.<ref>Nemhauser, George L., Laurence A. Wolsey, and Marshall L. Fisher. \"An analysis of approximations for maximizing submodular set functions\u2014I.\" Mathematical Programming 14.1 (1978): 265-294.</ref> Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\n\nSubmodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012<ref>Hui Lin, Jeff Bilmes. \"[https://arxiv.org/pdf/1210.4871 Learning mixtures of submodular shells with application to document summarization]\", UAI, 2012</ref> shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,<ref>Hui Lin, Jeff Bilmes. \"[http://www.aclweb.org/anthology/P11-1052 A Class of Submodular Functions for Document Summarization]\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011</ref> shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.{{citation needed|date=June 2018}}\n\nSubmodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show<ref>Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, [http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf Learning Mixtures of Submodular Functions for Image Collection Summarization], In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.</ref> that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015<ref>Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, [http://www.aclweb.org/anthology/P15-1054 Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures], To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015</ref> show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.<ref>Kai Wei, Rishabh Iyer, and Jeff Bilmes, [http://www.jmlr.org/proceedings/papers/v37/wei15.pdf Submodularity in Data Subset Selection and Active Learning], To Appear In Proc. International Conference on Machine Learning (ICML), Lille, France, June - 2015</ref>\n\n===Applications===\n{{Expand section|date=February 2017}}\nSpecific applications of automatic summarization include:\n* The [[Reddit]] [[Internet bot|bot]] \"autotldr\",<ref>{{cite web|title=overview for autotldr|url=https://www.reddit.com/user/autotldr|website=reddit|accessdate=9 February 2017|language=en}}</ref> created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.<ref>{{cite book|last1=Squire|first1=Megan|title=Mastering Data Mining with Python \u2013 Find patterns hidden in your data|publisher=Packt Publishing Ltd|isbn=9781785885914|url=https://books.google.com/books?id=_qXWDQAAQBAJ&pg=PA185|accessdate=9 February 2017|language=en|date=2016-08-29}}</ref> The name is reference to [[TL;DR]] \u2212 [[Internet slang]] for \"too long; didn't read\".<ref>{{cite web|title=What Is 'TLDR'?|url=https://www.lifewire.com/what-is-tldr-2483633|website=Lifewire|accessdate=9 February 2017}}</ref><ref>{{cite web|title=What Does TL;DR Mean? AMA? TIL? Glossary Of Reddit Terms And Abbreviations|url=http://www.ibtimes.com/what-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704|publisher=International Business Times|accessdate=9 February 2017|date=29 March 2012}}</ref>\n\n==Evaluation techniques==\n<!-- IMPORTANT: This section needs to be tied in to the above article so it fits in.  Currently, it is not clear what the relation of evaluation is to any of the above topics. The following questions need to be answered: First, in the context of automatic summarization, what is evaluation?  Second, what is the significance of evaluation?  That is, what is evaluation used for?\n-->\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\n\nEvaluation techniques fall into intrinsic and extrinsic,<ref>[http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings2/sum-mani.pdf Mani, I. Summarization evaluation: an overview]</ref> inter-textual and intra-textual.<ref>{{Cite journal | doi=10.3103/S0005105507030041|title = A method for evaluating modern systems of automatic text summarization| journal=Automatic Documentation and Mathematical Linguistics| volume=41| issue=3| pages=93\u2013103|year = 2007|last1 = Yatsko|first1 = V. A.| last2=Vishnyakov| first2=T. N.}}</ref>\n\n=== Intrinsic and extrinsic evaluation ===\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task. Intrinsic evaluations have\nassessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\n\n=== Inter-textual and intra-textual ===\nIntra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.\n\nHuman judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning [[coherence (linguistics)|coherence]] and coverage.\n\nOne of the metrics used in [[NIST]]'s annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [https://web.archive.org/web/20060408135021/http://haydn.isi.edu/ROUGE/]). It essentially calculates [[n-gram]] overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary's coherence. [[anaphora (linguistics)|Anaphor resolution]] remains another problem yet to be fully solved. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.<ref>Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, [http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf Learning Mixtures of Submodular Functions for Image Collection Summarization], In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014. (PDF)</ref>\n\n===Domain specific versus domain independent summarization techniques===\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments. Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text. For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.<ref>{{Cite book|last1=Sarker|first1=Abeed|last2=Molla|first2=Diego|last3=Paris|first3=Cecile|title=An Approach for Query-focused Text Summarization for Evidence-based medicine|date=2013|volume=7885|pages=295\u2013304|doi=10.1007/978-3-642-38326-7_41|series=Lecture Notes in Computer Science|isbn=978-3-642-38325-0}}</ref>\n\n===Evaluating summaries qualitatively===\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries. Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\n\n==See also==\n* [[Sentence extraction]]\n* [[Text mining]]\n* [[Multi-document summarization]]\n\n==References==\n{{Reflist|2}}\n\n== Further reading ==\n*{{cite book |last=Hercules |first=Dalianis |year=2003 |title=Porting and evaluation of automatic summarization|url=https://www.researchgate.net/publication/277288103}}\n*{{cite book |last=Roxana |first=Angheluta |year=2002 |title=The Use of Topic Segmentation for Automatic Summarization|url=https://www.researchgate.net/publication/2553088}}\n*{{cite book |last=Anne |first=Buist |year=2004 |title=Automatic Summarization of Meeting Data: A Feasibility Study|url=http://www.cs.ru.nl/~kraaijw/pubs/Biblio/papers/meeting_sum_tno.pdf}}\n*{{cite book |last=Annie |first=Louis |year=2009 |title=Performance Confidence Estimation for Automatic Summarization|url=https://repository.upenn.edu/cgi/viewcontent.cgi?article=1762&context=cis_papers}}\n*{{cite book |last=Elena |first=Lloret and Manuel, Palomar  |year=2009 |title=Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation|url=http://www.informatica.si/ojs-2.4.3/index.php/informatica/article/download/273/269}}\n*{{cite book |last=Andrew |first=Goldberg |year=2007 |title=Automatic Summarization}}\n*{{cite book |last=Alrehamy |first=Hassan |year=2017 |title=Automatic Keyphrases Extraction |volume=650 |pages=222\u2013235 |doi=10.1007/978-3-319-66939-7_19 |chapter=SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation |series=Advances in Intelligent Systems and Computing |isbn=978-3-319-66938-0 }}\n*{{cite book |last=Endres-Niggemeyer |first=Brigitte |year=1998 |title=Summarizing Information |url=https://archive.org/details/springer_10.1007-978-3-642-72025-3 |isbn=978-3-540-63735-6}}\n*{{cite book |last=Marcu |first=Daniel |year=2000 |title=The Theory and Practice of Discourse Parsing and Summarization |isbn=978-0-262-13372-2}}\n*{{cite book |last=Mani |first=Inderjeet |year=2001 |title=Automatic Summarization |isbn=978-1-58811-060-2}}\n*{{cite book |last=Huff |first=Jason |year=2010 |title=AutoSummarize |url=http://www.jason-huff.com/projects/autosummarize/}}, Conceptual artwork using automatic summarization software in Microsoft Word 2008.\n*{{cite book |last=Lehmam |first=Abderrafih |year=2010 |title=Essential summarizer: innovative automatic text summarization software in twenty languages - ACM Digital Library |pages=216\u2013217 |url=http://portal.acm.org/citation.cfm?id=1937055.1937111&coll=DL&dl=GUIDE&CFID=23185814&CFTOKEN=40272014/|series=Riao '10 }}, Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\n*{{cite book |last=Xiaojin |first=Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski |year=2007 |title=Improving diversity in ranking using absorbing random walks |url=http://pages.cs.wisc.edu/~jerryzhu/pub/grasshopper.pdf}}, The GRASSHOPPER algorithm\n*{{cite book |last=Miranda-Jim\u00e9nez |first=Sabino, Gelbukh, Alexander, and Sidorov, Grigori |year=2013 |doi=10.1007/978-3-642-35786-2_18 |title=Conceptual Structures for STEM Research and Education |volume=7735 |pages=245\u2013253 |series=Lecture Notes in Computer Science |isbn=978-3-642-35785-5 |chapter=Summarizing Conceptual Graphs for Automatic Summarization Task }}, Conceptual Structures for STEM Research and Education.\n\n{{Natural Language Processing}}\n\n[[Category:Computational linguistics]]\n[[Category:Natural language processing]]\n[[Category:Tasks of natural language processing]]\n[[Category:Data mining]]\n", "text_old": "{{Multiple issues|\n{{more footnotes|date=March 2015}}\n{{tone|date=March 2015}}\n}}\n\n'''Automatic summarization''' is the process of shortening a set of data computationally, to create a subset (a [[Abstract (summary)|summary]]) that represents the most important or relevant information within the original content. \n\nIn addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document<ref name=\"Torres2014\">{{cite book|author1=Torres-Moreno, Juan-Manuel|title=Automatic Text Summarization|url=https://www.wiley.com/en-gb/Automatic+Text+Summarization-p-9781848216686|date=1 October 2014|publisher=Wiley|isbn=978-1-848-21668-6|pages=320\u2013}}</ref>\n; image summarization finds the most representative images within an image collection {{citation needed|date=February 2019}}; video summarization extracts the most important frames from the video content.<ref name=\"PalPetrosino2012\">{{cite book|author1=Sankar K. Pal|author2=Alfredo Petrosino|author3=Lucia Maddalena|title=Handbook on Soft Computing for Video Surveillance|url=https://books.google.com/?id=O0fNBQAAQBAJ&pg=PA81&dq=video+surveillance+summarization#v=onepage&q=summarization&f=false|date=25 January 2012|publisher=CRC Press|isbn=978-1-4398-5685-7|pages=81\u2013}}</ref>\n\n==Approaches==\n\nThere are two general approaches to automatic summarization: [[Information extraction|extraction]] and [[abstract (summary)|abstraction]]. \n\n===Extraction-based summarization===\n\nHere, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.<ref> Richard Sutz, Peter Weverka. How to skim text. https://www.dummies.com/education/language-arts/speed-reading/how-to-skim-text/ Accessed Dec 2019. </ref>\n\n===Abstraction-based summarization===\n\nThis has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by [[automated paraphrasing|paraphrasing]] sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both [[natural language processing]] and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.\n\n===Aided summarization===\n\nApproaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\n\n==Applications and systems for summarization==\n<!--needs to make more clear about how to categorize the type of summaries-->\n\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is ''generic summarization'', which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  ''query relevant summarization'', sometimes called ''query-based summarization'', which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a [[cluster analysis|cluster]] of articles on the same topic). This problem is called [[multi-document summarization]]. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.<ref>Jorge E. Camargo and Fabio A. Gonz\u00e1lez. A Multi-class Kernel Alignment Method for Image Collection Summarization. In Proceedings of the 14th Iberoamerican Conference on Pattern Recognition: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications (CIARP '09), Eduardo Bayro-Corrochano and Jan-Olof Eklundh (Eds.). Springer-Verlag, Berlin, Heidelberg, 545-552.  {{DOI|10.1007/978-3-642-10268-4_64}}</ref> A summary in this context is useful to show the most representative images of results in an [[image collection exploration]] system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n\nAt a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the ''core-set''. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, [[Submodular set function]], [[Determinantal point process]], maximal marginal relevance (MMR) etc.\n\n===Keyphrase extraction===\nThe task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.<ref>{{Cite book |doi = 10.1007/978-3-319-66939-7_19|chapter = SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation|title = Advances in Computational Intelligence Systems|volume = 650|pages = 222\u2013235|series = Advances in Intelligent Systems and Computing|year = 2018|last1 = Alrehamy|first1 = Hassan H|last2 = Walker|first2 = Coral|isbn = 978-3-319-66938-0}}</ref> In the case of [[research article]]s, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\nConsider the example text from a news article:\n\n:\"The Army Corps of Engineers, rushing to meet President Bush's promise to protect New Orleans by the start of the 2006 hurricane season, installed defective flood-control pumps last year despite warnings from its own expert that the equipment would fail during a storm, according to documents obtained by The Associated Press\".\n\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep [[natural language understanding|understanding of the text]], which makes it difficult for a computer system.\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve [[information retrieval]] (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a [[full-text search]]), and be employed in generating index entries for a large text corpus.\n\nDepending on the different literature and the definition of key terms, words or phrases, [[keyword extraction]] is a highly related theme.\n\n====Supervised learning approaches====\nBeginning with the work of Turney,<ref>{{Cite journal |arxiv = cs/0212020|last1 = Turney|first1 = Peter D|title = Learning Algorithms for Keyphrase Extraction|journal = Information Retrieval, )|volume = 2|issue = 4|pages = 303\u2013336|year = 2002|doi = 10.1023/A:1009976227802}}</ref> many researchers have approached keyphrase extraction as a [[supervised machine learning]] problem.\nGiven a document, we construct an example for each [[unigram]], [[bigram]], and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a [[binary classification]] for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\nKeyphrase extractors are generally evaluated using precision and recall. Precision measures how\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\nharmonic mean of the two (''F''&nbsp;=&nbsp;2''PR''/(''P''&nbsp;+&nbsp;''R'') ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\n\nDesigning a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\n\nWe also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney\u2019s seminal paper.\n\nIn the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\n\nOnce examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, [[Naive Bayes]], and rule induction. In the case of Turney's GenEx algorithm, a [[genetic algorithm]] is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\n\n====Unsupervised approach: TextRank====\nAnother keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of [[training set|training data]]. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm<ref>Rada Mihalcea and Paul Tarau, 2004: ''TextRank: Bringing Order into Texts'', Department of Computer Science University of North Texas {{cite web|url=http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf |title=Archived copy |accessdate=2012-07-20 |url-status=dead |archiveurl=https://web.archive.org/web/20120617170501/http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf |archivedate=2012-06-17 }}</ref> exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that [[PageRank]] selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from [[social network]]s. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\n\nTextRank is a general purpose [[Graph (abstract data type)|graph]]-based ranking algorithm for [[natural language processing|NLP]]. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or [[lexical (semiotics)|lexical]] [[semantic similarity|similarity]] between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to [[eigenvalue]] 1 (i.e., the [[stationary distribution]] of the [[random walk]] on the graph).\n\nThe vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\n\nEdges are created based on word [[co-occurrence]] in this application of TextRank. Two vertices are connected by an edge if the [[unigram]]s appear within a window of size N in the original text. N is typically around 2\u201310. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text [[Cohesion (linguistics)|cohesion]]\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\n\nSince this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\n\nIt is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\n\nIn short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\n\n===Document summarization===\nLike keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units\u2014whole sentences instead of words and phrases.\n\nBefore getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated. The most common way is using the so-called [[ROUGE (metric)|ROUGE]] (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.\n\nIf there are multiple references, the ROUGE-1 scores are averaged. Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner. High-order n-gram ROUGE measures try to judge fluency to some degree.\nNote that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.\n\nA promising line in document summarization is adaptive document/text summarization.<ref>{{Cite journal | doi=10.3103/S0005105510030027|title = Automatic genre recognition and adaptive text summarization| journal=Automatic Documentation and Mathematical Linguistics| volume=44| issue=3| pages=111\u2013120|year = 2010|last1 = Yatsko|first1 = V. A.| last2=Starikov| first2=M. S.| last3=Butakov| first3=A. V.}}</ref> The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre. First summarizes that perform adaptive summarization have been created.<ref>[http://yatsko.zohosites.com/universal-summarizer-unis.html UNIS (Universal Summarizer)]</ref>\n\n====Supervised learning approaches====\nSupervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.\n\n====Maximum entropy-based summarization====\nDuring the DUC 2001 and 2002 evaluation workshops, [[Netherlands Organisation for Applied Scientific Research|TNO]] developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a [[naive Bayes]] classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a [[maximum entropy classifier|maximum entropy]] (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\n\n====TextRank and LexRank====\nThe unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"[[centroid]]\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\n\nA more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank<ref>G\u00fcne\u015f Erkan and Dragomir R. Radev: ''LexRank: Graph-based Lexical Centrality as Salience in Text Summarization [https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html]''</ref> is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n\nIn both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\n\nThe edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses [[cosine similarity]] of [[TF-IDF]] vectors, TextRank uses a very similar measure based on the number of words two sentences have in common ([[Quantile normalization|normalized]] by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous [[similarity score]]s as weights.\n\nIn both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\n\nIt is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ([[MEAD]]) that combines the LexRank score (stationary probability) with other features like sentence position and length using a [[linear combination]] with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\n\nAnother important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization. The task remains the same in both cases\u2014only the number of sentences to choose from has grown. However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary. The method used is called Cross-Sentence Information Subsumption (CSIS).\n\nThese methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. The importance of this sentence also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\n\n====Multi-document summarization====\n{{Main|Multi-document summarization}}\n'''Multi-document summarization''' is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]]. Multi-document summarization may also be done in response to a question.<ref>\"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]\", International Journal of Intelligent Information Database Systems, 5(2), 119-142, 2011.</ref>\n\nMulti-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\nAutomatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. {{dubious|date=June 2018}}\n\n=====Incorporating diversity=====\nMulti-document extractive summarization faces a problem of potential redundancy. Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),<ref>Carbonell, Jaime, and Jade Goldstein. \"[https://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/jgc/publication/MMR_DiversityBased_Reranking_SIGIR_1998.pdf The use of MMR, diversity-based reranking for reordering documents and producing summaries].\" Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1998.</ref> in trying to eliminate redundancy in information retrieval results. There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on [[absorbing Markov chain]] random walks. (An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.) The algorithm is called GRASSHOPPER.<ref>Zhu, Xiaojin, et al. \"[http://www.aclweb.org/anthology/N07-1013 Improving Diversity in Ranking using Absorbing Random Walks].\" HLT-NAACL. 2007.</ref> In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\n\nThe state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.<ref>Hui Lin, Jeff Bilmes. \"[https://arxiv.org/pdf/1210.4871 Learning mixtures of submodular shells with application to document summarization]</ref> Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.<ref>Alex Kulesza and Ben Taskar, [http://www.nowpublishers.com/article/DownloadSummary/MAL-044 Determinantal point processes for machine learning]. Foundations and Trends in Machine Learning, December 2012.</ref>\n\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed. This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\n\n===Submodular functions as generic tools for summarization===\nThe idea of a [[submodular set function]] has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of ''coverage'', ''information'', ''representation'' and ''diversity''. Moreover, several important [[combinatorial optimization]] problems occur as special instances of submodular optimization. For example, the [[set cover problem]] is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which ''cover'' a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the [[facility location problem]] is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a [[determinantal point process]] to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\n\nWhile submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple [[greedy algorithm]] admits a constant factor guarantee.<ref>Nemhauser, George L., Laurence A. Wolsey, and Marshall L. Fisher. \"An analysis of approximations for maximizing submodular set functions\u2014I.\" Mathematical Programming 14.1 (1978): 265-294.</ref> Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\n\nSubmodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012<ref>Hui Lin, Jeff Bilmes. \"[https://arxiv.org/pdf/1210.4871 Learning mixtures of submodular shells with application to document summarization]\", UAI, 2012</ref> shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,<ref>Hui Lin, Jeff Bilmes. \"[http://www.aclweb.org/anthology/P11-1052 A Class of Submodular Functions for Document Summarization]\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011</ref> shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.{{citation needed|date=June 2018}}\n\nSubmodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show<ref>Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, [http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf Learning Mixtures of Submodular Functions for Image Collection Summarization], In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.</ref> that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015<ref>Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, [http://www.aclweb.org/anthology/P15-1054 Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures], To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015</ref> show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.<ref>Kai Wei, Rishabh Iyer, and Jeff Bilmes, [http://www.jmlr.org/proceedings/papers/v37/wei15.pdf Submodularity in Data Subset Selection and Active Learning], To Appear In Proc. International Conference on Machine Learning (ICML), Lille, France, June - 2015</ref>\n\n===Applications===\n{{Expand section|date=February 2017}}\nSpecific applications of automatic summarization include:\n* The [[Reddit]] [[Internet bot|bot]] \"autotldr\",<ref>{{cite web|title=overview for autotldr|url=https://www.reddit.com/user/autotldr|website=reddit|accessdate=9 February 2017|language=en}}</ref> created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.<ref>{{cite book|last1=Squire|first1=Megan|title=Mastering Data Mining with Python \u2013 Find patterns hidden in your data|publisher=Packt Publishing Ltd|isbn=9781785885914|url=https://books.google.com/books?id=_qXWDQAAQBAJ&pg=PA185|accessdate=9 February 2017|language=en|date=2016-08-29}}</ref> The name is reference to [[TL;DR]] \u2212 [[Internet slang]] for \"too long; didn't read\".<ref>{{cite web|title=What Is 'TLDR'?|url=https://www.lifewire.com/what-is-tldr-2483633|website=Lifewire|accessdate=9 February 2017}}</ref><ref>{{cite web|title=What Does TL;DR Mean? AMA? TIL? Glossary Of Reddit Terms And Abbreviations|url=http://www.ibtimes.com/what-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704|publisher=International Business Times|accessdate=9 February 2017|date=29 March 2012}}</ref>\n\n==Evaluation techniques==\n<!-- IMPORTANT: This section needs to be tied in to the above article so it fits in.  Currently, it is not clear what the relation of evaluation is to any of the above topics. The following questions need to be answered: First, in the context of automatic summarization, what is evaluation?  Second, what is the significance of evaluation?  That is, what is evaluation used for?\n-->\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\n\nEvaluation techniques fall into intrinsic and extrinsic,<ref>[http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings2/sum-mani.pdf Mani, I. Summarization evaluation: an overview]</ref> inter-textual and intra-textual.<ref>{{Cite journal | doi=10.3103/S0005105507030041|title = A method for evaluating modern systems of automatic text summarization| journal=Automatic Documentation and Mathematical Linguistics| volume=41| issue=3| pages=93\u2013103|year = 2007|last1 = Yatsko|first1 = V. A.| last2=Vishnyakov| first2=T. N.}}</ref>\n\n=== Intrinsic and extrinsic evaluation ===\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task. Intrinsic evaluations have\nassessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\n\n=== Inter-textual and intra-textual ===\nIntra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.\n\nHuman judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning [[coherence (linguistics)|coherence]] and coverage.\n\nOne of the metrics used in [[NIST]]'s annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [https://web.archive.org/web/20060408135021/http://haydn.isi.edu/ROUGE/]). It essentially calculates [[n-gram]] overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary's coherence. [[anaphora (linguistics)|Anaphor resolution]] remains another problem yet to be fully solved. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.<ref>Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, [http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf Learning Mixtures of Submodular Functions for Image Collection Summarization], In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014. (PDF)</ref>\n\n===Domain specific versus domain independent summarization techniques===\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments. Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text. For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.<ref>{{Cite book|last1=Sarker|first1=Abeed|last2=Molla|first2=Diego|last3=Paris|first3=Cecile|title=An Approach for Query-focused Text Summarization for Evidence-based medicine|date=2013|volume=7885|pages=295\u2013304|doi=10.1007/978-3-642-38326-7_41|series=Lecture Notes in Computer Science|isbn=978-3-642-38325-0}}</ref>\n\n===Evaluating summaries qualitatively===\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries. Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\n\n==See also==\n* [[Sentence extraction]]\n* [[Text mining]]\n* [[Multi-document summarization]]\n\n==References==\n{{Reflist|2}}\n\n== Further reading ==\n*{{cite book |last=Hercules |first=Dalianis |year=2003 |title=Porting and evaluation of automatic summarization|url=https://www.researchgate.net/publication/277288103}}\n*{{cite book |last=Roxana |first=Angheluta |year=2002 |title=The Use of Topic Segmentation for Automatic Summarization|url=https://www.researchgate.net/publication/2553088}}\n*{{cite book |last=Anne |first=Buist |year=2004 |title=Automatic Summarization of Meeting Data: A Feasibility Study|url=http://www.cs.ru.nl/~kraaijw/pubs/Biblio/papers/meeting_sum_tno.pdf}}\n*{{cite book |last=Annie |first=Louis |year=2009 |title=Performance Confidence Estimation for Automatic Summarization|url=https://repository.upenn.edu/cgi/viewcontent.cgi?article=1762&context=cis_papers}}\n*{{cite book |last=Elena |first=Lloret and Manuel, Palomar  |year=2009 |title=Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation|url=http://www.informatica.si/ojs-2.4.3/index.php/informatica/article/download/273/269}}\n*{{cite book |last=Andrew |first=Goldberg |year=2007 |title=Automatic Summarization}}\n*{{cite book |last=Alrehamy |first=Hassan |year=2017 |title=Automatic Keyphrases Extraction |volume=650 |pages=222\u2013235 |doi=10.1007/978-3-319-66939-7_19 |chapter=SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation |series=Advances in Intelligent Systems and Computing |isbn=978-3-319-66938-0 }}\n*{{cite book |last=Endres-Niggemeyer |first=Brigitte |year=1998 |title=Summarizing Information |url=https://archive.org/details/springer_10.1007-978-3-642-72025-3 |isbn=978-3-540-63735-6}}\n*{{cite book |last=Marcu |first=Daniel |year=2000 |title=The Theory and Practice of Discourse Parsing and Summarization |isbn=978-0-262-13372-2}}\n*{{cite book |last=Mani |first=Inderjeet |year=2001 |title=Automatic Summarization |isbn=978-1-58811-060-2}}\n*{{cite book |last=Huff |first=Jason |year=2010 |title=AutoSummarize |url=http://www.jason-huff.com/projects/autosummarize/}}, Conceptual artwork using automatic summarization software in Microsoft Word 2008.\n*{{cite book |last=Lehmam |first=Abderrafih |year=2010 |title=Essential summarizer: innovative automatic text summarization software in twenty languages - ACM Digital Library |pages=216\u2013217 |url=http://portal.acm.org/citation.cfm?id=1937055.1937111&coll=DL&dl=GUIDE&CFID=23185814&CFTOKEN=40272014/|series=Riao '10 }}, Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\n*{{cite book |last=Xiaojin |first=Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski |year=2007 |title=Improving diversity in ranking using absorbing random walks |url=http://pages.cs.wisc.edu/~jerryzhu/pub/grasshopper.pdf}}, The GRASSHOPPER algorithm\n*{{cite book |last=Miranda-Jim\u00e9nez |first=Sabino, Gelbukh, Alexander, and Sidorov, Grigori |year=2013 |doi=10.1007/978-3-642-35786-2_18 |title=Conceptual Structures for STEM Research and Education |volume=7735 |pages=245\u2013253 |series=Lecture Notes in Computer Science |isbn=978-3-642-35785-5 |chapter=Summarizing Conceptual Graphs for Automatic Summarization Task }}, Conceptual Structures for STEM Research and Education.\n\n{{Natural Language Processing}}\n\n[[Category:Computational linguistics]]\n[[Category:Natural language processing]]\n[[Category:Tasks of natural language processing]]\n[[Category:Data mining]]\n", "name_user": "Juan manuel torres", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Automatic_summarization"}
{"title_page": "Charles F. Hockett", "text_new": "{{more footnotes|date=March 2013}}\n{{Infobox academic\n| honorific_prefix   = \n| name               = Charles F. Hockett\n| honorific_suffix   = \n| image              = Charles Francis Hockett (1916\u20132000).jpg\n| image_size         = \n| alt                = \n| caption            = \n| native_name        = \n| native_name_lang   = \n| birth_name         = Charles Francis Hockett\n| birth_date         = {{birth date|1916|1|17}}\n| birth_place        = [[Columbus, Ohio]], U.S.\n| death_date         = {{death date and age|2000|11|3|1916|1|17}}\n| death_place        = [[Ithaca, New York]], U.S.\n| death_cause        = \n| region             = \n| nationality        = [[Americans|American]]\n| citizenship        = \n| residence          = \n| other_names        = \n| occupation         = \n| period             = \n| known_for          = \n| home_town          = \n| title              = \n| boards             = \n| spouse             = Shirley Orlinoff Hockett\n| partner            = \n| children           = 5\n| parents            = \n| relatives          = \n| awards             = \n| website            = \n| education          = {{ubl|[[Ohio State University]] {{small|(B.A., M.A.)}}|[[Yale University]] {{small|(Ph.D.)}}}}\n| alma_mater         = \n| thesis_title       = The Potawatomi Language: A Descriptive Grammar\n| thesis_url         = \n| thesis_year        = 1939\n| school_tradition   = \n| doctoral_advisor   = \n| academic_advisors  = \n| influences         = [[Leonard Bloomfield]]\n| era                = \n| discipline         = [[Linguistics|Linguist]]\n| sub_discipline     = \n| workplaces         = {{ubl|[[Cornell University]] (1946{{ndash}}1982)|[[Rice University]] (1986{{ndash}}2000)}}\n| doctoral_students  = \n| notable_students   = \n| main_interests     = {{ubl|[[Structural linguistics]]|[[Linguistic anthropology]]}}\n| notable_works      = \n| notable_ideas      = \n| influenced         = \n| signature          = \n| signature_alt      = \n| signature_size     = \n| footnotes          = \n}}\n'''Charles Francis Hockett''' (January 17, 1916 \u2013 November 3, 2000) was an American [[linguist]] who developed many influential ideas in American [[structuralism#Structuralism in linguistics|structuralist]] linguistics. He represents the post-[[Leonard Bloomfield|Bloomfieldian]] phase of [[structuralism (linguistics)|structuralism]] often referred to as \"[[distributionalism]]\" or \"taxonomic structuralism\". His academic career spanned over half a century at Cornell and Rice universities. Hockett was also a firm believer of linguistics as a branch of anthropology, making contributions that were significant to the field of anthropology as well.\n\n==Professional and academic career <ref>{{Cite web|url=http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/hockett-charles.pdf|title=National Academy of Sciences Biographical Memoir|last=Gair|first=James W.|date=2006|website=National Academy of Sciences online|url-status=live|archive-url=|archive-date=|access-date=}}</ref>==\n\n===Education===\nAt the age of 16, Hockett enrolled at [[Ohio State University]] in [[Columbus, Ohio]] where he received a [[Bachelor of Arts]] and [[Master of Arts]] in [[ancient history]]. While enrolled at [[Ohio State University|Ohio State]], Hockett became interested in the work of [[Leonard Bloomfield]], a leading figure in the field of [[structural linguistics]]. Hockett continued his education at [[Yale University]] where he studied [[anthropology]] and [[linguistics]] and received his [[doctor of philosophy|PhD]] in anthropology in 1939. While studying at Yale, Hockett studied with several other influential linguists such as [[Edward Sapir]], [[George P. Murdock]], and [[Benjamin Whorf]]. Hockett's dissertation was based on his fieldwork in [[Potawatomi language|Potawatomi]]; his paper on Potawatomi [[syntax]] was published in ''[[Language (journal)|Language]]'' in 1939. In 1948 his [[dissertation]] was published as a series in the [[International Journal of American Linguistics]]. Following fieldwork in [[Kickapoo language|Kickapoo]] and [[Michoac\u00e1n]], [[Mexico]], Hockett did two years of [[postdoctoral]] study with [[Leonard Bloomfield]] in [[Chicago]] and [[Michigan]].\n\n===Career===\nHockett began his teaching career in 1946 as an assistant professor of linguistics in the Division of [[Modern Languages]] at [[Cornell University]] where he was responsible for directing the [[Chinese language]] program. In 1957, Hockett became a member of Cornell's anthropology department and continued to teach anthropology and linguistics until he retired to [[emeritus|emeritus status]] in 1982. In 1986, he took up an adjunct post at [[Rice University]] in [[Houston, Texas]], where he remained active until his death in 2000.\n\n===Achievements===\nCharles Hockett held membership among many academic institutions such as the [[United States National Academy of Sciences|National Academy of Sciences]] the [[American Academy of Arts and Sciences]], and the [[Society of Fellows]] at [[Harvard University]]. He served as president of both the [[Linguistic Society of America]] and the Linguistic Association of Canada and the United States.\n\nIn addition to making many contributions to the field of [[structural linguistics]], Hockett also considered such things as [[Benjamin Whorf|Whorfian Theory]], [[jokes]], the nature of [[writing systems]], slips of the tongue, and [[animal communication]] and their relativeness to [[speech]].\n\nOutside the realm of linguistics and anthropology, Hockett practiced [[Musical theatre|musical]] performance and [[Musical composition|composition]]. Hockett composed a full-length [[opera]] called ''The Love of Do\u00f1a Rosita'' which was based on a play by [[Federico Garc\u00eda Lorca]] and premiered at [[Ithaca College]] by the [[Ithaca, New York|Ithaca]] [[Opera]].\n\nHockett and his wife Shirley were vital leaders in the development of the [[Cayuga, New York|Cayuga]] Chamber Orchestra in Ithaca, New York. In appreciation of the Hocketts' hard work and dedication to the Ithaca community, Ithaca College established the Charles F. Hockett Music Scholarship, the Shirley and Chas Hockett Chamber Music Concert Series, and the Hockett Family Recital Hall.\n\n==View on linguistics==\nIn his paper \"A Note on Structure\", he proposes that [[linguistics]] can be seen as \"a game and as a science.\" A linguist as a player in the game of langauges has the freedom to experiment on all utterances of a language, but must ensure that \"all the utterances of the corpus must be taken into account.\"<ref>{{Cite web|url=http://www.journals.uchicago.edu.remotexs.ntu.edu.sg/doi/abs/10.1086/464015|title=A Note on 'Structure' [Review of de Goeje by W. D. Preston]|last=Hockett|first=Charles F.|date=Oct 1948|website=The University Of Chicago Press Journals|url-status=live|archive-url=|archive-date=|access-date=23 April 2020}}</ref> Late in his career, he was known for his stinging criticism of [[Noam Chomsky|Chomskyan]] linguistics.\n\n==Key contributions==\n\n===Criticisms of Noam Chomsky and the Generative Programme===\nAfter carefully examining the generative school's proposed innovations in Linguistics, Hockett decided that this approach was of little value. His book ''The State of the Art'' outlined his criticisms of the generative approach. In his paraphrase a key principle of the Chomskyan paradigm is that there are an infinite number of grammatical sentences in any particular language. \n<blockquote>The grammar of a language is a finite system that characterizes an infinite set of (well-formed) sentences. More specifically, the grammar of a language is a ''well-defined system'' by definition not more powerful than a universal Turing machine (and, in fact, surely a great deal weaker).<ref>''The State of the Art'', p. 40</ref></blockquote>\n\nThe crux of Hockett's rebuttal is that the set of grammatical sentences in a language is not infinite, but rather ill-defined.<ref>p. 52 et passim</ref> Hockett proposes that \"no physical system is well-defined\".<ref>p. 52</ref>\n\nLater in \"Where the tongue slips, there slip I\" he writes as follows. \n<blockquote>\nIt is currently fashionable to assume that, underlying the actual more or less bumbling speech behavior of any human being, there is a subtle and complicated but determinate linguistic \"competence\": a sentence-generating device whose design can only be roughly guessed at by any techniques so far available to us. This point of view makes linguistics very hard and very erudite, so that anyone who actually does discover facts about underlying \"competence\" is entitled to considerable kudos.\n\nWithin this popular frame of reference, a theory of \"performance\" -- of the \"generation of speech\" -- must take more or less the following form. If a sentence is to be uttered aloud, or even thought silently to oneself, it must first be built by the internal \"competence\" of the speaker, the functioning of which is by definition such that the sentence will be legal (\"grammatical\") in every respect. But that is not enough; the sentence as thus constructed must then be ''performed'', either overtly so that others may hear it, or covertly so that it is perceived only by the speaker himself. It is in this second step that blunders may appear. That which is generated by the speaker's internal \"competence\"is what the speaker \"intends to say,\" and is the only real concern of linguistics: blunders in actually performed speech are instructions from elsewhere. Just if there are no such intrusions is what is performed an instance of \"smooth speech\".\n\nI believe this view is unmitigated nonsense, unsupported by any empirical evidence of any sort. In its place, I propose the following.\n\n''All'' speech, smooth as well as blunderful, can be and must be accounted for essentially in terms of the three mechanisms we have listed: analogy, blending, and editing. An individual's language, at a given moment, is a set of habits--that is, of analogies, where different analogies are in conflict, one may appear as a constraint on the working of another. Speech actualizes habits--and changes the habits as it does so. Speech reflects awareness of norms; but norms are themselves entirely a matter of analogy (that is, of habit), not some different kind of thing.<ref>''The View From Language'', pp. 254-255.</ref> </blockquote>\n\nDespite his criticisms, Hockett always expressed gratitude to the generative school for seeing real problems in the preexisting approaches.\n\n<blockquote>There are many situations in which bracketing does not serve to disambiguate. As already noted, words that belong together cannot always be spoken together, and when they are not, bracketing is difficult or impossible. In the 1950s this drove some grammarians to drink and other to transformations, but both are only anodynes, not answers<ref>Hockett, Refurbishing our Foundations. John Benjamins, 1987, p. 23</ref></blockquote>\n\n===Design features of language===\nOne of Hockett's most important contributions was his development of the [[Design Features of Language|design-feature]] approach to comparative linguistics. He attempted to distinguish the similarities and differences among [[animal communication]] systems and [[human language]].\n\nHockett initially developed seven features, which were published in the 1959 paper \u201cAnimal \u2018Languages\u2019 and Human Language.\u201d However, after many revisions, he settled on 13 design-features in the ''[[Scientific American]] '' \"The Origin of Speech.\"\n\nHockett argued that while every communication system has some of the 13 design features, only human, spoken language has all 13 features. In turn, that differentiates human spoken language from animal communication and other human communication systems such as [[written language]].\n\n====Hockett's 13 design features of language====\n{{Main|Design Features of Language}}\n#[[Vocal-Auditory Channel]]: Much of human language is performed using the [[vocal]] tract and [[auditory system|auditory]] channel. Hockett viewed this as an advantage for human [[primates]] because it allowed for the ability to participate in other activities while simultaneously communicating through spoken language.\n#[[Broadcast transmission and directional reception]]: All human language can be heard if it is within range of another person's auditory channel. Additionally, a listener has the ability to determine the source of a sound by [[binaural direction]] finding.\n#[[Rapid Fading (transitoriness)]]: Wave forms of human language dissipate over time and do not persist. A hearer can only receive specific auditory information at the time it is spoken.\n#Interchangeability: A person has the ability to speak and hear the same [[signaling theory|signal]]. Anything that a person is able to hear can be reproduced in spoken language.\n#[[Total Feedback]]: Speakers can hear themselves speak and monitor their [[speech production]] and internalize what they are producing by language.\n#Specialization: Human language sounds are specialized for communication. When dogs pant it is to cool themselves off. When humans speak, it is to transmit information.\n#[[Semanticity]]: Specific signals can be matched with a specific [[meaning (linguistics)|meaning]].\n#[[Arbitrariness]]: There is no limitation to what can be communicated about and no specific or necessary connection between the sounds used and the message being sent.\n#[[Discrete mathematics|Discreteness]]: [[Phonemes]] can be placed in distinct categories which differentiate them from one another, like the distinct sound of /p/ versus /b/.\n#[[Displacement (linguistics)|Displacement]]: People can refer to things in space and time and communicate about things that are not present.\n#[[Productivity (linguistics)|Productivity]]: People can create new and unique meanings of utterances from previously existing utterances and sounds.\n#[[Traditional Transmission]]: Human language is not completely [[innate]], and acquisition depends in part on the learning of a language.\n#[[Duality of patterning]]: Meaningless phonic segments ([[phoneme]]s) are combined to make meaningful words, which, in turn, are combined again to make sentences.\n\nWhile Hockett believed that all communication systems, animal and human alike, share many of these features, only human language contains all 13 design features. Additionally, [[Traditional Transmission|traditional transmission]], and [[duality of patterning]] are key to human language.\n\n====Hockett's design features and their implications for human language====\n#Hockett suggests that the importance of a vocal-auditory channel lies in the fact that the animal can communicate while also performing other tasks, such as eating, or using tools.\n#[[Broadcast Transmission and Directional Reception]]: An auditory|audible human language signal is sent out in all directions but is perceived in a limited direction. For example, humans are more proficient in determining the location of a sound source when the sound is projecting directly in front of them, as opposed to a sound source projected directly behind them.\n#[[Rapid Fading]] of a signal in human communication differs from such things as animal tracks and written language because an utterance does not continue to exist after it has been broadcast. With that in mind, it is important to note that Hockett viewed spoken language as the primary concern for investigation. Written language was seen as being secondary because of its recent evolution in culture.\n#Interchangeability represents a human's ability to act out or reproduce any linguistic message that they are able to comprehend. That differs from many animal communication systems, particularly in regards to mating. For example, humans have the ability to say and do anything that they feel may benefit them in attracting a mate. [[Stickleback]]s, on the other hand, have different male and female courtship motions; a male cannot replicate a female's motions and vice versa.\n#[[Total Feedback]] is important in differentiating a human's ability to internalize their own productions of speech and behavior. That design-feature incorporates the idea that humans have insight into their actions.\n#Specialization is apparent in the anatomy of human [[speech organs]] and our ability to exhibit some control over these organs. For example, a key assumption in the evolution of language is that the descent of the [[larynx]] has allowed humans to produce speech sounds. Additionally, in terms of control, humans are generally able to control the movements of their tongue and mouth. Dogs however, do not have control over these organs. When dogs pant they are communicating a signal, but the panting is an uncontrollable response reflex of being hot [https://web.archive.org/web/20080709091601/http://www.columbia.edu/itc/psychology/rmk/Readings/Hockett.pdf].\n#[[Semanticity]]: A specific signal can be matched with a specific meaning within a particular language system. For example, all people who understand [[English language|English]] have the ability to make a connection between a specific word and what that word represents or refers to. (Hockett notes that [[gibbon]]s also show semanticity in their signals, but their calls are far more broad than human language.)\n#[[Arbitrariness]] within human language suggests that there is no direct connection between the type of signal (word) and what is being referenced. For example, an animal as large as a cow can be referred to by a very short word {{webarchive |url=https://web.archive.org/web/20091027014002/http://www.geocities.com/pkd_du/dfhl.htm |date=October 27, 2009 }}.\n#[[Discrete mathematics|Discreteness]]: Each basic unit of speech can be categorized and is distinct from other categories. In human language, there are only a small set of sound ranges that are used and the differences between these bits of sound are absolute. In contrast, the [[waggle dance]] of [[honey bee]]s is continuous.\n#[[Displacement (linguistics)|Displacement]] refers to the human language system's ability to communicate about things that are not present spatially, temporally, or realistically. For example, humans have the ability to communicate about unicorns and outer space.\n#[[Productivity (linguistics)|Productivity]]: Human language is open and productive in the sense that humans have the ability to say things that have never before been spoken or heard. In contrast, apes such as the gibbon have a closed communication system because all of their vocal sounds are part of a finite repertoire of familiar calls.\n#[[Traditional Transmission]]:: suggests that while certain aspects of [[language]] may be [[innate]], humans acquire words and their [[native language]] from other speakers. That is different from many [[animal communication]] systems because most animals are born with the [[innate]] [[knowledge]], and [[skills]] necessary for [[wiktionary:survival|survival]]. ([[Honey bee]]s have an inborn ability to perform and understand the [[waggle dance]]).\n#[[Duality of patterning]]: Humans have the ability to recombine a finite set of [[phoneme]]s to create an infinite number of words, which, in turn, can be combined to make an unlimited number of different sentences.\n\n====Design feature representation in other communication systems====\n;Honeybees\n\nForaging [[honey bee]]s communicate with other members of their hive when they have discovered a relevant source of [[pollen]], [[nectar]], or water. In an effort to convey information about the location and the distance of such resources, honeybees participate in a particular figure-eight dance known as the [[waggle dance]].\n\nIn Hockett's \"The Origin of Speech\", he determined that the honeybee communication system of the [[waggle dance]] holds the following [[Design Features of Language|design features]]:\n\n#Broadcast Transmission and Directional Reception: By the use of this dance, honeybees are able to send out a signal that informs other members of the hive as to what direction the source of food, or water can be located.\n#[[Semanticity]]: Evidence that the specific signals of a communication system can be matched with specific meanings is apparent because other members of the hive are able to locate the food source after a performance of the waggle dance.\n#[[Displacement (linguistics)|Displacement]]: Foraging honeybees can communicate about a resource that is not currently present within the hive.\n#[[Productivity (linguistics)|Productivity]]: [[Waggle dance]]s change based on the direction, amount, and type of resource.\n\nGibbons are small apes in the family Hylobatidae. While they share the same [[kingdom (biology)|kingdom]], [[phylum]], [[class (biology)|class]], and [[order (biology)|order]] of humans and are relatively close to man, Hockett distinguishes between the gibbon communication system and human language by noting that gibbons are devoid of the last four design features.\n\nGibbons possess the first nine [[Design Features of Language|design features]], but do not possess the last four (displacement, productivity, [[Traditional Transmission|traditional transmission]], and [[duality of patterning]]).\n\n#Displacement, according to Hockett, appears to be lacking in the vocal [[Signalling theory|signal]]ing of apes.\n#Productivity does not exist among gibbons because if any vocal sound is produced, it is one of a finite set of repetitive and familiar calls.\n#Hockett supports the idea that humans learn language extra genetically through the process of [[Traditional Transmission|traditional transmission]]. Hockett distinguishes gibbons from humans by stating that despite any similarities in communication among a species of apes, one cannot attribute these similarities to acquisition through the teaching and learning ([[Traditional Transmission|traditional transmission]]) of signals; the only explanation must be a genetic basis.\n#Finally, [[duality of patterning]] explains a human's ability to create multiple [[meanings (linguistics)|meanings]] from somewhat meaningless sounds. For example, the [[phonemes]]s /t/, /a/, /c/ can be used to create the words \"cat,\" \"tack,\" and \"act.\" Hockett states that no other [[Hominoid]] communication system besides human language maintains this ability.\n\n===Later additions to the features===\nIn a report published in 1968 with anthropologist and scientist Stuart A. Altmann, Hockett derived three more [[Design Features of Language|Design Features]], bringing the total to 16. These are the additional three:\n\n#<li value=14> '''[[Deception|Prevarication]]''': A speaker can say falsehoods, lies, and meaningless statements.\n# '''Reflexiveness''': Language can be used communicate about the very system it is, and language can discuss language\n# '''[[Learnability]]''': A speaker of a language can learn another language\n\n====Other additions====\nCognitive scientist and linguist at the University of Sussex [[Larry Trask]] offered an alternative term and definition for number 14, '''[[Deception|Prevarication]]''':\n:14. (a) '''Stimulus Freedom''': One can choose to say anything nothing in any given situation\n\nThere has since been one more Feature added to the list, by [https://web.archive.org/web/20100612132306/http://www.bsos.umd.edu/anth/People/FacStaff/faculty/wstuart/index.html Dr. William Taft Stuart], a director of the Undergraduate Studies program at the University of Maryland: College Park's Anthropology school, part of the College of Behavioral and Social Sciences. His \u201cextra\u201d Feature is:\n\n:17. '''Grammaticality''': A speaker\u2019s sayings conform to the rules of grammar\n\nThis follows the definition of Grammar and Syntax, as given by Merriam-Webster's Dictionary:\n\n:'''[[Grammar]]''':\n::1. (a) the study of the classes of words, their inflections, and their functions and relations in the sentence (b) a study of what is to be preferred and what avoided in inflection and syntax\n\n::2. (a) the characteristic system of inflections and syntax of a language (b) a system of rules that defines the grammatical structure of a language\n\n:'''[[Syntax]]''':\n::1. (a) the way in which linguistic elements (as words) are put together to form constituents (as phrases or clauses) (b) the part of grammar\n\n====Relationship between design features and animal communication====\n\nAdditionally, Dr. Stuart defends his postulation with references to famous linguist Noam Chomsky and University of New York psychologist Gary Marcus. Chomsky theorized that humans are unique in the animal world because of their ability to utilize Design Feature 5: Total Feedback, or recursive grammar. This includes being able to correct oneself and insert explanatory or even non sequitur statements into a sentence, without breaking stride, and keeping proper grammar throughout.\n\nWhile there have been studies attempting to disprove Chomsky, Marcus states that, \"An intriguing possibility is that the capacity to recognize recursion might be found only in species that can acquire new patterns of vocalization, for example, songbirds, humans and perhaps some cetaceans.\" This is in response to a [http://www.cbc.ca/health/story/2006/04/26/birds-grammar060426.html study performed] by psychologist Timothy Gentner of the University of California at San Diego. Gentner's study found that starling songbirds use recursive grammar to identify \u201codd\u201d statements within a given \u201csong.\u201d However, the study does not necessarily debunk Chomsky's observation because it has not yet been proven that songbirds have the semantic ability to generalize from patterns.\n\n[http://tuvalu.santafe.edu/~johnson/articles.chimp.html There is also thought] that symbolic thought is necessary for grammar-based speech, and thus Homo Erectus and all preceding \u201chumans\u201d would have been unable to comprehend modern speech. Rather, their utterances would have been halting and even quite confusing to us, \ntoday.\n\n=====Hockett's \"design features\" of language and other animal communication systems=====\n\nThe [http://www.phon.ox.ac.uk/jcoleman/design_features.htm [[University of Oxford]]]: Phonetics Laboratory Faculty of Linguistics, Philology and Phonetics published the following chart, detailing how Hockett's (and Altmann's) Design Features fit into other forms of communication, in animals:\n{| class=\"wikitable\"\n|-\n! Feature !! [[Cricket (insect)|Cricket]]s !! [[Bee Dance language|Bee dancing]] !! [[Western meadowlark|Western meadowlark song]] !! [[Gibbon|Gibbon calls]] !! [[Ape language|Signing apes]] !! [[Grey parrot|Alex, a grey parrot]] !! Paralinguistic phenomena !! [[Sign languages|Human sign languages]] !! [[Spoken dialogue|Spoken language]]\n|-\n| Vocal-Auditory Channel || Auditory, not vocal || No || Yes || Yes || No || Yes || Yes || No || Yes\n|-\n| Broadcast Transmission and Directional Reception || Yes || Yes || Yes || Yes || Yes || Yes || Yes || Yes || Yes\n|-\n| Rapid Fading || Yes (repeating) || ? || Yes || Yes (repeating) || Yes || Yes || Yes || Yes || Yes\n|-\n| Interchangeability || Limited || Limited || ? || Yes || Yes || Yes || Largely Yes || Yes || Yes\n|-\n| Total Feedback || Yes || ? || Yes || Yes || No || Yes || Yes || No || Yes\n|-\n| Specialization || Yes? || ? || Yes || Yes || Yes || Yes || Yes? || Yes || Yes\n|-\n| Semanticity || No? || Yes || In Part || Yes || Yes || Yes || Yes? || Yes || Yes\n|-\n| Arbitrariness || ? || No || If semantic, Yes || Yes || Largely Yes || Yes || In Part || Largely Yes || Yes\n|-\n| Discreteness || Yes? || No || ? || Yes || Yes || Yes || Largely No || Yes || Yes\n|-\n| Displacement || \u2013 || Yes, always || ? || No || Yes || No || In Part || Yes, often || Yes, often\n|-\n| Productivity || No || Yes || ? || No || Debatable || Limited || Yes || Yes || Yes\n|-\n| Traditional Transmission || No? || Probably not || ? || ? || Limited || Limited || Yes || Yes || Yes\n|-\n| Duality of Patterning || ? || No || ? || No (Cotton-top Tamarin: Yes) || Yes || Yes || No || Yes || Yes\n|-\n| Prevarication || \u2013 || \u2013 || \u2013 || \u2013 || Yes || No || \u2013 || Yes || Yes\n|-\n| Reflexiveness || \u2013 || \u2013 || \u2013 || \u2013 || No? || No || \u2013 || Yes || Yes\n|-\n| Learnability || \u2013 || \u2013 || \u2013 || \u2013 || Yes || Yes || \u2013 || Yes || Yes\n|}\n\n==Selected works==\n* 1939: \"Potowatomi Syntax\", ''Language'' 15: 235\u2013248.\n* 1942: \"A System of Descriptive Phonology\", ''Language'' 18: 3-21.\n* 1944: ''Spoken Chinese; Basic Course''. With C. Fang. Holt, New York.\n* 1947: \"Peiping phonology\", in: ''Journal of the American Oriental Society'', 67, pp.&nbsp;253\u2013267. [= Martin Joos (ed.), ''Readings in Linguistics'', vol. I, 4th edition. Chicago and London 1966, pp.&nbsp;217\u2013228].\n* 1947: \"Problems of morphemic analysis\", in: ''[[Language (journal)|Language]]'', 24, pp.&nbsp;414\u201341. [= ''Readings in Linguistics'', vol. I, pp.&nbsp;229\u2013242].\n* 1948: \"Biophysics, linguistics, and the unity of science\", in: ''[[American Scientist]]'', 36, pp.&nbsp;558\u2013572.\n* 1950: \"Peiping morphophonemics\", in: ''Language'', 26, pp.&nbsp;63\u201385. [= ''Readings in Linguistics'', vol. I, pp.&nbsp;315\u2013328].\n* 1954: \"Two models of grammatical description\", in: ''Word'', 10, pp.&nbsp;210\u2013234. [= ''Readings in Linguistics'', vol. I, pp.&nbsp;386\u2013399].\n* 1955: ''A Manual of Phonology''. Indiana University Publications in Anthropology and Linguistics 11.\n* 1958: ''A Course in Modern Linguistics''. The Macmillan Company: New York.\n* 1960: \"The Origin of Speech\". in ''[[Scientific American]]'', 203, pp.&nbsp;89\u201397.\n* 1961: \"Linguistic Elements and Their Relation\" in ''[[Language (journal)|Language]]'', 37: 29\u201353.\n* 1967: ''The State of the Art''. The Haag: Mouton\n* 1973: Man's Place in Nature. New York: McGraw-Hill.\n* 1977: The View From Language. Athens: The University of Georgia Press.\n* 1987: Refurbishing Our Foundations. Amsterdam: John Benjamins.\n\n==See also==\n* [[Animal communication]]\n* [[Design features of language]]\n* [[Language acquisition]]\n* [[Linguistic anthropology]]\n* [[Linguistic universals]]\n* [[Origin of language]]\n* [[Origin of speech]]\n\n==References==\n{{Reflist}}\n*Falk, Julia S. 2003. \"Turn to the history of linguistics : Noam Chomsky and Charles Hockett in the 1960s\". ''Historiographia linguistica'' (international journal for the history of the language sciences) 30/1-2, pp.&nbsp;129\u2013185. [https://www.researchgate.net/publication/233602133_Turn_to_the_History_of_Linguistics_Noam_Chomsky_and_Charles_Hockett_in_the_1960s]\n*Gair, James W. 2003. [Obituary] Charles F. Hockett. ''Language''. 79, 600\u2013613.\n*Hockett, Charles F. (1960), \"The Origin of Speech,\" ''Scientific American'', 203, 89\u201397.\n*Fox, Margalit 2003 (Obituary) \"Champion of structural linguistics\" The New York Times\n\n==External links==\n* [http://specgram.com/JLSSCNC.I.2/02.whitcomb.hockett.html Old Professor Hockett]: A poem written in honor of Hockett by one of his students during his 1991 visit to [[Rice University]].\n* [https://web.archive.org/web/20071023012311/http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0011b&L=linganth&D=0&O=A&P=723 Linguist List]: Obituary of Charles Hockett from [[the New York Times]] (November 13, 2000), reproduced on the Linguist List. The NY Times link to the obituary is at [https://www.nytimes.com/2000/11/13/national/13HOCK.html NY Times]\n* [https://books.google.com/books?id=Is0UAAAAIAAJ&pg=PA342&dq=four+traditional+grammar+historical+linguistics&lr=&as_brr=0&hl=tr#PPA13,M1 Essays in Honor of Charles F. Hockett]\n* [https://web.archive.org/web/20070716054015/http://www.people.ex.ac.uk/bosthaus/Lecture/hockett1.htm Features of Human Language]\n* [https://web.archive.org/web/20090306051026/http://books.nap.edu/html/biomems/chockett.pdf Charles Hockett-Biography]\n* [https://web.archive.org/web/20110125091520/http://www.pdfqueen.com/html/aHR0cDovL2hvbWVwYWdlLnJ1aHItdW5pLWJvY2h1bS5kZS9VZG8uRmlnZ2UvZG93bmxvYWQvZGVzaWduZmVhdHVyZXMucGRm Design Features of Human Language, Udo L. Figge]: A brief analysis of the 16 Design Features of Language, as published by Hockett and Altmann in 1968\n* {{worldcat id|lccn-n50-34609}}\n* [https://web.archive.org/web/20120425091558/http://www.searchinpdf.com/CHARLES-FRANCIS-HOCKETT-1916-2000 Charles Hockett Life Summary] {{dead link|date=April 2020}}\n*[http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/hockett-charles.pdf National Academy of Sciences Biographical Memoir]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Hockett, Charles F.}}\n[[Category:1916 births]]\n[[Category:2000 deaths]]\n[[Category:Linguists from the United States]]\n[[Category:People from Columbus, Ohio]]\n[[Category:Ohio State University alumni]]\n[[Category:Yale University alumni]]\n[[Category:Harvard Fellows]]\n[[Category:Cornell University faculty]]\n[[Category:Rice University faculty]]\n[[Category:Members of the United States National Academy of Sciences]]\n[[Category:Linguists of Algic languages]]\n[[Category:Linguistic Society of America presidents]]\n", "text_old": "{{more footnotes|date=March 2013}}\n{{Infobox academic\n| honorific_prefix   = \n| name               = Charles F. Hockett\n| honorific_suffix   = \n| image              = Charles Francis Hockett (1916\u20132000).jpg\n| image_size         = \n| alt                = \n| caption            = \n| native_name        = \n| native_name_lang   = \n| birth_name         = Charles Francis Hockett\n| birth_date         = {{birth date|1916|1|17}}\n| birth_place        = [[Columbus, Ohio]], U.S.\n| death_date         = {{death date and age|2000|11|3|1916|1|17}}\n| death_place        = [[Ithaca, New York]], U.S.\n| death_cause        = \n| region             = \n| nationality        = [[Americans|American]]\n| citizenship        = \n| residence          = \n| other_names        = \n| occupation         = \n| period             = \n| known_for          = \n| home_town          = \n| title              = \n| boards             = \n| spouse             = Shirley Orlinoff Hockett\n| partner            = \n| children           = 5\n| parents            = \n| relatives          = \n| awards             = \n| website            = \n| education          = {{ubl|[[Ohio State University]] {{small|(B.A., M.A.)}}|[[Yale University]] {{small|(Ph.D.)}}}}\n| alma_mater         = \n| thesis_title       = The Potawatomi Language: A Descriptive Grammar\n| thesis_url         = \n| thesis_year        = 1939\n| school_tradition   = \n| doctoral_advisor   = \n| academic_advisors  = \n| influences         = [[Leonard Bloomfield]]\n| era                = \n| discipline         = [[Linguistics|Linguist]]\n| sub_discipline     = \n| workplaces         = {{ubl|[[Cornell University]] (1946{{ndash}}1982)|[[Rice University]] (1986{{ndash}}2000)}}\n| doctoral_students  = \n| notable_students   = \n| main_interests     = {{ubl|[[Structural linguistics]]|[[Linguistic anthropology]]}}\n| notable_works      = \n| notable_ideas      = \n| influenced         = \n| signature          = \n| signature_alt      = \n| signature_size     = \n| footnotes          = \n}}\n'''Charles Francis Hockett''' (January 17, 1916 \u2013 November 3, 2000) was an American [[linguist]] who developed many influential ideas in American [[structuralism#Structuralism in linguistics|structuralist]] linguistics. He represents the post-[[Leonard Bloomfield|Bloomfieldian]] phase of [[structuralism (linguistics)|structuralism]] often referred to as \"[[distributionalism]]\" or \"taxonomic structuralism\". His academic career spanned over half a century at Cornell and Rice universities.\n\n==Professional and academic career <ref>{{Cite web|url=http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/hockett-charles.pdf|title=National Academy of Sciences Biographical Memoir|last=Gair|first=James W.|date=2006|website=National Academy of Sciences online|url-status=live|archive-url=|archive-date=|access-date=}}</ref>==\n\n===Education===\nAt the age of 16, Hockett enrolled at [[Ohio State University]] in [[Columbus, Ohio]] where he received a [[Bachelor of Arts]] and [[Master of Arts]] in [[ancient history]]. While enrolled at [[Ohio State University|Ohio State]], Hockett became interested in the work of [[Leonard Bloomfield]], a leading figure in the field of [[structural linguistics]]. Hockett continued his education at [[Yale University]] where he studied [[anthropology]] and [[linguistics]] and received his [[doctor of philosophy|PhD]] in anthropology in 1939. While studying at Yale, Hockett studied with several other influential linguists such as [[Edward Sapir]], [[George P. Murdock]], and [[Benjamin Whorf]]. Hockett's dissertation was based on his fieldwork in [[Potawatomi language|Potawatomi]]; his paper on Potawatomi [[syntax]] was published in ''[[Language (journal)|Language]]'' in 1939. In 1948 his [[dissertation]] was published as a series in the [[International Journal of American Linguistics]]. Following fieldwork in [[Kickapoo language|Kickapoo]] and [[Michoac\u00e1n]], [[Mexico]], Hockett did two years of [[postdoctoral]] study with [[Leonard Bloomfield]] in [[Chicago]] and [[Michigan]].\n\n===Career===\nHockett began his teaching career in 1946 as an assistant professor of linguistics in the Division of [[Modern Languages]] at [[Cornell University]] where he was responsible for directing the [[Chinese language]] program. In 1957, Hockett became a member of Cornell's anthropology department and continued to teach anthropology and linguistics until he retired to [[emeritus|emeritus status]] in 1982. In 1986, he took up an adjunct post at [[Rice University]] in [[Houston, Texas]], where he remained active until his death in 2000.\n\n===Achievements===\nCharles Hockett held membership among many academic institutions such as the [[United States National Academy of Sciences|National Academy of Sciences]] the [[American Academy of Arts and Sciences]], and the [[Society of Fellows]] at [[Harvard University]]. He served as president of both the [[Linguistic Society of America]] and the Linguistic Association of Canada and the United States.\n\nIn addition to making many contributions to the field of [[structural linguistics]], Hockett also considered such things as [[Benjamin Whorf|Whorfian Theory]], [[jokes]], the nature of [[writing systems]], slips of the tongue, and [[animal communication]] and their relativeness to [[speech]].\n\nOutside the realm of linguistics and anthropology, Hockett practiced [[Musical theatre|musical]] performance and [[Musical composition|composition]]. Hockett composed a full-length [[opera]] called ''The Love of Do\u00f1a Rosita'' which was based on a play by [[Federico Garc\u00eda Lorca]] and premiered at [[Ithaca College]] by the [[Ithaca, New York|Ithaca]] [[Opera]].\n\nHockett and his wife Shirley were vital leaders in the development of the [[Cayuga, New York|Cayuga]] Chamber Orchestra in Ithaca, New York. In appreciation of the Hocketts' hard work and dedication to the Ithaca community, Ithaca College established the Charles F. Hockett Music Scholarship, the Shirley and Chas Hockett Chamber Music Concert Series, and the Hockett Family Recital Hall.\n\n==View on linguistics==\nIn his paper \"A Note on Structure\", he proposes that [[linguistics]] can be seen as \"a game and as a science.\" A linguist as a player in the game of langauges has the freedom to experiment on all utterances of a language, but must ensure that \"all the utterances of the corpus must be taken into account.\"<ref>{{Cite web|url=http://www.journals.uchicago.edu.remotexs.ntu.edu.sg/doi/abs/10.1086/464015|title=A Note on 'Structure' [Review of de Goeje by W. D. Preston]|last=Hockett|first=Charles F.|date=Oct 1948|website=The University Of Chicago Press Journals|url-status=live|archive-url=|archive-date=|access-date=23 April 2020}}</ref> Late in his career, he was known for his stinging criticism of [[Noam Chomsky|Chomskyan]] linguistics.\n\n==Key contributions==\n\n===Criticisms of Noam Chomsky and the Generative Programme===\nAfter carefully examining the generative school's proposed innovations in Linguistics, Hockett decided that this approach was of little value. His book ''The State of the Art'' outlined his criticisms of the generative approach. In his paraphrase a key principle of the Chomskyan paradigm is that there are an infinite number of grammatical sentences in any particular language. \n<blockquote>The grammar of a language is a finite system that characterizes an infinite set of (well-formed) sentences. More specifically, the grammar of a language is a ''well-defined system'' by definition not more powerful than a universal Turing machine (and, in fact, surely a great deal weaker).<ref>''The State of the Art'', p. 40</ref></blockquote>\n\nThe crux of Hockett's rebuttal is that the set of grammatical sentences in a language is not infinite, but rather ill-defined.<ref>p. 52 et passim</ref> Hockett proposes that \"no physical system is well-defined\".<ref>p. 52</ref>\n\nLater in \"Where the tongue slips, there slip I\" he writes as follows. \n<blockquote>\nIt is currently fashionable to assume that, underlying the actual more or less bumbling speech behavior of any human being, there is a subtle and complicated but determinate linguistic \"competence\": a sentence-generating device whose design can only be roughly guessed at by any techniques so far available to us. This point of view makes linguistics very hard and very erudite, so that anyone who actually does discover facts about underlying \"competence\" is entitled to considerable kudos.\n\nWithin this popular frame of reference, a theory of \"performance\" -- of the \"generation of speech\" -- must take more or less the following form. If a sentence is to be uttered aloud, or even thought silently to oneself, it must first be built by the internal \"competence\" of the speaker, the functioning of which is by definition such that the sentence will be legal (\"grammatical\") in every respect. But that is not enough; the sentence as thus constructed must then be ''performed'', either overtly so that others may hear it, or covertly so that it is perceived only by the speaker himself. It is in this second step that blunders may appear. That which is generated by the speaker's internal \"competence\"is what the speaker \"intends to say,\" and is the only real concern of linguistics: blunders in actually performed speech are instructions from elsewhere. Just if there are no such intrusions is what is performed an instance of \"smooth speech\".\n\nI believe this view is unmitigated nonsense, unsupported by any empirical evidence of any sort. In its place, I propose the following.\n\n''All'' speech, smooth as well as blunderful, can be and must be accounted for essentially in terms of the three mechanisms we have listed: analogy, blending, and editing. An individual's language, at a given moment, is a set of habits--that is, of analogies, where different analogies are in conflict, one may appear as a constraint on the working of another. Speech actualizes habits--and changes the habits as it does so. Speech reflects awareness of norms; but norms are themselves entirely a matter of analogy (that is, of habit), not some different kind of thing.<ref>''The View From Language'', pp. 254-255.</ref> </blockquote>\n\nDespite his criticisms, Hockett always expressed gratitude to the generative school for seeing real problems in the preexisting approaches.\n\n<blockquote>There are many situations in which bracketing does not serve to disambiguate. As already noted, words that belong together cannot always be spoken together, and when they are not, bracketing is difficult or impossible. In the 1950s this drove some grammarians to drink and other to transformations, but both are only anodynes, not answers<ref>Hockett, Refurbishing our Foundations. John Benjamins, 1987, p. 23</ref></blockquote>\n\n===Design features of language===\nOne of Hockett's most important contributions was his development of the [[Design Features of Language|design-feature]] approach to comparative linguistics. He attempted to distinguish the similarities and differences among [[animal communication]] systems and [[human language]].\n\nHockett initially developed seven features, which were published in the 1959 paper \u201cAnimal \u2018Languages\u2019 and Human Language.\u201d However, after many revisions, he settled on 13 design-features in the ''[[Scientific American]] '' \"The Origin of Speech.\"\n\nHockett argued that while every communication system has some of the 13 design features, only human, spoken language has all 13 features. In turn, that differentiates human spoken language from animal communication and other human communication systems such as [[written language]].\n\n====Hockett's 13 design features of language====\n{{Main|Design Features of Language}}\n#[[Vocal-Auditory Channel]]: Much of human language is performed using the [[vocal]] tract and [[auditory system|auditory]] channel. Hockett viewed this as an advantage for human [[primates]] because it allowed for the ability to participate in other activities while simultaneously communicating through spoken language.\n#[[Broadcast transmission and directional reception]]: All human language can be heard if it is within range of another person's auditory channel. Additionally, a listener has the ability to determine the source of a sound by [[binaural direction]] finding.\n#[[Rapid Fading (transitoriness)]]: Wave forms of human language dissipate over time and do not persist. A hearer can only receive specific auditory information at the time it is spoken.\n#Interchangeability: A person has the ability to speak and hear the same [[signaling theory|signal]]. Anything that a person is able to hear can be reproduced in spoken language.\n#[[Total Feedback]]: Speakers can hear themselves speak and monitor their [[speech production]] and internalize what they are producing by language.\n#Specialization: Human language sounds are specialized for communication. When dogs pant it is to cool themselves off. When humans speak, it is to transmit information.\n#[[Semanticity]]: Specific signals can be matched with a specific [[meaning (linguistics)|meaning]].\n#[[Arbitrariness]]: There is no limitation to what can be communicated about and no specific or necessary connection between the sounds used and the message being sent.\n#[[Discrete mathematics|Discreteness]]: [[Phonemes]] can be placed in distinct categories which differentiate them from one another, like the distinct sound of /p/ versus /b/.\n#[[Displacement (linguistics)|Displacement]]: People can refer to things in space and time and communicate about things that are not present.\n#[[Productivity (linguistics)|Productivity]]: People can create new and unique meanings of utterances from previously existing utterances and sounds.\n#[[Traditional Transmission]]: Human language is not completely [[innate]], and acquisition depends in part on the learning of a language.\n#[[Duality of patterning]]: Meaningless phonic segments ([[phoneme]]s) are combined to make meaningful words, which, in turn, are combined again to make sentences.\n\nWhile Hockett believed that all communication systems, animal and human alike, share many of these features, only human language contains all 13 design features. Additionally, [[Traditional Transmission|traditional transmission]], and [[duality of patterning]] are key to human language.\n\n====Hockett's design features and their implications for human language====\n#Hockett suggests that the importance of a vocal-auditory channel lies in the fact that the animal can communicate while also performing other tasks, such as eating, or using tools.\n#[[Broadcast Transmission and Directional Reception]]: An auditory|audible human language signal is sent out in all directions but is perceived in a limited direction. For example, humans are more proficient in determining the location of a sound source when the sound is projecting directly in front of them, as opposed to a sound source projected directly behind them.\n#[[Rapid Fading]] of a signal in human communication differs from such things as animal tracks and written language because an utterance does not continue to exist after it has been broadcast. With that in mind, it is important to note that Hockett viewed spoken language as the primary concern for investigation. Written language was seen as being secondary because of its recent evolution in culture.\n#Interchangeability represents a human's ability to act out or reproduce any linguistic message that they are able to comprehend. That differs from many animal communication systems, particularly in regards to mating. For example, humans have the ability to say and do anything that they feel may benefit them in attracting a mate. [[Stickleback]]s, on the other hand, have different male and female courtship motions; a male cannot replicate a female's motions and vice versa.\n#[[Total Feedback]] is important in differentiating a human's ability to internalize their own productions of speech and behavior. That design-feature incorporates the idea that humans have insight into their actions.\n#Specialization is apparent in the anatomy of human [[speech organs]] and our ability to exhibit some control over these organs. For example, a key assumption in the evolution of language is that the descent of the [[larynx]] has allowed humans to produce speech sounds. Additionally, in terms of control, humans are generally able to control the movements of their tongue and mouth. Dogs however, do not have control over these organs. When dogs pant they are communicating a signal, but the panting is an uncontrollable response reflex of being hot [https://web.archive.org/web/20080709091601/http://www.columbia.edu/itc/psychology/rmk/Readings/Hockett.pdf].\n#[[Semanticity]]: A specific signal can be matched with a specific meaning within a particular language system. For example, all people who understand [[English language|English]] have the ability to make a connection between a specific word and what that word represents or refers to. (Hockett notes that [[gibbon]]s also show semanticity in their signals, but their calls are far more broad than human language.)\n#[[Arbitrariness]] within human language suggests that there is no direct connection between the type of signal (word) and what is being referenced. For example, an animal as large as a cow can be referred to by a very short word {{webarchive |url=https://web.archive.org/web/20091027014002/http://www.geocities.com/pkd_du/dfhl.htm |date=October 27, 2009 }}.\n#[[Discrete mathematics|Discreteness]]: Each basic unit of speech can be categorized and is distinct from other categories. In human language, there are only a small set of sound ranges that are used and the differences between these bits of sound are absolute. In contrast, the [[waggle dance]] of [[honey bee]]s is continuous.\n#[[Displacement (linguistics)|Displacement]] refers to the human language system's ability to communicate about things that are not present spatially, temporally, or realistically. For example, humans have the ability to communicate about unicorns and outer space.\n#[[Productivity (linguistics)|Productivity]]: Human language is open and productive in the sense that humans have the ability to say things that have never before been spoken or heard. In contrast, apes such as the gibbon have a closed communication system because all of their vocal sounds are part of a finite repertoire of familiar calls.\n#[[Traditional Transmission]]:: suggests that while certain aspects of [[language]] may be [[innate]], humans acquire words and their [[native language]] from other speakers. That is different from many [[animal communication]] systems because most animals are born with the [[innate]] [[knowledge]], and [[skills]] necessary for [[wiktionary:survival|survival]]. ([[Honey bee]]s have an inborn ability to perform and understand the [[waggle dance]]).\n#[[Duality of patterning]]: Humans have the ability to recombine a finite set of [[phoneme]]s to create an infinite number of words, which, in turn, can be combined to make an unlimited number of different sentences.\n\n====Design feature representation in other communication systems====\n;Honeybees\n\nForaging [[honey bee]]s communicate with other members of their hive when they have discovered a relevant source of [[pollen]], [[nectar]], or water. In an effort to convey information about the location and the distance of such resources, honeybees participate in a particular figure-eight dance known as the [[waggle dance]].\n\nIn Hockett's \"The Origin of Speech\", he determined that the honeybee communication system of the [[waggle dance]] holds the following [[Design Features of Language|design features]]:\n\n#Broadcast Transmission and Directional Reception: By the use of this dance, honeybees are able to send out a signal that informs other members of the hive as to what direction the source of food, or water can be located.\n#[[Semanticity]]: Evidence that the specific signals of a communication system can be matched with specific meanings is apparent because other members of the hive are able to locate the food source after a performance of the waggle dance.\n#[[Displacement (linguistics)|Displacement]]: Foraging honeybees can communicate about a resource that is not currently present within the hive.\n#[[Productivity (linguistics)|Productivity]]: [[Waggle dance]]s change based on the direction, amount, and type of resource.\n\nGibbons are small apes in the family Hylobatidae. While they share the same [[kingdom (biology)|kingdom]], [[phylum]], [[class (biology)|class]], and [[order (biology)|order]] of humans and are relatively close to man, Hockett distinguishes between the gibbon communication system and human language by noting that gibbons are devoid of the last four design features.\n\nGibbons possess the first nine [[Design Features of Language|design features]], but do not possess the last four (displacement, productivity, [[Traditional Transmission|traditional transmission]], and [[duality of patterning]]).\n\n#Displacement, according to Hockett, appears to be lacking in the vocal [[Signalling theory|signal]]ing of apes.\n#Productivity does not exist among gibbons because if any vocal sound is produced, it is one of a finite set of repetitive and familiar calls.\n#Hockett supports the idea that humans learn language extra genetically through the process of [[Traditional Transmission|traditional transmission]]. Hockett distinguishes gibbons from humans by stating that despite any similarities in communication among a species of apes, one cannot attribute these similarities to acquisition through the teaching and learning ([[Traditional Transmission|traditional transmission]]) of signals; the only explanation must be a genetic basis.\n#Finally, [[duality of patterning]] explains a human's ability to create multiple [[meanings (linguistics)|meanings]] from somewhat meaningless sounds. For example, the [[phonemes]]s /t/, /a/, /c/ can be used to create the words \"cat,\" \"tack,\" and \"act.\" Hockett states that no other [[Hominoid]] communication system besides human language maintains this ability.\n\n===Later additions to the features===\nIn a report published in 1968 with anthropologist and scientist Stuart A. Altmann, Hockett derived three more [[Design Features of Language|Design Features]], bringing the total to 16. These are the additional three:\n\n#<li value=14> '''[[Deception|Prevarication]]''': A speaker can say falsehoods, lies, and meaningless statements.\n# '''Reflexiveness''': Language can be used communicate about the very system it is, and language can discuss language\n# '''[[Learnability]]''': A speaker of a language can learn another language\n\n====Other additions====\nCognitive scientist and linguist at the University of Sussex [[Larry Trask]] offered an alternative term and definition for number 14, '''[[Deception|Prevarication]]''':\n:14. (a) '''Stimulus Freedom''': One can choose to say anything nothing in any given situation\n\nThere has since been one more Feature added to the list, by [https://web.archive.org/web/20100612132306/http://www.bsos.umd.edu/anth/People/FacStaff/faculty/wstuart/index.html Dr. William Taft Stuart], a director of the Undergraduate Studies program at the University of Maryland: College Park's Anthropology school, part of the College of Behavioral and Social Sciences. His \u201cextra\u201d Feature is:\n\n:17. '''Grammaticality''': A speaker\u2019s sayings conform to the rules of grammar\n\nThis follows the definition of Grammar and Syntax, as given by Merriam-Webster's Dictionary:\n\n:'''[[Grammar]]''':\n::1. (a) the study of the classes of words, their inflections, and their functions and relations in the sentence (b) a study of what is to be preferred and what avoided in inflection and syntax\n\n::2. (a) the characteristic system of inflections and syntax of a language (b) a system of rules that defines the grammatical structure of a language\n\n:'''[[Syntax]]''':\n::1. (a) the way in which linguistic elements (as words) are put together to form constituents (as phrases or clauses) (b) the part of grammar\n\n====Relationship between design features and animal communication====\n\nAdditionally, Dr. Stuart defends his postulation with references to famous linguist Noam Chomsky and University of New York psychologist Gary Marcus. Chomsky theorized that humans are unique in the animal world because of their ability to utilize Design Feature 5: Total Feedback, or recursive grammar. This includes being able to correct oneself and insert explanatory or even non sequitur statements into a sentence, without breaking stride, and keeping proper grammar throughout.\n\nWhile there have been studies attempting to disprove Chomsky, Marcus states that, \"An intriguing possibility is that the capacity to recognize recursion might be found only in species that can acquire new patterns of vocalization, for example, songbirds, humans and perhaps some cetaceans.\" This is in response to a [http://www.cbc.ca/health/story/2006/04/26/birds-grammar060426.html study performed] by psychologist Timothy Gentner of the University of California at San Diego. Gentner's study found that starling songbirds use recursive grammar to identify \u201codd\u201d statements within a given \u201csong.\u201d However, the study does not necessarily debunk Chomsky's observation because it has not yet been proven that songbirds have the semantic ability to generalize from patterns.\n\n[http://tuvalu.santafe.edu/~johnson/articles.chimp.html There is also thought] that symbolic thought is necessary for grammar-based speech, and thus Homo Erectus and all preceding \u201chumans\u201d would have been unable to comprehend modern speech. Rather, their utterances would have been halting and even quite confusing to us, \ntoday.\n\n=====Hockett's \"design features\" of language and other animal communication systems=====\n\nThe [http://www.phon.ox.ac.uk/jcoleman/design_features.htm [[University of Oxford]]]: Phonetics Laboratory Faculty of Linguistics, Philology and Phonetics published the following chart, detailing how Hockett's (and Altmann's) Design Features fit into other forms of communication, in animals:\n{| class=\"wikitable\"\n|-\n! Feature !! [[Cricket (insect)|Cricket]]s !! [[Bee Dance language|Bee dancing]] !! [[Western meadowlark|Western meadowlark song]] !! [[Gibbon|Gibbon calls]] !! [[Ape language|Signing apes]] !! [[Grey parrot|Alex, a grey parrot]] !! Paralinguistic phenomena !! [[Sign languages|Human sign languages]] !! [[Spoken dialogue|Spoken language]]\n|-\n| Vocal-Auditory Channel || Auditory, not vocal || No || Yes || Yes || No || Yes || Yes || No || Yes\n|-\n| Broadcast Transmission and Directional Reception || Yes || Yes || Yes || Yes || Yes || Yes || Yes || Yes || Yes\n|-\n| Rapid Fading || Yes (repeating) || ? || Yes || Yes (repeating) || Yes || Yes || Yes || Yes || Yes\n|-\n| Interchangeability || Limited || Limited || ? || Yes || Yes || Yes || Largely Yes || Yes || Yes\n|-\n| Total Feedback || Yes || ? || Yes || Yes || No || Yes || Yes || No || Yes\n|-\n| Specialization || Yes? || ? || Yes || Yes || Yes || Yes || Yes? || Yes || Yes\n|-\n| Semanticity || No? || Yes || In Part || Yes || Yes || Yes || Yes? || Yes || Yes\n|-\n| Arbitrariness || ? || No || If semantic, Yes || Yes || Largely Yes || Yes || In Part || Largely Yes || Yes\n|-\n| Discreteness || Yes? || No || ? || Yes || Yes || Yes || Largely No || Yes || Yes\n|-\n| Displacement || \u2013 || Yes, always || ? || No || Yes || No || In Part || Yes, often || Yes, often\n|-\n| Productivity || No || Yes || ? || No || Debatable || Limited || Yes || Yes || Yes\n|-\n| Traditional Transmission || No? || Probably not || ? || ? || Limited || Limited || Yes || Yes || Yes\n|-\n| Duality of Patterning || ? || No || ? || No (Cotton-top Tamarin: Yes) || Yes || Yes || No || Yes || Yes\n|-\n| Prevarication || \u2013 || \u2013 || \u2013 || \u2013 || Yes || No || \u2013 || Yes || Yes\n|-\n| Reflexiveness || \u2013 || \u2013 || \u2013 || \u2013 || No? || No || \u2013 || Yes || Yes\n|-\n| Learnability || \u2013 || \u2013 || \u2013 || \u2013 || Yes || Yes || \u2013 || Yes || Yes\n|}\n\n==Selected works==\n* 1939: \"Potowatomi Syntax\", ''Language'' 15: 235\u2013248.\n* 1942: \"A System of Descriptive Phonology\", ''Language'' 18: 3-21.\n* 1944: ''Spoken Chinese; Basic Course''. With C. Fang. Holt, New York.\n* 1947: \"Peiping phonology\", in: ''Journal of the American Oriental Society'', 67, pp.&nbsp;253\u2013267. [= Martin Joos (ed.), ''Readings in Linguistics'', vol. I, 4th edition. Chicago and London 1966, pp.&nbsp;217\u2013228].\n* 1947: \"Problems of morphemic analysis\", in: ''[[Language (journal)|Language]]'', 24, pp.&nbsp;414\u201341. [= ''Readings in Linguistics'', vol. I, pp.&nbsp;229\u2013242].\n* 1948: \"Biophysics, linguistics, and the unity of science\", in: ''[[American Scientist]]'', 36, pp.&nbsp;558\u2013572.\n* 1950: \"Peiping morphophonemics\", in: ''Language'', 26, pp.&nbsp;63\u201385. [= ''Readings in Linguistics'', vol. I, pp.&nbsp;315\u2013328].\n* 1954: \"Two models of grammatical description\", in: ''Word'', 10, pp.&nbsp;210\u2013234. [= ''Readings in Linguistics'', vol. I, pp.&nbsp;386\u2013399].\n* 1955: ''A Manual of Phonology''. Indiana University Publications in Anthropology and Linguistics 11.\n* 1958: ''A Course in Modern Linguistics''. The Macmillan Company: New York.\n* 1960: \"The Origin of Speech\". in ''[[Scientific American]]'', 203, pp.&nbsp;89\u201397.\n* 1961: \"Linguistic Elements and Their Relation\" in ''[[Language (journal)|Language]]'', 37: 29\u201353.\n* 1967: ''The State of the Art''. The Haag: Mouton\n* 1973: Man's Place in Nature. New York: McGraw-Hill.\n* 1977: The View From Language. Athens: The University of Georgia Press.\n* 1987: Refurbishing Our Foundations. Amsterdam: John Benjamins.\n\n==See also==\n* [[Animal communication]]\n* [[Design features of language]]\n* [[Language acquisition]]\n* [[Linguistic anthropology]]\n* [[Linguistic universals]]\n* [[Origin of language]]\n* [[Origin of speech]]\n\n==References==\n{{Reflist}}\n*Falk, Julia S. 2003. \"Turn to the history of linguistics : Noam Chomsky and Charles Hockett in the 1960s\". ''Historiographia linguistica'' (international journal for the history of the language sciences) 30/1-2, pp.&nbsp;129\u2013185. [https://www.researchgate.net/publication/233602133_Turn_to_the_History_of_Linguistics_Noam_Chomsky_and_Charles_Hockett_in_the_1960s]\n*Gair, James W. 2003. [Obituary] Charles F. Hockett. ''Language''. 79, 600\u2013613.\n*Hockett, Charles F. (1960), \"The Origin of Speech,\" ''Scientific American'', 203, 89\u201397.\n*Fox, Margalit 2003 (Obituary) \"Champion of structural linguistics\" The New York Times\n\n==External links==\n* [http://specgram.com/JLSSCNC.I.2/02.whitcomb.hockett.html Old Professor Hockett]: A poem written in honor of Hockett by one of his students during his 1991 visit to [[Rice University]].\n* [https://web.archive.org/web/20071023012311/http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0011b&L=linganth&D=0&O=A&P=723 Linguist List]: Obituary of Charles Hockett from [[the New York Times]] (November 13, 2000), reproduced on the Linguist List. The NY Times link to the obituary is at [https://www.nytimes.com/2000/11/13/national/13HOCK.html NY Times]\n* [https://books.google.com/books?id=Is0UAAAAIAAJ&pg=PA342&dq=four+traditional+grammar+historical+linguistics&lr=&as_brr=0&hl=tr#PPA13,M1 Essays in Honor of Charles F. Hockett]\n* [https://web.archive.org/web/20070716054015/http://www.people.ex.ac.uk/bosthaus/Lecture/hockett1.htm Features of Human Language]\n* [https://web.archive.org/web/20090306051026/http://books.nap.edu/html/biomems/chockett.pdf Charles Hockett-Biography]\n* [https://web.archive.org/web/20110125091520/http://www.pdfqueen.com/html/aHR0cDovL2hvbWVwYWdlLnJ1aHItdW5pLWJvY2h1bS5kZS9VZG8uRmlnZ2UvZG93bmxvYWQvZGVzaWduZmVhdHVyZXMucGRm Design Features of Human Language, Udo L. Figge]: A brief analysis of the 16 Design Features of Language, as published by Hockett and Altmann in 1968\n* {{worldcat id|lccn-n50-34609}}\n* [https://web.archive.org/web/20120425091558/http://www.searchinpdf.com/CHARLES-FRANCIS-HOCKETT-1916-2000 Charles Hockett Life Summary] {{dead link|date=April 2020}}\n*[http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/hockett-charles.pdf National Academy of Sciences Biographical Memoir]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Hockett, Charles F.}}\n[[Category:1916 births]]\n[[Category:2000 deaths]]\n[[Category:Linguists from the United States]]\n[[Category:People from Columbus, Ohio]]\n[[Category:Ohio State University alumni]]\n[[Category:Yale University alumni]]\n[[Category:Harvard Fellows]]\n[[Category:Cornell University faculty]]\n[[Category:Rice University faculty]]\n[[Category:Members of the United States National Academy of Sciences]]\n[[Category:Linguists of Algic languages]]\n[[Category:Linguistic Society of America presidents]]\n", "name_user": "Travalleir", "label": "safe", "comment": "", "url_page": "//en.wikipedia.org/wiki/Charles_F._Hockett"}
